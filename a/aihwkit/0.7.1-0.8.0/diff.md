# Comparing `tmp/aihwkit-0.7.1.tar.gz` & `tmp/aihwkit-0.8.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "aihwkit-0.7.1.tar", last modified: Tue Mar 28 21:47:52 2023, max compression
+gzip compressed data, was "aihwkit-0.8.0.tar", last modified: Fri Jul 14 20:10:45 2023, max compression
```

## Comparing `aihwkit-0.7.1.tar` & `aihwkit-0.8.0.tar`

### file list

```diff
@@ -1,332 +1,377 @@
-drwxrwxr-x   0 kvtran    (1003) kvtran    (1003)        0 2023-03-28 21:47:52.352347 aihwkit-0.7.1/
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     4079 2023-03-28 21:45:23.000000 aihwkit-0.7.1/CMakeLists.txt
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    11368 2023-03-28 21:45:23.000000 aihwkit-0.7.1/LICENSE.txt
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)      407 2023-03-28 21:45:23.000000 aihwkit-0.7.1/MANIFEST.in
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     9773 2023-03-28 21:47:52.352347 aihwkit-0.7.1/PKG-INFO
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     8791 2023-03-28 21:45:23.000000 aihwkit-0.7.1/README.md
-drwxrwxr-x   0 kvtran    (1003) kvtran    (1003)        0 2023-03-28 21:47:52.308347 aihwkit-0.7.1/cmake/
-drwxrwxr-x   0 kvtran    (1003) kvtran    (1003)        0 2023-03-28 21:47:52.312347 aihwkit-0.7.1/cmake/Modules/
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     2308 2023-03-28 21:45:23.000000 aihwkit-0.7.1/cmake/Modules/FindAVX.cmake
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)      869 2023-03-28 21:45:23.000000 aihwkit-0.7.1/cmake/Modules/FindCUB.cmake
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    14068 2023-03-28 21:45:23.000000 aihwkit-0.7.1/cmake/Modules/FindMKL.cmake
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     2829 2023-03-28 21:45:23.000000 aihwkit-0.7.1/cmake/Modules/FindOpenBLAS.cmake
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     2386 2023-03-28 21:45:23.000000 aihwkit-0.7.1/cmake/Modules/FindTorch.cmake
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     4172 2023-03-28 21:45:23.000000 aihwkit-0.7.1/cmake/dependencies.cmake
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     1461 2023-03-28 21:45:23.000000 aihwkit-0.7.1/cmake/dependencies_cuda.cmake
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     1168 2023-03-28 21:45:23.000000 aihwkit-0.7.1/cmake/dependencies_test.cmake
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)      681 2023-03-28 21:47:52.352347 aihwkit-0.7.1/setup.cfg
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     2772 2023-03-28 21:45:23.000000 aihwkit-0.7.1/setup.py
-drwxrwxr-x   0 kvtran    (1003) kvtran    (1003)        0 2023-03-28 21:47:52.308347 aihwkit-0.7.1/src/
-drwxrwxr-x   0 kvtran    (1003) kvtran    (1003)        0 2023-03-28 21:47:52.312347 aihwkit-0.7.1/src/aihwkit/
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)        6 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/VERSION.txt
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)      580 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/__init__.py
-drwxrwxr-x   0 kvtran    (1003) kvtran    (1003)        0 2023-03-28 21:47:52.312347 aihwkit-0.7.1/src/aihwkit/cloud/
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)      574 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/cloud/__init__.py
-drwxrwxr-x   0 kvtran    (1003) kvtran    (1003)        0 2023-03-28 21:47:52.312347 aihwkit-0.7.1/src/aihwkit/cloud/client/
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)      561 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/cloud/client/__init__.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     5458 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/cloud/client/entities.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     1845 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/cloud/client/exceptions.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     4021 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/cloud/client/session.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     2370 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/cloud/client/utils.py
-drwxrwxr-x   0 kvtran    (1003) kvtran    (1003)        0 2023-03-28 21:47:52.312347 aihwkit-0.7.1/src/aihwkit/cloud/client/v1/
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)      556 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/cloud/client/v1/__init__.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     6576 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/cloud/client/v1/api_client.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     6857 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/cloud/client/v1/i_api_client.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     5338 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/cloud/client/v1/parsers.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     4661 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/cloud/client/v1/stubs.py
-drwxrwxr-x   0 kvtran    (1003) kvtran    (1003)        0 2023-03-28 21:47:52.312347 aihwkit-0.7.1/src/aihwkit/cloud/converter/
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)      574 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/cloud/converter/__init__.py
-drwxrwxr-x   0 kvtran    (1003) kvtran    (1003)        0 2023-03-28 21:47:52.316347 aihwkit-0.7.1/src/aihwkit/cloud/converter/definitions/
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)      557 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/cloud/converter/definitions/__init__.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     2987 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/cloud/converter/definitions/common_pb2.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     3190 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/cloud/converter/definitions/i_common_pb2.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     3437 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/cloud/converter/definitions/i_input_file_pb2.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     3565 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/cloud/converter/definitions/i_onnx_common_pb2.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     1865 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/cloud/converter/definitions/i_output_file_pb2.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     1804 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/cloud/converter/definitions/input_file_pb2.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     3505 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/cloud/converter/definitions/onnx_common_pb2.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     1545 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/cloud/converter/definitions/output_file_pb2.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)      683 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/cloud/converter/exceptions.py
-drwxrwxr-x   0 kvtran    (1003) kvtran    (1003)        0 2023-03-28 21:47:52.316347 aihwkit-0.7.1/src/aihwkit/cloud/converter/v1/
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)      560 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/cloud/converter/v1/__init__.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     1233 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/cloud/converter/v1/analog_info.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    10417 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/cloud/converter/v1/i_mappings.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    17586 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/cloud/converter/v1/inferencing.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    10749 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/cloud/converter/v1/mappings.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     2707 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/cloud/converter/v1/noise_model_info.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     9991 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/cloud/converter/v1/rpu_config_info.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     7119 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/cloud/converter/v1/training.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     1169 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/exceptions.py
-drwxrwxr-x   0 kvtran    (1003) kvtran    (1003)        0 2023-03-28 21:47:52.316347 aihwkit-0.7.1/src/aihwkit/experiments/
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)      755 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/experiments/__init__.py
-drwxrwxr-x   0 kvtran    (1003) kvtran    (1003)        0 2023-03-28 21:47:52.316347 aihwkit-0.7.1/src/aihwkit/experiments/experiments/
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)      534 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/experiments/experiments/__init__.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     3323 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/experiments/experiments/base.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    15997 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/experiments/experiments/inferencing.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    11906 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/experiments/experiments/training.py
-drwxrwxr-x   0 kvtran    (1003) kvtran    (1003)        0 2023-03-28 21:47:52.320347 aihwkit-0.7.1/src/aihwkit/experiments/runners/
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)      984 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/experiments/runners/__init__.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)      876 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/experiments/runners/base.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     4362 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/experiments/runners/cloud.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     4670 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/experiments/runners/i_cloud.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     3378 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/experiments/runners/i_local.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     2524 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/experiments/runners/i_metrics.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     3718 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/experiments/runners/local.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     3801 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/experiments/runners/metrics.py
-drwxrwxr-x   0 kvtran    (1003) kvtran    (1003)        0 2023-03-28 21:47:52.320347 aihwkit-0.7.1/src/aihwkit/inference/
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     1156 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/inference/__init__.py
-drwxrwxr-x   0 kvtran    (1003) kvtran    (1003)        0 2023-03-28 21:47:52.320347 aihwkit-0.7.1/src/aihwkit/inference/compensation/
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)      582 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/inference/compensation/__init__.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     1898 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/inference/compensation/base.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     1339 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/inference/compensation/drift.py
-drwxrwxr-x   0 kvtran    (1003) kvtran    (1003)        0 2023-03-28 21:47:52.320347 aihwkit-0.7.1/src/aihwkit/inference/converter/
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)      598 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/inference/converter/__init__.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     1927 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/inference/converter/base.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     3076 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/inference/converter/conductance.py
-drwxrwxr-x   0 kvtran    (1003) kvtran    (1003)        0 2023-03-28 21:47:52.320347 aihwkit-0.7.1/src/aihwkit/inference/noise/
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)      583 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/inference/noise/__init__.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     5917 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/inference/noise/base.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     7611 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/inference/noise/custom.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     5926 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/inference/noise/pcm.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     2059 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/inference/utils.py
-drwxrwxr-x   0 kvtran    (1003) kvtran    (1003)        0 2023-03-28 21:47:52.320347 aihwkit-0.7.1/src/aihwkit/nn/
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     1129 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/nn/__init__.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     7605 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/nn/conversion.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     4670 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/nn/functions.py
-drwxrwxr-x   0 kvtran    (1003) kvtran    (1003)        0 2023-03-28 21:47:52.320347 aihwkit-0.7.1/src/aihwkit/nn/modules/
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)      533 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/nn/modules/__init__.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    26355 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/nn/modules/base.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    10548 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/nn/modules/container.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    28338 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/nn/modules/conv.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    39499 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/nn/modules/conv_mapped.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     5565 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/nn/modules/linear.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    14206 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/nn/modules/linear_mapped.py
-drwxrwxr-x   0 kvtran    (1003) kvtran    (1003)        0 2023-03-28 21:47:52.324347 aihwkit-0.7.1/src/aihwkit/nn/modules/rnn/
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)      537 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/nn/modules/rnn/__init__.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    10371 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/nn/modules/rnn/cells.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     4676 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/nn/modules/rnn/layers.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     8508 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/nn/modules/rnn/rnn.py
-drwxrwxr-x   0 kvtran    (1003) kvtran    (1003)        0 2023-03-28 21:47:52.324347 aihwkit-0.7.1/src/aihwkit/optim/
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)      656 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/optim/__init__.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     8190 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/optim/analog_optimizer.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     5126 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/optim/context.py
-drwxrwxr-x   0 kvtran    (1003) kvtran    (1003)        0 2023-03-28 21:47:52.324347 aihwkit-0.7.1/src/aihwkit/simulator/
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     1001 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/simulator/CMakeLists.txt
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)      676 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/simulator/__init__.py
-drwxrwxr-x   0 kvtran    (1003) kvtran    (1003)        0 2023-03-28 21:47:52.324347 aihwkit-0.7.1/src/aihwkit/simulator/configs/
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     1572 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/simulator/configs/__init__.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    29077 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/simulator/configs/compounds.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     9228 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/simulator/configs/configs.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    41091 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/simulator/configs/devices.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     8394 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/simulator/configs/enums.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     8073 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/simulator/configs/helpers.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    26933 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/simulator/configs/utils.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     1009 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/simulator/noise_models.py
-drwxrwxr-x   0 kvtran    (1003) kvtran    (1003)        0 2023-03-28 21:47:52.324347 aihwkit-0.7.1/src/aihwkit/simulator/presets/
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     1930 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/simulator/presets/__init__.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     1721 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/simulator/presets/compounds.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    42180 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/simulator/presets/configs.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    10245 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/simulator/presets/devices.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     3400 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/simulator/presets/utils.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     4532 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/simulator/presets/web.py
-drwxrwxr-x   0 kvtran    (1003) kvtran    (1003)        0 2023-03-28 21:47:52.324347 aihwkit-0.7.1/src/aihwkit/simulator/rpu_base_src/
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     1337 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/simulator/rpu_base_src/rpu_base.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     1656 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/simulator/rpu_base_src/rpu_base.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    38796 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/simulator/rpu_base_src/rpu_base_devices.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    32908 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/simulator/rpu_base_src/rpu_base_tiles.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    20805 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/simulator/rpu_base_src/rpu_base_tiles_cuda.cpp
-drwxrwxr-x   0 kvtran    (1003) kvtran    (1003)        0 2023-03-28 21:47:52.328347 aihwkit-0.7.1/src/aihwkit/simulator/tiles/
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)      833 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/simulator/tiles/__init__.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    10004 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/simulator/tiles/analog.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    58410 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/simulator/tiles/base.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     5648 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/simulator/tiles/floating_point.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    10536 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/simulator/tiles/inference.py
-drwxrwxr-x   0 kvtran    (1003) kvtran    (1003)        0 2023-03-28 21:47:52.328347 aihwkit-0.7.1/src/aihwkit/utils/
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)      544 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/utils/__init__.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    11159 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/utils/analog_info.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    10776 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/utils/fitting.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    24317 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/utils/visualization.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     5013 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/utils/visualization_web.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)      720 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/aihwkit/version.py
-drwxrwxr-x   0 kvtran    (1003) kvtran    (1003)        0 2023-03-28 21:47:52.312347 aihwkit-0.7.1/src/aihwkit.egg-info/
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     9773 2023-03-28 21:47:52.000000 aihwkit-0.7.1/src/aihwkit.egg-info/PKG-INFO
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    11272 2023-03-28 21:47:52.000000 aihwkit-0.7.1/src/aihwkit.egg-info/SOURCES.txt
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)        1 2023-03-28 21:47:52.000000 aihwkit-0.7.1/src/aihwkit.egg-info/dependency_links.txt
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)        1 2023-03-28 21:47:52.000000 aihwkit-0.7.1/src/aihwkit.egg-info/not-zip-safe
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)      183 2023-03-28 21:47:52.000000 aihwkit-0.7.1/src/aihwkit.egg-info/requires.txt
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)        8 2023-03-28 21:47:52.000000 aihwkit-0.7.1/src/aihwkit.egg-info/top_level.txt
-drwxrwxr-x   0 kvtran    (1003) kvtran    (1003)        0 2023-03-28 21:47:52.336347 aihwkit-0.7.1/src/rpucuda/
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)      756 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/CMakeLists.txt
-drwxrwxr-x   0 kvtran    (1003) kvtran    (1003)        0 2023-03-28 21:47:52.352347 aihwkit-0.7.1/src/rpucuda/cuda/
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)      700 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/CMakeLists.txt
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    88219 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/bit_line_maker.cu
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     5721 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/bit_line_maker.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    19829 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/bit_line_maker_test.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    12150 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/chopped_weight_output.cu
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     7641 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/chopped_weight_output.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    42315 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/cuda_math_util.cu
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     7712 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/cuda_math_util.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    33293 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/cuda_util.cu
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    23812 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/cuda_util.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    25981 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/forward_backward_pass.cu
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    10191 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/forward_backward_pass.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    12480 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/forward_backward_pass_test.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    19345 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/io_iterator.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    17468 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/io_iterator_test.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    32077 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/io_manager.cu
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     5166 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/io_manager.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    17438 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/io_manager_test.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    13359 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/maximizer.cu
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     2150 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/maximizer.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     6747 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/maximizer_test.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    19566 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/noise_manager.cu
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     3452 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/noise_manager.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    16535 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/pulsed_weight_updater.cu
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     3808 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/pulsed_weight_updater.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    20866 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/pulsed_weight_updater_test.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    66735 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/pwu_kernel.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    37675 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/pwu_kernel_parameter.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     5765 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/pwu_kernel_parameter_base.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)      707 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpu_cub.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    25665 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda.cu
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     8607 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     7910 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_buffered_transfer_device.cu
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     2360 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_buffered_transfer_device.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    10684 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_buffered_transfer_device_test.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     4465 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_constantstep_device.cu
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     1944 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_constantstep_device.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     8962 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_expstep_device.cu
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     2543 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_expstep_device.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    13267 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_expstep_test.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     6242 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_linearstep_device.cu
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     3119 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_linearstep_device.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    13024 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_linearstep_test.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     9274 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_mixedprec_device.cu
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     2760 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_mixedprec_device.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    10803 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_mixedprec_device_base.cu
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     5072 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_mixedprec_device_base.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    12009 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_mixedprec_device_test.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    13384 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_onesided_device.cu
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     3856 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_onesided_device.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    12049 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_onesided_device_test.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     5921 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_piecewisestep_device.cu
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     3137 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_piecewisestep_device.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     4311 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_powstep_device.cu
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     3107 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_powstep_device.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     4265 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_powstep_reference_device.cu
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     2930 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_powstep_reference_device.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    10219 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_powstep_test.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    28530 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_pulsed.cu
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     7870 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_pulsed.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    14316 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_pulsed_device.cu
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    16696 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_pulsed_device.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     8380 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_pulsed_device_test.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    50172 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_pulsed_test.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     9581 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_simple_device.cu
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     5448 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_simple_device.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     7503 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_simple_device_test.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     6568 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_softbounds_reference_device.cu
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     3124 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_softbounds_reference_device.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    21077 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_test.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    17925 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_transfer_device.cu
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     4753 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_transfer_device.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    16310 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_transfer_device_test.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    14129 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_vector_device.cu
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     4225 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_vector_device.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     9039 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_vector_device_test.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    16614 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/test_helper.cu
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     1393 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/test_helper.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    26011 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/update_management_helper.cu
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     3650 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/update_management_helper.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     3880 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/update_management_helper_test.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     4230 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/weight_clipper_cuda.cu
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     1243 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/weight_clipper_cuda.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     3762 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/weight_clipper_cuda_test.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     6763 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/weight_drifter_cuda.cu
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     1925 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/weight_drifter_cuda.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     6306 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/weight_drifter_cuda_test.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    13773 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/weight_modifier_cuda.cu
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     1436 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/weight_modifier_cuda.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     6678 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/weight_remapper_cuda.cu
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     1311 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/cuda/weight_remapper_cuda.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     9086 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/dense_bit_line_maker.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     2967 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/dense_bit_line_maker.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     7071 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/math_util.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     2460 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/math_util.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     4022 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rng.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     3537 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rng.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    48316 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rpu.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    24954 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rpu.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     9031 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rpu_buffered_transfer_device.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     5637 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rpu_buffered_transfer_device.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     2843 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rpu_constantstep_device.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     1842 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rpu_constantstep_device.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     5597 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rpu_expstep_device.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     3345 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rpu_expstep_device.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    30083 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rpu_forward_backward_pass.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     6808 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rpu_forward_backward_pass.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     7976 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rpu_linearstep_device.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     7038 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rpu_linearstep_device.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    10251 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rpu_mixedprec_device.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     4639 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rpu_mixedprec_device.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    13992 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rpu_mixedprec_device_base.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     6456 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rpu_mixedprec_device_base.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    14043 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rpu_onesided_device.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     5633 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rpu_onesided_device.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     8090 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rpu_onesided_device_test.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     4581 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rpu_piecewisestep_device.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     2744 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rpu_piecewisestep_device.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     5499 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rpu_powstep_device.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     3928 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rpu_powstep_device.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     6171 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rpu_powstep_reference_device.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     5205 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rpu_powstep_reference_device.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    17005 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rpu_pulsed.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     4910 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rpu_pulsed.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    22998 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rpu_pulsed_device.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    26224 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rpu_pulsed_device.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     7363 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rpu_pulsed_meta_parameter.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    12042 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rpu_pulsed_meta_parameter.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     5018 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rpu_pulsed_test.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     4559 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rpu_simple_device.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     9526 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rpu_simple_device.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     7643 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rpu_softbounds_reference_device.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     3543 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rpu_softbounds_reference_device.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    21627 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rpu_transfer_device.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     8740 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rpu_transfer_device.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    10852 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rpu_transfer_device_test.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    17816 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rpu_vector_device.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     8625 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rpu_vector_device.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    10460 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rpu_weight_updater.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     3321 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/rpu_weight_updater.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    10173 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/sparse_bit_line_maker.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     2516 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/sparse_bit_line_maker.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     7363 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/utility_functions.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     2925 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/weight_clipper.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     2092 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/weight_clipper.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     4164 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/weight_drifter.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     4881 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/weight_drifter.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     4955 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/weight_modifier.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     4187 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/weight_modifier.h
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     4158 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/weight_remapper.cpp
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     2387 2023-03-28 21:45:23.000000 aihwkit-0.7.1/src/rpucuda/weight_remapper.h
-drwxrwxr-x   0 kvtran    (1003) kvtran    (1003)        0 2023-03-28 21:47:52.352347 aihwkit-0.7.1/tests/
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)      524 2023-03-28 21:45:23.000000 aihwkit-0.7.1/tests/__init__.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    11488 2023-03-28 21:45:23.000000 aihwkit-0.7.1/tests/test_bindings_tiles.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     4112 2023-03-28 21:45:23.000000 aihwkit-0.7.1/tests/test_client.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     4764 2023-03-28 21:45:23.000000 aihwkit-0.7.1/tests/test_cloud_runner.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     5023 2023-03-28 21:45:23.000000 aihwkit-0.7.1/tests/test_conversions.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     2915 2023-03-28 21:45:23.000000 aihwkit-0.7.1/tests/test_experiment_runners.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     2869 2023-03-28 21:45:23.000000 aihwkit-0.7.1/tests/test_experiments.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     2534 2023-03-28 21:45:23.000000 aihwkit-0.7.1/tests/test_inference.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    11586 2023-03-28 21:45:23.000000 aihwkit-0.7.1/tests/test_inference_tiles.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    16987 2023-03-28 21:45:23.000000 aihwkit-0.7.1/tests/test_layers.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    30892 2023-03-28 21:45:23.000000 aihwkit-0.7.1/tests/test_layers_convolution.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    17287 2023-03-28 21:45:23.000000 aihwkit-0.7.1/tests/test_layers_linear.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     8797 2023-03-28 21:45:23.000000 aihwkit-0.7.1/tests/test_layers_mapped.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    18344 2023-03-28 21:45:23.000000 aihwkit-0.7.1/tests/test_layers_rnn.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     3384 2023-03-28 21:45:23.000000 aihwkit-0.7.1/tests/test_optimizers.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     3767 2023-03-28 21:45:23.000000 aihwkit-0.7.1/tests/test_presets.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     5482 2023-03-28 21:45:23.000000 aihwkit-0.7.1/tests/test_rpu_configurations.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    19650 2023-03-28 21:45:23.000000 aihwkit-0.7.1/tests/test_simulator_tiles.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)     6092 2023-03-28 21:45:23.000000 aihwkit-0.7.1/tests/test_specific_tiles.py
--rw-rw-r--   0 kvtran    (1003) kvtran    (1003)    32971 2023-03-28 21:45:23.000000 aihwkit-0.7.1/tests/test_utils.py
+drwxr-xr-x   0 kaoutar    (501) staff       (20)        0 2023-07-14 20:10:45.153345 aihwkit-0.8.0/
+-rw-r--r--   0 kaoutar    (501) staff       (20)     4317 2023-07-14 20:02:54.000000 aihwkit-0.8.0/CMakeLists.txt
+-rw-r--r--   0 kaoutar    (501) staff       (20)    11374 2023-07-14 20:02:54.000000 aihwkit-0.8.0/LICENSE.txt
+-rw-r--r--   0 kaoutar    (501) staff       (20)      407 2023-07-14 20:02:54.000000 aihwkit-0.8.0/MANIFEST.in
+-rw-r--r--   0 kaoutar    (501) staff       (20)    10839 2023-07-14 20:10:45.153638 aihwkit-0.8.0/PKG-INFO
+-rw-r--r--   0 kaoutar    (501) staff       (20)     9738 2023-07-14 20:02:54.000000 aihwkit-0.8.0/README.md
+drwxr-xr-x   0 kaoutar    (501) staff       (20)        0 2023-07-14 20:10:44.809515 aihwkit-0.8.0/cmake/
+drwxr-xr-x   0 kaoutar    (501) staff       (20)        0 2023-07-14 20:10:44.814425 aihwkit-0.8.0/cmake/Modules/
+-rw-r--r--   0 kaoutar    (501) staff       (20)     2308 2023-07-14 20:02:54.000000 aihwkit-0.8.0/cmake/Modules/FindAVX.cmake
+-rw-r--r--   0 kaoutar    (501) staff       (20)      875 2023-07-14 20:02:54.000000 aihwkit-0.8.0/cmake/Modules/FindCUB.cmake
+-rw-r--r--   0 kaoutar    (501) staff       (20)    14068 2023-07-14 20:02:54.000000 aihwkit-0.8.0/cmake/Modules/FindMKL.cmake
+-rw-r--r--   0 kaoutar    (501) staff       (20)     2829 2023-07-14 20:02:54.000000 aihwkit-0.8.0/cmake/Modules/FindOpenBLAS.cmake
+-rw-r--r--   0 kaoutar    (501) staff       (20)     2392 2023-07-14 20:02:54.000000 aihwkit-0.8.0/cmake/Modules/FindTorch.cmake
+-rw-r--r--   0 kaoutar    (501) staff       (20)     4347 2023-07-14 20:02:54.000000 aihwkit-0.8.0/cmake/dependencies.cmake
+-rw-r--r--   0 kaoutar    (501) staff       (20)     1693 2023-07-14 20:02:54.000000 aihwkit-0.8.0/cmake/dependencies_cuda.cmake
+-rw-r--r--   0 kaoutar    (501) staff       (20)     1174 2023-07-14 20:02:54.000000 aihwkit-0.8.0/cmake/dependencies_test.cmake
+-rw-r--r--   0 kaoutar    (501) staff       (20)      721 2023-07-14 20:10:45.155072 aihwkit-0.8.0/setup.cfg
+-rw-r--r--   0 kaoutar    (501) staff       (20)     2925 2023-07-14 20:02:54.000000 aihwkit-0.8.0/setup.py
+drwxr-xr-x   0 kaoutar    (501) staff       (20)        0 2023-07-14 20:10:44.799670 aihwkit-0.8.0/src/
+drwxr-xr-x   0 kaoutar    (501) staff       (20)        0 2023-07-14 20:10:44.818119 aihwkit-0.8.0/src/aihwkit/
+-rw-r--r--   0 kaoutar    (501) staff       (20)        6 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/VERSION.txt
+-rw-r--r--   0 kaoutar    (501) staff       (20)      586 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/__init__.py
+drwxr-xr-x   0 kaoutar    (501) staff       (20)        0 2023-07-14 20:10:44.823866 aihwkit-0.8.0/src/aihwkit/cloud/
+-rw-r--r--   0 kaoutar    (501) staff       (20)      580 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/cloud/__init__.py
+drwxr-xr-x   0 kaoutar    (501) staff       (20)        0 2023-07-14 20:10:44.828006 aihwkit-0.8.0/src/aihwkit/cloud/client/
+-rw-r--r--   0 kaoutar    (501) staff       (20)      567 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/cloud/client/__init__.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     5451 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/cloud/client/entities.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     1815 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/cloud/client/exceptions.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     3869 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/cloud/client/session.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     2414 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/cloud/client/utils.py
+drwxr-xr-x   0 kaoutar    (501) staff       (20)        0 2023-07-14 20:10:44.832343 aihwkit-0.8.0/src/aihwkit/cloud/client/v1/
+-rw-r--r--   0 kaoutar    (501) staff       (20)      562 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/cloud/client/v1/__init__.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     6485 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/cloud/client/v1/api_client.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     6783 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/cloud/client/v1/i_api_client.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     5346 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/cloud/client/v1/parsers.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     4546 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/cloud/client/v1/stubs.py
+drwxr-xr-x   0 kaoutar    (501) staff       (20)        0 2023-07-14 20:10:44.834395 aihwkit-0.8.0/src/aihwkit/cloud/converter/
+-rw-r--r--   0 kaoutar    (501) staff       (20)      580 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/cloud/converter/__init__.py
+drwxr-xr-x   0 kaoutar    (501) staff       (20)        0 2023-07-14 20:10:44.843003 aihwkit-0.8.0/src/aihwkit/cloud/converter/definitions/
+-rw-r--r--   0 kaoutar    (501) staff       (20)      563 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/cloud/converter/definitions/__init__.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     3064 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/cloud/converter/definitions/common_pb2.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     3268 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/cloud/converter/definitions/i_common_pb2.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     3493 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/cloud/converter/definitions/i_input_file_pb2.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     3626 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/cloud/converter/definitions/i_onnx_common_pb2.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     1894 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/cloud/converter/definitions/i_output_file_pb2.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     1833 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/cloud/converter/definitions/input_file_pb2.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     3566 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/cloud/converter/definitions/onnx_common_pb2.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     1567 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/cloud/converter/definitions/output_file_pb2.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)      689 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/cloud/converter/exceptions.py
+drwxr-xr-x   0 kaoutar    (501) staff       (20)        0 2023-07-14 20:10:44.850130 aihwkit-0.8.0/src/aihwkit/cloud/converter/v1/
+-rw-r--r--   0 kaoutar    (501) staff       (20)      566 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/cloud/converter/v1/__init__.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     1240 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/cloud/converter/v1/analog_info.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)    10387 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/cloud/converter/v1/i_mappings.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)    16653 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/cloud/converter/v1/inferencing.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)    11307 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/cloud/converter/v1/mappings.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     2724 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/cloud/converter/v1/noise_model_info.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     9945 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/cloud/converter/v1/rpu_config_info.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     6799 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/cloud/converter/v1/training.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     1462 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/exceptions.py
+drwxr-xr-x   0 kaoutar    (501) staff       (20)        0 2023-07-14 20:10:44.852465 aihwkit-0.8.0/src/aihwkit/experiments/
+-rw-r--r--   0 kaoutar    (501) staff       (20)      761 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/experiments/__init__.py
+drwxr-xr-x   0 kaoutar    (501) staff       (20)        0 2023-07-14 20:10:44.856594 aihwkit-0.8.0/src/aihwkit/experiments/experiments/
+-rw-r--r--   0 kaoutar    (501) staff       (20)      540 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/experiments/experiments/__init__.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     3330 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/experiments/experiments/base.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)    15697 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/experiments/experiments/inferencing.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)    11435 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/experiments/experiments/training.py
+drwxr-xr-x   0 kaoutar    (501) staff       (20)        0 2023-07-14 20:10:44.862255 aihwkit-0.8.0/src/aihwkit/experiments/runners/
+-rw-r--r--   0 kaoutar    (501) staff       (20)      990 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/experiments/runners/__init__.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)      882 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/experiments/runners/base.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     4264 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/experiments/runners/cloud.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     4570 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/experiments/runners/i_cloud.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     3348 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/experiments/runners/i_local.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     2475 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/experiments/runners/i_metrics.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     3682 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/experiments/runners/local.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     3743 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/experiments/runners/metrics.py
+drwxr-xr-x   0 kaoutar    (501) staff       (20)        0 2023-07-14 20:10:44.863716 aihwkit-0.8.0/src/aihwkit/inference/
+-rw-r--r--   0 kaoutar    (501) staff       (20)     1226 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/inference/__init__.py
+drwxr-xr-x   0 kaoutar    (501) staff       (20)        0 2023-07-14 20:10:44.865306 aihwkit-0.8.0/src/aihwkit/inference/calibration/
+-rw-r--r--   0 kaoutar    (501) staff       (20)      675 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/inference/calibration/__init__.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     9344 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/inference/calibration/calibration.py
+drwxr-xr-x   0 kaoutar    (501) staff       (20)        0 2023-07-14 20:10:44.867825 aihwkit-0.8.0/src/aihwkit/inference/compensation/
+-rw-r--r--   0 kaoutar    (501) staff       (20)      588 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/inference/compensation/__init__.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     1904 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/inference/compensation/base.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     1345 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/inference/compensation/drift.py
+drwxr-xr-x   0 kaoutar    (501) staff       (20)        0 2023-07-14 20:10:44.870090 aihwkit-0.8.0/src/aihwkit/inference/converter/
+-rw-r--r--   0 kaoutar    (501) staff       (20)      604 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/inference/converter/__init__.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     1933 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/inference/converter/base.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     3056 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/inference/converter/conductance.py
+drwxr-xr-x   0 kaoutar    (501) staff       (20)        0 2023-07-14 20:10:44.875703 aihwkit-0.8.0/src/aihwkit/inference/noise/
+-rw-r--r--   0 kaoutar    (501) staff       (20)      589 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/inference/noise/__init__.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     6095 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/inference/noise/base.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     7679 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/inference/noise/custom.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     6949 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/inference/noise/pcm.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     6587 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/inference/noise/reram.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     2006 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/inference/utils.py
+drwxr-xr-x   0 kaoutar    (501) staff       (20)        0 2023-07-14 20:10:44.877435 aihwkit-0.8.0/src/aihwkit/nn/
+-rw-r--r--   0 kaoutar    (501) staff       (20)     1174 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/nn/__init__.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)    11521 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/nn/conversion.py
+drwxr-xr-x   0 kaoutar    (501) staff       (20)        0 2023-07-14 20:10:44.883986 aihwkit-0.8.0/src/aihwkit/nn/modules/
+-rw-r--r--   0 kaoutar    (501) staff       (20)      539 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/nn/modules/__init__.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)    13923 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/nn/modules/base.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     3800 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/nn/modules/container.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)    29263 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/nn/modules/conv.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)    36398 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/nn/modules/conv_mapped.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     5642 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/nn/modules/linear.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     4005 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/nn/modules/linear_mapped.py
+drwxr-xr-x   0 kaoutar    (501) staff       (20)        0 2023-07-14 20:10:44.887047 aihwkit-0.8.0/src/aihwkit/nn/modules/rnn/
+-rw-r--r--   0 kaoutar    (501) staff       (20)      543 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/nn/modules/rnn/__init__.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     9284 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/nn/modules/rnn/cells.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     4632 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/nn/modules/rnn/layers.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     8470 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/nn/modules/rnn/rnn.py
+drwxr-xr-x   0 kaoutar    (501) staff       (20)        0 2023-07-14 20:10:44.889306 aihwkit-0.8.0/src/aihwkit/optim/
+-rw-r--r--   0 kaoutar    (501) staff       (20)      674 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/optim/__init__.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     8319 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/optim/analog_optimizer.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     5208 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/optim/context.py
+drwxr-xr-x   0 kaoutar    (501) staff       (20)        0 2023-07-14 20:10:44.891445 aihwkit-0.8.0/src/aihwkit/simulator/
+-rw-r--r--   0 kaoutar    (501) staff       (20)     1007 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/simulator/CMakeLists.txt
+-rw-r--r--   0 kaoutar    (501) staff       (20)      682 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/simulator/__init__.py
+drwxr-xr-x   0 kaoutar    (501) staff       (20)        0 2023-07-14 20:10:44.896232 aihwkit-0.8.0/src/aihwkit/simulator/configs/
+-rw-r--r--   0 kaoutar    (501) staff       (20)     1865 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/simulator/configs/__init__.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)    43522 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/simulator/configs/compounds.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)    10242 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/simulator/configs/configs.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)    43121 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/simulator/configs/devices.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)    10328 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/simulator/configs/helpers.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     1188 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/simulator/configs/utils.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     1000 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/simulator/noise_models.py
+drwxr-xr-x   0 kaoutar    (501) staff       (20)        0 2023-07-14 20:10:44.899869 aihwkit-0.8.0/src/aihwkit/simulator/parameters/
+-rw-r--r--   0 kaoutar    (501) staff       (20)     1162 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/simulator/parameters/__init__.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     3900 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/simulator/parameters/base.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     8962 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/simulator/parameters/enums.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     8482 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/simulator/parameters/helpers.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)    27374 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/simulator/parameters/utils.py
+drwxr-xr-x   0 kaoutar    (501) staff       (20)        0 2023-07-14 20:10:44.905159 aihwkit-0.8.0/src/aihwkit/simulator/presets/
+-rw-r--r--   0 kaoutar    (501) staff       (20)     2547 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/simulator/presets/__init__.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     1718 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/simulator/presets/compounds.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)    62911 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/simulator/presets/configs.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)    13751 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/simulator/presets/devices.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     4058 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/simulator/presets/inference.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     4507 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/simulator/presets/utils.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     4613 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/simulator/presets/web.py
+drwxr-xr-x   0 kaoutar    (501) staff       (20)        0 2023-07-14 20:10:44.908969 aihwkit-0.8.0/src/aihwkit/simulator/rpu_base_src/
+-rw-r--r--   0 kaoutar    (501) staff       (20)     1343 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/simulator/rpu_base_src/rpu_base.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)     1744 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/simulator/rpu_base_src/rpu_base.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)    43144 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/simulator/rpu_base_src/rpu_base_devices.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)    35740 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/simulator/rpu_base_src/rpu_base_tiles.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)    22129 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/simulator/rpu_base_src/rpu_base_tiles_cuda.cpp
+drwxr-xr-x   0 kaoutar    (501) staff       (20)        0 2023-07-14 20:10:44.919080 aihwkit-0.8.0/src/aihwkit/simulator/tiles/
+-rw-r--r--   0 kaoutar    (501) staff       (20)      852 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/simulator/tiles/__init__.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     9034 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/simulator/tiles/analog.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     6121 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/simulator/tiles/array.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)    22377 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/simulator/tiles/base.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)    11339 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/simulator/tiles/custom.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     5212 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/simulator/tiles/floating_point.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     3763 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/simulator/tiles/functions.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)    13584 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/simulator/tiles/inference.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     9846 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/simulator/tiles/inference_torch.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)    13856 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/simulator/tiles/module.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)    43950 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/simulator/tiles/periphery.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)    15921 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/simulator/tiles/rpucuda.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)    23860 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/simulator/tiles/torch_tile.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     2920 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/simulator/tiles/utils.py
+drwxr-xr-x   0 kaoutar    (501) staff       (20)        0 2023-07-14 20:10:44.962488 aihwkit-0.8.0/src/aihwkit/utils/
+-rw-r--r--   0 kaoutar    (501) staff       (20)      550 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/utils/__init__.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)    11494 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/utils/analog_info.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)    10530 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/utils/fitting.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     7779 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/utils/legacy.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)    28960 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/utils/visualization.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     4947 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/utils/visualization_web.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)      726 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/aihwkit/version.py
+drwxr-xr-x   0 kaoutar    (501) staff       (20)        0 2023-07-14 20:10:44.823140 aihwkit-0.8.0/src/aihwkit.egg-info/
+-rw-r--r--   0 kaoutar    (501) staff       (20)    10839 2023-07-14 20:10:44.000000 aihwkit-0.8.0/src/aihwkit.egg-info/PKG-INFO
+-rw-r--r--   0 kaoutar    (501) staff       (20)    13046 2023-07-14 20:10:44.000000 aihwkit-0.8.0/src/aihwkit.egg-info/SOURCES.txt
+-rw-r--r--   0 kaoutar    (501) staff       (20)        1 2023-07-14 20:10:44.000000 aihwkit-0.8.0/src/aihwkit.egg-info/dependency_links.txt
+-rw-r--r--   0 kaoutar    (501) staff       (20)        1 2023-07-14 20:10:44.000000 aihwkit-0.8.0/src/aihwkit.egg-info/not-zip-safe
+-rw-r--r--   0 kaoutar    (501) staff       (20)      183 2023-07-14 20:10:44.000000 aihwkit-0.8.0/src/aihwkit.egg-info/requires.txt
+-rw-r--r--   0 kaoutar    (501) staff       (20)        8 2023-07-14 20:10:44.000000 aihwkit-0.8.0/src/aihwkit.egg-info/top_level.txt
+drwxr-xr-x   0 kaoutar    (501) staff       (20)        0 2023-07-14 20:10:45.031432 aihwkit-0.8.0/src/rpucuda/
+-rw-r--r--   0 kaoutar    (501) staff       (20)      762 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/CMakeLists.txt
+drwxr-xr-x   0 kaoutar    (501) staff       (20)        0 2023-07-14 20:10:45.132066 aihwkit-0.8.0/src/rpucuda/cuda/
+-rw-r--r--   0 kaoutar    (501) staff       (20)      706 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/CMakeLists.txt
+-rw-r--r--   0 kaoutar    (501) staff       (20)    88633 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/bit_line_maker.cu
+-rw-r--r--   0 kaoutar    (501) staff       (20)     5876 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/bit_line_maker.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)    19835 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/bit_line_maker_test.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)    14613 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/chopped_weight_output.cu
+-rw-r--r--   0 kaoutar    (501) staff       (20)     7798 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/chopped_weight_output.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)     3854 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/cuda_buffer.cu
+-rw-r--r--   0 kaoutar    (501) staff       (20)     1680 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/cuda_buffer.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)    43703 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/cuda_math_util.cu
+-rw-r--r--   0 kaoutar    (501) staff       (20)     7976 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/cuda_math_util.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)    38792 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/cuda_util.cu
+-rw-r--r--   0 kaoutar    (501) staff       (20)    23307 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/cuda_util.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)    29985 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/forward_backward_pass.cu
+-rw-r--r--   0 kaoutar    (501) staff       (20)    10770 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/forward_backward_pass.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)    12486 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/forward_backward_pass_test.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)    19351 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/io_iterator.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)    17474 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/io_iterator_test.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)    33044 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/io_manager.cu
+-rw-r--r--   0 kaoutar    (501) staff       (20)     5455 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/io_manager.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)    17444 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/io_manager_test.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)    13517 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/maximizer.cu
+-rw-r--r--   0 kaoutar    (501) staff       (20)     2156 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/maximizer.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)     6753 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/maximizer_test.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)    20967 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/noise_manager.cu
+-rw-r--r--   0 kaoutar    (501) staff       (20)     3607 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/noise_manager.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)    17375 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/pulsed_weight_updater.cu
+-rw-r--r--   0 kaoutar    (501) staff       (20)     3943 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/pulsed_weight_updater.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)    20872 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/pulsed_weight_updater_test.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)    69621 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/pwu_kernel.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)    41134 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/pwu_kernel_parameter.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)     5824 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/pwu_kernel_parameter_base.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)      789 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpu_cub.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)    28072 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda.cu
+-rw-r--r--   0 kaoutar    (501) staff       (20)     8781 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)     7916 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_buffered_transfer_device.cu
+-rw-r--r--   0 kaoutar    (501) staff       (20)     2366 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_buffered_transfer_device.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)    10690 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_buffered_transfer_device_test.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)    18171 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_chopped_transfer_device.cu
+-rw-r--r--   0 kaoutar    (501) staff       (20)     3957 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_chopped_transfer_device.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)    13563 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_chopped_transfer_device_test.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)     4471 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_constantstep_device.cu
+-rw-r--r--   0 kaoutar    (501) staff       (20)     1950 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_constantstep_device.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)    16325 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_dynamic_transfer_device.cu
+-rw-r--r--   0 kaoutar    (501) staff       (20)     3388 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_dynamic_transfer_device.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)     8968 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_expstep_device.cu
+-rw-r--r--   0 kaoutar    (501) staff       (20)     2549 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_expstep_device.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)    13273 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_expstep_test.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)     3854 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_hidden_device.cu
+-rw-r--r--   0 kaoutar    (501) staff       (20)     3669 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_hidden_device.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)    13439 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_hidden_test.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)     6248 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_linearstep_device.cu
+-rw-r--r--   0 kaoutar    (501) staff       (20)     3125 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_linearstep_device.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)    13030 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_linearstep_test.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)     9280 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_mixedprec_device.cu
+-rw-r--r--   0 kaoutar    (501) staff       (20)     2766 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_mixedprec_device.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)    12699 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_mixedprec_device_base.cu
+-rw-r--r--   0 kaoutar    (501) staff       (20)     5245 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_mixedprec_device_base.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)    12015 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_mixedprec_device_test.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)     9585 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_mixedprec_int_device.cu
+-rw-r--r--   0 kaoutar    (501) staff       (20)     2900 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_mixedprec_int_device.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)    12089 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_mixedprec_int_device_test.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)    13390 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_onesided_device.cu
+-rw-r--r--   0 kaoutar    (501) staff       (20)     3862 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_onesided_device.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)    12055 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_onesided_device_test.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)     5927 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_piecewisestep_device.cu
+-rw-r--r--   0 kaoutar    (501) staff       (20)     3143 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_piecewisestep_device.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)     4317 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_powstep_device.cu
+-rw-r--r--   0 kaoutar    (501) staff       (20)     3113 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_powstep_device.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)     4271 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_powstep_reference_device.cu
+-rw-r--r--   0 kaoutar    (501) staff       (20)     2936 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_powstep_reference_device.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)    10225 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_powstep_test.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)    29782 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_pulsed.cu
+-rw-r--r--   0 kaoutar    (501) staff       (20)     8362 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_pulsed.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)    17223 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_pulsed_device.cu
+-rw-r--r--   0 kaoutar    (501) staff       (20)    17726 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_pulsed_device.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)     8466 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_pulsed_device_test.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)    50178 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_pulsed_test.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)    13625 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_simple_device.cu
+-rw-r--r--   0 kaoutar    (501) staff       (20)     6166 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_simple_device.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)     7509 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_simple_device_test.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)     6574 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_softbounds_reference_device.cu
+-rw-r--r--   0 kaoutar    (501) staff       (20)     3130 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_softbounds_reference_device.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)    21083 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_test.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)    19099 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_transfer_device.cu
+-rw-r--r--   0 kaoutar    (501) staff       (20)     4926 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_transfer_device.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)    16316 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_transfer_device_test.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)    15671 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_vector_device.cu
+-rw-r--r--   0 kaoutar    (501) staff       (20)     4457 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_vector_device.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)     9045 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_vector_device_test.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)    16620 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/test_helper.cu
+-rw-r--r--   0 kaoutar    (501) staff       (20)     1399 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/test_helper.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)    26151 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/update_management_helper.cu
+-rw-r--r--   0 kaoutar    (501) staff       (20)     3656 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/update_management_helper.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)     3886 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/update_management_helper_test.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)     4311 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/weight_clipper_cuda.cu
+-rw-r--r--   0 kaoutar    (501) staff       (20)     1403 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/weight_clipper_cuda.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)     3768 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/weight_clipper_cuda_test.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)     7773 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/weight_drifter_cuda.cu
+-rw-r--r--   0 kaoutar    (501) staff       (20)     2081 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/weight_drifter_cuda.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)     6312 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/weight_drifter_cuda_test.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)    16322 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/weight_modifier_cuda.cu
+-rw-r--r--   0 kaoutar    (501) staff       (20)     1571 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/weight_modifier_cuda.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)     8365 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/weight_remapper_cuda.cu
+-rw-r--r--   0 kaoutar    (501) staff       (20)     1473 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/cuda/weight_remapper_cuda.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)     9092 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/dense_bit_line_maker.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)     3206 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/dense_bit_line_maker.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)     7077 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/math_util.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)     2466 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/math_util.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)     4028 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rng.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)     3543 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rng.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)    50637 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)    25304 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)     9037 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_buffered_transfer_device.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)     5643 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_buffered_transfer_device.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)    15870 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_chopped_transfer_device.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)     6642 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_chopped_transfer_device.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)     2849 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_constantstep_device.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)     1848 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_constantstep_device.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)    14344 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_dynamic_transfer_device.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)     5290 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_dynamic_transfer_device.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)     5603 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_expstep_device.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)     3351 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_expstep_device.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)    33041 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_forward_backward_pass.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)     7183 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_forward_backward_pass.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)     4798 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_hidden_device.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)     5169 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_hidden_device.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)     7982 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_linearstep_device.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)     7044 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_linearstep_device.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)    10257 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_mixedprec_device.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)     4682 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_mixedprec_device.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)    15364 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_mixedprec_device_base.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)     6631 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_mixedprec_device_base.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)     9868 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_mixedprec_int_device.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)     5236 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_mixedprec_int_device.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)    14049 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_onesided_device.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)     5639 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_onesided_device.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)     8096 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_onesided_device_test.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)     4587 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_piecewisestep_device.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)     2750 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_piecewisestep_device.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)     5505 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_powstep_device.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)     3934 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_powstep_device.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)     6177 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_powstep_reference_device.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)     5211 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_powstep_reference_device.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)    17819 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_pulsed.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)     5083 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_pulsed.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)    24375 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_pulsed_device.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)    27234 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_pulsed_device.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)     7524 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_pulsed_meta_parameter.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)    12124 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_pulsed_meta_parameter.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)     5024 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_pulsed_test.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)     5883 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_simple_device.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)     9975 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_simple_device.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)     7649 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_softbounds_reference_device.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)     3549 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_softbounds_reference_device.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)    22825 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_transfer_device.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)     8913 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_transfer_device.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)    10858 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_transfer_device_test.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)    18963 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_vector_device.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)     8794 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_vector_device.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)    11427 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_weight_updater.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)     3666 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/rpu_weight_updater.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)    10179 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/sparse_bit_line_maker.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)     2756 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/sparse_bit_line_maker.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)     4665 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/utility_functions.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)     7902 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/utility_functions.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)     2931 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/weight_clipper.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)     2252 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/weight_clipper.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)     5048 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/weight_drifter.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)     5037 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/weight_drifter.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)     6451 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/weight_modifier.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)     4756 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/weight_modifier.h
+-rw-r--r--   0 kaoutar    (501) staff       (20)     4123 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/weight_remapper.cpp
+-rw-r--r--   0 kaoutar    (501) staff       (20)     2554 2023-07-14 20:02:54.000000 aihwkit-0.8.0/src/rpucuda/weight_remapper.h
+drwxr-xr-x   0 kaoutar    (501) staff       (20)        0 2023-07-14 20:10:45.152610 aihwkit-0.8.0/tests/
+-rw-r--r--   0 kaoutar    (501) staff       (20)      530 2023-07-14 20:02:54.000000 aihwkit-0.8.0/tests/__init__.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)    11453 2023-07-14 20:02:54.000000 aihwkit-0.8.0/tests/test_bindings_tiles.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     4353 2023-07-14 20:02:54.000000 aihwkit-0.8.0/tests/test_calibration.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     4072 2023-07-14 20:02:54.000000 aihwkit-0.8.0/tests/test_client.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     4716 2023-07-14 20:02:54.000000 aihwkit-0.8.0/tests/test_cloud_runner.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)    10308 2023-07-14 20:02:54.000000 aihwkit-0.8.0/tests/test_conversions.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     2893 2023-07-14 20:02:54.000000 aihwkit-0.8.0/tests/test_experiment_runners.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     2977 2023-07-14 20:02:54.000000 aihwkit-0.8.0/tests/test_experiments.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     2544 2023-07-14 20:02:54.000000 aihwkit-0.8.0/tests/test_inference.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)    12641 2023-07-14 20:02:54.000000 aihwkit-0.8.0/tests/test_inference_tiles.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     6668 2023-07-14 20:02:54.000000 aihwkit-0.8.0/tests/test_layer_base.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)    14844 2023-07-14 20:02:54.000000 aihwkit-0.8.0/tests/test_layers.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)    31045 2023-07-14 20:02:54.000000 aihwkit-0.8.0/tests/test_layers_convolution.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)    17192 2023-07-14 20:02:54.000000 aihwkit-0.8.0/tests/test_layers_linear.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     8765 2023-07-14 20:02:54.000000 aihwkit-0.8.0/tests/test_layers_mapped.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)    16338 2023-07-14 20:02:54.000000 aihwkit-0.8.0/tests/test_layers_rnn.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     2885 2023-07-14 20:02:54.000000 aihwkit-0.8.0/tests/test_localrunner_infer.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     3390 2023-07-14 20:02:54.000000 aihwkit-0.8.0/tests/test_optimizers.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     4109 2023-07-14 20:02:54.000000 aihwkit-0.8.0/tests/test_presets.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     5603 2023-07-14 20:02:54.000000 aihwkit-0.8.0/tests/test_rpu_configurations.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)    22798 2023-07-14 20:02:54.000000 aihwkit-0.8.0/tests/test_simulator_tiles.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)     6076 2023-07-14 20:02:54.000000 aihwkit-0.8.0/tests/test_specific_tiles.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)    14169 2023-07-14 20:02:54.000000 aihwkit-0.8.0/tests/test_torch_tiles.py
+-rw-r--r--   0 kaoutar    (501) staff       (20)    34554 2023-07-14 20:02:54.000000 aihwkit-0.8.0/tests/test_utils.py
```

### Comparing `aihwkit-0.7.1/CMakeLists.txt` & `aihwkit-0.8.0/CMakeLists.txt`

 * *Files 6% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -13,24 +13,24 @@
 
 # Project options.
 option(BUILD_TEST "Build C++ test binaries" OFF)
 option(USE_CUDA "Build with CUDA support" $ENV{USE_CUDA})
 option(RPU_DEBUG "Enable debug printing" OFF)
 option(RPU_USE_FASTMOD "Use fast mod" ON)
 option(RPU_USE_FASTRAND "Use fastrand" OFF)
+option(RPU_USE_TORCH_BUFFERS "Use torch buffers for RPUCuda" ON)
 
 set(RPU_BLAS "OpenBLAS" CACHE STRING "BLAS backend of choice (OpenBLAS, MKL)")
 set(RPU_CUDA_ARCHITECTURES "60;70;75;80" CACHE STRING "Target CUDA architectures")
 
 # Internal variables.
 set(CUDA_TARGET_PROPERTIES POSITION_INDEPENDENT_CODE ON
                            CUDA_RESOLVE_DEVICE_SYMBOLS ON
                            CUDA_SEPARABLE_COMPILATION ON
-                           CXX_STANDARD 11
-                           POSITION_INDEPENDENT_CODE ON)
+                           CXX_STANDARD 14)
 
 # Append the virtualenv library path to cmake.
 if(DEFINED ENV{VIRTUAL_ENV})
   include_directories("$ENV{VIRTUAL_ENV}/include")
   link_directories("$ENV{VIRTUAL_ENV}/lib")
   set(CMAKE_PREFIX_PATH "$ENV{VIRTUAL_ENV}")
 endif()
@@ -39,15 +39,15 @@
 include(cmake/dependencies.cmake)
 include(cmake/dependencies_cuda.cmake)
 include(cmake/dependencies_test.cmake)
 
 # Set compilation flags.
 if(WIN32)
   set(CMAKE_CXX_FLAGS_RELEASE "${CMAKE_CXX_FLAGS_RELEASE} /O2")
-  set(CMAKE_CXX_STANDARD 11)
+  set(CMAKE_CXX_STANDARD 14)
   set(CMAKE_CXX_STANDARD_REQUIRED ON)
 else()
   set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -Wall -Wno-narrowing -Wno-strict-overflow")
   set(CMAKE_CXX_FLAGS_RELEASE "${CMAKE_CXX_FLAGS_RELEASE} -O3 -ftree-vectorize")
 endif()
 
 if (APPLE)
@@ -61,37 +61,42 @@
 add_library(RPU_CPU ${RPU_CPU_SRCS})
 
 target_link_libraries(RPU_CPU ${RPU_DEPENDENCY_LIBS})
 if(WIN32)
   target_link_libraries(RPU_CPU c10.lib torch_cpu.lib)
 endif()
 
-set_target_properties(RPU_CPU PROPERTIES CXX_STANDARD 11
-                                         POSITION_INDEPENDENT_CODE ON)
+set_target_properties(RPU_CPU PROPERTIES CXX_STANDARD 14
+  POSITION_INDEPENDENT_CODE ON)
 
 if(USE_CUDA)
   add_subdirectory(src/rpucuda/cuda)
   include_directories(SYSTEM src/rpucuda/cuda)
   add_library(RPU_GPU ${RPU_GPU_SRCS})
 
   target_link_libraries(RPU_GPU RPU_CPU cublas curand ${RPU_DEPENDENCY_LIBS})
   if(WIN32)
     target_link_libraries(RPU_GPU c10_cuda.lib torch_cuda.lib)
   endif(WIN32)
 
   set_target_properties(RPU_GPU PROPERTIES ${CUDA_TARGET_PROPERTIES})
   set_property(TARGET RPU_GPU PROPERTY CUDA_ARCHITECTURES ${RPU_CUDA_ARCHITECTURES})
 
+  if (RPU_USE_TORCH_BUFFERS)
+    add_compile_definitions(RPU_TORCH_CUDA_BUFFERS)
+    set(CMAKE_CUDA_FLAGS "${CMAKE_CUDA_FLAGS} --expt-relaxed-constexpr")
+    set(CMAKE_CUDA_FLAGS "${CMAKE_CUDA_FLAGS} -Xcudafe --diag_suppress=186")
+  endif(RPU_USE_TORCH_BUFFERS)
+
   if(${CUDAToolkit_VERSION_MAJOR} LESS 11)
     # The "cub" target only exists if cub was downloaded during build.
     if(TARGET cub)
         add_dependencies(RPU_GPU cub)
     endif()
   endif()
-
 endif(USE_CUDA)
 
 # Add aihwkit targets.
 add_subdirectory(src/aihwkit/simulator)
 
 # Add tests.
 if(BUILD_TEST)
```

### Comparing `aihwkit-0.7.1/LICENSE.txt` & `aihwkit-0.8.0/LICENSE.txt`

 * *Files 1% similar despite different names*

```diff
@@ -183,15 +183,15 @@
       replaced with your own identifying information. (Don't include
       the brackets!)  The text should be enclosed in the appropriate
       comment syntax for the file format. We also recommend that a
       file or class name and description of purpose be included on the
       same "printed page" as the copyright notice for easier
       identification within third-party archives.
 
-   Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+   Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 
    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at
 
        http://www.apache.org/licenses/LICENSE-2.0
```

### Comparing `aihwkit-0.7.1/PKG-INFO` & `aihwkit-0.8.0/PKG-INFO`

 * *Files 6% similar despite different names*

```diff
@@ -1,16 +1,17 @@
 Metadata-Version: 2.1
 Name: aihwkit
-Version: 0.7.1
+Version: 0.8.0
 Summary: IBM Analog Hardware Acceleration Kit
 Home-page: https://github.com/IBM/aihwkit
 Author: IBM Research
 Author-email: aihwkit@us.ibm.com
 License: Apache 2.0
-Keywords: ai,analog,rpu,torch
+Keywords: ai,analog,rpu,torch,memristor,pcm,reram,crossbar,in-memory,nvm,non-von-neumann,non-volatile memory,phase-change material
+Platform: UNKNOWN
 Classifier: Development Status :: 4 - Beta
 Classifier: Environment :: Console
 Classifier: Environment :: GPU :: NVIDIA CUDA
 Classifier: Intended Audience :: Science/Research
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Operating System :: MacOS
 Classifier: Operating System :: Microsoft :: Windows
@@ -84,20 +85,20 @@
 
 ### Other features
 
 Along with the two main components, the toolkit includes other
 functionalities such as:
 
 * A library of device presets that are calibrated to real hardware data and
-  based on models in the literature, along with configuration that specifies a particular device and optimizer choice.
+  based on models in the literature, along with a configuration that specifies a particular device and optimizer choice.
 * A module for executing high-level use cases ("experiments"), such as neural
   network training with minimal code overhead.
 * A utility to automatically convert a downloaded model (e.g., pre-trained) to its equivalent Analog
   model by replacing all linear/conv layers to Analog layers (e.g., for convenient hardware-aware training).
-* Integration with the [AIHW Composer] platform, a no-code web experience, that allows executing
+* Integration with the [AIHW Composer] platform, a no-code web experience that allows executing
   experiments in the cloud.
 
 ## Example
 
 ### Training example
 
 ```python
@@ -128,28 +129,29 @@
     print('Loss error: {:.16f}'.format(loss))
 ```
 
 You can find more examples in the [`examples/`] folder of the project, and
 more information about the library in the [documentation]. Please note that
 the examples have some additional dependencies - you can install them via
 `pip install -r requirements-examples.txt`.
+You can find interactive notebooks and tutorials in the [`notebooks/`] directory. 
 
 
 ## What is Analog AI?
 
 In traditional hardware architecture, computation and memory are siloed in
 different locations. Information is moved back and forth between computation
 and memory units every time an operation is performed, creating a limitation
 called the [von Neumann bottleneck].
 
 Analog AI delivers radical performance improvements by combining compute and
 memory in a single device, eliminating the von Neumann bottleneck. By leveraging
 the physical properties of memory devices, computation happens at the same place
 where the data is stored. Such in-memory computing hardware increases the speed
-and energy-efficiency needed for next generation AI workloads.
+and energy efficiency needed for next-generation AI workloads.
 
 ## What is an in-memory computing chip?
 
 An in-memory computing chip typically consists of multiple arrays of memory
 devices that communicate with each other. Many types of memory devices such as
 [phase-change memory] (PCM), [resistive random-access memory] (RRAM), and
 [Flash memory] can be used for in-memory computing.
@@ -176,28 +178,50 @@
 > Malte J. Rasch, Diego Moreda, Tayfun Gokmen, Manuel Le Gallo, Fabio Carta,
 > Cindy Goldberg, Kaoutar El Maghraoui, Abu Sebastian, Vijay Narayanan.
 > "A flexible and fast PyTorch toolkit for simulating training and inference on
 > analog crossbar arrays" (2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems)
 >
 > https://ieeexplore.ieee.org/abstract/document/9458494
 
+## Awards and Media Mentions 
+We are proud to share the AIHWKIT and the companion cloud composer received the IEEE OPEN SOURCE SCIENCE [award] in 2023. 
+![image](https://github.com/IBM/aihwkit/assets/7916630/1eb2ee6a-31c6-42c1-aa30-da5d396b24d7)
+
 ## Installation
 
 ### Installing from PyPI
 
-The preferred way to install this package is by using the
+The preferred way to install this package is by using the 
 [Python package index]:
 
 ```bash
 $ pip install aihwkit
 ```
+### Conda-based Installation
+We are working on publishing the package in the conda-forge channel. Until then, you need to manually download the package for installation.
+
+Download the aihwkit conda package tar file::
+
+    $ wget https://aihwkit-gpu-demo.s3.us-east.cloud-object-storage.appdomain.cloud/aihwkit-condapkg.tar
+
+Untar the file to a directory such as $HOME/aihwkit-condapkg
+Create a conda environment::
+
+    $ conda create -n aihwkit
+    $ conda activate aihwkit
 
-> :warning: Note that currently we provide CPU-only pre-built packages for
-> specific combinations of architectures and versions, and in some cases a
-> pre-built package might still not be available.
+Install one of the conda packages.  For example:
+
+  - CPU::
+
+    $ conda install python=3.9 aihwkit -c conda-forge -c file:/$HOME/aihwkit-condapkg
+
+  - GPU::
+
+    $ conda install python=3.9 aihwkit-gpu -c conda-forge -c file:/$HOME/aihwkit-condapkg
 
 If you encounter any issues during download or want to compile the package
 for your environment, please refer to the [advanced installation] guide.
 That section describes the additional libraries and tools required for
 compiling the sources, using a build system based on `cmake`.
 
 ## Authors
@@ -216,18 +240,21 @@
 [Apache License 2.0]: LICENSE.txt
 [`CUDA Toolkit`]: https://developer.nvidia.com/accelerated-computing-toolkit
 [`OpenBLAS`]: https://www.openblas.net/
 [Python package index]: https://pypi.org/project/aihwkit
 [`PyTorch`]: https://pytorch.org/
 
 [`examples/`]: examples/
+[`notebooks/`]: notebooks/
 [documentation]: https://aihwkit.readthedocs.io/
 [contributors]: https://github.com/IBM/aihwkit/graphs/contributors
 [advanced installation]: https://aihwkit.readthedocs.io/en/latest/advanced_install.html
 
 [von Neumann bottleneck]: https://en.wikipedia.org/wiki/Von_Neumann_architecture#Von_Neumann_bottleneck
 [phase-change memory]: https://en.wikipedia.org/wiki/Phase-change_memory
 [resistive random-access memory]: https://en.wikipedia.org/wiki/Resistive_random-access_memory
 [Flash memory]: https://en.wikipedia.org/wiki/Flash_memory
 [Kirchhoff’s circuits laws]: https://en.wikipedia.org/wiki/Kirchhoff%27s_circuit_laws
 [online demo]: https://analog-ai-demo.mybluemix.net/
 [AIHW Composer]: https://aihw-composer.draco.res.ibm.com
+[award]: https://conferences.computer.org/services/2023/awards/
+
```

### Comparing `aihwkit-0.7.1/README.md` & `aihwkit-0.8.0/README.md`

 * *Files 7% similar despite different names*

```diff
@@ -56,20 +56,20 @@
 
 ### Other features
 
 Along with the two main components, the toolkit includes other
 functionalities such as:
 
 * A library of device presets that are calibrated to real hardware data and
-  based on models in the literature, along with configuration that specifies a particular device and optimizer choice.
+  based on models in the literature, along with a configuration that specifies a particular device and optimizer choice.
 * A module for executing high-level use cases ("experiments"), such as neural
   network training with minimal code overhead.
 * A utility to automatically convert a downloaded model (e.g., pre-trained) to its equivalent Analog
   model by replacing all linear/conv layers to Analog layers (e.g., for convenient hardware-aware training).
-* Integration with the [AIHW Composer] platform, a no-code web experience, that allows executing
+* Integration with the [AIHW Composer] platform, a no-code web experience that allows executing
   experiments in the cloud.
 
 ## Example
 
 ### Training example
 
 ```python
@@ -100,28 +100,29 @@
     print('Loss error: {:.16f}'.format(loss))
 ```
 
 You can find more examples in the [`examples/`] folder of the project, and
 more information about the library in the [documentation]. Please note that
 the examples have some additional dependencies - you can install them via
 `pip install -r requirements-examples.txt`.
+You can find interactive notebooks and tutorials in the [`notebooks/`] directory. 
 
 
 ## What is Analog AI?
 
 In traditional hardware architecture, computation and memory are siloed in
 different locations. Information is moved back and forth between computation
 and memory units every time an operation is performed, creating a limitation
 called the [von Neumann bottleneck].
 
 Analog AI delivers radical performance improvements by combining compute and
 memory in a single device, eliminating the von Neumann bottleneck. By leveraging
 the physical properties of memory devices, computation happens at the same place
 where the data is stored. Such in-memory computing hardware increases the speed
-and energy-efficiency needed for next generation AI workloads.
+and energy efficiency needed for next-generation AI workloads.
 
 ## What is an in-memory computing chip?
 
 An in-memory computing chip typically consists of multiple arrays of memory
 devices that communicate with each other. Many types of memory devices such as
 [phase-change memory] (PCM), [resistive random-access memory] (RRAM), and
 [Flash memory] can be used for in-memory computing.
@@ -148,28 +149,50 @@
 > Malte J. Rasch, Diego Moreda, Tayfun Gokmen, Manuel Le Gallo, Fabio Carta,
 > Cindy Goldberg, Kaoutar El Maghraoui, Abu Sebastian, Vijay Narayanan.
 > "A flexible and fast PyTorch toolkit for simulating training and inference on
 > analog crossbar arrays" (2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems)
 >
 > https://ieeexplore.ieee.org/abstract/document/9458494
 
+## Awards and Media Mentions 
+We are proud to share the AIHWKIT and the companion cloud composer received the IEEE OPEN SOURCE SCIENCE [award] in 2023. 
+![image](https://github.com/IBM/aihwkit/assets/7916630/1eb2ee6a-31c6-42c1-aa30-da5d396b24d7)
+
 ## Installation
 
 ### Installing from PyPI
 
-The preferred way to install this package is by using the
+The preferred way to install this package is by using the 
 [Python package index]:
 
 ```bash
 $ pip install aihwkit
 ```
+### Conda-based Installation
+We are working on publishing the package in the conda-forge channel. Until then, you need to manually download the package for installation.
+
+Download the aihwkit conda package tar file::
+
+    $ wget https://aihwkit-gpu-demo.s3.us-east.cloud-object-storage.appdomain.cloud/aihwkit-condapkg.tar
+
+Untar the file to a directory such as $HOME/aihwkit-condapkg
+Create a conda environment::
+
+    $ conda create -n aihwkit
+    $ conda activate aihwkit
+
+Install one of the conda packages.  For example:
+
+  - CPU::
+
+    $ conda install python=3.9 aihwkit -c conda-forge -c file:/$HOME/aihwkit-condapkg
+
+  - GPU::
 
-> :warning: Note that currently we provide CPU-only pre-built packages for
-> specific combinations of architectures and versions, and in some cases a
-> pre-built package might still not be available.
+    $ conda install python=3.9 aihwkit-gpu -c conda-forge -c file:/$HOME/aihwkit-condapkg
 
 If you encounter any issues during download or want to compile the package
 for your environment, please refer to the [advanced installation] guide.
 That section describes the additional libraries and tools required for
 compiling the sources, using a build system based on `cmake`.
 
 ## Authors
@@ -188,18 +211,20 @@
 [Apache License 2.0]: LICENSE.txt
 [`CUDA Toolkit`]: https://developer.nvidia.com/accelerated-computing-toolkit
 [`OpenBLAS`]: https://www.openblas.net/
 [Python package index]: https://pypi.org/project/aihwkit
 [`PyTorch`]: https://pytorch.org/
 
 [`examples/`]: examples/
+[`notebooks/`]: notebooks/
 [documentation]: https://aihwkit.readthedocs.io/
 [contributors]: https://github.com/IBM/aihwkit/graphs/contributors
 [advanced installation]: https://aihwkit.readthedocs.io/en/latest/advanced_install.html
 
 [von Neumann bottleneck]: https://en.wikipedia.org/wiki/Von_Neumann_architecture#Von_Neumann_bottleneck
 [phase-change memory]: https://en.wikipedia.org/wiki/Phase-change_memory
 [resistive random-access memory]: https://en.wikipedia.org/wiki/Resistive_random-access_memory
 [Flash memory]: https://en.wikipedia.org/wiki/Flash_memory
 [Kirchhoff’s circuits laws]: https://en.wikipedia.org/wiki/Kirchhoff%27s_circuit_laws
 [online demo]: https://analog-ai-demo.mybluemix.net/
 [AIHW Composer]: https://aihw-composer.draco.res.ibm.com
+[award]: https://conferences.computer.org/services/2023/awards/
```

### Comparing `aihwkit-0.7.1/cmake/Modules/FindAVX.cmake` & `aihwkit-0.8.0/cmake/Modules/FindAVX.cmake`

 * *Files identical despite different names*

### Comparing `aihwkit-0.7.1/cmake/Modules/FindCUB.cmake` & `aihwkit-0.8.0/cmake/Modules/FindCUB.cmake`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/cmake/Modules/FindMKL.cmake` & `aihwkit-0.8.0/cmake/Modules/FindMKL.cmake`

 * *Files identical despite different names*

### Comparing `aihwkit-0.7.1/cmake/Modules/FindOpenBLAS.cmake` & `aihwkit-0.8.0/cmake/Modules/FindOpenBLAS.cmake`

 * *Files identical despite different names*

### Comparing `aihwkit-0.7.1/cmake/Modules/FindTorch.cmake` & `aihwkit-0.8.0/cmake/Modules/FindTorch.cmake`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/cmake/dependencies.cmake` & `aihwkit-0.8.0/cmake/dependencies.cmake`

 * *Files 6% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -79,18 +79,19 @@
   add_compile_definitions(RPU_USE_MKL)
   message(STATUS "MKL include for RPU is ${RPU_DEPENDENCY_LIBS}")
 else()
   message(FATAL_ERROR "Invalid BLAS backend: ${RPU_BLAS}")
 endif()
 
 # Python and pybind11
-find_package(PythonInterp REQUIRED)
-find_package(PythonLibs REQUIRED)
+find_package(Python3 COMPONENTS Interpreter Development)
 include_directories(${PYTHON_INCLUDE_DIRS})  # order matters (before pybind)
 
+set(ignoreMe "${Python3_EXECUTABLE}${Python3_FIND_REGISTRY}${Python3_INCLUDE_DIR}${Python3_NumPy_INCLUDE_DIRS}${Python3_ROOT_DIR}${Python_INCLUDE_DIR}${Python_NumPy_INCLUDE_DIRS}")
+
 # Find pybind11Config.cmake
 execute_process(COMMAND ${PYTHON_EXECUTABLE} -c "import pybind11; print(pybind11.get_cmake_dir())"
     OUTPUT_VARIABLE CUSTOM_PYTHON_PYBIND11_PATH
     OUTPUT_STRIP_TRAILING_WHITESPACE
     ERROR_QUIET)
 set(pybind11_DIR ${CUSTOM_PYTHON_PYBIND11_PATH})
```

### Comparing `aihwkit-0.7.1/cmake/dependencies_cuda.cmake` & `aihwkit-0.8.0/cmake/dependencies_cuda.cmake`

 * *Files 22% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -38,8 +38,13 @@
       include_directories(SYSTEM ${CUB_INCLUDE_DIR})
     endif(CUB_FOUND)
   endif()
 
   include_directories(SYSTEM ${CMAKE_CUDA_TOOLKIT_INCLUDE_DIRECTORIES})
 
   add_compile_definitions(RPU_USE_CUDA)
+  add_compile_definitions(__CUDA_NO_HALF_OPERATORS__)
+  add_compile_definitions(__CUDA_NO_HALF_CONVERSIONS__)
+  add_compile_definitions(__CUDA_NO_BFLOAT16_CONVERSIONS__)
+  add_compile_definitions(__CUDA_NO_HALF2_OPERATORS__)
+
 endif()
```

### Comparing `aihwkit-0.7.1/cmake/dependencies_test.cmake` & `aihwkit-0.8.0/cmake/dependencies_test.cmake`

 * *Files 13% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/setup.cfg` & `aihwkit-0.8.0/setup.cfg`

 * *Files 25% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 [pycodestyle]
 max-line-length = 100
 exclude = *_pb2.py
+ignore = E203, W503
 
 [pydocstyle]
 convention = google
 add_ignore = D105,D107,D205,D400,D415
 add_select = D204,D215,D401,D404
 match-dir = ^(?!helpers|definitions).*
 
@@ -25,12 +26,13 @@
 follow_imports_for_stubs = True
 
 [mypy-aihwkit.cloud.converter.definitions.*]
 ignore_errors = True
 
 [flake8]
 max-line-length = 100
+ignore = E203, W503
 
 [egg_info]
 tag_build = 
 tag_date = 0
```

### Comparing `aihwkit-0.7.1/setup.py` & `aihwkit-0.8.0/setup.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -14,75 +14,81 @@
 
 import os
 
 from setuptools import find_packages
 from skbuild import setup
 
 INSTALL_REQUIRES = [
-    'torch{}'.format(os.getenv('TORCH_VERSION_SPECIFIER', '>=1.9')),
-    'torchvision',
-    'scipy',
-    'requests>=2.25,<3',
-    'numpy>=1.19',
-    'protobuf>=4.21.6',
+    "torch{}".format(os.getenv("TORCH_VERSION_SPECIFIER", ">=1.9")),
+    "torchvision",
+    "scipy",
+    "requests>=2.25,<3",
+    "numpy>=1.22",
+    "protobuf>=4.21.6",
 ]
 
+
 def get_version() -> str:
     """Get the package version."""
-    version_path = os.path.join(
-        os.path.dirname(__file__), 'src', 'aihwkit', 'VERSION.txt')
-    with open(version_path, encoding='utf-8') as version_file:
+    version_path = os.path.join(os.path.dirname(__file__), "src", "aihwkit", "VERSION.txt")
+    with open(version_path, encoding="utf-8") as version_file:
         return version_file.read().strip()
 
 
 def get_long_description() -> str:
     """Get the package long description."""
-    readme_path = os.path.join(os.path.dirname(__file__), 'README.md')
-    with open(readme_path, encoding='utf-8') as readme_file:
+    readme_path = os.path.join(os.path.dirname(__file__), "README.md")
+    with open(readme_path, encoding="utf-8") as readme_file:
         return readme_file.read().strip()
 
 
 setup(
-    name='aihwkit',
+    name="aihwkit",
     version=get_version(),
-    description='IBM Analog Hardware Acceleration Kit',
+    description="IBM Analog Hardware Acceleration Kit",
     long_description=get_long_description(),
-    long_description_content_type='text/markdown',
-    url='https://github.com/IBM/aihwkit',
-    author='IBM Research',
-    author_email='aihwkit@us.ibm.com',
-    license='Apache 2.0',
+    long_description_content_type="text/markdown",
+    url="https://github.com/IBM/aihwkit",
+    author="IBM Research",
+    author_email="aihwkit@us.ibm.com",
+    license="Apache 2.0",
     classifiers=[
-        'Development Status :: 4 - Beta',
-        'Environment :: Console',
-        'Environment :: GPU :: NVIDIA CUDA',
-        'Intended Audience :: Science/Research',
-        'License :: OSI Approved :: Apache Software License',
-        'Operating System :: MacOS',
-        'Operating System :: Microsoft :: Windows',
-        'Operating System :: POSIX :: Linux',
-        'Programming Language :: Python :: 3 :: Only',
-        'Topic :: Scientific/Engineering',
-        'Topic :: Scientific/Engineering :: Artificial Intelligence',
-        'Typing :: Typed',
+        "Development Status :: 4 - Beta",
+        "Environment :: Console",
+        "Environment :: GPU :: NVIDIA CUDA",
+        "Intended Audience :: Science/Research",
+        "License :: OSI Approved :: Apache Software License",
+        "Operating System :: MacOS",
+        "Operating System :: Microsoft :: Windows",
+        "Operating System :: POSIX :: Linux",
+        "Programming Language :: Python :: 3 :: Only",
+        "Topic :: Scientific/Engineering",
+        "Topic :: Scientific/Engineering :: Artificial Intelligence",
+        "Typing :: Typed",
     ],
-    keywords=['ai', 'analog', 'rpu', 'torch'],
-    package_dir={'': 'src'},
-    packages=find_packages('src'),
-    package_data={
-        'aihwkit': ['VERSION.txt']
-    },
+    keywords=[
+        "ai",
+        "analog",
+        "rpu",
+        "torch",
+        "memristor",
+        "pcm",
+        "reram",
+        "crossbar",
+        "in-memory",
+        "nvm",
+        "non-von-neumann",
+        "non-volatile memory",
+        "phase-change material",
+    ],
+    package_dir={"": "src"},
+    packages=find_packages("src"),
+    package_data={"aihwkit": ["VERSION.txt"]},
     install_requires=INSTALL_REQUIRES,
-    python_requires='>=3.7',
+    python_requires=">=3.7",
     zip_safe=False,
     extras_require={
-        'visualization': ['matplotlib>=3.0'],
-        'fitting': ['lmfit'],
-        'bert': [
-            'transformers',
-            'evaluate',
-            'datasets',
-            'wandb',
-            'tensorboard',
-        ],
-    }
+        "visualization": ["matplotlib>=3.0"],
+        "fitting": ["lmfit"],
+        "bert": ["transformers", "evaluate", "datasets", "wandb", "tensorboard"],
+    },
 )
```

### Comparing `aihwkit-0.7.1/src/aihwkit/__init__.py` & `aihwkit-0.8.0/src/aihwkit/cloud/converter/exceptions.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,15 +1,19 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
 # that they have been altered from the originals.
 
-"""Analog hardware library for PyTorch."""
+"""Conversion-related Exceptions."""
 
-from .version import __version__
+from aihwkit.exceptions import AihwkitException
+
+
+class ConversionError(AihwkitException):
+    """Errors related to Experiment conversion."""
```

### Comparing `aihwkit-0.7.1/src/aihwkit/cloud/__init__.py` & `aihwkit-0.8.0/src/aihwkit/experiments/experiments/__init__.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
 # that they have been altered from the originals.
 
-"""Functionality related to the cloud client for AIHW Composer API."""
+"""Experiments for aihwkit."""
```

### Comparing `aihwkit-0.7.1/src/aihwkit/cloud/client/__init__.py` & `aihwkit-0.8.0/src/aihwkit/cloud/client/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/aihwkit/cloud/client/entities.py` & `aihwkit-0.8.0/src/aihwkit/cloud/client/entities.py`

 * *Files 2% similar despite different names*

```diff
@@ -14,31 +14,34 @@
 
 from dataclasses import dataclass, field
 from datetime import datetime
 from enum import Enum
 from typing import Any, Optional
 
 from aihwkit.cloud.client.exceptions import ExperimentStatusError
+
 # pylint: disable=no-name-in-module,import-error
 from aihwkit.cloud.converter.definitions.input_file_pb2 import (  # type: ignore[attr-defined]
-     TrainingInput
+    TrainingInput,
 )
 from aihwkit.cloud.converter.definitions.i_input_file_pb2 import (  # type: ignore[attr-defined]
-     InferenceInput
+    InferenceInput,
 )
 from aihwkit.cloud.converter.definitions.output_file_pb2 import (  # type: ignore[attr-defined]
-     TrainingOutput
+    TrainingOutput,
 )
 from aihwkit.cloud.converter.definitions.i_output_file_pb2 import (  # type: ignore[attr-defined]
-     InferencingOutput
+    InferencingOutput,
 )
 from aihwkit.cloud.converter.v1.training import BasicTrainingConverter, BasicTrainingResultConverter
 from aihwkit.cloud.converter.v1.inferencing import (
-     BasicInferencingConverter, BasicInferencingResultConverter
+    BasicInferencingConverter,
+    BasicInferencingResultConverter,
 )
+
 # from aihwkit.experiments import BasicTraining, BasicInferencing
 
 
 class CloudJobStatus(Enum):
     """Status for a CloudJob."""
 
     UNKNOWN = 0
@@ -82,19 +85,19 @@
         Returns:
             The experiment.
 
         Raises:
             ExperimentStatusError: if the Experiment is not in a valid status.
         """
         if self.status() == CloudJobStatus.UNKNOWN:
-            raise ExperimentStatusError('Experiment input is not available')
+            raise ExperimentStatusError("Experiment input is not available")
 
         input_ = self._api_client.input_get(self.input_id)
 
-        if 'InferenceRPUConfig' in str(input_):
+        if "InferenceRPUConfig" in str(input_):
             input_proto = InferenceInput()
             input_proto.ParseFromString(input_)
             proto = BasicInferencingConverter().from_proto(input_proto)
         else:
             input_proto = TrainingInput()
             input_proto.ParseFromString(input_)
             proto = BasicTrainingConverter().from_proto(input_proto)
@@ -107,26 +110,27 @@
         Returns:
             The experiment result.
 
         Raises:
             ExperimentStatusError: if the Experiment is not completed.
         """
         if self.status() != CloudJobStatus.COMPLETED:
-            raise ExperimentStatusError('Output cannot be retrieved unless the '
-                                        'experiment is completed')
+            raise ExperimentStatusError(
+                "Output cannot be retrieved unless the experiment is completed"
+            )
 
         if self.category == CloudExperimentCategory.BASIC_TRAINING:
             # Fetch the protobuf output.
             output_ = self._api_client.output_get(self.job.output_id)  # type: ignore
             # Convert from protobuf.
             training_output = TrainingOutput()
             training_output.ParseFromString(output_)
             converter = BasicTrainingResultConverter()
             output = converter.from_proto(training_output)
-            result = output['epochs']
+            result = output["epochs"]
         if self.category == CloudExperimentCategory.BASIC_INFERENCE:
             output_ = self._api_client.output_get(self.job.output_id)  # type: ignore
             # Convert from protobuf.
             inferencing_output = InferencingOutput()
             inferencing_output.ParseFromString(output_)
             iconverter = BasicInferencingResultConverter()
             i_output = iconverter.result_from_proto(inferencing_output)
```

### Comparing `aihwkit-0.7.1/src/aihwkit/cloud/client/exceptions.py` & `aihwkit-0.8.0/src/aihwkit/cloud/client/exceptions.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -25,19 +25,16 @@
     def __init__(self, response: Response):
         self.response = response
         self.url = self._sanitize_url(response.url)
 
         super().__init__(str(self))
 
     def __str__(self) -> str:
-        return '{} {} for url: {} {}'.format(
-            self.response.status_code,
-            self.response.reason,
-            self.response.request.method,
-            self.url
+        return "{} {} for url: {} {}".format(
+            self.response.status_code, self.response.reason, self.response.request.method, self.url
         )
 
     @staticmethod
     def _sanitize_url(url: str) -> str:
         """Remove sensitive parts from an url."""
         return url
 
@@ -46,15 +43,15 @@
     """Error retrieving a response (object storage)."""
 
     @staticmethod
     def _sanitize_url(url: str) -> str:
         """Remove sensitive parts from an url."""
         parts = urlparse(url)
 
-        return '{}{}'.format(parts.path, '?...' if parts.query else '')
+        return "{}{}".format(parts.path, "?..." if parts.query else "")
 
 
 class InvalidResponseFieldError(CloudError):
     """Invalid or unsupported field in the response."""
 
 
 class ExperimentStatusError(CloudError):
```

### Comparing `aihwkit-0.7.1/src/aihwkit/cloud/client/session.py` & `aihwkit-0.8.0/src/aihwkit/cloud/client/session.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -25,19 +25,15 @@
     from typing import Optional
 
 
 class ObjectStorageSession(Session):
     """Session handler for requests to object storage."""
 
     def request(  # type: ignore
-            self,
-            method: str,
-            url: Union[str, bytes, Text],
-            *args: Any,
-            **kwargs: Any
+        self, method: str, url: Union[str, bytes, Text], *args: Any, **kwargs: Any
     ) -> Any:
         """Construct a Request, prepares it and sends it.
 
         Args:
             method: method for the new ``Request`` object.
             url: URL for the new ``Request`` object.
             args: additional arguments for the original ``requests`` method.
@@ -68,43 +64,34 @@
     * authorization based on jwt token.
     * custom user agent for the requests.
 
     Additionally, this class stores information about the API URL and base
     token.
     """
 
-    def __init__(
-            self,
-            api_url: str,
-            api_token: str,
-            verify: bool = True
-    ):
+    def __init__(self, api_url: str, api_token: str, verify: bool = True):
         super().__init__()
 
         self.api_url = api_url
         self.api_token = api_token
         self.verify = verify
         if not verify:
             urllib3.disable_warnings(InsecureRequestWarning)  # type: ignore[no-untyped-call]
 
         self.jwt_token = None  # type: Optional[str]
 
-        self.headers.update({'User-Agent': 'aihwkit/{}'.format(__version__)})
+        self.headers.update({"User-Agent": "aihwkit/{}".format(__version__)})
 
     def update_jwt_token(self, jwt_token: str) -> None:
         """Set the jwt token for the session."""
         self.jwt_token = jwt_token
-        self.headers.update({'Authorization': 'Bearer {}'.format(jwt_token)})
+        self.headers.update({"Authorization": "Bearer {}".format(jwt_token)})
 
     def request(  # type: ignore
-            self,
-            method: str,
-            url: Union[str, bytes, Text],
-            *args: Any,
-            **kwargs: Any
+        self, method: str, url: Union[str, bytes, Text], *args: Any, **kwargs: Any
     ) -> Any:
         """Construct a Request, prepares it and sends it.
 
         Args:
             method: method for the new ``Request`` object.
             url: URL for the new ``Request`` object.
             args: additional arguments for the original ``requests`` method.
@@ -113,15 +100,15 @@
         Returns:
             A new ``Response`` object.
 
         Raises:
             ApiResponseError: if the response did not have a valid status code.
         """
         # pylint: disable=signature-differs
-        full_url = '{}/{}'.format(self.api_url, str(url))
+        full_url = "{}/{}".format(self.api_url, str(url))
 
         response = super().request(method, full_url, *args, **kwargs)
 
         try:
             response.raise_for_status()
         except HTTPError:
             raise ApiResponseError(response) from None
```

### Comparing `aihwkit-0.7.1/src/aihwkit/cloud/client/utils.py` & `aihwkit-0.8.0/src/aihwkit/cloud/client/utils.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -12,15 +12,15 @@
 
 """Utilities for the AIHW Composer API."""
 
 from os import getenv, path
 from typing import Dict
 from configparser import ConfigParser
 
-DEFAULT_URL = 'https://api-aihw-composer.draco.res.ibm.com'
+DEFAULT_URL = "https://api-aihw-composer.draco.res.ibm.com"
 
 
 class ClientConfiguration:
     """Helper for retrieving the user configuration.
 
     Utility for retrieving the user configuration. The API token will be
     retrieved from, in order of preference:
@@ -47,27 +47,31 @@
         """Read the configuration from a config file.
 
         Returns:
             A dictionary with the contents of the ``aihwkit.conf``
             configuration file.
         """
         parser = ConfigParser()
-        parser.read(['aihwkit.conf',
-                     path.expanduser('{}/aihwkit/aihwkit.conf'.format(
-                         getenv('XDG_CONFIG_HOME', '~/.config')))])
+        parser.read(
+            [
+                "aihwkit.conf",
+                path.expanduser(
+                    "{}/aihwkit/aihwkit.conf".format(getenv("XDG_CONFIG_HOME", "~/.config"))
+                ),
+            ]
+        )
 
         # Check that the expected section is present.
-        if 'cloud' in parser:
-            return dict(parser['cloud'])
+        if "cloud" in parser:
+            return dict(parser["cloud"])
 
         return {}
 
     @property
     def token(self) -> str:
         """Return the user token."""
-        return getenv('AIHW_API_TOKEN',
-                      self.stored_config.get('api_token', None))
+        return getenv("AIHW_API_TOKEN", self.stored_config.get("api_token", None))
 
     @property
     def url(self) -> str:
         """Return the API URL."""
-        return self.stored_config.get('api_url', DEFAULT_URL)
+        return self.stored_config.get("api_url", DEFAULT_URL)
```

### Comparing `aihwkit-0.7.1/src/aihwkit/cloud/client/v1/__init__.py` & `aihwkit-0.8.0/src/aihwkit/cloud/client/v1/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/aihwkit/cloud/client/v1/api_client.py` & `aihwkit-0.8.0/src/aihwkit/cloud/client/v1/api_client.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -14,17 +14,15 @@
 
 from typing import List
 
 from aihwkit.cloud.client.entities import CloudExperiment, CloudJob
 from aihwkit.cloud.client.exceptions import ApiResponseError, CredentialsError
 from aihwkit.cloud.client.session import ObjectStorageSession, ApiSession
 from aihwkit.cloud.client.v1.parsers import ExperimentParser, GeneralParser
-from aihwkit.cloud.client.v1.stubs import (
-    ExperimentStub, InputStub, JobStub, LoginStub, OutputStub
-)
+from aihwkit.cloud.client.v1.stubs import ExperimentStub, InputStub, JobStub, LoginStub, OutputStub
 from aihwkit.cloud.converter.v1.training import BasicTrainingConverter
 from aihwkit.experiments.experiments.training import BasicTraining
 
 
 class ApiClient:
     """API client the AIHW Composer API.
 
@@ -62,41 +60,39 @@
         """Login into the application.
 
         Raises:
             CredentialsError: if the credentials are not valid.
             ApiResponseError: if the request was not successful.
         """
         try:
-            response = self.login_.post({'token': self.session.api_token})
+            response = self.login_.post({"token": self.session.api_token})
         except ApiResponseError as ex:
             if ex.response.status_code == 400:
                 try:
                     json_response = ex.response.json()
                 except Exception:  # pylint: disable=broad-except
                     json_response = {}
-                raise CredentialsError('Error while trying to log in: {}'.format(
-                    json_response.get('message', 'unknown')
-                )) from ex
+                raise CredentialsError(
+                    "Error while trying to log in: {}".format(
+                        json_response.get("message", "unknown")
+                    )
+                ) from ex
             raise
 
         jwt_token = GeneralParser.parse_login(response)
         self.session.update_jwt_token(jwt_token)
 
     def experiments_list(self) -> List[CloudExperiment]:
         """Return a list of experiments."""
         response = self.experiments.list()
 
-        return [ExperimentParser.parse_experiment(experiment, self)
-                for experiment in response]
+        return [ExperimentParser.parse_experiment(experiment, self) for experiment in response]
 
     def experiment_create(
-            self,
-            input_: BasicTraining,
-            name: str,
-            device: str = 'gpu'
+        self, input_: BasicTraining, name: str, device: str = "gpu"
     ) -> CloudExperiment:
         """Create a new experiment, queuing its execution.
 
         Args:
             input_: the experiment to be executed.
             name: the name of the experiment.
             device: the desired device.
@@ -106,28 +102,26 @@
         """
         # Prepare the API data.
         # debug: print('input_: ', input_)
         payload = self.converter.to_proto(input_).SerializeToString()
         # debug: print('payload after convert to proto and serialize to string: \n', payload)
 
         # Create the experiment.
-        response = self.experiments.post({'name': name,
-                                          'category': 'train'})
+        response = self.experiments.post({"name": name, "category": "train"})
         # debug: print('response from experiments.post: ', response)
         experiment = ExperimentParser.parse_experiment(response, self)
 
         # Create the input.
-        response = self.inputs.post({'experiment': experiment.id_,
-                                     'device': device})
-        object_storage_url = response['url']
+        response = self.inputs.post({"experiment": experiment.id_, "device": device})
+        object_storage_url = response["url"]
         # debug: print ('url: \n', object_storage_url)
         _ = self.object_storage_session.put(url=object_storage_url, data=payload)
 
         # Create the job.
-        response = self.jobs.post({'device': device, 'experiment': experiment.id_})
+        response = self.jobs.post({"device": device, "experiment": experiment.id_})
         # debug: print('response from jobs.post: ', response)
         experiment.job = ExperimentParser.parse_job(response)
         # debug: print('In experiment_create: experiment.job: \n', experiment.job)
 
         return experiment
 
     def experiment_get(self, experiment_id: str) -> CloudExperiment:
@@ -162,15 +156,15 @@
         Args:
             input_id: id of the input.
 
         Returns:
             The input with the specified id, in protobuf format.
         """
         response = self.inputs.get(input_id)
-        object_storage_url = response['url']
+        object_storage_url = response["url"]
 
         object_storage_response = self.object_storage_session.get(object_storage_url)
 
         return object_storage_response.content
 
     def output_get(self, output_id: str) -> bytes:
         """Get an existing output by id.
@@ -178,12 +172,12 @@
         Args:
             output_id: id of the output.
 
         Returns:
             The output with the specified id, in protobuf format.
         """
         response = self.outputs.get(output_id)
-        object_storage_url = response['url']
+        object_storage_url = response["url"]
 
         object_storage_response = self.object_storage_session.get(object_storage_url)
 
         return object_storage_response.content
```

### Comparing `aihwkit-0.7.1/src/aihwkit/cloud/client/v1/i_api_client.py` & `aihwkit-0.8.0/src/aihwkit/cloud/client/v1/i_api_client.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -14,17 +14,15 @@
 
 from typing import List, Dict
 
 from aihwkit.cloud.client.entities import CloudExperiment, CloudJob
 from aihwkit.cloud.client.exceptions import ApiResponseError, CredentialsError
 from aihwkit.cloud.client.session import ObjectStorageSession, ApiSession
 from aihwkit.cloud.client.v1.parsers import ExperimentParser, GeneralParser
-from aihwkit.cloud.client.v1.stubs import (
-    ExperimentStub, InputStub, JobStub, LoginStub, OutputStub
-)
+from aihwkit.cloud.client.v1.stubs import ExperimentStub, InputStub, JobStub, LoginStub, OutputStub
 from aihwkit.cloud.converter.v1.inferencing import BasicInferencingConverter
 from aihwkit.experiments.experiments.inferencing import BasicInferencing
 
 
 class InferenceApiClient:
     """API client the AIHW Composer API.
 
@@ -62,43 +60,44 @@
         """Login into the application.
 
         Raises:
             CredentialsError: if the credentials are not valid.
             ApiResponseError: if the request was not successful.
         """
         try:
-            response = self.login_.post({'token': self.session.api_token})
+            response = self.login_.post({"token": self.session.api_token})
         except ApiResponseError as ex:
             if ex.response.status_code == 400:
                 try:
                     json_response = ex.response.json()
                 except Exception:  # pylint: disable=broad-except
                     json_response = {}
-                raise CredentialsError('Error while trying to log in: {}'.format(
-                    json_response.get('message', 'unknown')
-                )) from ex
+                raise CredentialsError(
+                    "Error while trying to log in: {}".format(
+                        json_response.get("message", "unknown")
+                    )
+                ) from ex
             raise
 
         jwt_token = GeneralParser.parse_login(response)
         self.session.update_jwt_token(jwt_token)
 
     def experiments_list(self) -> List[CloudExperiment]:
         """Return a list of experiments."""
         response = self.experiments.list()
 
-        return [ExperimentParser.parse_experiment(experiment, self)
-                for experiment in response]
+        return [ExperimentParser.parse_experiment(experiment, self) for experiment in response]
 
     def experiment_create(
-            self,
-            input_: BasicInferencing,
-            analog_info: Dict,
-            noise_model_info: Dict,
-            name: str,
-            device: str = 'gpu'
+        self,
+        input_: BasicInferencing,
+        analog_info: Dict,
+        noise_model_info: Dict,
+        name: str,
+        device: str = "gpu",
     ) -> CloudExperiment:
         """Create a new experiment, queuing its execution.
 
         Args:
             input_: the experiment to be executed.
             analog_info: analog information.
             noise_model_info: noise information.
@@ -111,28 +110,26 @@
         # Prepare the API data.
         # debug: print('input_: ', input_)
         payload = self.converter.to_proto(input_, analog_info, noise_model_info).SerializeToString()
         # payload = self.converter.to_proto(input_)
         # debug: print('payload after convert to proto and serialize to string: \n', payload)
 
         # Create the experiment.
-        response = self.experiments.post({'name': name,
-                                          'category': 'inference'})
+        response = self.experiments.post({"name": name, "category": "inference"})
         # debug: print('response from experiments.post: ', response)
         experiment = ExperimentParser.parse_experiment(response, self)
 
         # Create the input.
-        response = self.inputs.post({'experiment': experiment.id_,
-                                     'device': device})
-        object_storage_url = response['url']
+        response = self.inputs.post({"experiment": experiment.id_, "device": device})
+        object_storage_url = response["url"]
         # debug: print ('url: \n', object_storage_url)
         _ = self.object_storage_session.put(url=object_storage_url, data=payload)
 
         # Create the job.
-        response = self.jobs.post({'device': device, 'experiment': experiment.id_})
+        response = self.jobs.post({"device": device, "experiment": experiment.id_})
         # debug: print('response from jobs.post: ', response)
         experiment.job = ExperimentParser.parse_job(response)
         # debug: print('In experiment_create: experiment.job: \n', experiment.job)
 
         return experiment
 
     def experiment_get(self, experiment_id: str) -> CloudExperiment:
@@ -167,15 +164,15 @@
         Args:
             input_id: id of the input.
 
         Returns:
             The input with the specified id, in protobuf format.
         """
         response = self.inputs.get(input_id)
-        object_storage_url = response['url']
+        object_storage_url = response["url"]
 
         object_storage_response = self.object_storage_session.get(object_storage_url)
 
         return object_storage_response.content
 
     def output_get(self, output_id: str) -> bytes:
         """Get an existing output by id.
@@ -183,12 +180,12 @@
         Args:
             output_id: id of the output.
 
         Returns:
             The output with the specified id, in protobuf format.
         """
         response = self.outputs.get(output_id)
-        object_storage_url = response['url']
+        object_storage_url = response["url"]
 
         object_storage_response = self.object_storage_session.get(object_storage_url)
 
         return object_storage_response.content
```

### Comparing `aihwkit-0.7.1/src/aihwkit/cloud/client/v1/parsers.py` & `aihwkit-0.8.0/src/aihwkit/cloud/client/v1/parsers.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -12,15 +12,18 @@
 
 """Parsers for the AIHW Composer API."""
 
 from datetime import datetime, timezone
 from typing import Any, Dict
 
 from aihwkit.cloud.client.entities import (
-    CloudExperiment, CloudExperimentCategory, CloudJobStatus, CloudJob
+    CloudExperiment,
+    CloudExperimentCategory,
+    CloudJobStatus,
+    CloudJob,
 )
 from aihwkit.cloud.client.exceptions import InvalidResponseFieldError
 
 
 class ExperimentParser:
     """Parser for Experiment API responses."""
 
@@ -34,31 +37,31 @@
 
         Returns:
             A `CloudExperiment` based on the response. Some of the fields might
             not be populated if they are not present in the response.
         """
         experiment = CloudExperiment(
             _api_client=api_client,
-            id_=api_response['id'],
-            name=api_response['name'],
+            id_=api_response["id"],
+            name=api_response["name"],
             category=ExperimentParser.parse_experiment_category(api_response),
-            created_at=ExperimentParser.parse_date_string(api_response['createdAt']),
+            created_at=ExperimentParser.parse_date_string(api_response["createdAt"]),
             input_id=None,
-            job=None
+            job=None,
         )
 
         # debug
         # print('api_response: ', api_response)
 
-        if api_response.get('input'):
-            if api_response.get('input', {}).get('id'):
-                experiment.input_id = api_response['input']['id']
+        if api_response.get("input"):
+            if api_response.get("input", {}).get("id"):
+                experiment.input_id = api_response["input"]["id"]
 
-        if api_response.get('job', None):
-            experiment.job = ExperimentParser.parse_job(api_response['job'])
+        if api_response.get("job", None):
+            experiment.job = ExperimentParser.parse_job(api_response["job"])
 
         return experiment
 
     @staticmethod
     def parse_job(api_response: Dict) -> CloudJob:
         """Return an CloudJob from an API response.
 
@@ -66,21 +69,21 @@
             api_response: the response from the API.
 
         Returns:
             A `CloudJob` based on the response. Some of the fields might
             not be populated if they are not present in the response.
         """
         job = CloudJob(
-            id_=api_response['id'],
+            id_=api_response["id"],
             output_id=None,
-            status=ExperimentParser.parse_experiment_status(api_response)
+            status=ExperimentParser.parse_experiment_status(api_response),
         )
 
-        if api_response.get('output', None):
-            job.output_id = api_response['output']
+        if api_response.get("output", None):
+            job.output_id = api_response["output"]
 
         return job
 
     @staticmethod
     def parse_experiment_status(api_response: Dict) -> CloudJobStatus:
         """Return an Experiment status from an API response.
 
@@ -90,26 +93,26 @@
         Returns:
             A value from the `CloudJobStatus` enum.
 
         Raises:
             InvalidResponseFieldError: if the API response contains an
                 unrecognized status code.
         """
-        job_status = api_response['status']
+        job_status = api_response["status"]
 
-        if job_status in ('waiting', 'validating', 'validated'):
+        if job_status in ("waiting", "validating", "validated"):
             return CloudJobStatus.WAITING
-        if job_status in ('running', ):
+        if job_status in ("running",):
             return CloudJobStatus.RUNNING
-        if job_status in ('failed', 'cancelled'):
+        if job_status in ("failed", "cancelled"):
             return CloudJobStatus.FAILED
-        if job_status in ('completed',):
+        if job_status in ("completed",):
             return CloudJobStatus.COMPLETED
 
-        raise InvalidResponseFieldError('Unsupported job status: {}'.format(job_status))
+        raise InvalidResponseFieldError("Unsupported job status: {}".format(job_status))
 
     @staticmethod
     def parse_experiment_category(api_response: Dict) -> CloudExperimentCategory:
         """Return an Experiment category from an API response.
 
         Args:
             api_response: the response from the API.
@@ -117,48 +120,48 @@
         Returns:
             A value from the `CloudExperimentCategory` enum.
 
         Raises:
             InvalidResponseFieldError: if the API response contains an
                 unrecognized category.
         """
-        job_category = api_response['category']
+        job_category = api_response["category"]
 
-        if job_category in ('train', 'trainweb'):
+        if job_category in ("train", "trainweb"):
             return CloudExperimentCategory.BASIC_TRAINING
 
-        if job_category in ('inference', 'inferenceweb'):
+        if job_category in ("inference", "inferenceweb"):
             return CloudExperimentCategory.BASIC_INFERENCE
 
-        raise InvalidResponseFieldError(
-            'Unsupported experiment category: {}'.format(job_category))
+        raise InvalidResponseFieldError("Unsupported experiment category: {}".format(job_category))
 
     @staticmethod
     def parse_date_string(date_string: str) -> datetime:
         """Return a datetime from a date string.
 
         Args:
             date_string: the date string from the API.
 
         Returns:
             A value from the `CloudExperimentCategory` enum.
         """
-        tmp_datetime = datetime.strptime(date_string, '%Y-%m-%dT%H:%M:%S.%fZ')
+        tmp_datetime = datetime.strptime(date_string, "%Y-%m-%dT%H:%M:%S.%fZ")
 
         return tmp_datetime.replace(tzinfo=timezone.utc)
 
 
 class GeneralParser:
     """Parser for generic responses."""
+
     # pylint: disable=too-few-public-methods
 
     @staticmethod
     def parse_login(api_response: Dict) -> str:
         """Return the jwt token from an API response.
 
         Args:
             api_response: the response from the API.
 
         Returns:
             A string with the jwt token.
         """
-        return api_response['jwt']
+        return api_response["jwt"]
```

### Comparing `aihwkit-0.7.1/src/aihwkit/cloud/client/v1/stubs.py` & `aihwkit-0.8.0/src/aihwkit/cloud/client/v1/stubs.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -14,15 +14,15 @@
 
 from collections import namedtuple
 from typing import Dict
 
 from aihwkit.cloud.client.session import ApiSession
 
 
-Endpoint = namedtuple('Endpoint', ['url', 'method'])
+Endpoint = namedtuple("Endpoint", ["url", "method"])
 
 
 class ApiStub:
     """Base API stub for the AIHW Composer.
 
     API stub for use in the client for the AIHW Composer in order to interact
     with REST endpoints.
@@ -34,15 +34,15 @@
         * ``get`` (via ``self.get()``): a ``GET`` operation returning a single
           object from an id.
         * ``post`` (via ``self.post()``): a ``POST`` operation.
         * ``list`` (via ``self.list()``): a ``GET`` operation returning
           multiple objects.
     """
 
-    base_url = ''
+    base_url = ""
     """Base url to be used in the endpoints."""
 
     def __init__(self, session: ApiSession):
         """Create a new ``ApiStub``.
 
         Args:
             session: the requests session to be used.
@@ -53,120 +53,118 @@
     def _endpoint_map(self) -> Dict[str, Endpoint]:
         """Generate the mappings to the endpoints.
 
         Returns:
             A dictionary of strings to endpoints.
         """
         return {
-            'list': Endpoint(self.base_url, 'GET'),
-            'get': Endpoint('{}/{{}}'.format(self.base_url), 'GET'),
-            'post': Endpoint(self.base_url, 'POST')
+            "list": Endpoint(self.base_url, "GET"),
+            "get": Endpoint("{}/{{}}".format(self.base_url), "GET"),
+            "post": Endpoint(self.base_url, "POST"),
         }
 
     def list(self) -> Dict:
         """Return a list of entities.
 
         Returns:
             A list of entities.
         """
-        endpoint = self.endpoints['list']
+        endpoint = self.endpoints["list"]
 
-        return self.session.request(method=endpoint.method,
-                                    url=endpoint.url).json()
+        return self.session.request(method=endpoint.method, url=endpoint.url).json()
 
     def get(self, object_id: str) -> Dict:
         """Return a single entity by id.
 
         Args:
             object_id: the id of the entity.
 
         Returns:
             A dictionary with the entity.
         """
-        endpoint = self.endpoints['get']
+        endpoint = self.endpoints["get"]
 
-        return self.session.request(method=endpoint.method,
-                                    url=endpoint.url.format(object_id)).json()
+        return self.session.request(
+            method=endpoint.method, url=endpoint.url.format(object_id)
+        ).json()
 
     def post(self, content: Dict) -> Dict:
         """Create a single entity.
 
         Args:
             content: the content of the entity.
 
         Returns:
             A dictionary with the API response.
         """
-        endpoint = self.endpoints['post']
+        endpoint = self.endpoints["post"]
 
-        return self.session.request(method=endpoint.method,
-                                    url=endpoint.url,
-                                    json=content).json()
+        return self.session.request(method=endpoint.method, url=endpoint.url, json=content).json()
 
 
 class ExperimentStub(ApiStub):
     """Stub for ``experiment``."""
 
-    base_url = 'experiments'
+    base_url = "experiments"
 
     def _endpoint_map(self) -> Dict[str, Endpoint]:
         """Generate the mappings to the endpoints.
 
         Returns:
             A dictionary of strings to endpoints.
         """
         ret = super()._endpoint_map()
 
         # Use a different url for listing.
-        ret['list'] = Endpoint('{}/me'.format(self.base_url), 'GET')
+        ret["list"] = Endpoint("{}/me".format(self.base_url), "GET")
 
         return ret
 
 
 class InputStub(ApiStub):
     """Stub for ``input``."""
 
-    base_url = 'inputs'
+    base_url = "inputs"
 
     def _endpoint_map(self) -> Dict[str, Endpoint]:
         """Generate the mappings to the endpoints.
 
         Returns:
             A dictionary of strings to endpoints.
         """
         ret = super()._endpoint_map()
 
         # Use a different url for getter.
-        ret['get'] = Endpoint('{}/{{}}/file'.format(self.base_url), 'GET')
+        ret["get"] = Endpoint("{}/{{}}/file".format(self.base_url), "GET")
 
         return ret
 
 
 class OutputStub(ApiStub):
     """Stub for ``output``."""
 
-    base_url = 'outputs'
+    base_url = "outputs"
 
     def _endpoint_map(self) -> Dict[str, Endpoint]:
         """Generate the mappings to the endpoints.
 
         Returns:
             A dictionary of strings to endpoints.
         """
         ret = super()._endpoint_map()
 
         # Use a different url for getter.
-        ret['get'] = Endpoint('{}/{{}}/file'.format(self.base_url), 'GET')
+        ret["get"] = Endpoint("{}/{{}}/file".format(self.base_url), "GET")
 
         return ret
 
 
 class JobStub(ApiStub):
     """Stub for ``job``."""
 
-    base_url = 'jobs'
+    base_url = "jobs"
 
 
 class LoginStub(ApiStub):
     """Stub for ``login``."""
 
-    base_url = 'token/login'
+    base_url = "token/login"
```

### Comparing `aihwkit-0.7.1/src/aihwkit/cloud/converter/__init__.py` & `aihwkit-0.8.0/src/aihwkit/cloud/converter/definitions/__init__.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
 # that they have been altered from the originals.
 
-"""Conversion utilities for interacting with the AIHW Composer API."""
+"""Protobuf definitions for the AIHW Composer API."""
```

### Comparing `aihwkit-0.7.1/src/aihwkit/cloud/converter/definitions/__init__.py` & `aihwkit-0.8.0/src/aihwkit/cloud/converter/v1/__init__.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
 # that they have been altered from the originals.
 
-"""Protobuf definitions for the AIHW Composer API."""
+"""Helpers for version 1 of the AIHW Composer format."""
```

### Comparing `aihwkit-0.7.1/src/aihwkit/cloud/converter/definitions/common_pb2.py` & `aihwkit-0.8.0/src/aihwkit/cloud/converter/definitions/common_pb2.py`

 * *Files 6% similar despite different names*

```diff
@@ -2,43 +2,45 @@
 # Generated by the protocol buffer compiler.  DO NOT EDIT!
 # source: common.proto
 """Generated protocol buffer code."""
 from google.protobuf.internal import builder as _builder
 from google.protobuf import descriptor as _descriptor
 from google.protobuf import descriptor_pool as _descriptor_pool
 from google.protobuf import symbol_database as _symbol_database
+
 # @@protoc_insertion_point(imports)
 
 _sym_db = _symbol_database.Default()
 
 
 from . import onnx_common_pb2 as onnx__common__pb2
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x0c\x63ommon.proto\x12\x05\x61ihwx\x1a\x11onnx_common.proto\"F\n\x0ePrimitiveProto\x12\n\n\x02id\x18\x01 \x02(\t\x12(\n\targuments\x18\x02 \x03(\x0b\x32\x15.aihwx.AttributeProto\"F\n\x0eTransformProto\x12\n\n\x02id\x18\x01 \x02(\t\x12(\n\targuments\x18\x02 \x03(\x0b\x32\x15.aihwx.AttributeProto\"O\n\x17\x41\x63tivationFunctionProto\x12\n\n\x02id\x18\x01 \x02(\t\x12(\n\targuments\x18\x02 \x03(\x0b\x32\x15.aihwx.AttributeProto\"m\n\nLayerProto\x12\n\n\x02id\x18\x01 \x02(\t\x12(\n\targuments\x18\x02 \x03(\x0b\x32\x15.aihwx.AttributeProto\x12)\n\nstate_dict\x18\x03 \x03(\x0b\x32\x15.aihwx.AttributeProto\"\x86\x01\n\x19LayerOrActivationFunction\x12=\n\x13\x61\x63tivation_function\x18\x01 \x01(\x0b\x32\x1e.aihwx.ActivationFunctionProtoH\x00\x12\"\n\x05layer\x18\x02 \x01(\x0b\x32\x11.aihwx.LayerProtoH\x00\x42\x06\n\x04item\"F\n\x0eOptimizerProto\x12\n\n\x02id\x18\x01 \x02(\t\x12(\n\targuments\x18\x02 \x03(\x0b\x32\x15.aihwx.AttributeProto\"F\n\x0eSchedulerProto\x12\n\n\x02id\x18\x01 \x02(\t\x12(\n\targuments\x18\x02 \x03(\x0b\x32\x15.aihwx.AttributeProto\"I\n\x11LossFunctionProto\x12\n\n\x02id\x18\x01 \x02(\t\x12(\n\targuments\x18\x02 \x03(\x0b\x32\x15.aihwx.AttributeProto\";\n\x07Network\x12\x30\n\x06layers\x18\x01 \x03(\x0b\x32 .aihwx.LayerOrActivationFunction\"(\n\x07Version\x12\x0e\n\x06schema\x18\x01 \x02(\x03\x12\r\n\x05opset\x18\x02 \x02(\x03')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(
+    b'\n\x0c\x63ommon.proto\x12\x05\x61ihwx\x1a\x11onnx_common.proto"F\n\x0ePrimitiveProto\x12\n\n\x02id\x18\x01 \x02(\t\x12(\n\targuments\x18\x02 \x03(\x0b\x32\x15.aihwx.AttributeProto"F\n\x0eTransformProto\x12\n\n\x02id\x18\x01 \x02(\t\x12(\n\targuments\x18\x02 \x03(\x0b\x32\x15.aihwx.AttributeProto"O\n\x17\x41\x63tivationFunctionProto\x12\n\n\x02id\x18\x01 \x02(\t\x12(\n\targuments\x18\x02 \x03(\x0b\x32\x15.aihwx.AttributeProto"m\n\nLayerProto\x12\n\n\x02id\x18\x01 \x02(\t\x12(\n\targuments\x18\x02 \x03(\x0b\x32\x15.aihwx.AttributeProto\x12)\n\nstate_dict\x18\x03 \x03(\x0b\x32\x15.aihwx.AttributeProto"\x86\x01\n\x19LayerOrActivationFunction\x12=\n\x13\x61\x63tivation_function\x18\x01 \x01(\x0b\x32\x1e.aihwx.ActivationFunctionProtoH\x00\x12"\n\x05layer\x18\x02 \x01(\x0b\x32\x11.aihwx.LayerProtoH\x00\x42\x06\n\x04item"F\n\x0eOptimizerProto\x12\n\n\x02id\x18\x01 \x02(\t\x12(\n\targuments\x18\x02 \x03(\x0b\x32\x15.aihwx.AttributeProto"F\n\x0eSchedulerProto\x12\n\n\x02id\x18\x01 \x02(\t\x12(\n\targuments\x18\x02 \x03(\x0b\x32\x15.aihwx.AttributeProto"I\n\x11LossFunctionProto\x12\n\n\x02id\x18\x01 \x02(\t\x12(\n\targuments\x18\x02 \x03(\x0b\x32\x15.aihwx.AttributeProto";\n\x07Network\x12\x30\n\x06layers\x18\x01 \x03(\x0b\x32 .aihwx.LayerOrActivationFunction"(\n\x07Version\x12\x0e\n\x06schema\x18\x01 \x02(\x03\x12\r\n\x05opset\x18\x02 \x02(\x03'
+)
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
-_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'common_pb2', globals())
+_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, "common_pb2", globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
-
-  DESCRIPTOR._options = None
-  _PRIMITIVEPROTO._serialized_start=42
-  _PRIMITIVEPROTO._serialized_end=112
-  _TRANSFORMPROTO._serialized_start=114
-  _TRANSFORMPROTO._serialized_end=184
-  _ACTIVATIONFUNCTIONPROTO._serialized_start=186
-  _ACTIVATIONFUNCTIONPROTO._serialized_end=265
-  _LAYERPROTO._serialized_start=267
-  _LAYERPROTO._serialized_end=376
-  _LAYERORACTIVATIONFUNCTION._serialized_start=379
-  _LAYERORACTIVATIONFUNCTION._serialized_end=513
-  _OPTIMIZERPROTO._serialized_start=515
-  _OPTIMIZERPROTO._serialized_end=585
-  _SCHEDULERPROTO._serialized_start=587
-  _SCHEDULERPROTO._serialized_end=657
-  _LOSSFUNCTIONPROTO._serialized_start=659
-  _LOSSFUNCTIONPROTO._serialized_end=732
-  _NETWORK._serialized_start=734
-  _NETWORK._serialized_end=793
-  _VERSION._serialized_start=795
-  _VERSION._serialized_end=835
+    DESCRIPTOR._options = None
+    _PRIMITIVEPROTO._serialized_start = 42
+    _PRIMITIVEPROTO._serialized_end = 112
+    _TRANSFORMPROTO._serialized_start = 114
+    _TRANSFORMPROTO._serialized_end = 184
+    _ACTIVATIONFUNCTIONPROTO._serialized_start = 186
+    _ACTIVATIONFUNCTIONPROTO._serialized_end = 265
+    _LAYERPROTO._serialized_start = 267
+    _LAYERPROTO._serialized_end = 376
+    _LAYERORACTIVATIONFUNCTION._serialized_start = 379
+    _LAYERORACTIVATIONFUNCTION._serialized_end = 513
+    _OPTIMIZERPROTO._serialized_start = 515
+    _OPTIMIZERPROTO._serialized_end = 585
+    _SCHEDULERPROTO._serialized_start = 587
+    _SCHEDULERPROTO._serialized_end = 657
+    _LOSSFUNCTIONPROTO._serialized_start = 659
+    _LOSSFUNCTIONPROTO._serialized_end = 732
+    _NETWORK._serialized_start = 734
+    _NETWORK._serialized_end = 793
+    _VERSION._serialized_start = 795
+    _VERSION._serialized_end = 835
 # @@protoc_insertion_point(module_scope)
```

### Comparing `aihwkit-0.7.1/src/aihwkit/cloud/converter/definitions/i_common_pb2.py` & `aihwkit-0.8.0/src/aihwkit/cloud/converter/definitions/i_common_pb2.py`

 * *Files 10% similar despite different names*

```diff
@@ -2,43 +2,45 @@
 # Generated by the protocol buffer compiler.  DO NOT EDIT!
 # source: i_common.proto
 """Generated protocol buffer code."""
 from google.protobuf.internal import builder as _builder
 from google.protobuf import descriptor as _descriptor
 from google.protobuf import descriptor_pool as _descriptor_pool
 from google.protobuf import symbol_database as _symbol_database
+
 # @@protoc_insertion_point(imports)
 
 _sym_db = _symbol_database.Default()
 
 
 from . import i_onnx_common_pb2 as i__onnx__common__pb2
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x0ei_common.proto\x12\x11\x61ihwx.inferencing\x1a\x13i_onnx_common.proto\"R\n\x0eOptimizerProto\x12\n\n\x02id\x18\x01 \x02(\t\x12\x34\n\targuments\x18\x02 \x03(\x0b\x32!.aihwx.inferencing.AttributeProto\"R\n\x0ePrimitiveProto\x12\n\n\x02id\x18\x01 \x02(\t\x12\x34\n\targuments\x18\x02 \x03(\x0b\x32!.aihwx.inferencing.AttributeProto\"R\n\x0eTransformProto\x12\n\n\x02id\x18\x01 \x02(\t\x12\x34\n\targuments\x18\x02 \x03(\x0b\x32!.aihwx.inferencing.AttributeProto\"\x85\x01\n\nLayerProto\x12\n\n\x02id\x18\x01 \x02(\t\x12\x34\n\targuments\x18\x02 \x03(\x0b\x32!.aihwx.inferencing.AttributeProto\x12\x35\n\nstate_dict\x18\x03 \x03(\x0b\x32!.aihwx.inferencing.AttributeProto\"[\n\x17\x41\x63tivationFunctionProto\x12\n\n\x02id\x18\x01 \x02(\t\x12\x34\n\targuments\x18\x02 \x03(\x0b\x32!.aihwx.inferencing.AttributeProto\"\x9e\x01\n\x19LayerOrActivationFunction\x12I\n\x13\x61\x63tivation_function\x18\x01 \x01(\x0b\x32*.aihwx.inferencing.ActivationFunctionProtoH\x00\x12.\n\x05layer\x18\x02 \x01(\x0b\x32\x1d.aihwx.inferencing.LayerProtoH\x00\x42\x06\n\x04item\"R\n\x0eSchedulerProto\x12\n\n\x02id\x18\x01 \x02(\t\x12\x34\n\targuments\x18\x02 \x03(\x0b\x32!.aihwx.inferencing.AttributeProto\"U\n\x11LossFunctionProto\x12\n\n\x02id\x18\x01 \x02(\t\x12\x34\n\targuments\x18\x02 \x03(\x0b\x32!.aihwx.inferencing.AttributeProto\"c\n\x07Network\x12\x1a\n\x12weight_template_id\x18\x01 \x02(\t\x12<\n\x06layers\x18\x02 \x03(\x0b\x32,.aihwx.inferencing.LayerOrActivationFunction\"(\n\x07Version\x12\x0e\n\x06schema\x18\x01 \x02(\x03\x12\r\n\x05opset\x18\x02 \x02(\x03')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(
+    b'\n\x0ei_common.proto\x12\x11\x61ihwx.inferencing\x1a\x13i_onnx_common.proto"R\n\x0eOptimizerProto\x12\n\n\x02id\x18\x01 \x02(\t\x12\x34\n\targuments\x18\x02 \x03(\x0b\x32!.aihwx.inferencing.AttributeProto"R\n\x0ePrimitiveProto\x12\n\n\x02id\x18\x01 \x02(\t\x12\x34\n\targuments\x18\x02 \x03(\x0b\x32!.aihwx.inferencing.AttributeProto"R\n\x0eTransformProto\x12\n\n\x02id\x18\x01 \x02(\t\x12\x34\n\targuments\x18\x02 \x03(\x0b\x32!.aihwx.inferencing.AttributeProto"\x85\x01\n\nLayerProto\x12\n\n\x02id\x18\x01 \x02(\t\x12\x34\n\targuments\x18\x02 \x03(\x0b\x32!.aihwx.inferencing.AttributeProto\x12\x35\n\nstate_dict\x18\x03 \x03(\x0b\x32!.aihwx.inferencing.AttributeProto"[\n\x17\x41\x63tivationFunctionProto\x12\n\n\x02id\x18\x01 \x02(\t\x12\x34\n\targuments\x18\x02 \x03(\x0b\x32!.aihwx.inferencing.AttributeProto"\x9e\x01\n\x19LayerOrActivationFunction\x12I\n\x13\x61\x63tivation_function\x18\x01 \x01(\x0b\x32*.aihwx.inferencing.ActivationFunctionProtoH\x00\x12.\n\x05layer\x18\x02 \x01(\x0b\x32\x1d.aihwx.inferencing.LayerProtoH\x00\x42\x06\n\x04item"R\n\x0eSchedulerProto\x12\n\n\x02id\x18\x01 \x02(\t\x12\x34\n\targuments\x18\x02 \x03(\x0b\x32!.aihwx.inferencing.AttributeProto"U\n\x11LossFunctionProto\x12\n\n\x02id\x18\x01 \x02(\t\x12\x34\n\targuments\x18\x02 \x03(\x0b\x32!.aihwx.inferencing.AttributeProto"c\n\x07Network\x12\x1a\n\x12weight_template_id\x18\x01 \x02(\t\x12<\n\x06layers\x18\x02 \x03(\x0b\x32,.aihwx.inferencing.LayerOrActivationFunction"(\n\x07Version\x12\x0e\n\x06schema\x18\x01 \x02(\x03\x12\r\n\x05opset\x18\x02 \x02(\x03'
+)
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
-_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'i_common_pb2', globals())
+_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, "i_common_pb2", globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
-
-  DESCRIPTOR._options = None
-  _OPTIMIZERPROTO._serialized_start=58
-  _OPTIMIZERPROTO._serialized_end=140
-  _PRIMITIVEPROTO._serialized_start=142
-  _PRIMITIVEPROTO._serialized_end=224
-  _TRANSFORMPROTO._serialized_start=226
-  _TRANSFORMPROTO._serialized_end=308
-  _LAYERPROTO._serialized_start=311
-  _LAYERPROTO._serialized_end=444
-  _ACTIVATIONFUNCTIONPROTO._serialized_start=446
-  _ACTIVATIONFUNCTIONPROTO._serialized_end=537
-  _LAYERORACTIVATIONFUNCTION._serialized_start=540
-  _LAYERORACTIVATIONFUNCTION._serialized_end=698
-  _SCHEDULERPROTO._serialized_start=700
-  _SCHEDULERPROTO._serialized_end=782
-  _LOSSFUNCTIONPROTO._serialized_start=784
-  _LOSSFUNCTIONPROTO._serialized_end=869
-  _NETWORK._serialized_start=871
-  _NETWORK._serialized_end=970
-  _VERSION._serialized_start=972
-  _VERSION._serialized_end=1012
+    DESCRIPTOR._options = None
+    _OPTIMIZERPROTO._serialized_start = 58
+    _OPTIMIZERPROTO._serialized_end = 140
+    _PRIMITIVEPROTO._serialized_start = 142
+    _PRIMITIVEPROTO._serialized_end = 224
+    _TRANSFORMPROTO._serialized_start = 226
+    _TRANSFORMPROTO._serialized_end = 308
+    _LAYERPROTO._serialized_start = 311
+    _LAYERPROTO._serialized_end = 444
+    _ACTIVATIONFUNCTIONPROTO._serialized_start = 446
+    _ACTIVATIONFUNCTIONPROTO._serialized_end = 537
+    _LAYERORACTIVATIONFUNCTION._serialized_start = 540
+    _LAYERORACTIVATIONFUNCTION._serialized_end = 698
+    _SCHEDULERPROTO._serialized_start = 700
+    _SCHEDULERPROTO._serialized_end = 782
+    _LOSSFUNCTIONPROTO._serialized_start = 784
+    _LOSSFUNCTIONPROTO._serialized_end = 869
+    _NETWORK._serialized_start = 871
+    _NETWORK._serialized_end = 970
+    _VERSION._serialized_start = 972
+    _VERSION._serialized_end = 1012
 # @@protoc_insertion_point(module_scope)
```

### Comparing `aihwkit-0.7.1/src/aihwkit/cloud/converter/definitions/i_input_file_pb2.py` & `aihwkit-0.8.0/src/aihwkit/cloud/converter/definitions/i_input_file_pb2.py`

 * *Files 5% similar despite different names*

```diff
@@ -2,37 +2,39 @@
 # Generated by the protocol buffer compiler.  DO NOT EDIT!
 # source: i_input_file.proto
 """Generated protocol buffer code."""
 from google.protobuf.internal import builder as _builder
 from google.protobuf import descriptor as _descriptor
 from google.protobuf import descriptor_pool as _descriptor_pool
 from google.protobuf import symbol_database as _symbol_database
+
 # @@protoc_insertion_point(imports)
 
 _sym_db = _symbol_database.Default()
 
 
 from . import i_common_pb2 as i__common__pb2
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x12i_input_file.proto\x12\x11\x61ihwx.inferencing\x1a\x0ei_common.proto\"1\n\x07\x44\x61taset\x12\x12\n\ndataset_id\x18\x01 \x02(\t\x12\x12\n\nbatch_size\x18\x02 \x01(\x03\"F\n\x0b\x41nalogProto\x12\x1d\n\x15output_noise_strength\x18\x01 \x02(\x02\x12\x0b\n\x03\x61\x64\x63\x18\x02 \x01(\x03\x12\x0b\n\x03\x64\x61\x63\x18\x03 \x01(\x03\"\xe9\x01\n\x08PCMProto\x12\x16\n\tdevice_id\x18\x01 \x02(\t:\x03pcm\x12\x1f\n\x17programming_noise_scale\x18\x02 \x02(\x02\x12\x18\n\x10read_noise_scale\x18\x03 \x02(\x02\x12\x13\n\x0b\x64rift_scale\x18\x04 \x02(\x02\x12\x1a\n\x12\x64rift_compensation\x18\x05 \x02(\x08\x12\x1d\n\x15poly_first_order_coef\x18\x06 \x02(\x02\x12\x1e\n\x16poly_second_order_coef\x18\x07 \x02(\x02\x12\x1a\n\x12poly_constant_coef\x18\x08 \x02(\x02\"\x98\x02\n\x0cGenericProto\x12\x1a\n\tdevice_id\x18\x01 \x02(\t:\x07generic\x12\x1f\n\x17programming_noise_scale\x18\x02 \x02(\x02\x12\x18\n\x10read_noise_scale\x18\x03 \x02(\x02\x12\x13\n\x0b\x64rift_scale\x18\x04 \x02(\x02\x12\x1a\n\x12\x64rift_compensation\x18\x05 \x02(\x08\x12\x1d\n\x15poly_first_order_coef\x18\x06 \x02(\x02\x12\x1e\n\x16poly_second_order_coef\x18\x07 \x02(\x02\x12\x1a\n\x12poly_constant_coef\x18\x08 \x02(\x02\x12\x12\n\ndrift_mean\x18\t \x02(\x02\x12\x11\n\tdrift_std\x18\n \x02(\x02\"y\n\x0fNoiseModelProto\x12*\n\x03pcm\x18\x01 \x01(\x0b\x32\x1b.aihwx.inferencing.PCMProtoH\x00\x12\x32\n\x07generic\x18\x02 \x01(\x0b\x32\x1f.aihwx.inferencing.GenericProtoH\x00\x42\x06\n\x04item\"\xb3\x01\n\x0bInferencing\x12\x19\n\x11inference_repeats\x18\x01 \x02(\x03\x12\x16\n\x0einference_time\x18\x02 \x02(\x03\x12\x33\n\x0b\x61nalog_info\x18\x03 \x02(\x0b\x32\x1e.aihwx.inferencing.AnalogProto\x12<\n\x10noise_model_info\x18\x04 \x02(\x0b\x32\".aihwx.inferencing.NoiseModelProto\"\xcc\x01\n\x0eInferenceInput\x12+\n\x07version\x18\x01 \x02(\x0b\x32\x1a.aihwx.inferencing.Version\x12+\n\x07\x64\x61taset\x18\x02 \x02(\x0b\x32\x1a.aihwx.inferencing.Dataset\x12+\n\x07network\x18\x03 \x02(\x0b\x32\x1a.aihwx.inferencing.Network\x12\x33\n\x0binferencing\x18\x04 \x02(\x0b\x32\x1e.aihwx.inferencing.Inferencing')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(
+    b'\n\x12i_input_file.proto\x12\x11\x61ihwx.inferencing\x1a\x0ei_common.proto"1\n\x07\x44\x61taset\x12\x12\n\ndataset_id\x18\x01 \x02(\t\x12\x12\n\nbatch_size\x18\x02 \x01(\x03"F\n\x0b\x41nalogProto\x12\x1d\n\x15output_noise_strength\x18\x01 \x02(\x02\x12\x0b\n\x03\x61\x64\x63\x18\x02 \x01(\x03\x12\x0b\n\x03\x64\x61\x63\x18\x03 \x01(\x03"\xe9\x01\n\x08PCMProto\x12\x16\n\tdevice_id\x18\x01 \x02(\t:\x03pcm\x12\x1f\n\x17programming_noise_scale\x18\x02 \x02(\x02\x12\x18\n\x10read_noise_scale\x18\x03 \x02(\x02\x12\x13\n\x0b\x64rift_scale\x18\x04 \x02(\x02\x12\x1a\n\x12\x64rift_compensation\x18\x05 \x02(\x08\x12\x1d\n\x15poly_first_order_coef\x18\x06 \x02(\x02\x12\x1e\n\x16poly_second_order_coef\x18\x07 \x02(\x02\x12\x1a\n\x12poly_constant_coef\x18\x08 \x02(\x02"\x98\x02\n\x0cGenericProto\x12\x1a\n\tdevice_id\x18\x01 \x02(\t:\x07generic\x12\x1f\n\x17programming_noise_scale\x18\x02 \x02(\x02\x12\x18\n\x10read_noise_scale\x18\x03 \x02(\x02\x12\x13\n\x0b\x64rift_scale\x18\x04 \x02(\x02\x12\x1a\n\x12\x64rift_compensation\x18\x05 \x02(\x08\x12\x1d\n\x15poly_first_order_coef\x18\x06 \x02(\x02\x12\x1e\n\x16poly_second_order_coef\x18\x07 \x02(\x02\x12\x1a\n\x12poly_constant_coef\x18\x08 \x02(\x02\x12\x12\n\ndrift_mean\x18\t \x02(\x02\x12\x11\n\tdrift_std\x18\n \x02(\x02"y\n\x0fNoiseModelProto\x12*\n\x03pcm\x18\x01 \x01(\x0b\x32\x1b.aihwx.inferencing.PCMProtoH\x00\x12\x32\n\x07generic\x18\x02 \x01(\x0b\x32\x1f.aihwx.inferencing.GenericProtoH\x00\x42\x06\n\x04item"\xb3\x01\n\x0bInferencing\x12\x19\n\x11inference_repeats\x18\x01 \x02(\x03\x12\x16\n\x0einference_time\x18\x02 \x02(\x03\x12\x33\n\x0b\x61nalog_info\x18\x03 \x02(\x0b\x32\x1e.aihwx.inferencing.AnalogProto\x12<\n\x10noise_model_info\x18\x04 \x02(\x0b\x32".aihwx.inferencing.NoiseModelProto"\xcc\x01\n\x0eInferenceInput\x12+\n\x07version\x18\x01 \x02(\x0b\x32\x1a.aihwx.inferencing.Version\x12+\n\x07\x64\x61taset\x18\x02 \x02(\x0b\x32\x1a.aihwx.inferencing.Dataset\x12+\n\x07network\x18\x03 \x02(\x0b\x32\x1a.aihwx.inferencing.Network\x12\x33\n\x0binferencing\x18\x04 \x02(\x0b\x32\x1e.aihwx.inferencing.Inferencing'
+)
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
-_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'i_input_file_pb2', globals())
+_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, "i_input_file_pb2", globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
-
-  DESCRIPTOR._options = None
-  _DATASET._serialized_start=57
-  _DATASET._serialized_end=106
-  _ANALOGPROTO._serialized_start=108
-  _ANALOGPROTO._serialized_end=178
-  _PCMPROTO._serialized_start=181
-  _PCMPROTO._serialized_end=414
-  _GENERICPROTO._serialized_start=417
-  _GENERICPROTO._serialized_end=697
-  _NOISEMODELPROTO._serialized_start=699
-  _NOISEMODELPROTO._serialized_end=820
-  _INFERENCING._serialized_start=823
-  _INFERENCING._serialized_end=1002
-  _INFERENCEINPUT._serialized_start=1005
-  _INFERENCEINPUT._serialized_end=1209
+    DESCRIPTOR._options = None
+    _DATASET._serialized_start = 57
+    _DATASET._serialized_end = 106
+    _ANALOGPROTO._serialized_start = 108
+    _ANALOGPROTO._serialized_end = 178
+    _PCMPROTO._serialized_start = 181
+    _PCMPROTO._serialized_end = 414
+    _GENERICPROTO._serialized_start = 417
+    _GENERICPROTO._serialized_end = 697
+    _NOISEMODELPROTO._serialized_start = 699
+    _NOISEMODELPROTO._serialized_end = 820
+    _INFERENCING._serialized_start = 823
+    _INFERENCING._serialized_end = 1002
+    _INFERENCEINPUT._serialized_start = 1005
+    _INFERENCEINPUT._serialized_end = 1209
 # @@protoc_insertion_point(module_scope)
```

### Comparing `aihwkit-0.7.1/src/aihwkit/cloud/converter/definitions/i_onnx_common_pb2.py` & `aihwkit-0.8.0/src/aihwkit/cloud/converter/definitions/i_onnx_common_pb2.py`

 * *Files 8% similar despite different names*

```diff
@@ -2,40 +2,40 @@
 # Generated by the protocol buffer compiler.  DO NOT EDIT!
 # source: i_onnx_common.proto
 """Generated protocol buffer code."""
 from google.protobuf.internal import builder as _builder
 from google.protobuf import descriptor as _descriptor
 from google.protobuf import descriptor_pool as _descriptor_pool
 from google.protobuf import symbol_database as _symbol_database
+
 # @@protoc_insertion_point(imports)
 
 _sym_db = _symbol_database.Default()
 
 
-
-
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x13i_onnx_common.proto\x12\x11\x61ihwx.inferencing\"\xb5\x03\n\x0e\x41ttributeProto\x12\x0c\n\x04name\x18\x01 \x01(\t\x12=\n\x04type\x18\x14 \x01(\x0e\x32/.aihwx.inferencing.AttributeProto.AttributeType\x12\t\n\x01\x66\x18\x02 \x01(\x02\x12\t\n\x01i\x18\x03 \x01(\x03\x12\t\n\x01s\x18\x04 \x01(\x0c\x12)\n\x01t\x18\x05 \x01(\x0b\x32\x1e.aihwx.inferencing.TensorProto\x12\t\n\x01\x62\x18\x65 \x01(\x08\x12\x0e\n\x06\x66loats\x18\x07 \x03(\x02\x12\x0c\n\x04ints\x18\x08 \x03(\x03\x12\x0f\n\x07strings\x18\t \x03(\x0c\x12/\n\x07tensors\x18\n \x03(\x0b\x32\x1e.aihwx.inferencing.TensorProto\x12\r\n\x05\x62ools\x18\x66 \x03(\x08\"\x8f\x01\n\rAttributeType\x12\r\n\tUNDEFINED\x10\x00\x12\t\n\x05\x46LOAT\x10\x01\x12\x07\n\x03INT\x10\x02\x12\n\n\x06STRING\x10\x03\x12\n\n\x06TENSOR\x10\x04\x12\x08\n\x04\x42OOL\x10\x65\x12\n\n\x06\x46LOATS\x10\x06\x12\x08\n\x04INTS\x10\x07\x12\x0b\n\x07STRINGS\x10\x08\x12\x0b\n\x07TENSORS\x10\t\x12\t\n\x05\x42OOLS\x10\x66\"\x89\x02\n\x0bTensorProto\x12\x0c\n\x04\x64ims\x18\x01 \x03(\x03\x12\x11\n\tdata_type\x18\x02 \x01(\x05\x12\x16\n\nfloat_data\x18\x04 \x03(\x02\x42\x02\x10\x01\x12\x16\n\nint32_data\x18\x05 \x03(\x05\x42\x02\x10\x01\x12\x13\n\x0bstring_data\x18\x06 \x03(\x0c\x12\x16\n\nint64_data\x18\x07 \x03(\x03\x42\x02\x10\x01\"|\n\x08\x44\x61taType\x12\r\n\tUNDEFINED\x10\x00\x12\t\n\x05\x46LOAT\x10\x01\x12\t\n\x05UINT8\x10\x02\x12\x08\n\x04INT8\x10\x03\x12\n\n\x06UINT16\x10\x04\x12\t\n\x05INT16\x10\x05\x12\t\n\x05INT32\x10\x06\x12\t\n\x05INT64\x10\x07\x12\n\n\x06STRING\x10\x08\x12\x08\n\x04\x42OOL\x10\t\"\xa2\x01\n\x10TensorShapeProto\x12:\n\x03\x64im\x18\x01 \x03(\x0b\x32-.aihwx.inferencing.TensorShapeProto.Dimension\x1aR\n\tDimension\x12\x13\n\tdim_value\x18\x01 \x01(\x03H\x00\x12\x13\n\tdim_param\x18\x02 \x01(\tH\x00\x12\x12\n\ndenotation\x18\x03 \x01(\tB\x07\n\x05value')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(
+    b'\n\x13i_onnx_common.proto\x12\x11\x61ihwx.inferencing"\xb5\x03\n\x0e\x41ttributeProto\x12\x0c\n\x04name\x18\x01 \x01(\t\x12=\n\x04type\x18\x14 \x01(\x0e\x32/.aihwx.inferencing.AttributeProto.AttributeType\x12\t\n\x01\x66\x18\x02 \x01(\x02\x12\t\n\x01i\x18\x03 \x01(\x03\x12\t\n\x01s\x18\x04 \x01(\x0c\x12)\n\x01t\x18\x05 \x01(\x0b\x32\x1e.aihwx.inferencing.TensorProto\x12\t\n\x01\x62\x18\x65 \x01(\x08\x12\x0e\n\x06\x66loats\x18\x07 \x03(\x02\x12\x0c\n\x04ints\x18\x08 \x03(\x03\x12\x0f\n\x07strings\x18\t \x03(\x0c\x12/\n\x07tensors\x18\n \x03(\x0b\x32\x1e.aihwx.inferencing.TensorProto\x12\r\n\x05\x62ools\x18\x66 \x03(\x08"\x8f\x01\n\rAttributeType\x12\r\n\tUNDEFINED\x10\x00\x12\t\n\x05\x46LOAT\x10\x01\x12\x07\n\x03INT\x10\x02\x12\n\n\x06STRING\x10\x03\x12\n\n\x06TENSOR\x10\x04\x12\x08\n\x04\x42OOL\x10\x65\x12\n\n\x06\x46LOATS\x10\x06\x12\x08\n\x04INTS\x10\x07\x12\x0b\n\x07STRINGS\x10\x08\x12\x0b\n\x07TENSORS\x10\t\x12\t\n\x05\x42OOLS\x10\x66"\x89\x02\n\x0bTensorProto\x12\x0c\n\x04\x64ims\x18\x01 \x03(\x03\x12\x11\n\tdata_type\x18\x02 \x01(\x05\x12\x16\n\nfloat_data\x18\x04 \x03(\x02\x42\x02\x10\x01\x12\x16\n\nint32_data\x18\x05 \x03(\x05\x42\x02\x10\x01\x12\x13\n\x0bstring_data\x18\x06 \x03(\x0c\x12\x16\n\nint64_data\x18\x07 \x03(\x03\x42\x02\x10\x01"|\n\x08\x44\x61taType\x12\r\n\tUNDEFINED\x10\x00\x12\t\n\x05\x46LOAT\x10\x01\x12\t\n\x05UINT8\x10\x02\x12\x08\n\x04INT8\x10\x03\x12\n\n\x06UINT16\x10\x04\x12\t\n\x05INT16\x10\x05\x12\t\n\x05INT32\x10\x06\x12\t\n\x05INT64\x10\x07\x12\n\n\x06STRING\x10\x08\x12\x08\n\x04\x42OOL\x10\t"\xa2\x01\n\x10TensorShapeProto\x12:\n\x03\x64im\x18\x01 \x03(\x0b\x32-.aihwx.inferencing.TensorShapeProto.Dimension\x1aR\n\tDimension\x12\x13\n\tdim_value\x18\x01 \x01(\x03H\x00\x12\x13\n\tdim_param\x18\x02 \x01(\tH\x00\x12\x12\n\ndenotation\x18\x03 \x01(\tB\x07\n\x05value'
+)
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
-_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'i_onnx_common_pb2', globals())
+_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, "i_onnx_common_pb2", globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
-
-  DESCRIPTOR._options = None
-  _TENSORPROTO.fields_by_name['float_data']._options = None
-  _TENSORPROTO.fields_by_name['float_data']._serialized_options = b'\020\001'
-  _TENSORPROTO.fields_by_name['int32_data']._options = None
-  _TENSORPROTO.fields_by_name['int32_data']._serialized_options = b'\020\001'
-  _TENSORPROTO.fields_by_name['int64_data']._options = None
-  _TENSORPROTO.fields_by_name['int64_data']._serialized_options = b'\020\001'
-  _ATTRIBUTEPROTO._serialized_start=43
-  _ATTRIBUTEPROTO._serialized_end=480
-  _ATTRIBUTEPROTO_ATTRIBUTETYPE._serialized_start=337
-  _ATTRIBUTEPROTO_ATTRIBUTETYPE._serialized_end=480
-  _TENSORPROTO._serialized_start=483
-  _TENSORPROTO._serialized_end=748
-  _TENSORPROTO_DATATYPE._serialized_start=624
-  _TENSORPROTO_DATATYPE._serialized_end=748
-  _TENSORSHAPEPROTO._serialized_start=751
-  _TENSORSHAPEPROTO._serialized_end=913
-  _TENSORSHAPEPROTO_DIMENSION._serialized_start=831
-  _TENSORSHAPEPROTO_DIMENSION._serialized_end=913
+    DESCRIPTOR._options = None
+    _TENSORPROTO.fields_by_name["float_data"]._options = None
+    _TENSORPROTO.fields_by_name["float_data"]._serialized_options = b"\020\001"
+    _TENSORPROTO.fields_by_name["int32_data"]._options = None
+    _TENSORPROTO.fields_by_name["int32_data"]._serialized_options = b"\020\001"
+    _TENSORPROTO.fields_by_name["int64_data"]._options = None
+    _TENSORPROTO.fields_by_name["int64_data"]._serialized_options = b"\020\001"
+    _ATTRIBUTEPROTO._serialized_start = 43
+    _ATTRIBUTEPROTO._serialized_end = 480
+    _ATTRIBUTEPROTO_ATTRIBUTETYPE._serialized_start = 337
+    _ATTRIBUTEPROTO_ATTRIBUTETYPE._serialized_end = 480
+    _TENSORPROTO._serialized_start = 483
+    _TENSORPROTO._serialized_end = 748
+    _TENSORPROTO_DATATYPE._serialized_start = 624
+    _TENSORPROTO_DATATYPE._serialized_end = 748
+    _TENSORSHAPEPROTO._serialized_start = 751
+    _TENSORSHAPEPROTO._serialized_end = 913
+    _TENSORSHAPEPROTO_DIMENSION._serialized_start = 831
+    _TENSORSHAPEPROTO_DIMENSION._serialized_end = 913
 # @@protoc_insertion_point(module_scope)
```

### Comparing `aihwkit-0.7.1/src/aihwkit/cloud/converter/definitions/i_output_file_pb2.py` & `aihwkit-0.8.0/src/aihwkit/cloud/converter/definitions/i_output_file_pb2.py`

 * *Files 6% similar despite different names*

```diff
@@ -2,29 +2,31 @@
 # Generated by the protocol buffer compiler.  DO NOT EDIT!
 # source: i_output_file.proto
 """Generated protocol buffer code."""
 from google.protobuf.internal import builder as _builder
 from google.protobuf import descriptor as _descriptor
 from google.protobuf import descriptor_pool as _descriptor_pool
 from google.protobuf import symbol_database as _symbol_database
+
 # @@protoc_insertion_point(imports)
 
 _sym_db = _symbol_database.Default()
 
 
 from . import i_common_pb2 as i__common__pb2
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x13i_output_file.proto\x12\x11\x61ihwx.inferencing\x1a\x0ei_common.proto\"}\n\x15InferenceResultsProto\x12\x13\n\x0bt_inference\x18\x01 \x02(\x02\x12\x14\n\x0c\x61vg_accuracy\x18\x02 \x02(\x02\x12\x14\n\x0cstd_accuracy\x18\x03 \x02(\x02\x12\x11\n\tavg_error\x18\x04 \x02(\x02\x12\x10\n\x08\x61vg_loss\x18\x05 \x02(\x02\"\x9d\x01\n\x12InferenceRunsProto\x12\x18\n\x10inference_repeat\x18\x01 \x02(\x03\x12\x12\n\nis_partial\x18\x02 \x02(\x08\x12\x14\n\x0ctime_elapsed\x18\x03 \x02(\x02\x12\x43\n\x11inference_results\x18\x04 \x03(\x0b\x32(.aihwx.inferencing.InferenceResultsProto\"\x7f\n\x11InferencingOutput\x12+\n\x07version\x18\x01 \x02(\x0b\x32\x1a.aihwx.inferencing.Version\x12=\n\x0einference_runs\x18\x02 \x02(\x0b\x32%.aihwx.inferencing.InferenceRunsProto')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(
+    b'\n\x13i_output_file.proto\x12\x11\x61ihwx.inferencing\x1a\x0ei_common.proto"}\n\x15InferenceResultsProto\x12\x13\n\x0bt_inference\x18\x01 \x02(\x02\x12\x14\n\x0c\x61vg_accuracy\x18\x02 \x02(\x02\x12\x14\n\x0cstd_accuracy\x18\x03 \x02(\x02\x12\x11\n\tavg_error\x18\x04 \x02(\x02\x12\x10\n\x08\x61vg_loss\x18\x05 \x02(\x02"\x9d\x01\n\x12InferenceRunsProto\x12\x18\n\x10inference_repeat\x18\x01 \x02(\x03\x12\x12\n\nis_partial\x18\x02 \x02(\x08\x12\x14\n\x0ctime_elapsed\x18\x03 \x02(\x02\x12\x43\n\x11inference_results\x18\x04 \x03(\x0b\x32(.aihwx.inferencing.InferenceResultsProto"\x7f\n\x11InferencingOutput\x12+\n\x07version\x18\x01 \x02(\x0b\x32\x1a.aihwx.inferencing.Version\x12=\n\x0einference_runs\x18\x02 \x02(\x0b\x32%.aihwx.inferencing.InferenceRunsProto'
+)
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
-_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'i_output_file_pb2', globals())
+_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, "i_output_file_pb2", globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
-
-  DESCRIPTOR._options = None
-  _INFERENCERESULTSPROTO._serialized_start=58
-  _INFERENCERESULTSPROTO._serialized_end=183
-  _INFERENCERUNSPROTO._serialized_start=186
-  _INFERENCERUNSPROTO._serialized_end=343
-  _INFERENCINGOUTPUT._serialized_start=345
-  _INFERENCINGOUTPUT._serialized_end=472
+    DESCRIPTOR._options = None
+    _INFERENCERESULTSPROTO._serialized_start = 58
+    _INFERENCERESULTSPROTO._serialized_end = 183
+    _INFERENCERUNSPROTO._serialized_start = 186
+    _INFERENCERUNSPROTO._serialized_end = 343
+    _INFERENCINGOUTPUT._serialized_start = 345
+    _INFERENCINGOUTPUT._serialized_end = 472
 # @@protoc_insertion_point(module_scope)
```

### Comparing `aihwkit-0.7.1/src/aihwkit/cloud/converter/definitions/input_file_pb2.py` & `aihwkit-0.8.0/src/aihwkit/cloud/converter/definitions/input_file_pb2.py`

 * *Files 4% similar despite different names*

```diff
@@ -2,29 +2,31 @@
 # Generated by the protocol buffer compiler.  DO NOT EDIT!
 # source: input_file.proto
 """Generated protocol buffer code."""
 from google.protobuf.internal import builder as _builder
 from google.protobuf import descriptor as _descriptor
 from google.protobuf import descriptor_pool as _descriptor_pool
 from google.protobuf import symbol_database as _symbol_database
+
 # @@protoc_insertion_point(imports)
 
 _sym_db = _symbol_database.Default()
 
 
 from . import common_pb2 as common__pb2
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x10input_file.proto\x12\x05\x61ihwx\x1a\x0c\x63ommon.proto\"\\\n\x07\x44\x61taset\x12\x12\n\ndataset_id\x18\x01 \x02(\t\x12)\n\ntransforms\x18\x02 \x03(\x0b\x32\x15.aihwx.TransformProto\x12\x12\n\nbatch_size\x18\x03 \x01(\x05\"\x9f\x01\n\x08Training\x12(\n\toptimizer\x18\x01 \x02(\x0b\x32\x15.aihwx.OptimizerProto\x12(\n\tscheduler\x18\x02 \x01(\x0b\x32\x15.aihwx.SchedulerProto\x12/\n\rloss_function\x18\x03 \x01(\x0b\x32\x18.aihwx.LossFunctionProto\x12\x0e\n\x06\x65pochs\x18\x04 \x01(\x05\"\x95\x01\n\rTrainingInput\x12\x1f\n\x07version\x18\x01 \x02(\x0b\x32\x0e.aihwx.Version\x12\x1f\n\x07\x64\x61taset\x18\x02 \x02(\x0b\x32\x0e.aihwx.Dataset\x12\x1f\n\x07network\x18\x03 \x02(\x0b\x32\x0e.aihwx.Network\x12!\n\x08training\x18\x04 \x02(\x0b\x32\x0f.aihwx.Training')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(
+    b'\n\x10input_file.proto\x12\x05\x61ihwx\x1a\x0c\x63ommon.proto"\\\n\x07\x44\x61taset\x12\x12\n\ndataset_id\x18\x01 \x02(\t\x12)\n\ntransforms\x18\x02 \x03(\x0b\x32\x15.aihwx.TransformProto\x12\x12\n\nbatch_size\x18\x03 \x01(\x05"\x9f\x01\n\x08Training\x12(\n\toptimizer\x18\x01 \x02(\x0b\x32\x15.aihwx.OptimizerProto\x12(\n\tscheduler\x18\x02 \x01(\x0b\x32\x15.aihwx.SchedulerProto\x12/\n\rloss_function\x18\x03 \x01(\x0b\x32\x18.aihwx.LossFunctionProto\x12\x0e\n\x06\x65pochs\x18\x04 \x01(\x05"\x95\x01\n\rTrainingInput\x12\x1f\n\x07version\x18\x01 \x02(\x0b\x32\x0e.aihwx.Version\x12\x1f\n\x07\x64\x61taset\x18\x02 \x02(\x0b\x32\x0e.aihwx.Dataset\x12\x1f\n\x07network\x18\x03 \x02(\x0b\x32\x0e.aihwx.Network\x12!\n\x08training\x18\x04 \x02(\x0b\x32\x0f.aihwx.Training'
+)
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
-_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'input_file_pb2', globals())
+_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, "input_file_pb2", globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
-
-  DESCRIPTOR._options = None
-  _DATASET._serialized_start=41
-  _DATASET._serialized_end=133
-  _TRAINING._serialized_start=136
-  _TRAINING._serialized_end=295
-  _TRAININGINPUT._serialized_start=298
-  _TRAININGINPUT._serialized_end=447
+    DESCRIPTOR._options = None
+    _DATASET._serialized_start = 41
+    _DATASET._serialized_end = 133
+    _TRAINING._serialized_start = 136
+    _TRAINING._serialized_end = 295
+    _TRAININGINPUT._serialized_start = 298
+    _TRAININGINPUT._serialized_end = 447
 # @@protoc_insertion_point(module_scope)
```

### Comparing `aihwkit-0.7.1/src/aihwkit/cloud/converter/definitions/onnx_common_pb2.py` & `aihwkit-0.8.0/src/aihwkit/cloud/converter/definitions/onnx_common_pb2.py`

 * *Files 8% similar despite different names*

```diff
@@ -2,40 +2,40 @@
 # Generated by the protocol buffer compiler.  DO NOT EDIT!
 # source: onnx_common.proto
 """Generated protocol buffer code."""
 from google.protobuf.internal import builder as _builder
 from google.protobuf import descriptor as _descriptor
 from google.protobuf import descriptor_pool as _descriptor_pool
 from google.protobuf import symbol_database as _symbol_database
+
 # @@protoc_insertion_point(imports)
 
 _sym_db = _symbol_database.Default()
 
 
-
-
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x11onnx_common.proto\x12\x05\x61ihwx\"\x91\x03\n\x0e\x41ttributeProto\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x31\n\x04type\x18\x14 \x01(\x0e\x32#.aihwx.AttributeProto.AttributeType\x12\t\n\x01\x66\x18\x02 \x01(\x02\x12\t\n\x01i\x18\x03 \x01(\x03\x12\t\n\x01s\x18\x04 \x01(\x0c\x12\x1d\n\x01t\x18\x05 \x01(\x0b\x32\x12.aihwx.TensorProto\x12\t\n\x01\x62\x18\x65 \x01(\x08\x12\x0e\n\x06\x66loats\x18\x07 \x03(\x02\x12\x0c\n\x04ints\x18\x08 \x03(\x03\x12\x0f\n\x07strings\x18\t \x03(\x0c\x12#\n\x07tensors\x18\n \x03(\x0b\x32\x12.aihwx.TensorProto\x12\r\n\x05\x62ools\x18\x66 \x03(\x08\"\x8f\x01\n\rAttributeType\x12\r\n\tUNDEFINED\x10\x00\x12\t\n\x05\x46LOAT\x10\x01\x12\x07\n\x03INT\x10\x02\x12\n\n\x06STRING\x10\x03\x12\n\n\x06TENSOR\x10\x04\x12\x08\n\x04\x42OOL\x10\x65\x12\n\n\x06\x46LOATS\x10\x06\x12\x08\n\x04INTS\x10\x07\x12\x0b\n\x07STRINGS\x10\x08\x12\x0b\n\x07TENSORS\x10\t\x12\t\n\x05\x42OOLS\x10\x66\"\x89\x02\n\x0bTensorProto\x12\x0c\n\x04\x64ims\x18\x01 \x03(\x03\x12\x11\n\tdata_type\x18\x02 \x01(\x05\x12\x16\n\nfloat_data\x18\x04 \x03(\x02\x42\x02\x10\x01\x12\x16\n\nint32_data\x18\x05 \x03(\x05\x42\x02\x10\x01\x12\x13\n\x0bstring_data\x18\x06 \x03(\x0c\x12\x16\n\nint64_data\x18\x07 \x03(\x03\x42\x02\x10\x01\"|\n\x08\x44\x61taType\x12\r\n\tUNDEFINED\x10\x00\x12\t\n\x05\x46LOAT\x10\x01\x12\t\n\x05UINT8\x10\x02\x12\x08\n\x04INT8\x10\x03\x12\n\n\x06UINT16\x10\x04\x12\t\n\x05INT16\x10\x05\x12\t\n\x05INT32\x10\x06\x12\t\n\x05INT64\x10\x07\x12\n\n\x06STRING\x10\x08\x12\x08\n\x04\x42OOL\x10\t\"\x96\x01\n\x10TensorShapeProto\x12.\n\x03\x64im\x18\x01 \x03(\x0b\x32!.aihwx.TensorShapeProto.Dimension\x1aR\n\tDimension\x12\x13\n\tdim_value\x18\x01 \x01(\x03H\x00\x12\x13\n\tdim_param\x18\x02 \x01(\tH\x00\x12\x12\n\ndenotation\x18\x03 \x01(\tB\x07\n\x05value')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(
+    b'\n\x11onnx_common.proto\x12\x05\x61ihwx"\x91\x03\n\x0e\x41ttributeProto\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x31\n\x04type\x18\x14 \x01(\x0e\x32#.aihwx.AttributeProto.AttributeType\x12\t\n\x01\x66\x18\x02 \x01(\x02\x12\t\n\x01i\x18\x03 \x01(\x03\x12\t\n\x01s\x18\x04 \x01(\x0c\x12\x1d\n\x01t\x18\x05 \x01(\x0b\x32\x12.aihwx.TensorProto\x12\t\n\x01\x62\x18\x65 \x01(\x08\x12\x0e\n\x06\x66loats\x18\x07 \x03(\x02\x12\x0c\n\x04ints\x18\x08 \x03(\x03\x12\x0f\n\x07strings\x18\t \x03(\x0c\x12#\n\x07tensors\x18\n \x03(\x0b\x32\x12.aihwx.TensorProto\x12\r\n\x05\x62ools\x18\x66 \x03(\x08"\x8f\x01\n\rAttributeType\x12\r\n\tUNDEFINED\x10\x00\x12\t\n\x05\x46LOAT\x10\x01\x12\x07\n\x03INT\x10\x02\x12\n\n\x06STRING\x10\x03\x12\n\n\x06TENSOR\x10\x04\x12\x08\n\x04\x42OOL\x10\x65\x12\n\n\x06\x46LOATS\x10\x06\x12\x08\n\x04INTS\x10\x07\x12\x0b\n\x07STRINGS\x10\x08\x12\x0b\n\x07TENSORS\x10\t\x12\t\n\x05\x42OOLS\x10\x66"\x89\x02\n\x0bTensorProto\x12\x0c\n\x04\x64ims\x18\x01 \x03(\x03\x12\x11\n\tdata_type\x18\x02 \x01(\x05\x12\x16\n\nfloat_data\x18\x04 \x03(\x02\x42\x02\x10\x01\x12\x16\n\nint32_data\x18\x05 \x03(\x05\x42\x02\x10\x01\x12\x13\n\x0bstring_data\x18\x06 \x03(\x0c\x12\x16\n\nint64_data\x18\x07 \x03(\x03\x42\x02\x10\x01"|\n\x08\x44\x61taType\x12\r\n\tUNDEFINED\x10\x00\x12\t\n\x05\x46LOAT\x10\x01\x12\t\n\x05UINT8\x10\x02\x12\x08\n\x04INT8\x10\x03\x12\n\n\x06UINT16\x10\x04\x12\t\n\x05INT16\x10\x05\x12\t\n\x05INT32\x10\x06\x12\t\n\x05INT64\x10\x07\x12\n\n\x06STRING\x10\x08\x12\x08\n\x04\x42OOL\x10\t"\x96\x01\n\x10TensorShapeProto\x12.\n\x03\x64im\x18\x01 \x03(\x0b\x32!.aihwx.TensorShapeProto.Dimension\x1aR\n\tDimension\x12\x13\n\tdim_value\x18\x01 \x01(\x03H\x00\x12\x13\n\tdim_param\x18\x02 \x01(\tH\x00\x12\x12\n\ndenotation\x18\x03 \x01(\tB\x07\n\x05value'
+)
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
-_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'onnx_common_pb2', globals())
+_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, "onnx_common_pb2", globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
-
-  DESCRIPTOR._options = None
-  _TENSORPROTO.fields_by_name['float_data']._options = None
-  _TENSORPROTO.fields_by_name['float_data']._serialized_options = b'\020\001'
-  _TENSORPROTO.fields_by_name['int32_data']._options = None
-  _TENSORPROTO.fields_by_name['int32_data']._serialized_options = b'\020\001'
-  _TENSORPROTO.fields_by_name['int64_data']._options = None
-  _TENSORPROTO.fields_by_name['int64_data']._serialized_options = b'\020\001'
-  _ATTRIBUTEPROTO._serialized_start=29
-  _ATTRIBUTEPROTO._serialized_end=430
-  _ATTRIBUTEPROTO_ATTRIBUTETYPE._serialized_start=287
-  _ATTRIBUTEPROTO_ATTRIBUTETYPE._serialized_end=430
-  _TENSORPROTO._serialized_start=433
-  _TENSORPROTO._serialized_end=698
-  _TENSORPROTO_DATATYPE._serialized_start=574
-  _TENSORPROTO_DATATYPE._serialized_end=698
-  _TENSORSHAPEPROTO._serialized_start=701
-  _TENSORSHAPEPROTO._serialized_end=851
-  _TENSORSHAPEPROTO_DIMENSION._serialized_start=769
-  _TENSORSHAPEPROTO_DIMENSION._serialized_end=851
+    DESCRIPTOR._options = None
+    _TENSORPROTO.fields_by_name["float_data"]._options = None
+    _TENSORPROTO.fields_by_name["float_data"]._serialized_options = b"\020\001"
+    _TENSORPROTO.fields_by_name["int32_data"]._options = None
+    _TENSORPROTO.fields_by_name["int32_data"]._serialized_options = b"\020\001"
+    _TENSORPROTO.fields_by_name["int64_data"]._options = None
+    _TENSORPROTO.fields_by_name["int64_data"]._serialized_options = b"\020\001"
+    _ATTRIBUTEPROTO._serialized_start = 29
+    _ATTRIBUTEPROTO._serialized_end = 430
+    _ATTRIBUTEPROTO_ATTRIBUTETYPE._serialized_start = 287
+    _ATTRIBUTEPROTO_ATTRIBUTETYPE._serialized_end = 430
+    _TENSORPROTO._serialized_start = 433
+    _TENSORPROTO._serialized_end = 698
+    _TENSORPROTO_DATATYPE._serialized_start = 574
+    _TENSORPROTO_DATATYPE._serialized_end = 698
+    _TENSORSHAPEPROTO._serialized_start = 701
+    _TENSORSHAPEPROTO._serialized_end = 851
+    _TENSORSHAPEPROTO_DIMENSION._serialized_start = 769
+    _TENSORSHAPEPROTO_DIMENSION._serialized_end = 851
 # @@protoc_insertion_point(module_scope)
```

### Comparing `aihwkit-0.7.1/src/aihwkit/cloud/converter/definitions/output_file_pb2.py` & `aihwkit-0.8.0/src/aihwkit/cloud/converter/definitions/output_file_pb2.py`

 * *Files 9% similar despite different names*

```diff
@@ -2,28 +2,30 @@
 # Generated by the protocol buffer compiler.  DO NOT EDIT!
 # source: output_file.proto
 """Generated protocol buffer code."""
 from google.protobuf.internal import builder as _builder
 from google.protobuf import descriptor as _descriptor
 from google.protobuf import descriptor_pool as _descriptor_pool
 from google.protobuf import symbol_database as _symbol_database
+
 # @@protoc_insertion_point(imports)
 
 _sym_db = _symbol_database.Default()
 
 
 from . import onnx_common_pb2 as onnx__common__pb2
 from . import common_pb2 as common__pb2
 
 
-DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x11output_file.proto\x12\x05\x61ihwx\x1a\x11onnx_common.proto\x1a\x0c\x63ommon.proto\"I\n\x10\x45pochInformation\x12\r\n\x05\x65poch\x18\x01 \x02(\x05\x12&\n\x07metrics\x18\x02 \x03(\x0b\x32\x15.aihwx.AttributeProto\"\xa3\x01\n\x0eTrainingOutput\x12\x1f\n\x07version\x18\x01 \x02(\x0b\x32\x0e.aihwx.Version\x12\x1f\n\x07network\x18\x02 \x02(\x0b\x32\x0e.aihwx.Network\x12\'\n\x06\x65pochs\x18\x03 \x03(\x0b\x32\x17.aihwx.EpochInformation\x12&\n\x07metrics\x18\x04 \x03(\x0b\x32\x15.aihwx.AttributeProto')
+DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(
+    b'\n\x11output_file.proto\x12\x05\x61ihwx\x1a\x11onnx_common.proto\x1a\x0c\x63ommon.proto"I\n\x10\x45pochInformation\x12\r\n\x05\x65poch\x18\x01 \x02(\x05\x12&\n\x07metrics\x18\x02 \x03(\x0b\x32\x15.aihwx.AttributeProto"\xa3\x01\n\x0eTrainingOutput\x12\x1f\n\x07version\x18\x01 \x02(\x0b\x32\x0e.aihwx.Version\x12\x1f\n\x07network\x18\x02 \x02(\x0b\x32\x0e.aihwx.Network\x12\'\n\x06\x65pochs\x18\x03 \x03(\x0b\x32\x17.aihwx.EpochInformation\x12&\n\x07metrics\x18\x04 \x03(\x0b\x32\x15.aihwx.AttributeProto'
+)
 
 _builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
-_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'output_file_pb2', globals())
+_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, "output_file_pb2", globals())
 if _descriptor._USE_C_DESCRIPTORS == False:
-
-  DESCRIPTOR._options = None
-  _EPOCHINFORMATION._serialized_start=61
-  _EPOCHINFORMATION._serialized_end=134
-  _TRAININGOUTPUT._serialized_start=137
-  _TRAININGOUTPUT._serialized_end=300
+    DESCRIPTOR._options = None
+    _EPOCHINFORMATION._serialized_start = 61
+    _EPOCHINFORMATION._serialized_end = 134
+    _TRAININGOUTPUT._serialized_start = 137
+    _TRAININGOUTPUT._serialized_end = 300
 # @@protoc_insertion_point(module_scope)
```

### Comparing `aihwkit-0.7.1/src/aihwkit/cloud/converter/v1/__init__.py` & `aihwkit-0.8.0/src/aihwkit/utils/__init__.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
 # that they have been altered from the originals.
 
-"""Helpers for version 1 of the AIHW Composer format."""
+"""Utilities and helpers for aihwkit."""
```

### Comparing `aihwkit-0.7.1/src/aihwkit/cloud/converter/v1/analog_info.py` & `aihwkit-0.8.0/src/aihwkit/cloud/converter/v1/analog_info.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,24 +1,24 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
 # that they have been altered from the originals.
 
 """Helper class for adding rpu_config to neural network model"""
 # pylint: disable=no-name-in-module,import-error
 
 from aihwkit.cloud.converter.definitions.i_input_file_pb2 import (  # type: ignore[attr-defined]
-    AnalogProto
+    AnalogProto,
 )
 
 
 # pylint: disable=too-few-public-methods)
 class AnalogInfo:
     """Data class for fields from protobuf AnalogProto message"""
```

### Comparing `aihwkit-0.7.1/src/aihwkit/cloud/converter/v1/i_mappings.py` & `aihwkit-0.8.0/src/aihwkit/cloud/converter/v1/i_mappings.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -14,65 +14,78 @@
 
 """Mappings for version 1 of the AIHW Composer format."""
 
 from collections import namedtuple
 from typing import Any, Dict
 
 from torch.nn import (
-    BCELoss, BatchNorm2d, Conv2d, ConvTranspose2d, CrossEntropyLoss, Flatten,
-    LeakyReLU, Linear, LogSigmoid, LogSoftmax, MSELoss, MaxPool2d, NLLLoss,
-    ReLU, Sigmoid, Softmax, Tanh
+    BCELoss,
+    BatchNorm2d,
+    Conv2d,
+    ConvTranspose2d,
+    CrossEntropyLoss,
+    Flatten,
+    LeakyReLU,
+    Linear,
+    LogSigmoid,
+    LogSoftmax,
+    MSELoss,
+    MaxPool2d,
+    NLLLoss,
+    ReLU,
+    Sigmoid,
+    Softmax,
+    Tanh,
 )
 from torchvision.datasets import FashionMNIST, SVHN  # type: ignore[import]
 
 from aihwkit.simulator.configs import InferenceRPUConfig
 from aihwkit.simulator.presets.web import (
-    WebComposerInferenceRPUConfig, OldWebComposerInferenceRPUConfig)
+    WebComposerInferenceRPUConfig,
+    OldWebComposerInferenceRPUConfig,
+)
 
-from aihwkit.cloud.converter.definitions.i_onnx_common_pb2 import (   # type: ignore[attr-defined]
-    AttributeProto
+from aihwkit.cloud.converter.definitions.i_onnx_common_pb2 import (  # type: ignore[attr-defined]
+    AttributeProto,
 )
 from aihwkit.cloud.converter.exceptions import ConversionError
-from aihwkit.nn import (
-    AnalogConv2d, AnalogConv2dMapped,
-    AnalogLinear, AnalogLinearMapped
-)
+from aihwkit.nn import AnalogConv2d, AnalogConv2dMapped, AnalogLinear, AnalogLinearMapped
 from aihwkit.optim import AnalogSGD
 from aihwkit.cloud.converter.v1.rpu_config_info import RPUconfigInfo
 
 
-Type = namedtuple('Type', ['attribute_type', 'field', 'fn'])
+Type = namedtuple("Type", ["attribute_type", "field", "fn"])
 
 # pylint: disable=no-member
 TYPES = {
-    int: Type(AttributeProto.AttributeType.INT, 'i', lambda x: x),  # type: ignore
-    bool: Type(AttributeProto.AttributeType.BOOL, 'b', lambda x: x),  # type: ignore
-    str: Type(AttributeProto.AttributeType.STRING, 's',  # type: ignore
-              lambda x: x.encode('utf-8')),
-    float: Type(AttributeProto.AttributeType.FLOAT, 'f', lambda x: x)  # type: ignore
+    int: Type(AttributeProto.AttributeType.INT, "i", lambda x: x),  # type: ignore
+    bool: Type(AttributeProto.AttributeType.BOOL, "b", lambda x: x),  # type: ignore
+    str: Type(
+        AttributeProto.AttributeType.STRING, "s", lambda x: x.encode("utf-8")  # type: ignore
+    ),
+    float: Type(AttributeProto.AttributeType.FLOAT, "f", lambda x: x),  # type: ignore
 }
 
 TYPES_LISTS = {
-    int: Type(AttributeProto.AttributeType.INTS, 'ints', lambda x: x),  # type: ignore
-    bool: Type(AttributeProto.AttributeType.BOOLS, 'bools', lambda x: x),  # type: ignore
-    str: Type(AttributeProto.AttributeType.STRINGS, 'strings',  # type: ignore
-              lambda x: [y.encode('utf-8') for y in x]),
-    float: Type(AttributeProto.AttributeType.FLOATS, 'floats', lambda x: x)  # type: ignore
+    int: Type(AttributeProto.AttributeType.INTS, "ints", lambda x: x),  # type: ignore
+    bool: Type(AttributeProto.AttributeType.BOOLS, "bools", lambda x: x),  # type: ignore
+    str: Type(
+        AttributeProto.AttributeType.STRINGS,
+        "strings",  # type: ignore
+        lambda x: [y.encode("utf-8") for y in x],
+    ),
+    float: Type(AttributeProto.AttributeType.FLOATS, "floats", lambda x: x),  # type: ignore
 }
 # pylint: enable=no-member
 
 
 class Function:
     """Mapping for a function-like entity."""
 
-    def __init__(
-            self,
-            id_: str,
-            args: Dict
-    ):
+    def __init__(self, id_: str, args: Dict):
         self.id_ = id_
         self.args = args
 
     def to_proto(self, source: object, proto_cls: type) -> object:
         """Convert a source object into a destination object."""
         instance = proto_cls(id=self.id_)
 
@@ -99,15 +112,14 @@
         return instance
 
     def from_proto(self, source: Any, cls: type, default: Any = None) -> object:
         """Convert a proto object into a destination object."""
         kwargs = {}
 
         for argument in source.arguments:
-
             if argument.name not in self.args:
                 continue
 
             type_ = self.args[argument.name]
 
             if isinstance(type_, list):
                 proto_type = TYPES_LISTS[type_[0]]
@@ -137,161 +149,147 @@
     """Mapping for a function-like entity (Layer)."""
 
     def get_field_value_to_proto(self, source: Any, field: str, default: Any = None) -> Any:
         """Get the value of a field.
 
         Raises ConversionError
         """
-        if field == 'bias':
-            return getattr(source, 'bias', None) is not None
-        if field == 'rpu_config':
+        if field == "bias":
+            return getattr(source, "bias", None) is not None
+        if field == "rpu_config":
             # preset_cls = type(source.analog_tile.rpu_config)
             analog_tile = next(source.analog_tiles())
             preset_cls = type(analog_tile.rpu_config)
             if preset_cls not in Mappings.presets:
-                raise ConversionError('Invalid rpu_config in layer: '
-                                      f'{preset_cls} not '
-                                      'among the presets')
+                raise ConversionError(
+                    "Invalid rpu_config in layer: " f"{preset_cls} not among the presets"
+                )
             return Mappings.presets[preset_cls]
         return super().get_field_value_to_proto(source, field, default)
 
     def get_argument_from_proto(self, source: Any, field: str, default: Any = None) -> Dict:
         """Get the value of an argument.
 
         Raises ConversionError
         """
-        if source.name == 'rpu_config':
-
+        if source.name == "rpu_config":
             if not isinstance(default, RPUconfigInfo):
-                raise ConversionError('Expect an new RPUconfigInfo as '
-                                      'default for layer creation.')
+                raise ConversionError("Expect an new RPUconfigInfo as default for layer creation.")
 
-            return {'rpu_config': default.create_inference_rpu_config(self.id_)}
+            return {"rpu_config": default.create_inference_rpu_config(self.id_)}
 
         return super().get_argument_from_proto(source, field, default)
 
 
 class Mappings:
     """Mappings between Python entities and AIHW format."""
 
-    datasets = {
-        FashionMNIST: 'fashion_mnist',
-        SVHN: 'svhn'
-    }
+    datasets = {FashionMNIST: "fashion_mnist", SVHN: "svhn"}
 
     layers = {
-        AnalogConv2d: LayerFunction('AnalogConv2d', {
-            'in_channels': int,
-            'out_channels': int,
-            'kernel_size': [int],
-            'stride': [int],
-            'padding': [int],
-            'dilation': [int],
-            'bias': bool,
-            'rpu_config': str,
-        }),
-        AnalogConv2dMapped: LayerFunction('AnalogConv2dMapped', {
-            'in_channels': int,
-            'out_channels': int,
-            'kernel_size': [int],
-            'stride': [int],
-            'padding': [int],
-            'dilation': [int],
-            'bias': bool,
-            'rpu_config': str,
-        }),
-        AnalogLinear: LayerFunction('AnalogLinear', {
-            'in_features': int,
-            'out_features': int,
-            'bias': bool,
-            'rpu_config': str,
-        }),
-        AnalogLinearMapped: LayerFunction('AnalogLinearMapped', {
-            'in_features': int,
-            'out_features': int,
-            'bias': bool,
-            'rpu_config': str,
-        }),
-        BatchNorm2d: LayerFunction('BatchNorm2d', {
-            'num_features': int
-        }),
-        Conv2d: LayerFunction('Conv2d', {
-            'in_channels': int,
-            'out_channels': int,
-            'kernel_size': [int],
-            'stride': [int],
-            'padding': [int],
-            'dilation': [int],
-            'bias': bool
-        }),
-        ConvTranspose2d: LayerFunction('ConvTranspose2d', {
-            'in_channels': int,
-            'out_channels': int,
-            'kernel_size': [int],
-            'stride': [int],
-            'padding': [int],
-            'output_padding': [int],
-            'dilation': [int],
-            'bias': bool
-        }),
-        Flatten: LayerFunction('Flatten', {}),
-        Linear: LayerFunction('Linear', {
-            'in_features': int,
-            'out_features': int,
-            'bias': bool
-        }),
-        MaxPool2d: LayerFunction('MaxPool2d', {
-            'kernel_size': int,
-            'stride': int,
-            'padding': int,
-            'dilation': int,
-            'ceil_mode': bool
-        })
+        AnalogConv2d: LayerFunction(
+            "AnalogConv2d",
+            {
+                "in_channels": int,
+                "out_channels": int,
+                "kernel_size": [int],
+                "stride": [int],
+                "padding": [int],
+                "dilation": [int],
+                "bias": bool,
+                "rpu_config": str,
+            },
+        ),
+        AnalogConv2dMapped: LayerFunction(
+            "AnalogConv2dMapped",
+            {
+                "in_channels": int,
+                "out_channels": int,
+                "kernel_size": [int],
+                "stride": [int],
+                "padding": [int],
+                "dilation": [int],
+                "bias": bool,
+                "rpu_config": str,
+            },
+        ),
+        AnalogLinear: LayerFunction(
+            "AnalogLinear",
+            {"in_features": int, "out_features": int, "bias": bool, "rpu_config": str},
+        ),
+        AnalogLinearMapped: LayerFunction(
+            "AnalogLinearMapped",
+            {"in_features": int, "out_features": int, "bias": bool, "rpu_config": str},
+        ),
+        BatchNorm2d: LayerFunction("BatchNorm2d", {"num_features": int}),
+        Conv2d: LayerFunction(
+            "Conv2d",
+            {
+                "in_channels": int,
+                "out_channels": int,
+                "kernel_size": [int],
+                "stride": [int],
+                "padding": [int],
+                "dilation": [int],
+                "bias": bool,
+            },
+        ),
+        ConvTranspose2d: LayerFunction(
+            "ConvTranspose2d",
+            {
+                "in_channels": int,
+                "out_channels": int,
+                "kernel_size": [int],
+                "stride": [int],
+                "padding": [int],
+                "output_padding": [int],
+                "dilation": [int],
+                "bias": bool,
+            },
+        ),
+        Flatten: LayerFunction("Flatten", {}),
+        Linear: LayerFunction("Linear", {"in_features": int, "out_features": int, "bias": bool}),
+        MaxPool2d: LayerFunction(
+            "MaxPool2d",
+            {"kernel_size": int, "stride": int, "padding": int, "dilation": int, "ceil_mode": bool},
+        ),
     }
 
     activation_functions = {
-        LeakyReLU: Function('LeakyReLU', {
-            'negative_slope': float
-        }),
-        LogSigmoid: Function('LogSigmoid', {}),
-        LogSoftmax: Function('LogSoftmax', {
-            'dim': int
-        }),
-        ReLU: Function('ReLU', {}),
-        Sigmoid: Function('Sigmoid', {}),
-        Softmax: Function('Softmax', {
-            'dim': int
-        }),
-        Tanh: Function('Tanh', {})
+        LeakyReLU: Function("LeakyReLU", {"negative_slope": float}),
+        LogSigmoid: Function("LogSigmoid", {}),
+        LogSoftmax: Function("LogSoftmax", {"dim": int}),
+        ReLU: Function("ReLU", {}),
+        Sigmoid: Function("Sigmoid", {}),
+        Softmax: Function("Softmax", {"dim": int}),
+        Tanh: Function("Tanh", {}),
     }
 
     loss_functions = {
-        BCELoss: Function('BCELoss', {}),
-        CrossEntropyLoss: Function('CrossEntropyLoss', {}),
-        MSELoss: Function('MSELoss', {}),
-        NLLLoss: Function('NLLLoss', {})
+        BCELoss: Function("BCELoss", {}),
+        CrossEntropyLoss: Function("CrossEntropyLoss", {}),
+        MSELoss: Function("MSELoss", {}),
+        NLLLoss: Function("NLLLoss", {}),
     }
 
-    optimizers = {
-        AnalogSGD: Function('AnalogSGD', {
-            'lr': float
-        })
-    }
+    optimizers = {AnalogSGD: Function("AnalogSGD", {"lr": float})}
 
     presets = {
-        InferenceRPUConfig: 'InferenceRPUConfig',
-        OldWebComposerInferenceRPUConfig: 'OldWebComposerInferenceRPUConfig',
-        WebComposerInferenceRPUConfig: 'WebComposerInferenceRPUConfig',
+        InferenceRPUConfig: "InferenceRPUConfig",
+        OldWebComposerInferenceRPUConfig: "OldWebComposerInferenceRPUConfig",
+        WebComposerInferenceRPUConfig: "WebComposerInferenceRPUConfig",
     }
 
 
 def build_inverse_mapping(mapping: Dict) -> Dict:
     """Create the inverse mapping between Python entities and AIHW Composer formats."""
-    return {value if not isinstance(value, Function) else value.id_: key
-            for key, value in mapping.items()}
+    return {
+        value if not isinstance(value, Function) else value.id_: key
+        for key, value in mapping.items()
+    }
 
 
 class InverseMappings:
     """Mappings between AIHW Composer format and Python entities."""
 
     datasets = build_inverse_mapping(Mappings.datasets)
     layers = build_inverse_mapping(Mappings.layers)
```

### Comparing `aihwkit-0.7.1/src/aihwkit/cloud/converter/v1/inferencing.py` & `aihwkit-0.8.0/src/aihwkit/cloud/converter/v1/inferencing.py`

 * *Files 16% similar despite different names*

```diff
@@ -6,80 +6,72 @@
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
 # that they have been altered from the originals.
 
+# pylint: disable=no-name-in-module, import-error
+
 """Converters for `BasicInferencing` Experiment."""
 
 from typing import Any, Dict, List
-# from collections import OrderedDict
-
 from torch.nn import Module, NLLLoss
 
 from aihwkit.simulator.configs import InferenceRPUConfig
 from aihwkit.cloud.converter.exceptions import ConversionError
-from aihwkit.cloud.converter.v1.i_mappings import (
-    InverseMappings, Mappings)
+from aihwkit.cloud.converter.v1.i_mappings import InverseMappings, Mappings
 
-# pylint: disable=no-name-in-module,import-error
-from aihwkit.experiments.experiments.inferencing import (  # type: ignore[import]
-    BasicInferencing
-)
-from aihwkit.cloud.converter.definitions.i_input_file_pb2 import (   # type: ignore[attr-defined]
+from aihwkit.experiments.experiments.inferencing import BasicInferencing  # type: ignore[import]
+from aihwkit.cloud.converter.definitions.i_input_file_pb2 import (  # type: ignore[attr-defined]
     InferenceInput,
     Dataset,
     Inferencing,
     NoiseModelProto,
-    PCMProto, GenericProto,
-    AnalogProto
+    PCMProto,
+    GenericProto,
+    AnalogProto,
 )
 
-from aihwkit.cloud.converter.definitions.i_common_pb2 import (   # type: ignore[attr-defined]
+from aihwkit.cloud.converter.definitions.i_common_pb2 import (  # type: ignore[attr-defined]
     LayerOrActivationFunction,
     Network,
     LayerProto,
     ActivationFunctionProto,
-    Version
+    Version,
 )
 from aihwkit.cloud.converter.definitions.i_output_file_pb2 import (  # type: ignore[attr-defined]
-    InferenceRunsProto, InferenceResultsProto, InferencingOutput
+    InferenceRunsProto,
+    InferenceResultsProto,
+    InferencingOutput,
 )
 
 from aihwkit.nn import AnalogSequential
 
 from aihwkit.cloud.converter.v1.analog_info import AnalogInfo
 from aihwkit.cloud.converter.v1.noise_model_info import NoiseModelInfo
 from aihwkit.cloud.converter.v1.rpu_config_info import RPUconfigInfo
 
 
 class BasicInferencingConverter:
     """Converter for `BasicInferencing` Experiment."""
 
-    def to_proto(self, experiment: BasicInferencing,
-                 analog_info: Dict,
-                 noise_model_info: Dict) -> Any:
+    def to_proto(
+        self, experiment: BasicInferencing, analog_info: Dict, noise_model_info: Dict
+    ) -> Any:
         """Convert an `Experiment` to its protobuf representation."""
 
         version = self._version_to_proto()
-        dataset = self._dataset_to_proto(experiment.dataset,
-                                         experiment.batch_size)
-        network = self._model_to_proto(experiment.model,
-                                       experiment.weight_template_id)
-        inferencing = self._inferencing_to_proto(experiment.inference_repeats,
-                                                 experiment.inference_time,
-                                                 analog_info,
-                                                 noise_model_info,
-                                                 )
+        dataset = self._dataset_to_proto(experiment.dataset, experiment.batch_size)
+        network = self._model_to_proto(experiment.model, experiment.weight_template_id)
+        inferencing = self._inferencing_to_proto(
+            experiment.inference_repeats, experiment.inference_time, analog_info, noise_model_info
+        )
         return InferenceInput(
-            version=version,
-            dataset=dataset,
-            network=network,
-            inferencing=inferencing
+            version=version, dataset=dataset, network=network, inferencing=inferencing
         )
 
     def from_proto(self, protobuf: Any) -> BasicInferencing:
         """Convert a protobuf representation to an `Experiment`."""
 
         dataset = InverseMappings.datasets[protobuf.dataset.dataset_id]
         layers = protobuf.network.layers
@@ -89,357 +81,340 @@
         rc_info = RPUconfigInfo(nm_info, alog_info, layers)
 
         model = self._model_from_proto(protobuf.network, rc_info)
 
         batch_size = protobuf.dataset.batch_size
         inference_info = self._inference_info_from_proto(protobuf)
         loss_function = NLLLoss
-        weight_template_id = inference_info['weight_template_id']
-        inference_repeats = inference_info['inference_repeats']
-        inference_time = inference_info['inference_time']
+        weight_template_id = inference_info["weight_template_id"]
+        inference_repeats = inference_info["inference_repeats"]
+        inference_time = inference_info["inference_time"]
 
         return BasicInferencing(
             dataset=dataset,
             model=model,
             batch_size=batch_size,
             loss_function=loss_function,
             weight_template_id=weight_template_id,
             inference_repeats=inference_repeats,
-            inference_time=inference_time
+            inference_time=inference_time,
         )
 
     # Methods for converting to proto.
 
     @staticmethod
     def _version_to_proto() -> Any:
-        return Version(
-            schema=1,
-            opset=1
-        )
+        return Version(schema=1, opset=1)
 
     @staticmethod
     def _dataset_to_proto(dataset: type, batch_size: int) -> Any:
         if dataset not in Mappings.datasets:
-            raise ConversionError(f'Unsupported dataset: {dataset}')
+            raise ConversionError(f"Unsupported dataset: {dataset}")
 
-        return Dataset(
-            dataset_id=Mappings.datasets[dataset],
-            batch_size=batch_size
-        )
+        return Dataset(dataset_id=Mappings.datasets[dataset], batch_size=batch_size)
 
     @staticmethod
     def _model_to_proto(model: Module, weight_template_id: str) -> Any:
-
         if not isinstance(model, AnalogSequential):
-            raise ConversionError('Unsupported model: only AnalogSequential is supported')
+            raise ConversionError("Unsupported model: only AnalogSequential is supported")
 
         children_types = {type(layer) for layer in model.children()}
         valid_types = set(Mappings.layers.keys()) | set(Mappings.activation_functions.keys())
         if children_types - valid_types:
-            raise ConversionError('Unsupported layers: '
-                                  f'{children_types - valid_types}')
+            raise ConversionError("Unsupported layers: " f"{children_types - valid_types}")
 
         # Create a new input_file pb Network object with weight_template_id
         network = Network(weight_template_id=weight_template_id)
 
         for child in model.children():
             child_type = type(child)
             if child_type in Mappings.layers:
                 item = LayerOrActivationFunction(
                     layer=Mappings.layers[child_type].to_proto(child, LayerProto)
                 )
             else:
                 item = LayerOrActivationFunction(
                     activation_function=Mappings.activation_functions[child_type].to_proto(
-                        child, ActivationFunctionProto)
+                        child, ActivationFunctionProto
+                    )
                 )
             # pylint: disable=no-member),undefined-variable
             network.layers.extend([item])
 
         return network
 
     @staticmethod
     def _noise_model_to_proto(
-            noise_model_info: Dict) -> NoiseModelProto:  # type: ignore[valid-type]
+        noise_model_info: Dict,
+    ) -> NoiseModelProto:  # type: ignore[valid-type]
         """Creates a protobuf NoiseModelProto object from input dictionaries"""
 
         model = None
-        device_id = noise_model_info['device_id']
-        if device_id == 'pcm':
+        device_id = noise_model_info["device_id"]
+        if device_id == "pcm":
             model = NoiseModelProto(
                 pcm=PCMProto(
                     device_id=device_id,
-                    programming_noise_scale=noise_model_info['programming_noise_scale'],
-                    read_noise_scale=noise_model_info['read_noise_scale'],
-                    drift_scale=noise_model_info['drift_scale'],
-                    drift_compensation=noise_model_info['drift_compensation'],
-                    poly_first_order_coef=noise_model_info['poly_first_order_coef'],
-                    poly_second_order_coef=noise_model_info['poly_second_order_coef'],
-                    poly_constant_coef=noise_model_info['poly_constant_coef']
-                    )
+                    programming_noise_scale=noise_model_info["programming_noise_scale"],
+                    read_noise_scale=noise_model_info["read_noise_scale"],
+                    drift_scale=noise_model_info["drift_scale"],
+                    drift_compensation=noise_model_info["drift_compensation"],
+                    poly_first_order_coef=noise_model_info["poly_first_order_coef"],
+                    poly_second_order_coef=noise_model_info["poly_second_order_coef"],
+                    poly_constant_coef=noise_model_info["poly_constant_coef"],
                 )
+            )
         else:
             model = NoiseModelProto(
                 generic=GenericProto(
                     device_id=device_id,
-                    programming_noise_scale=noise_model_info['programming_noise_scale'],
-                    read_noise_scale=noise_model_info['read_noise_scale'],
-                    drift_scale=noise_model_info['drift_scale'],
-                    drift_compensation=noise_model_info['drift_compensation'],
-                    poly_first_order_coef=noise_model_info['poly_first_order_coef'],
-                    poly_second_order_coef=noise_model_info['poly_second_order_coef'],
-                    poly_constant_coef=noise_model_info['poly_constant_coef'],
-                    drift_mean=noise_model_info['drift_mean'],
-                    drift_std=noise_model_info['drift_std'],
-                    )
+                    programming_noise_scale=noise_model_info["programming_noise_scale"],
+                    read_noise_scale=noise_model_info["read_noise_scale"],
+                    drift_scale=noise_model_info["drift_scale"],
+                    drift_compensation=noise_model_info["drift_compensation"],
+                    poly_first_order_coef=noise_model_info["poly_first_order_coef"],
+                    poly_second_order_coef=noise_model_info["poly_second_order_coef"],
+                    poly_constant_coef=noise_model_info["poly_constant_coef"],
+                    drift_mean=noise_model_info["drift_mean"],
+                    drift_std=noise_model_info["drift_std"],
+                )
             )
         return model
 
     @staticmethod
-    def rpu_config_info_from_info(analog_info: Dict,
-                                  noise_model_info: Dict
-                                  ) -> RPUconfigInfo:
-        """ Creates RPUconfigInfo """
+    def rpu_config_info_from_info(analog_info: Dict, noise_model_info: Dict) -> RPUconfigInfo:
+        """Creates RPUconfigInfo"""
 
         # TODO: Remove the "Info" objects: NoiseModelInfo, AnalogInfo. Seems unecessary
-        nm_info = NoiseModelInfo(BasicInferencingConverter._noise_model_to_proto(
-            noise_model_info))  # type: ignore[name-defined]
+        nm_info = NoiseModelInfo(
+            BasicInferencingConverter._noise_model_to_proto(noise_model_info)
+        )  # type: ignore[name-defined]
         a_info = AnalogInfo(AnalogProto(**analog_info))
         return RPUconfigInfo(nm_info, a_info, None)
 
     @staticmethod
-    def rpu_config_from_info(analog_info: Dict,
-                             noise_model_info: Dict,
-                             func_id: str = 'id-not-provided'
-                             ) -> InferenceRPUConfig:
-        """ Creates RPUConfig """
-        nm_info = NoiseModelInfo(BasicInferencingConverter._noise_model_to_proto(
-            noise_model_info))  # type: ignore[name-defined]
+    def rpu_config_from_info(
+        analog_info: Dict, noise_model_info: Dict, func_id: str = "id-not-provided"
+    ) -> InferenceRPUConfig:
+        """Creates RPUConfig"""
+        nm_info = NoiseModelInfo(
+            BasicInferencingConverter._noise_model_to_proto(noise_model_info)
+        )  # type: ignore[name-defined]
         a_info = AnalogInfo(AnalogProto(**analog_info))
-        return RPUconfigInfo(nm_info,
-                             a_info, None).create_inference_rpu_config(func_id)
+        return RPUconfigInfo(nm_info, a_info, None).create_inference_rpu_config(func_id)
 
     @staticmethod
     def _inferencing_to_proto(
-            inference_repeats: int,
-            inference_time: float,
-            analog_info: Dict,
-            noise_model_info: Dict) -> Inferencing:  # type: ignore[valid-type]
+        inference_repeats: int, inference_time: float, analog_info: Dict, noise_model_info: Dict
+    ) -> Inferencing:  # type: ignore[valid-type]
         """Creates protobuf Inferencing object"""
 
         # Not sure why mypy and pylint cannot find following method, python3 can.
         # pylint: disable=undefined-variable
         nm_info = BasicInferencingConverter._noise_model_to_proto(
-            noise_model_info)  # type: ignore[name-defined]
+            noise_model_info
+        )  # type: ignore[name-defined]
         a_info = AnalogProto(**analog_info)
 
-        return Inferencing(inference_repeats=inference_repeats,
-                           inference_time=inference_time,
-                           analog_info=a_info,
-                           noise_model_info=nm_info
-                           )
+        return Inferencing(
+            inference_repeats=inference_repeats,
+            inference_time=inference_time,
+            analog_info=a_info,
+            noise_model_info=nm_info,
+        )
 
     # Methods for converting from proto.
 
     @staticmethod
     def _inference_info_from_proto(
-            inference_pb: InferenceInput) -> Dict:  # type: ignore[valid-type]
+        inference_pb: InferenceInput,
+    ) -> Dict:  # type: ignore[valid-type]
         """Converts inference_info from protobuf to a dictionary"""
 
         inferencing = inference_pb.inferencing  # type: ignore[attr-defined]
         network = inference_pb.network  # type: ignore[attr-defined]
         return {
-                    'inference_repeats': inferencing.inference_repeats,
-                    'inference_time': inferencing.inference_time,
-                    'weight_template_id': network.weight_template_id
-                }
+            "inference_repeats": inferencing.inference_repeats,
+            "inference_time": inferencing.inference_time,
+            "weight_template_id": network.weight_template_id,
+        }
 
     @staticmethod
-    def _model_from_proto(network: Network,  # type: ignore[valid-type]
-                          rc_info: RPUconfigInfo) -> Module:
+    def _model_from_proto(
+        network: Network, rc_info: RPUconfigInfo  # type: ignore[valid-type]
+    ) -> Module:
         layers = []
         for layer_proto in network.layers:  # type: ignore[attr-defined]
-            if layer_proto.WhichOneof('item') == 'layer':
+            if layer_proto.WhichOneof("item") == "layer":
                 layer_cls = InverseMappings.layers[layer_proto.layer.id]
                 layer = Mappings.layers[layer_cls].from_proto(
-                    layer_proto.layer, layer_cls, {'rpu_config': rc_info})
+                    layer_proto.layer, layer_cls, {"rpu_config": rc_info}
+                )
             else:
                 layer_cls = InverseMappings.activation_functions[layer_proto.activation_function.id]
                 layer = Mappings.activation_functions[layer_cls].from_proto(
-                    layer_proto.activation_function, layer_cls)
+                    layer_proto.activation_function, layer_cls
+                )
 
             layers.append(layer)
 
         return AnalogSequential(*layers)
 
     @staticmethod
-    def _analog_info_from_proto(
-            analog_info: AnalogProto) -> Dict:  # type: ignore[valid-type]
+    def _analog_info_from_proto(analog_info: AnalogProto) -> Dict:  # type: ignore[valid-type]
         """Converts from protobuf analog_info to a dictionary"""
 
         return {
-                    'output_noise_strength': (
-                        analog_info.output_noise_strength  # type: ignore[attr-defined]
-                    ),
-                    'adc': analog_info.adc,  # type: ignore[attr-defined]
-                    'dac': analog_info.dac  # type: ignore[attr-defined]
-                }
+            "output_noise_strength": (
+                analog_info.output_noise_strength  # type: ignore[attr-defined]
+            ),
+            "adc": analog_info.adc,  # type: ignore[attr-defined]
+            "dac": analog_info.dac,  # type: ignore[attr-defined]
+        }
 
     @staticmethod
     def _noise_model_from_proto(
-            noise_model_info: NoiseModelProto) -> Dict:  # type: ignore[valid-type]
+        noise_model_info: NoiseModelProto,
+    ) -> Dict:  # type: ignore[valid-type]
         """Converts from protobuf noise_model to a dictionary"""
 
         extra = {}
-        typ = noise_model_info.WhichOneof('item')  # type: ignore[attr-defined]
+        typ = noise_model_info.WhichOneof("item")  # type: ignore[attr-defined]
 
-        if typ == 'pcm':
+        if typ == "pcm":
             # pcm does not have 2 extra fields
             info = noise_model_info.pcm  # type: ignore[attr-defined]
 
-        elif typ == 'generic':
+        elif typ == "generic":
             info = noise_model_info.generic  # type: ignore[attr-defined]
             # There are 2 extra fields in generic
-            extra = {
-                'drift_mean': info.drift_mean,
-                'drift_std': info.drift_std
-            }
+            extra = {"drift_mean": info.drift_mean, "drift_std": info.drift_std}
         else:
             raise TypeError
 
         base = {
-            'device_id': typ,
-            'programming_noise_scale': info.programming_noise_scale,
-            'read_noise_scale': info.read_noise_scale,
-            'drift_scale': info.drift_scale,
-            'drift_compensation': info.drift_compensation,
-            'poly_first_order_coef': info.poly_first_order_coef,
-            'poly_second_order_coef': info.poly_second_order_coef,
-            'poly_constant_coef': info.poly_constant_coef,
+            "device_id": typ,
+            "programming_noise_scale": info.programming_noise_scale,
+            "read_noise_scale": info.read_noise_scale,
+            "drift_scale": info.drift_scale,
+            "drift_compensation": info.drift_compensation,
+            "poly_first_order_coef": info.poly_first_order_coef,
+            "poly_second_order_coef": info.poly_second_order_coef,
+            "poly_constant_coef": info.poly_constant_coef,
         }
 
         # add the extra fields if any in return value
         return {**base, **extra}
 
 
 class BasicInferencingResultConverter:
     """Converter for `BasicInferencing` results."""
 
     def to_proto(self, results: Dict) -> Any:
         """Convert a result to its InferenceOutput object in i_output_file protobuf"""
 
         version = self._version_to_proto()
-        inference_runs = self._runs_to_proto(results['inference_runs'])
+        inference_runs = self._runs_to_proto(results["inference_runs"])
 
-        return InferencingOutput(
-            version=version,
-            inference_runs=inference_runs
-        )
+        return InferencingOutput(version=version, inference_runs=inference_runs)
 
     @staticmethod
     def to_json_from_pb(inference_input: Any) -> Dict:
         """Convert a result to its json representation (inverse of to_proto())"""
 
         # Create an InferenceRunsProto object
         i_runs = inference_input.inference_runs  # type: ignore
 
-        results = []        # this is a list
+        results = []  # this is a list
 
         # loop through all the results and append directly to InferenceRunsProto field
         for result in i_runs.inference_results:
             results.append(
                 {
-                    't_inference': result.t_inference,
-                    'avg_accuracy': result.avg_accuracy,
-                    'std_accuracy': result.std_accuracy,
-                    'avg_error': result.avg_error,
-                    'avg_loss': result.avg_loss
+                    "t_inference": result.t_inference,
+                    "avg_accuracy": result.avg_accuracy,
+                    "std_accuracy": result.std_accuracy,
+                    "avg_error": result.avg_error,
+                    "avg_loss": result.avg_loss,
                 }
             )
 
         # need to add 'inference_runs' dictionary key and value because to_proto() input
         #   contained a leading index.
         inference_runs = {
-            'inference_runs': {
-                'inference_repeat': i_runs.inference_repeat,
-                'is_partial': i_runs.is_partial,
-                'time_elapsed': i_runs.time_elapsed,
-                'inference_results': results
+            "inference_runs": {
+                "inference_repeat": i_runs.inference_repeat,
+                "is_partial": i_runs.is_partial,
+                "time_elapsed": i_runs.time_elapsed,
+                "inference_results": results,
             }
         }
         return inference_runs
 
     @staticmethod
     def result_from_proto(inference_input: Any) -> List[Dict]:
         """Convert a result to its json representation (inverse of to_proto())"""
 
         # Create an InferenceRunsProto object
         i_runs = inference_input.inference_runs  # type: ignore
 
-        results = []        # this is a list
+        results = []  # this is a list
 
         # loop through all the results and append directly to InferenceRunsProto field
         for result in i_runs.inference_results:
             results.append(
                 {
-                    't_inference': result.t_inference,
-                    'avg_accuracy': result.avg_accuracy,
-                    'std_accuracy': result.std_accuracy,
-                    'avg_error': result.avg_error,
-                    'avg_loss': result.avg_loss
+                    "t_inference": result.t_inference,
+                    "avg_accuracy": result.avg_accuracy,
+                    "std_accuracy": result.std_accuracy,
+                    "avg_error": result.avg_error,
+                    "avg_loss": result.avg_loss,
                 }
             )
 
         return results
 
     @staticmethod
     def to_json(results: Dict) -> Dict:
         """Convert a result to its json representation."""
 
         # concatenate the results dict to a static one
-        return dict({
-            'version': {
-                'schema': 1,
-                'opset': 1
-            }}, **results)
+        return dict({"version": {"schema": 1, "opset": 1}}, **results)
 
     # Methods for converting to proto.
 
     @staticmethod
     def _version_to_proto() -> Dict:
-        return Version(
-            schema=1,
-            opset=1
-        )
+        return Version(schema=1, opset=1)
 
     @staticmethod
     def _runs_to_proto(results: Dict) -> Any:
         """converts results dictionary to protobuf InferenceRunsProto object"""
 
         # There are 4 fields in InferenceRunsProto, 3 are scalar
-        inference_repeat = results['inference_repeat']
-        is_partial = results['is_partial']
-        time_elapsed = results['time_elapsed']
+        inference_repeat = results["inference_repeat"]
+        is_partial = results["is_partial"]
+        time_elapsed = results["time_elapsed"]
 
         # Create object with constructor specifying the scalar values only
         irp = InferenceRunsProto(
-                inference_repeat=inference_repeat,
-                is_partial=is_partial,
-                time_elapsed=time_elapsed)
+            inference_repeat=inference_repeat, is_partial=is_partial, time_elapsed=time_elapsed
+        )
 
         # inference_results field is an array in protobuf and a list of
         #    dictionaries in the passed results
 
         # Build the InferenceResultsProto objects by appending each
         #    to the InferenceRunsProto object field
-        i_results = results['inference_results']
+        i_results = results["inference_results"]
         for result in i_results:
-            irp.inference_results.append(       # pylint: disable=no-member
-               InferenceResultsProto(
-                   t_inference=result['t_inference'],
-                   avg_accuracy=result['avg_accuracy'],
-                   std_accuracy=result['std_accuracy'],
-                   avg_error=result['avg_error'],
-                   avg_loss=result['avg_loss']
-               ))
+            irp.inference_results.append(  # pylint: disable=no-member
+                InferenceResultsProto(
+                    t_inference=result["t_inference"],
+                    avg_accuracy=result["avg_accuracy"],
+                    std_accuracy=result["std_accuracy"],
+                    avg_error=result["avg_error"],
+                    avg_loss=result["avg_loss"],
+                )
+            )
 
         return irp
```

### Comparing `aihwkit-0.7.1/src/aihwkit/cloud/converter/v1/mappings.py` & `aihwkit-0.8.0/src/aihwkit/cloud/converter/v1/mappings.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -14,63 +14,87 @@
 
 """Mappings for version 1 of the AIHW Composer format."""
 
 from collections import namedtuple
 from typing import Any, Dict
 
 from torch.nn import (
-    BCELoss, BatchNorm2d, Conv2d, ConvTranspose2d, CrossEntropyLoss, Flatten,
-    LeakyReLU, Linear, LogSigmoid, LogSoftmax, MSELoss, MaxPool2d, NLLLoss,
-    ReLU, Sigmoid, Softmax, Tanh
+    BCELoss,
+    BatchNorm2d,
+    Conv2d,
+    ConvTranspose2d,
+    CrossEntropyLoss,
+    Flatten,
+    LeakyReLU,
+    Linear,
+    LogSigmoid,
+    LogSoftmax,
+    MSELoss,
+    MaxPool2d,
+    NLLLoss,
+    ReLU,
+    Sigmoid,
+    Softmax,
+    Tanh,
 )
 from torchvision.datasets import FashionMNIST, SVHN
 
 from aihwkit.cloud.converter.definitions.onnx_common_pb2 import (  # type: ignore[attr-defined]
-    AttributeProto
+    AttributeProto,
 )
 from aihwkit.cloud.converter.exceptions import ConversionError
 from aihwkit.nn import AnalogConv2d, AnalogLinear
 from aihwkit.optim import AnalogSGD
 from aihwkit.simulator.presets import (
-    CapacitorPreset, EcRamPreset, IdealizedPreset, ReRamESPreset, ReRamSBPreset,
+    CapacitorPreset,
+    EcRamPreset,
+    IdealizedPreset,
+    ReRamESPreset,
+    ReRamSBPreset,
     PCMPreset,
-    MixedPrecisionCapacitorPreset, MixedPrecisionEcRamPreset,
-    MixedPrecisionIdealizedPreset, MixedPrecisionPCMPreset,
-    MixedPrecisionReRamESPreset, MixedPrecisionReRamSBPreset,
-    TikiTakaCapacitorPreset, TikiTakaEcRamPreset, TikiTakaIdealizedPreset,
-    TikiTakaReRamESPreset, TikiTakaReRamSBPreset
+    MixedPrecisionCapacitorPreset,
+    MixedPrecisionEcRamPreset,
+    MixedPrecisionIdealizedPreset,
+    MixedPrecisionPCMPreset,
+    MixedPrecisionReRamESPreset,
+    MixedPrecisionReRamSBPreset,
+    TikiTakaCapacitorPreset,
+    TikiTakaEcRamPreset,
+    TikiTakaIdealizedPreset,
+    TikiTakaReRamESPreset,
+    TikiTakaReRamSBPreset,
 )
 
-Type = namedtuple('Type', ['attribute_type', 'field', 'fn'])
+Type = namedtuple("Type", ["attribute_type", "field", "fn"])
 
 TYPES = {
-    int: Type(AttributeProto.AttributeType.INT, 'i', lambda x: x),  # type: ignore
-    bool: Type(AttributeProto.AttributeType.BOOL, 'b', lambda x: x),  # type: ignore
-    str: Type(AttributeProto.AttributeType.STRING, 's',  # type: ignore
-              lambda x: x.encode('utf-8')),
-    float: Type(AttributeProto.AttributeType.FLOAT, 'f', lambda x: x)  # type: ignore
+    int: Type(AttributeProto.AttributeType.INT, "i", lambda x: x),  # type: ignore
+    bool: Type(AttributeProto.AttributeType.BOOL, "b", lambda x: x),  # type: ignore
+    str: Type(
+        AttributeProto.AttributeType.STRING, "s", lambda x: x.encode("utf-8")  # type: ignore
+    ),
+    float: Type(AttributeProto.AttributeType.FLOAT, "f", lambda x: x),  # type: ignore
 }
 
 TYPES_LISTS = {
-    int: Type(AttributeProto.AttributeType.INTS, 'ints', lambda x: x),  # type: ignore
-    bool: Type(AttributeProto.AttributeType.BOOLS, 'bools', lambda x: x),  # type: ignore
-    str: Type(AttributeProto.AttributeType.STRINGS, 'strings',  # type: ignore
-              lambda x: [y.encode('utf-8') for y in x]),
-    float: Type(AttributeProto.AttributeType.FLOATS, 'floats', lambda x: x)  # type: ignore
+    int: Type(AttributeProto.AttributeType.INTS, "ints", lambda x: x),  # type: ignore
+    bool: Type(AttributeProto.AttributeType.BOOLS, "bools", lambda x: x),  # type: ignore
+    str: Type(
+        AttributeProto.AttributeType.STRINGS,
+        "strings",  # type: ignore
+        lambda x: [y.encode("utf-8") for y in x],
+    ),
+    float: Type(AttributeProto.AttributeType.FLOATS, "floats", lambda x: x),  # type: ignore
 }
 
 
 class Function:
     """Mapping for a function-like entity."""
 
-    def __init__(
-            self,
-            id_: str,
-            args: Dict
-    ):
+    def __init__(self, id_: str, args: Dict):
         self.id_ = id_
         self.args = args
 
     def to_proto(self, source: object, proto_cls: type) -> object:
         """Convert a source object into a destination object."""
         instance = proto_cls(id=self.id_)
 
@@ -109,14 +133,21 @@
                 proto_type = TYPES[type_]
 
             new_argument = self.get_argument_from_proto(argument, proto_type.field, None)
             if isinstance(type_, list):
                 new_argument[argument.name] = list(new_argument[argument.name])
             kwargs.update(new_argument)
 
+        # handle the weights_scaling_omega legacy
+        weight_scaling_omega = kwargs.pop("weight_scaling_omega", None)
+        if weight_scaling_omega is not None:
+            if "rpu_config" not in kwargs or not hasattr(kwargs["rpu_config"], "mapping"):
+                raise ConversionError("Expect Mappable RPUConfig")
+            kwargs["rpu_config"].mapping.weights_scaling_omega = weight_scaling_omega
+
         return cls(**kwargs)
 
     def get_field_value_to_proto(self, source: Any, field: str, default: Any = None) -> Any:
         """Get the value of a field."""
         return getattr(source, field, default)
 
     def get_argument_from_proto(self, source: Any, field: str, default: Any = None) -> Dict:
@@ -125,167 +156,163 @@
 
 
 class LayerFunction(Function):
     """Mapping for a function-like entity (Layer)."""
 
     def get_field_value_to_proto(self, source: Any, field: str, default: Any = None) -> Any:
         """Get the value of a field."""
-        if field == 'bias':
-            return getattr(source, 'bias', None) is not None
+        if field == "bias":
+            return getattr(source, "bias", None) is not None
 
-        if field == 'weight_scaling_omega':
-            return list(source.analog_tiles())[0].rpu_config.mapping.weight_scaling_omega
+        if field == "weight_scaling_omega":
+            return next(source.analog_tiles()).rpu_config.mapping.weight_scaling_omega
 
-        if field == 'rpu_config':
-            preset_cls = type(source.analog_tile.rpu_config)
+        if field == "rpu_config":
+            preset_cls = type(next(source.analog_tiles()).rpu_config)
             try:
                 return Mappings.presets[preset_cls]
             except KeyError as ex:
-                raise ConversionError('Invalid rpu_config in layer: {} not '
-                                      'among the presets'.format(preset_cls)) from ex
+                raise ConversionError(
+                    "Invalid rpu_config in layer: {} not among the presets".format(preset_cls)
+                ) from ex
 
         return super().get_field_value_to_proto(source, field, default)
 
     def get_argument_from_proto(self, source: Any, field: str, default: Any = None) -> Dict:
         """Get the value of an argument."""
-        if source.name == 'rpu_config':
-            preset_str = getattr(source, field, default).decode('utf-8')
+        if source.name == "rpu_config":
+            preset_str = getattr(source, field, default).decode("utf-8")
             try:
                 preset = InverseMappings.presets[preset_str]
             except KeyError as ex:
-                raise ConversionError('Invalid rpu_config in layer: {} not '
-                                      'among the presets'.format(preset_str)) from ex
-            return {'rpu_config': preset()}
+                raise ConversionError(
+                    "Invalid rpu_config in layer: {} not among the presets".format(preset_str)
+                ) from ex
+            return {"rpu_config": preset()}
 
         return super().get_argument_from_proto(source, field, default)
 
 
 class Mappings:
     """Mappings between Python entities and AIHW format."""
+
     # pylint: disable=too-few-public-methods
 
-    datasets = {
-        FashionMNIST: 'fashion_mnist',
-        SVHN: 'svhn'
-    }
+    datasets = {FashionMNIST: "fashion_mnist", SVHN: "svhn"}
 
     layers = {
-        AnalogConv2d: LayerFunction('AnalogConv2d', {
-            'in_channels': int,
-            'out_channels': int,
-            'kernel_size': [int],
-            'stride': [int],
-            'padding': [int],
-            'dilation': [int],
-            'bias': bool,
-            'rpu_config': str,
-            'weight_scaling_omega': float
-        }),
-        AnalogLinear: LayerFunction('AnalogLinear', {
-            'in_features': int,
-            'out_features': int,
-            'bias': bool,
-            'rpu_config': str,
-            'weight_scaling_omega': float
-        }),
-        BatchNorm2d: LayerFunction('BatchNorm2d', {
-            'num_features': int
-        }),
-        Conv2d: LayerFunction('Conv2d', {
-            'in_channels': int,
-            'out_channels': int,
-            'kernel_size': [int],
-            'stride': [int],
-            'padding': [int],
-            'dilation': [int],
-            'bias': bool
-        }),
-        ConvTranspose2d: LayerFunction('ConvTranspose2d', {
-            'in_channels': int,
-            'out_channels': int,
-            'kernel_size': [int],
-            'stride': [int],
-            'padding': [int],
-            'output_padding': [int],
-            'dilation': [int],
-            'bias': bool
-        }),
-        Flatten: LayerFunction('Flatten', {}),
-        Linear: LayerFunction('Linear', {
-            'in_features': int,
-            'out_features': int,
-            'bias': bool
-        }),
-        MaxPool2d: LayerFunction('MaxPool2d', {
-            'kernel_size': int,
-            'stride': int,
-            'padding': int,
-            'dilation': int,
-            'ceil_mode': bool
-        })
+        AnalogConv2d: LayerFunction(
+            "AnalogConv2d",
+            {
+                "in_channels": int,
+                "out_channels": int,
+                "kernel_size": [int],
+                "stride": [int],
+                "padding": [int],
+                "dilation": [int],
+                "bias": bool,
+                "rpu_config": str,
+                "weight_scaling_omega": float,
+            },
+        ),
+        AnalogLinear: LayerFunction(
+            "AnalogLinear",
+            {
+                "in_features": int,
+                "out_features": int,
+                "bias": bool,
+                "rpu_config": str,
+                "weight_scaling_omega": float,
+            },
+        ),
+        BatchNorm2d: LayerFunction("BatchNorm2d", {"num_features": int}),
+        Conv2d: LayerFunction(
+            "Conv2d",
+            {
+                "in_channels": int,
+                "out_channels": int,
+                "kernel_size": [int],
+                "stride": [int],
+                "padding": [int],
+                "dilation": [int],
+                "bias": bool,
+            },
+        ),
+        ConvTranspose2d: LayerFunction(
+            "ConvTranspose2d",
+            {
+                "in_channels": int,
+                "out_channels": int,
+                "kernel_size": [int],
+                "stride": [int],
+                "padding": [int],
+                "output_padding": [int],
+                "dilation": [int],
+                "bias": bool,
+            },
+        ),
+        Flatten: LayerFunction("Flatten", {}),
+        Linear: LayerFunction("Linear", {"in_features": int, "out_features": int, "bias": bool}),
+        MaxPool2d: LayerFunction(
+            "MaxPool2d",
+            {"kernel_size": int, "stride": int, "padding": int, "dilation": int, "ceil_mode": bool},
+        ),
     }
 
     activation_functions = {
-        LeakyReLU: Function('LeakyReLU', {
-            'negative_slope': float
-        }),
-        LogSigmoid: Function('LogSigmoid', {}),
-        LogSoftmax: Function('LogSoftmax', {
-            'dim': int
-        }),
-        ReLU: Function('ReLU', {}),
-        Sigmoid: Function('Sigmoid', {}),
-        Softmax: Function('Softmax', {
-            'dim': int
-        }),
-        Tanh: Function('Tanh', {})
+        LeakyReLU: Function("LeakyReLU", {"negative_slope": float}),
+        LogSigmoid: Function("LogSigmoid", {}),
+        LogSoftmax: Function("LogSoftmax", {"dim": int}),
+        ReLU: Function("ReLU", {}),
+        Sigmoid: Function("Sigmoid", {}),
+        Softmax: Function("Softmax", {"dim": int}),
+        Tanh: Function("Tanh", {}),
     }
 
     loss_functions = {
-        BCELoss: Function('BCELoss', {}),
-        CrossEntropyLoss: Function('CrossEntropyLoss', {}),
-        MSELoss: Function('MSELoss', {}),
-        NLLLoss: Function('NLLLoss', {})
+        BCELoss: Function("BCELoss", {}),
+        CrossEntropyLoss: Function("CrossEntropyLoss", {}),
+        MSELoss: Function("MSELoss", {}),
+        NLLLoss: Function("NLLLoss", {}),
     }
 
-    optimizers = {
-        AnalogSGD: Function('AnalogSGD', {
-            'lr': float
-        })
-    }
+    optimizers = {AnalogSGD: Function("AnalogSGD", {"lr": float})}
 
     presets = {
-        ReRamESPreset: 'ReRamESPreset',
-        ReRamSBPreset: 'ReRamSBPreset',
-        CapacitorPreset: 'CapacitorPreset',
-        EcRamPreset: 'EcRamPreset',
-        IdealizedPreset: 'IdealizedPreset',
-        PCMPreset: 'PCMPreset',
-        MixedPrecisionReRamESPreset: 'MixedPrecisionReRamESPreset',
-        MixedPrecisionReRamSBPreset: 'MixedPrecisionReRamSBPreset',
-        MixedPrecisionCapacitorPreset: 'MixedPrecisionCapacitorPreset',
-        MixedPrecisionEcRamPreset: 'MixedPrecisionEcRamPreset',
-        MixedPrecisionIdealizedPreset: 'MixedPrecisionIdealizedPreset',
-        MixedPrecisionPCMPreset: 'MixedPrecisionPCMPreset',
-        TikiTakaReRamESPreset: 'TikiTakaReRamESPreset',
-        TikiTakaReRamSBPreset: 'TikiTakaReRamSBPreset',
-        TikiTakaCapacitorPreset: 'TikiTakaCapacitorPreset',
-        TikiTakaEcRamPreset: 'TikiTakaEcRamPreset',
-        TikiTakaIdealizedPreset: 'TikiTakaIdealizedPreset',
+        ReRamESPreset: "ReRamESPreset",
+        ReRamSBPreset: "ReRamSBPreset",
+        CapacitorPreset: "CapacitorPreset",
+        EcRamPreset: "EcRamPreset",
+        IdealizedPreset: "IdealizedPreset",
+        PCMPreset: "PCMPreset",
+        MixedPrecisionReRamESPreset: "MixedPrecisionReRamESPreset",
+        MixedPrecisionReRamSBPreset: "MixedPrecisionReRamSBPreset",
+        MixedPrecisionCapacitorPreset: "MixedPrecisionCapacitorPreset",
+        MixedPrecisionEcRamPreset: "MixedPrecisionEcRamPreset",
+        MixedPrecisionIdealizedPreset: "MixedPrecisionIdealizedPreset",
+        MixedPrecisionPCMPreset: "MixedPrecisionPCMPreset",
+        TikiTakaReRamESPreset: "TikiTakaReRamESPreset",
+        TikiTakaReRamSBPreset: "TikiTakaReRamSBPreset",
+        TikiTakaCapacitorPreset: "TikiTakaCapacitorPreset",
+        TikiTakaEcRamPreset: "TikiTakaEcRamPreset",
+        TikiTakaIdealizedPreset: "TikiTakaIdealizedPreset",
     }
 
 
 def build_inverse_mapping(mapping: Dict) -> Dict:
     """Create the inverse mapping between Python entities and AIHW Composer formats."""
-    return {value if not isinstance(value, Function) else value.id_: key
-            for key, value in mapping.items()}
+    return {
+        value if not isinstance(value, Function) else value.id_: key
+        for key, value in mapping.items()
+    }
 
 
 class InverseMappings:
     """Mappings between AIHW Composer format and Python entities."""
+
     # pylint: disable=too-few-public-methods
 
     datasets = build_inverse_mapping(Mappings.datasets)
     layers = build_inverse_mapping(Mappings.layers)
     activation_functions = build_inverse_mapping(Mappings.activation_functions)
     loss_functions = build_inverse_mapping(Mappings.loss_functions)
     optimizers = build_inverse_mapping(Mappings.optimizers)
```

### Comparing `aihwkit-0.7.1/src/aihwkit/cloud/converter/v1/noise_model_info.py` & `aihwkit-0.8.0/src/aihwkit/cloud/converter/v1/noise_model_info.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,41 +1,41 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
 # that they have been altered from the originals.
 
 """Noise model info in rpu_config to neural network model"""
 
 # pylint: disable=no-name-in-module,import-error
 from aihwkit.cloud.converter.definitions.i_input_file_pb2 import (  # type: ignore[attr-defined]
-    NoiseModelProto
+    NoiseModelProto,
 )
 
 
 # pylint: disable=too-many-instance-attributes
 class NoiseModelInfo:
     """Data only class for fields from protobuf NoiseModelProto message"""
 
-    PCM = 'pcm'
-    GENERIC = 'generic'
+    PCM = "pcm"
+    GENERIC = "generic"
 
     def __init__(self, nm_proto: NoiseModelProto):  # type: ignore[valid-type]
         """Constructor for this class"""
 
-        type_ = nm_proto.WhichOneof('item')  # type: ignore[attr-defined]
+        type_ = nm_proto.WhichOneof("item")  # type: ignore[attr-defined]
 
         info = None
-        if type_ == 'pcm':
+        if type_ == "pcm":
             # pcm does NOT have 2 extra fields
             info = nm_proto.pcm  # type: ignore[attr-defined]
         else:
             # generic HAS 2 extra fields
             info = nm_proto.generic  # type: ignore[attr-defined]
 
         self.device_id = info.device_id
@@ -54,24 +54,27 @@
         if info.device_id == self.GENERIC:
             self._drift_mean = info.drift_mean
             self._drift_std = info.drift_std
 
     def _assert_generic(self) -> None:
         """Check is device is generic"""
 
-        assert self.device_id == self.GENERIC, \
-            "device_id does not have value '{}'".format(self.GENERIC)
+        assert self.device_id == self.GENERIC, "device_id does not have value '{}'".format(
+            self.GENERIC
+        )
 
     @property
     def drift_mean(self) -> float:
         """Enforce access to drift_mean if this is a generic device"""
 
         self._assert_generic()
         return self._drift_mean
 
     @property
     def drift_std(self) -> float:
         """Enforce access to drift_std if this is a generic device"""
 
         self._assert_generic()
         return self._drift_std
+
+
 # pylint: enable=too-many-instance-attributes
```

### Comparing `aihwkit-0.7.1/src/aihwkit/cloud/converter/v1/rpu_config_info.py` & `aihwkit-0.8.0/src/aihwkit/cloud/converter/v1/rpu_config_info.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -13,206 +13,213 @@
 """Creates InferenceRPUConfig to add to nn model"""
 from typing import Dict, Any
 from collections import OrderedDict
 
 from aihwkit.simulator.configs.configs import InferenceRPUConfig
 from aihwkit.simulator.presets.web import (
     WebComposerInferenceRPUConfig,
-    OldWebComposerInferenceRPUConfig
+    OldWebComposerInferenceRPUConfig,
 )
 from aihwkit.inference.noise.pcm import PCMLikeNoiseModel
 from aihwkit.inference.noise.custom import StateIndependentNoiseModel
 from aihwkit.inference.compensation.drift import GlobalDriftCompensation
 from aihwkit.cloud.converter.v1.analog_info import AnalogInfo
 from aihwkit.cloud.converter.v1.noise_model_info import NoiseModelInfo
+from aihwkit.exceptions import ConfigError
 
 RPU_CLASSES = {
-    'InferenceRPUConfig': InferenceRPUConfig,
-    'WebComposerInferenceRPUConfig': WebComposerInferenceRPUConfig,
-    'OldWebComposerInferenceRPUConfig': OldWebComposerInferenceRPUConfig
+    "InferenceRPUConfig": InferenceRPUConfig,
+    "WebComposerInferenceRPUConfig": WebComposerInferenceRPUConfig,
+    "OldWebComposerInferenceRPUConfig": OldWebComposerInferenceRPUConfig,
 }
 
 
 # pylint: disable=too-few-public-methods
 class NoiseModelDeviceIDException(Exception):
     """Exception raised if noise model device id is not correct"""
 
 
 class RPUconfigInfo:
     """Data only class for RPUConfig fields"""
 
-    def __init__(self, nm_info: NoiseModelInfo,
-                 a_info: AnalogInfo,
-                 layers: Any = None):
+    def __init__(self, nm_info: NoiseModelInfo, a_info: AnalogInfo, layers: Any = None):
         """
         The only constructor for this class
         """
         self._noise_model_info = nm_info
         self._analog_info = a_info
         self._layers = layers
-        self._device_id = ''
+        self._device_id = ""
 
     @staticmethod
     def _get_common_rpucfg_name(layers: Any) -> Any:
         """Set common rpu config name by search all analog layers"""
         # Use default RPU config for Composer
         if layers is None:
-            return 'WebComposerInferenceRPUConfig'
+            return "WebComposerInferenceRPUConfig"
         # Need to loop through protobuf layers and figure out
         #   common rpu_config value.
         names: Dict[str, int] = {}
         # pylint: disable=too-many-nested-blocks
         for layer_proto in layers:  # type: ignore[attr-defined]
-            if layer_proto.WhichOneof('item') == 'layer':
+            if layer_proto.WhichOneof("item") == "layer":
                 layer = layer_proto.layer
-                if layer.id.startswith('Analog'):
+                if layer.id.startswith("Analog"):
                     # Loop though all AttributeProto objecs in layer.arguments
                     for argument in layer.arguments:
-                        if argument.name == 'rpu_config':
+                        if argument.name == "rpu_config":
                             # stored as UTF8 byte string in attribute s
-                            arg_value = getattr(argument, 's')
+                            arg_value = getattr(argument, "s")
                             # update count of this rpu_config in all analog layers
                             if arg_value in names:
                                 names[arg_value] += 1
                             else:
                                 names[arg_value] = 1
         # pylint: enable=too-many-nested-blocks
         # should have exactly on in dictionary 'names'
         if len(names) > 1:
-            print(f'>>> ERROR: more than one rpu_config: {names}')
+            print(f">>> ERROR: more than one rpu_config: {names}")
             return None
         if len(names) == 1:
             # keys() returns dict_keys object, need a list
-            return list(names.keys())[0].decode('UTF-8')  # type: ignore[attr-defined]
-        print('>>> INFO: experiment has not analog layers')
-        return ''
-
-    def _print_rpu_config(
-            self,
-            rpu_config: InferenceRPUConfig,
-            func_id: str,
-            dev_id: str) -> None:
+            return list(names.keys())[0].decode("UTF-8")  # type: ignore[attr-defined]
+        print(">>> INFO: experiment has not analog layers")
+        return ""
+
+    def _print_rpu_config(self, rpu_config: InferenceRPUConfig, func_id: str, dev_id: str) -> None:
         """Pretty-print rpu_config"""
 
         # Create ordered dictionary with field name as key and value as field value
         #   Include ADC and DAC in input/output resolution
         print_order = OrderedDict(
             {
-                ('forward.out_noise', rpu_config.forward.out_noise),
-                ('forward.out_res/adc', '{}/{}'.format(
-                    rpu_config.forward.out_res, self._analog_info.adc)),
-                ('forward.inp_res/dac', '{}/{}'.format(
-                    rpu_config.forward.inp_res, self._analog_info.dac)),
-                ('noise_model', rpu_config.noise_model),
-                ('noise_model.read_noise_scale',
-                    rpu_config.noise_model.read_noise_scale),  # type: ignore[attr-defined]
-                ('noise_model.prog_noise_scale',
-                    rpu_config.noise_model.prog_noise_scale),  # type: ignore[attr-defined]
-                ('drift_compensation',
-                    rpu_config.drift_compensation),  # type: ignore[attr-defined]
-                ('noise_model.drift_scale',
-                    rpu_config.noise_model.drift_scale),  # type: ignore[attr-defined]
+                ("forward.out_noise", rpu_config.forward.out_noise),
+                (
+                    "forward.out_res/adc",
+                    "{}/{}".format(rpu_config.forward.out_res, self._analog_info.adc),
+                ),
+                (
+                    "forward.inp_res/dac",
+                    "{}/{}".format(rpu_config.forward.inp_res, self._analog_info.dac),
+                ),
+                ("noise_model", rpu_config.noise_model),
+                (
+                    "noise_model.read_noise_scale",
+                    rpu_config.noise_model.read_noise_scale,  # type: ignore[attr-defined]
+                ),
+                (
+                    "noise_model.prog_noise_scale",
+                    rpu_config.noise_model.prog_noise_scale,  # type: ignore[attr-defined]
+                ),
+                ("drift_compensation", rpu_config.drift_compensation),  # type: ignore[attr-defined]
+                (
+                    "noise_model.drift_scale",
+                    rpu_config.noise_model.drift_scale,  # type: ignore[attr-defined]
+                ),
             }
-
         )
 
         # add extra fields for GENERIC device
         if dev_id == NoiseModelInfo.GENERIC:
-            print_order['noise_model.drift_nu_mean'] = (
-                rpu_config.noise_model.drift_nu_mean)  # type: ignore[attr-defined]
-            print_order['noise_model.drift_nu_std'] = (
-                rpu_config.noise_model.drift_nu_std)  # type: ignore[attr-defined]
+            print_order[
+                "noise_model.drift_nu_mean"
+            ] = rpu_config.noise_model.drift_nu_mean  # type: ignore[attr-defined]
+            print_order[
+                "noise_model.drift_nu_std"
+            ] = rpu_config.noise_model.drift_nu_std  # type: ignore[attr-defined]
 
-        output = '  rpu_config: function_id={} device_id={}\n'.format(func_id, dev_id)
+        output = "  rpu_config: function_id={} device_id={}\n".format(func_id, dev_id)
         for key in print_order:
-            output += '    {:30}: {}\n'.format(key, print_order[key])
-            if key == 'noise_model' and dev_id == NoiseModelInfo.PCM:
+            output += "    {:30}: {}\n".format(key, print_order[key])
+            if key == "noise_model" and dev_id == NoiseModelInfo.PCM:
                 # need to print out prog_coeff in PCMLikeNoiseModel object
                 for i in range(3):
-                    field_name = 'noise_model.prog_coeff[{}]'.format(i)
-                    output += '    {:30}: {}\n'.format(
+                    field_name = "noise_model.prog_coeff[{}]".format(i)
+                    output += "    {:30}: {}\n".format(
                         field_name,
-                        rpu_config.noise_model.prog_coeff[i]  # type: ignore[attr-defined]
+                        rpu_config.noise_model.prog_coeff[i],  # type: ignore[attr-defined]
                     )
 
-        print('Here is the variable content of the rpu_config:\n{}'.format(output))
+        print("Here is the variable content of the rpu_config:\n{}".format(output))
 
-    def create_inference_rpu_config(self, func_id: str,
-                                    verbose: bool = False) -> InferenceRPUConfig:
+    def create_inference_rpu_config(
+        self, func_id: str, verbose: bool = False
+    ) -> InferenceRPUConfig:
         """Creates a InferenceRPUConfig class using noise and analog info"""
         # Need to find name of 'common-rpu-conf-class-name' in protobuf
         #   This should be the consistent across all layers.
         #   The Composer Validator should have already caught this but
         #   it is checked here for testcases and other unknown environments
         rpu_class_name = self._get_common_rpucfg_name(self._layers)
-        print(f'>>> INFO: rpu_class_name={rpu_class_name}')
+        print(f">>> INFO: rpu_class_name={rpu_class_name}")
         if rpu_class_name is None or len(rpu_class_name) == 0:
-            raise Exception('class name error. see previous messages')
+            raise ConfigError("class name error. see previous messages")
         rpu_config_class = None
         if rpu_class_name in RPU_CLASSES:
             rpu_config_class = RPU_CLASSES[rpu_class_name]
         else:
-            raise Exception(f"rpu class name '{rpu_class_name}' not one of '{RPU_CLASSES.keys()}'")
+            raise ConfigError(
+                f"rpu class name '{rpu_class_name}' not one of '{RPU_CLASSES.keys()}'"
+            )
 
         # Dynamically create the right InferenceRPUConfig class
         rpu_config = rpu_config_class()
         # Assign values from AnalogProto
         rpu_config.forward.out_noise = self._analog_info.output_noise_strength
 
         # changed input/output res to use a formula. print out adc and dac
         rpu_config.forward.out_res = 1.0 / (2**self._analog_info.adc - 2)
         rpu_config.forward.inp_res = 1.0 / (2**self._analog_info.dac - 2)
 
         # Assign values from NoiseModelProto (CM Noise model)
         self._device_id = self._noise_model_info.device_id
         if self._device_id == NoiseModelInfo.PCM:
-            rpu_config.noise_model = (
-                PCMLikeNoiseModel(
-                                g_max=25.0,
-                                prog_coeff=[
-                                    self._noise_model_info.poly_constant_coef,
-                                    self._noise_model_info.poly_first_order_coef,
-                                    self._noise_model_info.poly_second_order_coef,
-                                ]
-                )
+            rpu_config.noise_model = PCMLikeNoiseModel(
+                g_max=25.0,
+                prog_coeff=[
+                    self._noise_model_info.poly_constant_coef,
+                    self._noise_model_info.poly_first_order_coef,
+                    self._noise_model_info.poly_second_order_coef,
+                ],
             )
         elif self._device_id == NoiseModelInfo.GENERIC:
             rpu_config.noise_model = StateIndependentNoiseModel(
-                                g_max=25.0,
-                                prog_coeff=[
-                                    self._noise_model_info.poly_constant_coef,
-                                    self._noise_model_info.poly_first_order_coef,
-                                    self._noise_model_info.poly_second_order_coef,
-                                ]
+                g_max=25.0,
+                prog_coeff=[
+                    self._noise_model_info.poly_constant_coef,
+                    self._noise_model_info.poly_first_order_coef,
+                    self._noise_model_info.poly_second_order_coef,
+                ],
             )
 
             # These are unique to generic
             rpu_config.noise_model.drift_nu_mean = (
                 self._noise_model_info.drift_mean  # type: ignore[attr-defined]
             )
             rpu_config.noise_model.drift_nu_std = (
                 self._noise_model_info.drift_std  # type: ignore[attr-defined]
             )
         else:
             raise NoiseModelDeviceIDException(
-                'invalid noise model device id {}'.format(self._device_id))
+                "invalid noise model device id {}".format(self._device_id)
+            )
 
         # common fields
         rpu_config.noise_model.prog_noise_scale = (  # type: ignore[attr-defined]
             self._noise_model_info.programming_noise_scale
         )
         rpu_config.noise_model.read_noise_scale = (  # type: ignore[attr-defined]
-                self._noise_model_info.read_noise_scale
+            self._noise_model_info.read_noise_scale
         )
 
         # Drift compensation in protobuf is boolean (bool)
         rpu_config.drift_compensation = None  # type: ignore[assignment]
         if self._noise_model_info.drift_compensation:
             rpu_config.drift_compensation = GlobalDriftCompensation()
 
         rpu_config.noise_model.drift_scale = (  # type: ignore[attr-defined]
-                self._noise_model_info.drift_scale
+            self._noise_model_info.drift_scale
         )
 
         if verbose:
             self._print_rpu_config(rpu_config, func_id, self._noise_model_info.device_id)
         return rpu_config
```

### Comparing `aihwkit-0.7.1/src/aihwkit/cloud/converter/v1/training.py` & `aihwkit-0.8.0/src/aihwkit/cloud/converter/v1/training.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -19,45 +19,48 @@
 from torch.nn import Module
 from torch.nn.modules.loss import _Loss
 
 from aihwkit.cloud.converter.exceptions import ConversionError
 from aihwkit.cloud.converter.v1.mappings import InverseMappings, Mappings
 from aihwkit.experiments.experiments.training import BasicTraining
 
-from aihwkit.cloud.converter.definitions.input_file_pb2 import (   # type: ignore[attr-defined]
-    TrainingInput, Dataset, Training
+from aihwkit.cloud.converter.definitions.input_file_pb2 import (  # type: ignore[attr-defined]
+    TrainingInput,
+    Dataset,
+    Training,
 )
-from aihwkit.cloud.converter.definitions.common_pb2 import (    # type: ignore[attr-defined]
-    LayerOrActivationFunction, LossFunctionProto, Network, LayerProto,
-    ActivationFunctionProto, OptimizerProto, Version
+from aihwkit.cloud.converter.definitions.common_pb2 import (  # type: ignore[attr-defined]
+    LayerOrActivationFunction,
+    LossFunctionProto,
+    Network,
+    LayerProto,
+    ActivationFunctionProto,
+    OptimizerProto,
+    Version,
 )
 from aihwkit.cloud.converter.definitions.onnx_common_pb2 import (  # type: ignore[attr-defined]
-    AttributeProto
+    AttributeProto,
 )
 from aihwkit.nn import AnalogSequential
 
 
 class BasicTrainingConverter:
     """Converter for `BasicTraining` Experiment."""
 
     def to_proto(self, experiment: BasicTraining) -> Any:
         """Convert an `Experiment` to its protobuf representation."""
         version = self._version_to_proto()
-        dataset = self._dataset_to_proto(experiment.dataset,
-                                         experiment.batch_size)
+        dataset = self._dataset_to_proto(experiment.dataset, experiment.batch_size)
         network = self._model_to_proto(experiment.model)
-        training = self._training_to_proto(experiment.epochs,
-                                           experiment.learning_rate,
-                                           experiment.loss_function)
+        training = self._training_to_proto(
+            experiment.epochs, experiment.learning_rate, experiment.loss_function
+        )
 
         training_input = TrainingInput(
-            version=version,
-            dataset=dataset,
-            network=network,
-            training=training
+            version=version, dataset=dataset, network=network, training=training
         )
 
         return training_input
 
     def from_proto(self, training_proto: Any) -> Any:
         """Convert a protobuf representation to an `Experiment`."""
         dataset = InverseMappings.datasets[training_proto.dataset.dataset_id]
@@ -69,130 +72,114 @@
 
         return BasicTraining(
             dataset=dataset,
             model=model,
             batch_size=batch_size,
             loss_function=loss_function,
             epochs=epochs,
-            learning_rate=learning_rate
+            learning_rate=learning_rate,
         )
 
     # Methods for converting to proto.
 
     @staticmethod
     def _version_to_proto() -> Any:
-        return Version(
-            schema=1,
-            opset=1
-        )
+        return Version(schema=1, opset=1)
 
     @staticmethod
     def _dataset_to_proto(dataset: type, batch_size: int) -> Any:
         if dataset not in Mappings.datasets:
-            raise ConversionError('Unsupported dataset: {}'.format(dataset))
+            raise ConversionError("Unsupported dataset: {}".format(dataset))
 
-        return Dataset(
-            dataset_id=Mappings.datasets[dataset],
-            batch_size=batch_size
-        )
+        return Dataset(dataset_id=Mappings.datasets[dataset], batch_size=batch_size)
 
     @staticmethod
     def _model_to_proto(model: Module) -> Any:
         if not isinstance(model, AnalogSequential):
-            raise ConversionError('Unsupported model: only AnalogSequential is supported')
+            raise ConversionError("Unsupported model: only AnalogSequential is supported")
 
         children_types = {type(layer) for layer in model.children()}
         valid_types = set(Mappings.layers.keys()) | set(Mappings.activation_functions.keys())
         if children_types - valid_types:
-            raise ConversionError('Unsupported layers: {}'.format(children_types - valid_types))
+            raise ConversionError("Unsupported layers: {}".format(children_types - valid_types))
 
         network = Network()
         for child in model.children():
             child_type = type(child)
             if child_type in Mappings.layers:
                 item = LayerOrActivationFunction(
                     layer=Mappings.layers[child_type].to_proto(child, LayerProto)
                 )
             else:
                 item = LayerOrActivationFunction(
                     activation_function=Mappings.activation_functions[child_type].to_proto(
-                        child, ActivationFunctionProto)
+                        child, ActivationFunctionProto
+                    )
                 )
             network.layers.extend([item])
 
         return network
 
     @staticmethod
-    def _training_to_proto(
-            epochs: int,
-            learning_rate: float,
-            loss_function: _Loss
-    ) -> Any:
+    def _training_to_proto(epochs: int, learning_rate: float, loss_function: _Loss) -> Any:
         if loss_function not in Mappings.loss_functions:
-            raise ConversionError('Unsupported loss function: {}'.format(loss_function))
+            raise ConversionError("Unsupported loss function: {}".format(loss_function))
 
         # Build optimizer manually.
-        optimizer = OptimizerProto(id='AnalogSGD')
+        optimizer = OptimizerProto(id="AnalogSGD")
         optimizer.arguments.append(
             AttributeProto(
-                name='lr',
-                type=AttributeProto.AttributeType.FLOAT,  # type: ignore
-                f=learning_rate))
+                name="lr", type=AttributeProto.AttributeType.FLOAT, f=learning_rate  # type: ignore
+            )
+        )
 
         training = Training(
             epochs=epochs,
             optimizer=optimizer,
             loss_function=Mappings.loss_functions[loss_function].to_proto(
-                loss_function(), LossFunctionProto)
+                loss_function(), LossFunctionProto
+            ),
         )
 
         return training
 
     # Methods for converting from proto.
 
     @staticmethod
     def _model_from_proto(model_proto: Any) -> Module:
         layers = []
         for layer_proto in model_proto.layers:
-            if layer_proto.WhichOneof('item') == 'layer':
+            if layer_proto.WhichOneof("item") == "layer":
                 layer_cls = InverseMappings.layers[layer_proto.layer.id]
-                layer = Mappings.layers[layer_cls].from_proto(
-                    layer_proto.layer, layer_cls)
+                layer = Mappings.layers[layer_cls].from_proto(layer_proto.layer, layer_cls)
             else:
                 layer_cls = InverseMappings.activation_functions[layer_proto.activation_function.id]
                 layer = Mappings.activation_functions[layer_cls].from_proto(
-                    layer_proto.activation_function, layer_cls)
+                    layer_proto.activation_function, layer_cls
+                )
 
             layers.append(layer)
 
         return AnalogSequential(*layers)
 
 
 class BasicTrainingResultConverter:
     """Converter for `BasicTraining` results."""
+
     # pylint: disable=too-few-public-methods
 
     def from_proto(self, results: Any) -> Any:
         """Convert a result to its json representation."""
-        return {
-            'version': {
-                'schema': 1,
-                'opset': 1
-            },
-            'epochs': self._epochs_from_proto(results)
-        }
+        return {"version": {"schema": 1, "opset": 1}, "epochs": self._epochs_from_proto(results)}
 
     # Methods for converting from proto.
     @staticmethod
     def _epochs_from_proto(epochs_proto: Any) -> List[Dict]:
         epochs = []
         for epoch in epochs_proto.epochs:
-            epoch_dict = {
-                'epoch': epoch.epoch,
-                'metrics': {}
-            }
+            epoch_dict = {"epoch": epoch.epoch, "metrics": {}}
             for metric in epoch.metrics:
-                epoch_dict['metrics'][metric.name] = metric.f
+                epoch_dict["metrics"][metric.name] = metric.f
 
             epochs.append(epoch_dict)
 
         return epochs
```

### Comparing `aihwkit-0.7.1/src/aihwkit/exceptions.py` & `aihwkit-0.8.0/src/aihwkit/exceptions.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -21,21 +21,33 @@
     """Exceptions related to analog neural network modules."""
 
 
 class TileError(AihwkitException):
     """Exceptions related to analog tiles."""
 
 
+class TileModuleError(TileError):
+    """Exceptions related to analog tile modules."""
+
+
 class ArgumentError(AihwkitException):
     """Exceptions related to wrong arguments."""
 
 
 class CudaError(AihwkitException):
     """Exceptions related to CUDA."""
 
 
 class ConfigError(AihwkitException):
     """Exceptions related to tile configuration."""
 
 
+class AnalogBiasConfigError(ConfigError):
+    """Exception that analog bias is wrongly set."""
+
+
+class TorchTileConfigError(ConfigError):
+    """Exceptions related to torch tile configuration."""
+
+
 class CloudError(AihwkitException):
     """Exceptions related to the cloud functionality."""
```

### Comparing `aihwkit-0.7.1/src/aihwkit/experiments/__init__.py` & `aihwkit-0.8.0/src/aihwkit/experiments/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/aihwkit/experiments/experiments/__init__.py` & `aihwkit-0.8.0/src/aihwkit/__init__.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,13 +1,15 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
 # that they have been altered from the originals.
 
-"""Experiments for aihwkit."""
+"""Analog hardware library for PyTorch."""
+
+from .version import __version__
```

### Comparing `aihwkit-0.7.1/src/aihwkit/experiments/experiments/base.py` & `aihwkit-0.8.0/src/aihwkit/experiments/experiments/base.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -58,15 +58,15 @@
             Signals.VALIDATION_EPOCH_START: [],
             Signals.VALIDATION_EPOCH_END: [],
             Signals.VALIDATION_EPOCH_BATCH_START: [],
             Signals.VALIDATION_EPOCH_BATCH_END: [],
             Signals.INFERENCE_START: [],
             Signals.INFERENCE_END: [],
             Signals.INFERENCE_REPEAT_START: [],
-            Signals.INFERENCE_REPEAT_END: []
+            Signals.INFERENCE_REPEAT_END: [],
         }
 
         self.results: Optional[Any] = None
 
     # add the specified routine to call with the specified hook key to the experiment.
     def add_hook(self, key: Signals, hook: Callable) -> None:
         """Register a hook for the experiment.
```

### Comparing `aihwkit-0.7.1/src/aihwkit/experiments/experiments/inferencing.py` & `aihwkit-0.8.0/src/aihwkit/experiments/experiments/inferencing.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -26,15 +26,16 @@
 from torch.nn import Module, CrossEntropyLoss
 from torch.nn.modules.loss import _Loss
 from torch.utils.data import DataLoader, Dataset, Subset
 from torchvision.datasets import FashionMNIST, SVHN
 from torchvision.transforms import Compose, Normalize, ToTensor
 
 from aihwkit.experiments.experiments.base import Experiment, Signals
-from aihwkit.nn.modules.base import AnalogModuleBase
+from aihwkit.nn.modules.base import AnalogLayerBase
+from aihwkit.utils.legacy import convert_legacy_checkpoint
 
 
 WEIGHT_TEMPLATE_URL = "https://github.com/IBM-AI-Hardware-Center/Composer/raw/main/"
 
 
 def download(url: str, destination: str) -> None:
     """Helper for downloading a file from url"""
@@ -61,23 +62,23 @@
         When executing a ``BasicInferencing`` in the cloud, additional constraints
         are applied to the data. For example, the model is restricted to
         sequential layers of specific types; the dataset choices are limited,
         etc. Please check the ``CloudRunner`` documentation.
     """
 
     def __init__(
-            self,
-            dataset: Type[Dataset],
-            model: Module,
-            batch_size: int = 10,
-            loss_function: type = CrossEntropyLoss,
-            weight_template_id: str = "",
-            inference_repeats: int = 2,
-            inference_time: int = 86400,
-            remap_weights: bool = True
+        self,
+        dataset: Type[Dataset],
+        model: Module,
+        batch_size: int = 10,
+        loss_function: type = CrossEntropyLoss,
+        weight_template_id: str = "",
+        inference_repeats: int = 2,
+        inference_time: int = 86400,
+        remap_weights: bool = True,
     ):
         """Create a new ``BasicInferencing``.
 
         Args:
             dataset: the dataset class to be used.
             model: the neural network to use for inferencing.
             batch_size: the batch size used for inferencing.
@@ -96,17 +97,17 @@
         self.weight_template_id = weight_template_id
         self.remap_weights = remap_weights
 
         super().__init__()
 
     def get_dataset_arguments(self, dataset: type) -> Tuple[Dict, Dict]:
         """Return the dataset constructor arguments for specifying subset."""
-        if dataset in (SVHN, ):
-            return {'split': 'train'}, {'split': 'test'}
-        return {'train': True}, {'train': False}
+        if dataset in (SVHN,):
+            return {"split": "train"}, {"split": "test"}
+        return {"train": True}, {"train": False}
 
     def get_dataset_transform(self, dataset: type) -> Any:
         """Return the dataset transform."""
         # Normalize supported datasets.
         if dataset == FashionMNIST:
             # mean = Tensor([0.2860])
             # std_dev = Tensor([0.3205])
@@ -120,19 +121,19 @@
             transform = Compose([ToTensor(), Normalize(mean, std_dev)])
         else:
             transform = Compose([ToTensor()])
 
         return transform
 
     def get_data_loader(
-            self,
-            dataset: type,
-            batch_size: int,
-            max_elements: int = 0,
-            dataset_root: str = '/tmp/datasets'
+        self,
+        dataset: type,
+        batch_size: int,
+        max_elements: int = 0,
+        dataset_root: str = "/tmp/datasets",
     ) -> DataLoader:
         """Return `DataLoaders` for the selected dataset.
 
         Args:
             dataset: the dataset class to be used.
             batch_size: the batch size used for inferencing.
             max_elements: the maximum number of elements of the dataset
@@ -153,19 +154,15 @@
         if max_elements > 0:
             validation_set = Subset(validation_set, range(max_elements))
 
         validation_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=False)
 
         return validation_loader
 
-    def get_model(
-            self,
-            weight_template_id: str,
-            device: torch_device
-    ) -> Module:
+    def get_model(self, weight_template_id: str, device: torch_device) -> Module:
         """Get a copy of the set-up model (with load the weights and biases)
         from the original experiment model.
 
         Args:
             weight_template_id: location/index for the file
                 that contains the state_dicts for the model.
             device: the torch device used for the model.
@@ -173,51 +170,51 @@
         Returns:
             a copied model with loaded weights and biases
 
         """
         model = deepcopy(self.model)
 
         if weight_template_id != "":
-            if weight_template_id[0:1] == "." or weight_template_id[0:1] == '/':
+            if weight_template_id[0:1] == "." or weight_template_id[0:1] == "/":
                 # This is the case where it is a local file
                 template_path = weight_template_id
             else:
                 template_dir = "/tmp/weight_templates"
-                if weight_template_id.startswith('http'):
+                if weight_template_id.startswith("http"):
                     template_url = weight_template_id
                 else:
                     # print('weights_template_id: ', weight_template_id)
                     template_path = template_dir + "/" + weight_template_id + ".pth"
                     template_url = WEIGHT_TEMPLATE_URL + weight_template_id + ".pth"
                 # check if the file exists
                 if not path.exists(template_dir):
                     mkdir(template_dir)
                 if not path.exists(template_path):
                     download(template_url, template_path)
 
             # print('template_path: ', template_path)
             if path.exists(template_path):
-                model.load_state_dict(load(template_path, map_location=device),
-                                      load_rpu_config=False)
+                state_dict = load(template_path, map_location=device)
+                state_dict, _ = convert_legacy_checkpoint(state_dict, model)
+                model.load_state_dict(state_dict, load_rpu_config=False)
             else:
-                print('Checkpoint file: ', template_path, ' does not exist.')
+                print("Checkpoint file: ", template_path, " does not exist.")
 
             if self.remap_weights:
-                for module in model.analog_modules():
-                    module.remap_weights()
+                model.remap_analog_weights()
 
         return model.to(device)
 
     def inferencing_step(
-            self,
-            validation_loader: DataLoader,
-            model: Module,
-            loss_function: _Loss,
-            t_inference_list: list,
-            device: torch_device
+        self,
+        validation_loader: DataLoader,
+        model: Module,
+        loss_function: _Loss,
+        t_inference_list: list,
+        device: torch_device,
     ) -> Tuple[ndarray, ndarray, ndarray]:
         """Run a single inferencing.
 
         Args:
             validation_loader: the data loader for the inferencing data.
             model: the neural network to be trained.
             loss_function: the loss function used for inferencing.
@@ -264,30 +261,30 @@
                 total_images += n_images
                 total_loss += loss.item() * n_images
 
                 _, predicted = torch_max(predict.data, 1)
                 predicted_ok += (predicted == labels).sum().item()
 
             # Save the information in the np arrays and return
-            accuracy_post = predicted_ok / total_images * 100.
+            accuracy_post = predicted_ok / total_images * 100.0
             infer_accuracy[idx] = accuracy_post
-            infer_error[idx] = 100. - accuracy_post
+            infer_error[idx] = 100.0 - accuracy_post
             infer_loss[idx] = total_loss / total_images
 
         return infer_accuracy, infer_error, infer_loss
 
     def inference(
-            self,
-            validation_loader: DataLoader,
-            model: Module,
-            loss_function: _Loss,
-            inference_repeats: int,
-            inference_time: int,
-            device: torch_device,
-            n_inference_times: int = 10,
+        self,
+        validation_loader: DataLoader,
+        model: Module,
+        loss_function: _Loss,
+        inference_repeats: int,
+        inference_time: int,
+        device: torch_device,
+        n_inference_times: int = 10,
     ) -> Dict:
         """Run the inferencing loop.
 
         Args:
             validation_loader: the data loader for the validation data.
             model: the neural network to be trained.
             loss_function: the loss function used for inferencing.
@@ -303,98 +300,105 @@
 
         # Move the model to the device if needed.
         if device:
             model = model.to(device)
 
         # Create the t_inference_list using inference_time.
         # Generate the 9 values between 0 and the inference time using log10
-        t_inference_list = [0.0] + logspace(0, log10(float(inference_time)),
-                                            n_inference_times - 1).tolist()
+        t_inference_list = [0.0] + logspace(
+            0, log10(float(inference_time)), n_inference_times - 1
+        ).tolist()
         repeat_results = {}
-        accuracy_array = array([])
-        error_array = array([])
-        loss_array = array([])
+        accuracy_array = array([], "float")
+        error_array = array([], "float")
+        loss_array = array([], "float")
 
         for repeat in range(inference_repeats):
             self._call_hook(Signals.INFERENCE_REPEAT_START, repeat)
             infer_accuracy, infer_error, infer_loss = self.inferencing_step(
-                validation_loader, model, loss_function, t_inference_list, device)
+                validation_loader, model, loss_function, t_inference_list, device
+            )
 
             # Save the info
             accuracy_array = concatenate([accuracy_array, infer_accuracy])  # type: ignore
             error_array = concatenate([error_array, infer_error])  # type: ignore
             loss_array = concatenate([loss_array, infer_loss])  # type: ignore
 
             # call the metric hook function with the average information
             # to write out the partial result to standard out.
             shape = (repeat + 1, n_inference_times)
-            repeat_results = self._call_hook(Signals.INFERENCE_REPEAT_END,
-                                             array(t_inference_list),
-                                             accuracy_array.reshape(shape).mean(axis=0),
-                                             accuracy_array.reshape(shape).std(axis=0),
-                                             error_array.reshape(shape).mean(axis=0),
-                                             loss_array.reshape(shape).mean(axis=0),
-                                             self.inference_repeats)
+            repeat_results = self._call_hook(
+                Signals.INFERENCE_REPEAT_END,
+                array(t_inference_list),
+                accuracy_array.reshape(shape).mean(axis=0),
+                accuracy_array.reshape(shape).std(axis=0),
+                error_array.reshape(shape).mean(axis=0),
+                loss_array.reshape(shape).mean(axis=0),
+                self.inference_repeats,
+            )
         return deepcopy(repeat_results)
 
     def _print_rpu_fields(self, model: Module) -> None:
         """Print the Inference RPU Config fields"""
 
-        print('\n>>> inferenceworker.py: STARTING _print_rpu_fields() ')
+        print("\n>>> inferenceworker.py: STARTING _print_rpu_fields() ")
 
         for name, module in model.named_modules():
-            if not isinstance(module, AnalogModuleBase):
+            if not isinstance(module, AnalogLayerBase):
                 continue
 
-            print(f'RPUConfig of module {name}:')
+            print(f"RPUConfig of module {name}:")
             tile = next(module.analog_tiles())
             print(tile.rpu_config)
             print(tile.tile)
-            print('-------------')
+            print("-------------")
 
-        print('\n>>> inferenceworker.py: ENDING _print_rpu_fields() ')
+        print("\n>>> inferenceworker.py: ENDING _print_rpu_fields() ")
 
-    def run(self, max_elements: int = 0,
-            dataset_root: str = '/tmp/data',
-            device: Optional[torch_device] = None,
-            ) -> Dict:
-        """ Sets up the internal model and runs the inference.
+    def run(
+        self,
+        max_elements: int = 0,
+        dataset_root: str = "/tmp/data",
+        device: Optional[torch_device] = None,
+    ) -> Dict:
+        """Sets up the internal model and runs the inference.
 
         Results are returned and the internal model is updated.
         """
 
         # Build the objects needed for inferencing.
         # Get valication dataset
         validation_loader = self.get_data_loader(
-            self.dataset, self.batch_size,
-            max_elements=max_elements,
-            dataset_root=dataset_root
+            self.dataset, self.batch_size, max_elements=max_elements, dataset_root=dataset_root
         )
 
         # Load the weights and biases to the model.
         # Assumption: the model already includes the customer-specified InferenceRPUConfig.
         model = self.get_model(self.weight_template_id, device)
-
         self._print_rpu_fields(model)
 
         # Invoke the inference step
-        result = self.inference(validation_loader,
-                                model,
-                                self.loss_function(),
-                                self.inference_repeats,
-                                self.inference_time,
-                                device)
+        result = self.inference(
+            validation_loader,
+            model,
+            self.loss_function(),
+            self.inference_repeats,
+            self.inference_time,
+            device,
+        )
         self.model = model  # update the stored model with the trained one
         return result
 
     def __str__(self) -> str:
         """Return a string representation of a BasicInferencing experiment."""
-        return ('{}(dataset={}, batch_size={}, loss_function={}, inference_repeats={}, '
-                'inference_time={}, model={})'.format(
-                    self.__class__.__name__,
-                    getattr(self.dataset, '__name__', self.dataset),
-                    self.batch_size,
-                    getattr(self.loss_function, '__name__', self.loss_function),
-                    self.inference_repeats,
-                    self.inference_time,
-                    self.model
-                ))
+        return (
+            "{}(dataset={}, batch_size={}, loss_function={}, inference_repeats={}, "
+            "inference_time={}, model={})".format(
+                self.__class__.__name__,
+                getattr(self.dataset, "__name__", self.dataset),
+                self.batch_size,
+                getattr(self.loss_function, "__name__", self.loss_function),
+                self.inference_repeats,
+                self.inference_time,
+                self.model,
+            )
+        )
```

### Comparing `aihwkit-0.7.1/src/aihwkit/experiments/experiments/training.py` & `aihwkit-0.8.0/src/aihwkit/experiments/experiments/training.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -44,21 +44,21 @@
         When executing a ``BasicTraining`` in the cloud, additional constraints
         are applied to the data. For example, the model is restricted to
         sequential layers of specific types; the dataset choices are limited,
         etc. Please check the ``CloudRunner`` documentation.
     """
 
     def __init__(
-            self,
-            dataset: Type[Dataset],
-            model: Module,
-            batch_size: int = 64,
-            loss_function: type = NLLLoss,
-            epochs: int = 30,
-            learning_rate: float = 0.05
+        self,
+        dataset: Type[Dataset],
+        model: Module,
+        batch_size: int = 64,
+        loss_function: type = NLLLoss,
+        epochs: int = 30,
+        learning_rate: float = 0.05,
     ):
         """Create a new ``BasicTraining``.
 
         Args:
             dataset: the dataset class to be used.
             model: the neural network to be trained.
             batch_size: the batch size used for training.
@@ -73,17 +73,17 @@
         self.epochs = epochs
         self.learning_rate = learning_rate
 
         super().__init__()
 
     def get_dataset_arguments(self, dataset: type) -> Tuple[Dict, Dict]:
         """Return the dataset constructor arguments for specifying subset."""
-        if dataset in (SVHN, ):
-            return {'split': 'train'}, {'split': 'test'}
-        return {'train': True}, {'train': False}
+        if dataset in (SVHN,):
+            return {"split": "train"}, {"split": "test"}
+        return {"train": True}, {"train": False}
 
     def get_dataset_transform(self, dataset: type) -> Any:
         """Return the dataset transform."""
         # Normalize supported datasets.
         if dataset == FashionMNIST:
             mean = Tensor([0.2860])
             std = Tensor([0.3205])
@@ -94,19 +94,19 @@
             transform = Compose([ToTensor(), Normalize(mean, std)])
         else:
             transform = Compose([ToTensor()])
 
         return transform
 
     def get_data_loaders(
-            self,
-            dataset: type,
-            batch_size: int,
-            max_elements_train: int = 0,
-            dataset_root: str = '/tmp/datasets'
+        self,
+        dataset: type,
+        batch_size: int,
+        max_elements_train: int = 0,
+        dataset_root: str = "/tmp/datasets",
     ) -> Tuple[DataLoader, DataLoader]:
         """Return `DataLoaders` for the selected dataset.
 
         Args:
             dataset: the dataset class to be used.
             batch_size: the batch size used for training.
             max_elements_train: the maximum number of elements of the dataset
@@ -130,19 +130,15 @@
             validation_set = Subset(validation_set, range(max_elements_train))
 
         training_loader = DataLoader(training_set, batch_size=batch_size, shuffle=True)
         validation_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=True)
 
         return training_loader, validation_loader
 
-    def get_optimizer(
-            self,
-            learning_rate: float,
-            model: Module
-    ) -> Optimizer:
+    def get_optimizer(self, learning_rate: float, model: Module) -> Optimizer:
         """Return the `Optimizer` for the experiment.
 
         Args:
             learning_rate: the learning rate used by the optimizer.
             model: the neural network to be trained.
 
         Returns:
@@ -150,20 +146,20 @@
         """
         optimizer = AnalogSGD(model.parameters(), lr=learning_rate)
         optimizer.regroup_param_groups(model)
 
         return optimizer
 
     def training_step(
-            self,
-            training_loader: DataLoader,
-            model: Module,
-            optimizer: Optimizer,
-            loss_function: _Loss,
-            device: torch_device
+        self,
+        training_loader: DataLoader,
+        model: Module,
+        optimizer: Optimizer,
+        loss_function: _Loss,
+        device: torch_device,
     ) -> None:
         """Run a single training step.
 
         Args:
             training_loader: the data loader for the training data.
             model: the neural network to be trained.
             optimizer: the optimizer used for the training.
@@ -185,24 +181,24 @@
 
             # Run training (backward propagation).
             loss.backward()
 
             # Optimize weights.
             optimizer.step()
 
-            self._call_hook(Signals.TRAIN_EPOCH_BATCH_END,
-                            batch_image_count,
-                            loss.item()*batch_image_count)
+            self._call_hook(
+                Signals.TRAIN_EPOCH_BATCH_END, batch_image_count, loss.item() * batch_image_count
+            )
 
     def validation_step(
-            self,
-            validation_loader: DataLoader,
-            model: Module,
-            loss_function: _Loss,
-            device: torch_device
+        self,
+        validation_loader: DataLoader,
+        model: Module,
+        loss_function: _Loss,
+        device: torch_device,
     ) -> None:
         """Run a single evaluation step.
 
         Args:
             validation_loader: the data loader for the validation data.
             model: the neural network to be trained.
             loss_function: the loss function used for training.
@@ -220,28 +216,30 @@
             prediction = model(images)
             loss = loss_function(prediction, labels)
 
             _, predicted = torch_max(prediction.data, 1)
             batch_image_count = labels.size(0)
             batch_correct_count = (predicted == labels).sum().item()
 
-            self._call_hook(Signals.VALIDATION_EPOCH_BATCH_END,
-                            batch_image_count,
-                            batch_correct_count,
-                            loss.item()*batch_image_count)
+            self._call_hook(
+                Signals.VALIDATION_EPOCH_BATCH_END,
+                batch_image_count,
+                batch_correct_count,
+                loss.item() * batch_image_count,
+            )
 
     def train(
-            self,
-            training_loader: DataLoader,
-            validation_loader: DataLoader,
-            model: Module,
-            optimizer: Optimizer,
-            loss_function: _Loss,
-            epochs: int,
-            device: torch_device
+        self,
+        training_loader: DataLoader,
+        validation_loader: DataLoader,
+        model: Module,
+        optimizer: Optimizer,
+        loss_function: _Loss,
+        epochs: int,
+        device: torch_device,
     ) -> List[Dict]:
         """Run the training loop.
 
         Args:
             training_loader: the data loader for the training data.
             validation_loader: the data loader for the validation data.
             model: the neural network to be trained.
@@ -254,71 +252,72 @@
             A list of the metrics for each epoch.
         """
         results = []
 
         for epoch_number in range(epochs):
             self._call_hook(Signals.EPOCH_START, epoch_number)
             self._call_hook(Signals.TRAIN_EPOCH_START, epoch_number)
-            self.training_step(training_loader,
-                               model,
-                               optimizer,
-                               loss_function,
-                               device)
+            self.training_step(training_loader, model, optimizer, loss_function, device)
             self._call_hook(Signals.TRAIN_EPOCH_END)
 
             self._call_hook(Signals.VALIDATION_EPOCH_START, epoch_number)
-            self.validation_step(validation_loader,
-                                 model,
-                                 loss_function,
-                                 device)
+            self.validation_step(validation_loader, model, loss_function, device)
             self._call_hook(Signals.VALIDATION_EPOCH_END)
 
-            epoch_results = {'epoch': epoch_number}
+            epoch_results = {"epoch": epoch_number}
             epoch_results.update(self._call_hook(Signals.EPOCH_END))
             results.append(epoch_results)
 
         return results
 
-    def run(self, max_elements: int = 0,
-            dataset_root: str = '/tmp/data',
-            device: Optional[torch_device] = None) -> List[Dict]:
-        """ Sets up and runs the training.
+    def run(
+        self,
+        max_elements: int = 0,
+        dataset_root: str = "/tmp/data",
+        device: Optional[torch_device] = None,
+    ) -> List[Dict]:
+        """Sets up and runs the training.
 
         Results are returned and the internal model is updated.
         """
 
         # Build the objects needed for training.
         training_loader, validation_loader = self.get_data_loaders(
-            self.dataset, self.batch_size,
+            self.dataset,
+            self.batch_size,
             max_elements_train=max_elements,
-            dataset_root=dataset_root
+            dataset_root=dataset_root,
         )
 
         optimizer = self.get_optimizer(self.learning_rate, self.model)
 
         # Move the model to the device if needed.
         model = self.model
         if device:
             model = model.to(device)
 
-        results = self.train(training_loader,
-                             validation_loader,
-                             model,
-                             optimizer,
-                             self.loss_function(),
-                             self.epochs,
-                             device)
+        results = self.train(
+            training_loader,
+            validation_loader,
+            model,
+            optimizer,
+            self.loss_function(),
+            self.epochs,
+            device,
+        )
         self.model = model  # update the stored model with the trained one
         return results
 
     def __str__(self) -> str:
         """Return a string representation of a BasicTraining experiment."""
-        return ('{}(dataset={}, batch_size={}, loss_function={}, epochs={}, '
-                'learning_rate={}, model={})'.format(
-                    self.__class__.__name__,
-                    getattr(self.dataset, '__name__', self.dataset),
-                    self.batch_size,
-                    getattr(self.loss_function, '__name__', self.loss_function),
-                    self.epochs,
-                    self.learning_rate,
-                    self.model
-                ))
+        return (
+            "{}(dataset={}, batch_size={}, loss_function={}, epochs={}, "
+            "learning_rate={}, model={})".format(
+                self.__class__.__name__,
+                getattr(self.dataset, "__name__", self.dataset),
+                self.batch_size,
+                getattr(self.loss_function, "__name__", self.loss_function),
+                self.epochs,
+                self.learning_rate,
+                self.model,
+            )
+        )
```

### Comparing `aihwkit-0.7.1/src/aihwkit/experiments/runners/__init__.py` & `aihwkit-0.8.0/src/aihwkit/experiments/runners/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/aihwkit/experiments/runners/base.py` & `aihwkit-0.8.0/src/aihwkit/experiments/runners/base.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/aihwkit/experiments/runners/cloud.py` & `aihwkit-0.8.0/src/aihwkit/experiments/runners/cloud.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -24,20 +24,20 @@
 
 
 class CloudRunner(Runner):
     """Runner that executes Experiments in the AIHW Composer cloud.
 
     Class that allows executing Experiments in the cloud.
     """
+
     # pylint: disable=too-few-public-methods
 
-    def __init__(self,
-                 api_url: Optional[str] = None,
-                 api_token: Optional[str] = None,
-                 verify: bool = True):
+    def __init__(
+        self, api_url: Optional[str] = None, api_token: Optional[str] = None, verify: bool = True
+    ):
         """Create a new ``CloudRunner``.
 
         Note:
             If no ``api_token`` or ``api_url`` is provided, this class will
             attempt to read them from the local configuration file (by default,
             at ``~/.config/aihwkit/aihwkit.conf`` or  environment variables
             (``AIHW_API_TOKEN``).
@@ -53,15 +53,15 @@
         # Attempt to load credentials if not present.
         if not api_url or not api_token:
             config = ClientConfiguration()
             api_url = api_url or config.url
             api_token = api_token or config.token
 
             if not api_url or not api_token:
-                raise CredentialsError('No credentials could be found')
+                raise CredentialsError("No credentials could be found")
 
         self.api_url = api_url
         self.api_token = api_token
 
         # Authenticate.
         self.session = ApiSession(self.api_url, self.api_token, verify)
         self.api_client = ApiClient(self.session)
@@ -82,19 +82,15 @@
 
         Returns:
             A list of ``CloudExperiments``.
         """
         return self.api_client.experiments_list()
 
     def run(  # type: ignore[override]
-            self,
-            experiment: BasicTraining,
-            name: str = '',
-            device: str = 'gpu',
-            **_: Any
+        self, experiment: BasicTraining, name: str = "", device: str = "gpu", **_: Any
     ) -> CloudExperiment:
         """Run a single Experiment.
 
         Starts the execution of an Experiment in the cloud. Upon successful
         invocation, this method will return a ``CloudExperiment`` object that
         can be used for inspecting the status of the remote execution.
 
@@ -110,13 +106,12 @@
 
         Returns:
             A ``CloudExperiment`` which represents the remote experiment.
         """
         # pylint: disable=arguments-differ
         # Generate an experiment name if not given.
         if not name:
-            name = 'aihwkit cloud experiment ({}, {} layers)'.format(
-                experiment.dataset.__name__,
-                len(experiment.model)
+            name = "aihwkit cloud experiment ({}, {} layers)".format(
+                experiment.dataset.__name__, len(experiment.model)
             )
 
         return self.api_client.experiment_create(experiment, name, device)
```

### Comparing `aihwkit-0.7.1/src/aihwkit/experiments/runners/i_cloud.py` & `aihwkit-0.8.0/src/aihwkit/experiments/runners/i_cloud.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -24,20 +24,20 @@
 
 
 class InferenceCloudRunner(Runner):
     """Runner that executes Experiments in the AIHW Composer cloud.
 
     Class that allows executing Experiments in the cloud.
     """
+
     # pylint: disable=too-few-public-methods
 
-    def __init__(self,
-                 api_url: Optional[str] = None,
-                 api_token: Optional[str] = None,
-                 verify: bool = False):
+    def __init__(
+        self, api_url: Optional[str] = None, api_token: Optional[str] = None, verify: bool = False
+    ):
         """Create a new ``InferenceCloudRunner``.
 
         Note:
             If no ``api_token`` or ``api_url`` is provided, this class will
             attempt to read them from the local configuration file (by default,
             at ``~/.config/aihwkit/aihwkit.conf`` or  environment variables
             (``AIHW_API_TOKEN``).
@@ -53,15 +53,15 @@
         # Attempt to load credentials if not present.
         if not api_url or not api_token:
             config = ClientConfiguration()
             api_url = api_url or config.url
             api_token = api_token or config.token
 
             if not api_url or not api_token:
-                raise CredentialsError('No credentials could be found')
+                raise CredentialsError("No credentials could be found")
 
         self.api_url = api_url
         self.api_token = api_token
 
         # Authenticate.
         self.session = ApiSession(self.api_url, self.api_token, verify)
         self.api_client = InferenceApiClient(self.session)
@@ -82,21 +82,21 @@
 
         Returns:
             A list of ``CloudExperiments``.
         """
         return self.api_client.experiments_list()
 
     def run(  # type: ignore[override]
-            self,
-            experiment: BasicInferencing,
-            analog_info: Dict,
-            noise_model_info: Dict,
-            name: str = '',
-            device: str = 'gpu',
-            **_: Any
+        self,
+        experiment: BasicInferencing,
+        analog_info: Dict,
+        noise_model_info: Dict,
+        name: str = "",
+        device: str = "gpu",
+        **_: Any,
     ) -> CloudExperiment:
         """Run a single Experiment.
 
         Starts the execution of an Experiment in the cloud. Upon successful
         invocation, this method will return a ``CloudExperiment`` object that
         can be used for inspecting the status of the remote execution.
 
@@ -114,14 +114,14 @@
 
         Returns:
             A ``CloudExperiment`` which represents the remote experiment.
         """
         # pylint: disable=arguments-differ
         # Generate an experiment name if not given.
         if not name:
-            name = 'aihwkit inference cloud experiment ({}, {} layers)'.format(
-                experiment.dataset.__name__,
-                len(experiment.model)
+            name = "aihwkit inference cloud experiment ({}, {} layers)".format(
+                experiment.dataset.__name__, len(experiment.model)
             )
 
-        return self.api_client.experiment_create(experiment, analog_info,
-                                                 noise_model_info, name, device)
+        return self.api_client.experiment_create(
+            experiment, analog_info, noise_model_info, name, device
+        )
```

### Comparing `aihwkit-0.7.1/src/aihwkit/experiments/runners/i_local.py` & `aihwkit-0.8.0/src/aihwkit/experiments/runners/i_local.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -24,31 +24,31 @@
 
 
 class InferenceLocalRunner(Runner):
     """Runner that executes Experiments locally.
 
     Class that allows executing Experiments locally.
     """
+
     # pylint: disable=too-few-public-methods
 
-    def __init__(self,
-                 device: Optional[torch_device] = None):
+    def __init__(self, device: Optional[torch_device] = None):
         """Create a new ``InferenceLocalRunner``.
 
         Args:
             device: the device where the model will be running on.
         """
         self.device = device
 
     def run(  # type: ignore[override]
-            self,
-            experiment: BasicInferencing,
-            max_elements: int = 0,
-            dataset_root: str = '/tmp/datasets',
-            stdout: bool = False,
+        self,
+        experiment: BasicInferencing,
+        max_elements: int = 0,
+        dataset_root: str = "/tmp/datasets",
+        stdout: bool = False,
     ) -> Dict:
         """Run a single Experiment.
 
         Executes an experiment locally, in the device specified by
         ``self.device``, optionally printing information to stdout.
 
         Note:
@@ -77,12 +77,12 @@
         experiment.add_hook(Signals.INFERENCE_REPEAT_START, metric.receive_repeat_start)
         experiment.add_hook(Signals.INFERENCE_REPEAT_END, metric.receive_repeat_end)
 
         # Download the FashionMNIST or SVHN dataset if needed.
         if experiment.dataset == FashionMNIST:
             _ = experiment.dataset(dataset_root, download=True)
         elif experiment.dataset == SVHN:
-            _ = experiment.dataset(dataset_root, download=True, split='train')
-            _ = experiment.dataset(dataset_root, download=True, split='test')
+            _ = experiment.dataset(dataset_root, download=True, split="train")
+            _ = experiment.dataset(dataset_root, download=True, split="test")
 
         # Invoke the inference step
         return experiment.run(max_elements, dataset_root, self.device)
```

### Comparing `aihwkit-0.7.1/src/aihwkit/experiments/runners/i_metrics.py` & `aihwkit-0.8.0/src/aihwkit/experiments/runners/i_metrics.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -23,47 +23,51 @@
     def __init__(self, stdout: bool = False) -> None:
         self.current_repeat: Dict = {}
         self.time_init = datetime.utcnow()
         self.stdout = stdout
 
     def receive_repeat_start(self, repeat: int) -> None:
         """Hook for `INFERENCE_REPEAT_START`."""
-        self.current_repeat = {
-                "number": repeat,
-                "inference_results": []
-        }
+        self.current_repeat = {"number": repeat, "inference_results": []}
 
-    def receive_repeat_end(self, t_inference_array: list, avg_acc_arr: list,
-                           std_acc_arr: list, avg_err_arr: list, avg_loss_arr: list,
-                           inference_repeats: int) -> Dict:
+    def receive_repeat_end(
+        self,
+        t_inference_array: list,
+        avg_acc_arr: list,
+        std_acc_arr: list,
+        avg_err_arr: list,
+        avg_loss_arr: list,
+        inference_repeats: int,
+    ) -> Dict:
         """Hook for `INFERENCE_REPEAT_END`."""
 
         inf_results = []
         n_inference = len(t_inference_array)
 
         # The input are the arrays of avg accuracy, avg error and avg loss.
         # Create the dict entry for the items in the arrays.
         for i in range(n_inference):
             new_dict = {
-                    "t_inference": t_inference_array[i],
-                    "avg_accuracy": avg_acc_arr[i],
-                    "std_accuracy": std_acc_arr[i],
-                    "avg_error": avg_err_arr[i],
-                    "avg_loss": avg_loss_arr[i]}
+                "t_inference": t_inference_array[i],
+                "avg_accuracy": avg_acc_arr[i],
+                "std_accuracy": std_acc_arr[i],
+                "avg_error": avg_err_arr[i],
+                "avg_loss": avg_loss_arr[i],
+            }
             inf_results.append(new_dict)
 
         repeat = self.current_repeat["number"] + 1
         time_elapsed = (datetime.utcnow() - self.time_init).total_seconds()
         is_partial = bool(repeat < inference_repeats)
         partial = {
             "inference_runs": {
-                    "inference_repeat": repeat,
-                    "is_partial": is_partial,
-                    "time_elapsed": time_elapsed,
-                    "inference_results":  inf_results
+                "inference_repeat": repeat,
+                "is_partial": is_partial,
+                "time_elapsed": time_elapsed,
+                "inference_results": inf_results,
             }
         }
 
         if self.stdout:
             print("{}".format(json.dumps(partial)))
 
         # Return the partial.
```

### Comparing `aihwkit-0.7.1/src/aihwkit/experiments/runners/local.py` & `aihwkit-0.8.0/src/aihwkit/experiments/runners/local.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -24,31 +24,31 @@
 
 
 class LocalRunner(Runner):
     """Runner that executes Experiments locally.
 
     Class that allows executing Experiments locally.
     """
+
     # pylint: disable=too-few-public-methods
 
-    def __init__(self,
-                 device: Optional[torch_device] = None):
+    def __init__(self, device: Optional[torch_device] = None):
         """Create a new ``LocalRunner``.
 
         Args:
             device: the device where the model will be moved to.
         """
         self.device = device
 
     def run(  # type: ignore[override]
-            self,
-            experiment: BasicTraining,
-            max_elements_train: int = 0,
-            dataset_root: str = '/tmp/datasets',
-            stdout: bool = False,
+        self,
+        experiment: BasicTraining,
+        max_elements_train: int = 0,
+        dataset_root: str = "/tmp/datasets",
+        stdout: bool = False,
     ) -> List[Dict]:
         """Run a single Experiment.
 
         Executes an experiment locally, in the device specified by
         ``self.device``, optionally printing information to stdout.
 
         Note:
@@ -74,19 +74,20 @@
         # Setup the metric helper for the experiment.
         metric = LocalMetric(stdout=stdout)
         experiment.clear_hooks()
         experiment.add_hook(Signals.EPOCH_START, metric.receive_epoch_start)
         experiment.add_hook(Signals.EPOCH_END, metric.receive_epoch_end)
         experiment.add_hook(Signals.TRAIN_EPOCH_END, metric.receive_train_epoch_end)
         experiment.add_hook(Signals.TRAIN_EPOCH_BATCH_END, metric.receive_train_epoch_batch_end)
-        experiment.add_hook(Signals.VALIDATION_EPOCH_BATCH_END,
-                            metric.receive_validation_epoch_batch_end)
+        experiment.add_hook(
+            Signals.VALIDATION_EPOCH_BATCH_END, metric.receive_validation_epoch_batch_end
+        )
         experiment.add_hook(Signals.VALIDATION_EPOCH_END, metric.receive_validation_epoch_end)
 
         # Download the dataset if needed.
         if experiment.dataset == FashionMNIST:
             _ = experiment.dataset(dataset_root, download=True)
         elif experiment.dataset == SVHN:
-            _ = experiment.dataset(dataset_root, download=True, split='train')
-            _ = experiment.dataset(dataset_root, download=True, split='test')
+            _ = experiment.dataset(dataset_root, download=True, split="train")
+            _ = experiment.dataset(dataset_root, download=True, split="test")
 
         return experiment.run(max_elements_train, dataset_root, self.device)
```

### Comparing `aihwkit-0.7.1/src/aihwkit/experiments/runners/metrics.py` & `aihwkit-0.8.0/src/aihwkit/experiments/runners/metrics.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -27,76 +27,75 @@
         self.epochs: List[Dict] = []
         self.current_epoch: Dict = {}
         self.stdout = stdout
 
     def receive_epoch_start(self, epoch: int) -> None:
         """Hook for `EPOCH_START`."""
         self.current_epoch = {
-            'number': epoch,
-            'start_time': datetime.utcnow(),
-            'total_loss': 0,
-            'training_images': 0,
-            'validation_images': 0,
-            'validation_correct': 0,
-            'validation_loss': 0
+            "number": epoch,
+            "start_time": datetime.utcnow(),
+            "total_loss": 0,
+            "training_images": 0,
+            "validation_images": 0,
+            "validation_correct": 0,
+            "validation_loss": 0,
         }
 
-    def receive_train_epoch_batch_end(
-            self,
-            total: int,
-            train_loss: float
-    ) -> None:
+    def receive_train_epoch_batch_end(self, total: int, train_loss: float) -> None:
         """Hook for `TRAIN_EPOCH_START`."""
-        self.current_epoch['training_images'] += total
-        self.current_epoch['total_loss'] += train_loss
+        self.current_epoch["training_images"] += total
+        self.current_epoch["total_loss"] += train_loss
 
     def receive_validation_epoch_batch_end(
-            self,
-            total: int,
-            correct: int,
-            validation_loss: float
+        self, total: int, correct: int, validation_loss: float
     ) -> None:
         """Hook for `VALIDATION_EPOCH_BATCH_END`."""
-        self.current_epoch['validation_images'] += total
-        self.current_epoch['validation_correct'] += int(correct)
-        self.current_epoch['validation_loss'] += validation_loss
+        self.current_epoch["validation_images"] += total
+        self.current_epoch["validation_correct"] += int(correct)
+        self.current_epoch["validation_loss"] += validation_loss
 
     def receive_train_epoch_end(self) -> None:
         """Hook for `TRAIN_EPOCH_END`."""
         if not self.stdout:
             return
 
-        print('Epoch: {}, loss: {:.8f}'.format(
-                self.current_epoch['number'],
-                self.current_epoch['total_loss'] / self.current_epoch['training_images'],
-              ))
+        print(
+            "Epoch: {}, loss: {:.8f}".format(
+                self.current_epoch["number"],
+                self.current_epoch["total_loss"] / self.current_epoch["training_images"],
+            )
+        )
 
     def receive_validation_epoch_end(self) -> None:
         """Hook for `VALIDATION_EPOCH_END`."""
         if not self.stdout:
             return
 
-        print('Number of images: {}, accuracy: {:.6%}, validation loss: {:.8f}'.format(
-            self.current_epoch['validation_images'],
-            self.current_epoch['validation_correct'] / self.current_epoch['validation_images'],
-            self.current_epoch['validation_loss'] / self.current_epoch['validation_images'],
-        ))
+        print(
+            "Number of images: {}, accuracy: {:.6%}, validation loss: {:.8f}".format(
+                self.current_epoch["validation_images"],
+                self.current_epoch["validation_correct"] / self.current_epoch["validation_images"],
+                self.current_epoch["validation_loss"] / self.current_epoch["validation_images"],
+            )
+        )
 
     def receive_epoch_end(self) -> Dict:
         """Hook for `EPOCH_END`."""
         end_time = datetime.utcnow()
-        time_epoch = (end_time - self.current_epoch['start_time']).total_seconds()
+        time_epoch = (end_time - self.current_epoch["start_time"]).total_seconds()
         if self.stdout:
-            print('Time for epoch {}: {:.4}s'.format(self.current_epoch['number'],
-                                                     time_epoch))
+            print("Time for epoch {}: {:.4}s".format(self.current_epoch["number"], time_epoch))
         self.epochs.append(self.current_epoch)
 
         return {
-            'epoch': self.current_epoch['number'],
-            'time_epoch': time_epoch,
-            'accuracy': (self.current_epoch['validation_correct'] /
-                         self.current_epoch['validation_images']),
-            'train_loss': (self.current_epoch['total_loss'] /
-                           self.current_epoch['training_images']),
-            'valid_loss': (self.current_epoch['validation_loss'] /
-                           self.current_epoch['validation_images'])
+            "epoch": self.current_epoch["number"],
+            "time_epoch": time_epoch,
+            "accuracy": (
+                self.current_epoch["validation_correct"] / self.current_epoch["validation_images"]
+            ),
+            "train_loss": (
+                self.current_epoch["total_loss"] / self.current_epoch["training_images"]
+            ),
+            "valid_loss": (
+                self.current_epoch["validation_loss"] / self.current_epoch["validation_images"]
+            ),
         }
```

### Comparing `aihwkit-0.7.1/src/aihwkit/inference/__init__.py` & `aihwkit-0.8.0/src/aihwkit/inference/__init__.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,11 +1,10 @@
-
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -14,11 +13,12 @@
 """High level inference tools."""
 
 # Convenience imports for easier access to the classes.
 from aihwkit.inference.converter.base import BaseConductanceConverter
 from aihwkit.inference.converter.conductance import SinglePairConductanceConverter
 from aihwkit.inference.noise.base import BaseNoiseModel
 from aihwkit.inference.noise.pcm import PCMLikeNoiseModel
+from aihwkit.inference.noise.reram import ReRamWan2022NoiseModel
 from aihwkit.inference.noise.custom import StateIndependentNoiseModel
 from aihwkit.inference.compensation.base import BaseDriftCompensation
 from aihwkit.inference.compensation.drift import GlobalDriftCompensation
 from aihwkit.inference.utils import drift_analog_weights, program_analog_weights
```

### Comparing `aihwkit-0.7.1/src/aihwkit/inference/compensation/__init__.py` & `aihwkit-0.8.0/src/aihwkit/inference/compensation/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/aihwkit/inference/compensation/base.py` & `aihwkit-0.8.0/src/aihwkit/inference/compensation/base.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/aihwkit/inference/compensation/drift.py` & `aihwkit-0.8.0/src/aihwkit/inference/compensation/drift.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -35,8 +35,8 @@
         """Return the read-out tensor.
 
         Uses the set of one-hot vectors (eye).
         """
         return eye(in_size)
 
     def __str__(self) -> str:
-        return '{}()'.format(self.__class__.__name__)
+        return "{}()".format(self.__class__.__name__)
```

### Comparing `aihwkit-0.7.1/src/aihwkit/inference/converter/__init__.py` & `aihwkit-0.8.0/src/aihwkit/inference/converter/__init__.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/aihwkit/inference/converter/base.py` & `aihwkit-0.8.0/src/aihwkit/inference/converter/base.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/aihwkit/inference/converter/conductance.py` & `aihwkit-0.8.0/src/aihwkit/inference/converter/conductance.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -33,51 +33,50 @@
     Args:
         g_max: In :math:`\mu S`, the maximal conductance, ie the value
             the absolute max of the weights will be mapped to.
         g_min: In :math:`\mu S`, the minimal conductance, ie the value
             the logical zero of the weights will be mapped to.
     """
 
-    def __init__(
-            self,
-            g_max: Optional[float] = None,
-            g_min: Optional[float] = None
-    ):
+    def __init__(self, g_max: Optional[float] = None, g_min: Optional[float] = None):
         self.g_max = 25.0 if g_max is None else g_max
         self.g_min = 0.0 if g_min is None else g_min
         self.scale_ratio = None
 
-        if self.g_max < 0.:
-            raise ValueError('g_max should be a positive value')
-        if self.g_min < 0.:
-            raise ValueError('g_min should be a positive value')
-        if self.g_min > self.g_max:
-            raise ValueError('g_min should be smaller than g_max')
+        if self.g_max < 0.0:
+            raise ValueError("g_max should be a positive value")
+        if self.g_min < 0.0:
+            raise ValueError("g_min should be a positive value")
+        if self.g_min >= self.g_max:
+            raise ValueError("g_min should be smaller than g_max")
 
     def __str__(self) -> str:
-        return '{}(g_max={:1.2f}, g_min={:1.2f})'.format(
+        return "{}(g_max={:1.2f}, g_min={:1.2f})".format(
             self.__class__.__name__, self.g_max, self.g_min
         )
 
     @no_grad()
     def convert_to_conductances(self, weights: Tensor) -> Tuple[List[Tensor], Dict]:
         abs_max = torch_abs(weights).max()
         scale_ratio = (self.g_max - self.g_min) / abs_max.clamp(min=_ZERO_CLIP)
         scaled_weights = weights * scale_ratio
 
-        conductances = [scaled_weights.clamp(min=0.0, max=self.g_max) + self.g_min,
-                        (- scaled_weights).clamp(min=0.0, max=self.g_max) + self.g_min]
-        params = {'scale_ratio': scale_ratio}
+        conductances = [
+            scaled_weights.clamp(min=0.0, max=self.g_max) + self.g_min,
+            (-scaled_weights).clamp(min=0.0, max=self.g_max) + self.g_min,
+        ]
+        params = {"scale_ratio": scale_ratio}
 
         return conductances, params
 
     @no_grad()
     def convert_back_to_weights(self, conductances: List[Tensor], params: Dict) -> Tensor:
         if len(conductances) != 2:
-            raise ValueError('conductances must contain exactly two elements')
-        if 'scale_ratio' not in params:
-            raise ValueError('params do not contain scale_ratio')
-
-        weights = ((conductances[0] - self.g_min) -
-                   (conductances[1] - self.g_min)) / params['scale_ratio']
+            raise ValueError("conductances must contain exactly two elements")
+        if "scale_ratio" not in params:
+            raise ValueError("params do not contain scale_ratio")
+
+        weights = ((conductances[0] - self.g_min) - (conductances[1] - self.g_min)) / params[
+            "scale_ratio"
+        ]
 
         return weights
```

### Comparing `aihwkit-0.7.1/src/aihwkit/inference/noise/__init__.py` & `aihwkit-0.8.0/src/aihwkit/inference/noise/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/aihwkit/inference/noise/base.py` & `aihwkit-0.8.0/src/aihwkit/inference/noise/base.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,11 +1,10 @@
-
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -20,18 +19,15 @@
 from aihwkit.inference.converter.base import BaseConductanceConverter
 from aihwkit.inference.converter.conductance import SinglePairConductanceConverter
 
 
 class BaseNoiseModel:
     """Base class for phenomenological noise models for inference."""
 
-    def __init__(
-            self,
-            g_converter: Optional[BaseConductanceConverter] = None
-    ):
+    def __init__(self, g_converter: Optional[BaseConductanceConverter] = None):
         self.g_converter = g_converter or SinglePairConductanceConverter()
 
     @no_grad()
     def apply_noise(self, weights: Tensor, t_inference: float) -> Tensor:
         """Apply the expected noise.
 
         Applies the noise to a non-perturbed conductance matrix ``weights``
@@ -49,16 +45,17 @@
         target_conductances, params = self.g_converter.convert_to_conductances(weights)
 
         noisy_conductances = []
         for g_target in target_conductances:
             g_prog = self.apply_programming_noise_to_conductance(g_target)
             if t_inference > 0:
                 nu_drift = self.generate_drift_coefficients(g_target)
-                noisy_conductances.append(self.apply_drift_noise_to_conductance(
-                    g_prog, nu_drift, t_inference))
+                noisy_conductances.append(
+                    self.apply_drift_noise_to_conductance(g_prog, nu_drift, t_inference)
+                )
 
         noisy_weights = self.g_converter.convert_back_to_weights(noisy_conductances, params)
 
         return noisy_weights
 
     @no_grad()
     def apply_programming_noise(self, weights: Tensor) -> Tuple[Tensor, List[Tensor]]:
@@ -76,27 +73,23 @@
             determined during programming.
         """
         target_conductances, params = self.g_converter.convert_to_conductances(weights)
 
         noisy_conductances = []
         nu_drift_list = []
         for g_target in target_conductances:
-
             noisy_conductances.append(self.apply_programming_noise_to_conductance(g_target))
             nu_drift_list.append(self.generate_drift_coefficients(g_target))
         noisy_weights = self.g_converter.convert_back_to_weights(noisy_conductances, params)
 
         return noisy_weights, nu_drift_list
 
     @no_grad()
     def apply_drift_noise(
-            self,
-            weights: Tensor,
-            nu_drift_list: List[Tensor],
-            t_inference: float
+        self, weights: Tensor, nu_drift_list: List[Tensor], t_inference: float
     ) -> Tensor:
         """Apply the expected drift noise to weights.
 
         Uses the :meth:`~apply_drift_noise_to_conductances` on
         each of the conductance slices.
 
         Args:
@@ -108,24 +101,35 @@
             weight tensor with drift noise applied
         """
         target_conductances, params = self.g_converter.convert_to_conductances(weights)
 
         noisy_conductances = []
         for g_target, nu_drift in zip(target_conductances, nu_drift_list):
             noisy_conductances.append(
-                self.apply_drift_noise_to_conductance(g_target, nu_drift, t_inference))
+                self.apply_drift_noise_to_conductance(g_target, nu_drift, t_inference)
+            )
 
         noisy_weights = self.g_converter.convert_back_to_weights(noisy_conductances, params)
 
         return noisy_weights
 
     @no_grad()
-    def generate_drift_coefficients(self, g_target: Tensor) -> Tensor:
-        """Generate drift coefficients ``nu`` based on the target conductances."""
-        raise NotImplementedError
+    def generate_drift_coefficients(self, g_target: Tensor) -> Optional[Tensor]:
+        """Generate drift coefficients.
+
+        Generate coefficients once and passed through when
+        long-term noise and drift is applied. Typical `nu_drift`.
+
+        Args:
+            g_target: Target conductances
+
+        Returns:
+            When not overriden, it simply returns None.
+        """
+        # pylint: disable=unused-argument
 
     @no_grad()
     def apply_programming_noise_to_conductance(self, g_target: Tensor) -> Tensor:
         r"""Apply programming noise to a target conductance ``Tensor``.
 
         Args:
             g_target: Target conductances
@@ -134,18 +138,15 @@
             Tensor of sampled drift coefficients :math:`\nu`, one for each
             target conductance value.
         """
         raise NotImplementedError
 
     @no_grad()
     def apply_drift_noise_to_conductance(
-            self,
-            g_prog: Tensor,
-            nu_drift: Tensor,
-            t_inference: float
+        self, g_prog: Tensor, nu_drift: Optional[Tensor], t_inference: float
     ) -> Tensor:
         r"""Apply the noise and drift up to the assumed inference time point.
 
         Args:
             g_prog: Tensor of conductance values after programming (in :math:`\muS`)
             nu_drift: drift nu
             t_inference: assumed time of inference (in sec)
```

### Comparing `aihwkit-0.7.1/src/aihwkit/inference/noise/custom.py` & `aihwkit-0.8.0/src/aihwkit/inference/noise/custom.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -24,137 +24,145 @@
 from aihwkit.inference.noise.base import BaseNoiseModel
 from aihwkit.inference.converter.base import BaseConductanceConverter
 from aihwkit.inference.converter.conductance import SinglePairConductanceConverter
 
 
 class StateIndependentNoiseModel(BaseNoiseModel):  # pylint: disable=too-many-instance-attributes
     r"""Standard noise model that has a non-conductance dependent drift and
-    multiplicative read (1/f) noise.
+     multiplicative read (1/f) noise.
 
-    Programming noise is state-independent by default, however, it can
-    be made conductance dependent, since the expected programming
-    noise strength is modeled with a second-order polynomial in
-    general.
+     Programming noise is state-independent by default, however, it can
+     be made conductance dependent, since the expected programming
+     noise strength is modeled with a second-order polynomial in
+     general.
 
-   **Programming noise** is thus given by:
+    **Programming noise** is thus given by:
 
-    .. math::
+     .. math::
 
-         \sigma_text{programming noise}=\gamma\,\left(c_0 + c_1 \frac{g_T}{g_\text{max}} +
-              + c_2 \frac{g_T^2}{g_\text{max}^2}\right)
+          \sigma_text{programming noise}=\gamma\,\left(c_0 + c_1 \frac{g_T}{g_\text{max}} +
+               + c_2 \frac{g_T^2}{g_\text{max}^2}\right)
 
-    where :math:`\gamma` is a additional convenience scale and :math:`g_T`
-    is the target conductance established from the given
-    ``g_converter`` from the weight matrix.  The default programming
-    noise is constant (state independent): :math:`c_0=0.2\mu\mathrm{S}`
-    and other coefficient set to :math:`0.0`.
+     where :math:`\gamma` is a additional convenience scale and :math:`g_T`
+     is the target conductance established from the given
+     ``g_converter`` from the weight matrix.  The default programming
+     noise is constant (state independent): :math:`c_0=0.2\mu\mathrm{S}`
+     and other coefficient set to :math:`0.0`.
 
-    **Drift** is for each device is computed as
+     **Drift** is for each device is computed as
 
-    .. math::
+     .. math::
 
-        g_\text{drift}(t) = g_\text{prog}(t / t_0) ^{- \nu}
+         g_\text{drift}(t) = g_\text{prog}(t / t_0) ^{- \nu}
 
-    with the drift coefficient determined at the beginning for each
-    device with
+     with the drift coefficient determined at the beginning for each
+     device with
 
-    .. math::
+     .. math::
 
-        \nu= \zeta\, |\nu_\text{mean} + \nu_\text{std}\xi|_+
+         \nu= \zeta\, |\nu_\text{mean} + \nu_\text{std}\xi|_+
 
-    where :math:`\xi` is a Gaussian random number and
-    :math:`|\cdot|_+` rectifies negative value to zero. :math:`\zeta`
-    is an additional drift scale.
+     where :math:`\xi` is a Gaussian random number and
+     :math:`|\cdot|_+` rectifies negative value to zero. :math:`\zeta`
+     is an additional drift scale.
 
-    **Read noise** is given by
+     **Read noise** is given by
 
-    .. math::
+     .. math::
 
-        \sigma_\text{read} = \rho \frac{g_\text{drift}(t)}{g_\text{max}} \sqrt{\log\left(\frac{t
-        + t_\text{read}}{2 t_\text{read}}\right)}
+         \sigma_\text{read} = \rho \frac{g_\text{drift}(t)}{g_\text{max}} \sqrt{\log\left(\frac{t
+         + t_\text{read}}{2 t_\text{read}}\right)}
 
-    This :math:`\sigma_\text{read}` is then used to add Gaussian noise
-    of this magnitude to the drifted conductance. The read noise scale
-    :math:`\rho` can be used to scale the read noise.
+     This :math:`\sigma_\text{read}` is then used to add Gaussian noise
+     of this magnitude to the drifted conductance. The read noise scale
+     :math:`\rho` can be used to scale the read noise.
 
-    Args:
-        g_converter: instantiated class of the conductance converter
-            (defaults to single pair)
+     Args:
+         g_converter: instantiated class of the conductance converter
+             (defaults to single pair)
 
-        g_max: In :math:`\mu S`, the maximal conductance, ie the value
-            the absolute max of the weights will be mapped to.
+         g_max: In :math:`\mu S`, the maximal conductance, ie the value
+             the absolute max of the weights will be mapped to.
 
-        prog_coeff: programming polynomial coefficients :math:`c_i` in
-            :math:`\mu S`. Default is constant :math:`c_0=0.2` and
-            other coefficient set to 0.0.
+         prog_coeff: programming polynomial coefficients :math:`c_i` in
+             :math:`\mu S`. Default is constant :math:`c_0=0.2` and
+             other coefficient set to 0.0.
 
-        prog_noise_scale: scale :math:\gamma: for the programming noise
+         prog_noise_scale: scale :math:\gamma: for the programming noise
 
-        drift_nu_mean: mean :math:`\nu_\text{mean}` of power-law drift
-            coefficient (:math:`\nu`) (before ``drift_scale``
-            :math:`\zeta` is applied).
+         drift_nu_mean: mean :math:`\nu_\text{mean}` of power-law drift
+             coefficient (:math:`\nu`) (before ``drift_scale``
+             :math:`\zeta` is applied).
 
-        drift_nu_std: device-to-device variability
-            :math:`\nu_\text{std}` of the power-law drift coefficient
-            (before ``drift_scale`` is applied)
+         drift_nu_std: device-to-device variability
+             :math:`\nu_\text{std}` of the power-law drift coefficient
+             (before ``drift_scale`` is applied)
 
-        drift_scale: additional scale :math:`\zeta` applied to all
-            drawn drift coefficients
+         drift_scale: additional scale :math:`\zeta` applied to all
+             drawn drift coefficients
 
-        t_0: parameter of the drift (first reading time), see above.
+         t_0: parameter of the drift (first reading time), see above.
 
-            Note:
-                The ``t_inference`` is relative to this time ``t0``
-                e.g. ``t_inference`` counts from the completion of the
-                programming of a device.
+             Note:
+                 The ``t_inference`` is relative to this time ``t0``
+                 e.g. ``t_inference`` counts from the completion of the
+                 programming of a device.
 
-        read_noise_scale: scale :math:`\rho` for scaling the read and
-            accumulated noise :math:`1/f`.
+         read_noise_scale: scale :math:`\rho` for scaling the read and
+             accumulated noise :math:`1/f`.
 
-        t_read: parameter of the :math:`1/f` noise (in seconds)
+         t_read: parameter of the :math:`1/f` noise (in seconds)
 
     """
 
     def __init__(  # pylint: disable=too-many-arguments
-            self,
-            g_converter: Optional[BaseConductanceConverter] = None,
-            g_max: Optional[float] = None,
-            prog_coeff: Optional[List[float]] = None,
-            prog_noise_scale: float = 1.0,
-            drift_nu_mean: float = 0.1,
-            drift_nu_std: float = 0.05,
-            drift_scale: float = 1.0,
-            t_0: float = 20.0,
-            read_noise_scale: float = 1.0,
-            t_read: float = 250.0e-9,
+        self,
+        g_converter: Optional[BaseConductanceConverter] = None,
+        g_max: Optional[float] = None,
+        prog_coeff: Optional[List[float]] = None,
+        prog_noise_scale: float = 1.0,
+        drift_nu_mean: float = 0.1,
+        drift_nu_std: float = 0.05,
+        drift_scale: float = 1.0,
+        t_0: float = 20.0,
+        read_noise_scale: float = 1.0,
+        t_read: float = 250.0e-9,
     ):
         g_converter = deepcopy(g_converter) or SinglePairConductanceConverter(g_max=g_max)
         super().__init__(g_converter)
 
-        self.g_max = getattr(self.g_converter, 'g_max', g_max)
+        self.g_max = getattr(self.g_converter, "g_max", g_max)
 
         if self.g_max is None:
-            raise ValueError('g_max cannot be established from g_converter')
+            raise ValueError("g_max cannot be established from g_converter")
 
         self.prog_coeff = [0.2, 0.0, 0.0] if prog_coeff is None else prog_coeff
         self.prog_noise_scale = prog_noise_scale
         self.drift_nu_mean = drift_nu_mean
         self.drift_nu_std = drift_nu_std
         self.drift_scale = drift_scale
         self.t_0 = t_0
         self.read_noise_scale = read_noise_scale
         self.t_read = t_read
 
     def __str__(self) -> str:
-        ret = self.__class__.__name__ + '('
-        for key in ['g_converter', 'prog_coeff', 'prog_noise_scale',
-                    'drift_nu_mean', 'drift_nu_std', 'drift_scale',
-                    't_0', 'read_noise_scale', 't_0']:
-            ret += key + '={}, '.format(self.__dict__[key])
-        ret = ret[:-2] + ')'
+        ret = self.__class__.__name__ + "("
+        for key in [
+            "g_converter",
+            "prog_coeff",
+            "prog_noise_scale",
+            "drift_nu_mean",
+            "drift_nu_std",
+            "drift_scale",
+            "t_0",
+            "read_noise_scale",
+            "t_0",
+        ]:
+            ret += key + "={}, ".format(self.__dict__[key])
+        ret = ret[:-2] + ")"
         return ret
 
     @no_grad()
     def apply_programming_noise_to_conductance(self, g_target: Tensor) -> Tensor:
         """Apply programming noise to a target conductance Tensor.
 
         Programming noise with additive Gaussian noise with
@@ -178,31 +186,29 @@
         mu_drift = self.drift_nu_mean
         sig_drift = self.drift_nu_std
         nu_drift = torch_abs(mu_drift + sig_drift * randn_like(g_target)).clamp(min=0.0)
         return nu_drift * self.drift_scale
 
     @no_grad()
     def apply_drift_noise_to_conductance(
-            self,
-            g_prog: Tensor,
-            nu_drift: Tensor,
-            t_inference: float
+        self, g_prog: Tensor, nu_drift: Tensor, t_inference: float
     ) -> Tensor:
         """Apply the noise and drift up to the assumed inference time
         point."""
         t = t_inference + self.t_0
 
         # drift
         if t > self.t_0:
-            g_drift = g_prog * ((t / self.t_0) ** (- nu_drift))
+            g_drift = g_prog * ((t / self.t_0) ** (-nu_drift))
         else:
             g_drift = g_prog
 
         # expected accumulated 1/f noise since start of programming at t=0
         if t > 0:
             sig_noise = sqrt(numpy_log((t + self.t_read) / (2 * self.t_read)))
-            g_final = g_drift + torch_abs(g_drift / self.g_max) * self.read_noise_scale \
-                * sig_noise * randn_like(g_drift)
+            g_final = g_drift + torch_abs(
+                g_drift / self.g_max
+            ) * self.read_noise_scale * sig_noise * randn_like(g_drift)
         else:
             g_final = g_prog
 
         return g_final.clamp(min=0.0)
```

### Comparing `aihwkit-0.7.1/src/aihwkit/inference/noise/pcm.py` & `aihwkit-0.8.0/src/aihwkit/inference/noise/pcm.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -32,68 +32,97 @@
 
 class PCMLikeNoiseModel(BaseNoiseModel):
     r"""Noise model that was fitted and characterized on real PCM devices.
 
     Expected weight noise at assumed time of inference with expected
     programming noise at 0.
 
-    The statistical noise model is based on measured PCM devices.
+    The statistical noise model is based on measured PCM devices. See
+    also `Nandakumar et al. ICECS (2019)`_
 
     Args:
-        prog_coeff: programming polynomial coeffs in :math:`\mu S`, c(0) + c(1)*gt + c(2)*gt^2)
-        g_converter: instantiated class of the conductance converter (defaults to single pair)
+        prog_coeff: Programming polynomial coeffs in
+            :math:`\sum_i c_i \left(\frac{g_t}{g_\max}\right)^i`
+        g_converter: instantiated class of the conductance converter
+            (defaults to single pair)
         g_max: In :math:`\mu S`, the maximal conductance, ie the value
             the absolute max of the weights will be mapped to.
-        t_read: parameter of the 1/f fit (in seconds)
-        t_0: parameter of the drift fit (first reading time)
+        t_read: Parameter of the 1/f fit (in seconds).
+        t_0: Parameter of the drift fit (first reading time).
 
             Note:
                 The ``t_inference`` is relative to this time `t0`
                 e.g. t_inference counts from the completion of the programming
                 of a device.
-        prog_noise_scale: scale for the programming noise
-        read_noise_scale: scale for the read and accumulated noise
-        drift_scale: scale for the  drift coefficient
+        prog_noise_scale: Scale for the programming noise.
+        read_noise_scale: Scale for the read and accumulated noise.
+        drift_scale: Scale for the  drift coefficient.
+        prog_coeff_g_max_reference: reference :math:`g_\max` value
+            when fitting the coefficients, since the result of the
+            polynomial fit is given in uS. If
+            ``prog_coeff_g_max_reference`` is not given and
+            `prog_coeffs` are given explicitly, it will be set to
+            ``g_max`` of the conductance converter.
+
+    .. _`Nandakumar et al. ICECS (2019)`: https://ieeexplore.ieee.org/abstract/document/8964852
 
     """
 
     def __init__(
-            self,
-            prog_coeff: Optional[List[float]] = None,
-            g_converter: Optional[BaseConductanceConverter] = None,
-            g_max: Optional[float] = None,
-            t_read: float = 250.0e-9,
-            t_0: float = 20.0,
-            prog_noise_scale: float = 1.0,
-            read_noise_scale: float = 1.0,
-            drift_scale: float = 1.0,
+        self,
+        prog_coeff: Optional[List[float]] = None,
+        g_converter: Optional[BaseConductanceConverter] = None,
+        g_max: Optional[float] = None,
+        t_read: float = 250.0e-9,
+        t_0: float = 20.0,
+        prog_noise_scale: float = 1.0,
+        read_noise_scale: float = 1.0,
+        drift_scale: float = 1.0,
+        prog_coeff_g_max_reference: Optional[float] = None,
     ):
         g_converter = deepcopy(g_converter) or SinglePairConductanceConverter(g_max=g_max)
         super().__init__(g_converter)
 
-        self.g_max = getattr(self.g_converter, 'g_max', g_max)
-
+        self.g_max = getattr(self.g_converter, "g_max", g_max)
         if self.g_max is None:
-            raise ValueError('g_max cannot be established from g_converter')
+            raise ValueError("g_max cannot be established from g_converter")
+
+        if prog_coeff_g_max_reference is None:
+            self.prog_coeff_g_max_reference = self.g_max
+
+        if prog_coeff is None:
+            # standard g_max are defined in respect to 25.0 uS. Need to
+            # adjust for that in case g_max is not equal to 25.0 uS
+            self.prog_coeff = [0.26348, 1.9650, -1.1731]
+            self.prog_coeff_g_max_reference = 25.0
+        else:
+            self.prog_coeff = prog_coeff
 
-        self.prog_coeff = [0.26348, 1.9650, -1.1731] if prog_coeff is None else prog_coeff
         self.t_0 = t_0
         self.t_read = t_read
         self.prog_noise_scale = prog_noise_scale
         self.read_noise_scale = read_noise_scale
         self.drift_scale = drift_scale
 
     def __str__(self) -> str:
-        return ('{}(prog_coeff={}, g_converter={}, g_max={:1.2f}, t_read={}, '
-                't_0={:1.2f}, prog_noise_scale={}, '
-                'read_noise_scale={}, drift_scale={})').format(  # type: ignore
-                    self.__class__.__name__, self.prog_coeff, self.g_converter,
-                    self.g_max,
-                    self.t_read, self.t_0, self.prog_noise_scale,
-                    self.read_noise_scale, self.drift_scale)
+        return (
+            "{}(prog_coeff={}, g_converter={}, g_max={:1.2f}, t_read={}, "
+            "t_0={:1.2f}, prog_noise_scale={}, "
+            "read_noise_scale={}, drift_scale={})"
+        ).format(  # type: ignore
+            self.__class__.__name__,
+            self.prog_coeff,
+            self.g_converter,
+            self.g_max,
+            self.t_read,
+            self.t_0,
+            self.prog_noise_scale,
+            self.read_noise_scale,
+            self.drift_scale,
+        )
 
     @no_grad()
     def apply_programming_noise_to_conductance(self, g_target: Tensor) -> Tensor:
         """Apply programming noise to a target conductance Tensor.
 
         Programming noise with additive Gaussian noise with
         conductance dependency of the variance given by a 2-degree
@@ -101,14 +130,15 @@
         """
         mat = 1
         sig_prog = self.prog_coeff[0]
         for coeff in self.prog_coeff[1:]:
             mat *= g_target / self.g_max
             sig_prog += mat * coeff
 
+        sig_prog *= self.g_max / self.prog_coeff_g_max_reference  # type: ignore
         g_prog = g_target + self.prog_noise_scale * sig_prog * randn_like(g_target)
         g_prog.clamp_(min=0.0)  # no negative conductances allowed
 
         return g_prog
 
     @no_grad()
     def generate_drift_coefficients(self, g_target: Tensor) -> Tensor:
@@ -120,33 +150,32 @@
         sig_drift = (-0.0125 * log(g_relative) - 0.0059).clamp(min=0.008, max=0.045)
         nu_drift = torch_abs(mu_drift + sig_drift * randn_like(g_relative)).clamp(min=0.0)
 
         return nu_drift * self.drift_scale
 
     @no_grad()
     def apply_drift_noise_to_conductance(
-            self,
-            g_prog: Tensor,
-            nu_drift: Tensor,
-            t_inference: float
+        self, g_prog: Tensor, nu_drift: Tensor, t_inference: float
     ) -> Tensor:
         """Apply the noise and drift up to the assumed inference time
         point based on PCM measurements."""
         t = t_inference + self.t_0
 
         # drift
         if t > self.t_0:
-            g_drift = g_prog * ((t / self.t_0) ** (- nu_drift))
+            g_drift = g_prog * ((t / self.t_0) ** (-nu_drift))
         else:
             g_drift = g_prog
 
         # expected accumulated 1/f noise since start of programming at t=0
         if t > 0:
-            q_s = (0.0088 / ((torch_abs(g_prog) /
-                              self.g_max) ** 0.65).clamp(min=1e-3)).clamp(max=0.2)
+            q_s = (0.0088 / ((torch_abs(g_prog) / self.g_max) ** 0.65).clamp(min=1e-3)).clamp(
+                max=0.2
+            )
             sig_noise = q_s * sqrt(numpy_log((t + self.t_read) / (2 * self.t_read)))
-            g_final = g_drift + torch_abs(g_drift) * self.read_noise_scale \
-                * sig_noise * randn_like(g_prog)
+            g_final = g_drift + torch_abs(g_drift) * self.read_noise_scale * sig_noise * randn_like(
+                g_prog
+            )
         else:
             g_final = g_prog
 
         return g_final.clamp(min=0.0)
```

### Comparing `aihwkit-0.7.1/src/aihwkit/inference/utils.py` & `aihwkit-0.8.0/src/aihwkit/inference/utils.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -24,21 +24,21 @@
         t_inference: assumed time of inference (in sec)
 
     Raises:
         ModuleError: if the layer is not in evaluation mode.
     """
     # avoid circular import
     # pylint: disable=import-outside-toplevel
-    from aihwkit.nn.modules.base import AnalogModuleBase
+    from aihwkit.nn.modules.base import AnalogLayerBase
+
     if model.training:
-        raise ModuleError('drift_analog_weights can only be applied in '
-                          'evaluation mode')
+        raise ModuleError("drift_analog_weights can only be applied in  evaluation mode")
 
     for module in model.modules():
-        if not isinstance(module, AnalogModuleBase):
+        if not isinstance(module, AnalogLayerBase):
             continue
         module.drift_analog_weights(t_inference)
 
 
 def program_analog_weights(model: Module) -> None:
     """Program all analog inference layers of a given model.
 
@@ -46,16 +46,16 @@
         model: torch model with analog layers
 
     Raises:
         ModuleError: if the layer is not in evaluation mode.
     """
     # avoid circular import
     # pylint: disable=import-outside-toplevel
-    from aihwkit.nn.modules.base import AnalogModuleBase
+    from aihwkit.nn.modules.base import AnalogLayerBase
+
     if model.training:
-        raise ModuleError('program_analog_weights can only be applied in '
-                          'evaluation mode')
+        raise ModuleError("program_analog_weights can only be applied in evaluation mode")
 
     for module in model.modules():
-        if not isinstance(module, AnalogModuleBase):
+        if not isinstance(module, AnalogLayerBase):
             continue
         module.program_analog_weights()
```

### Comparing `aihwkit-0.7.1/src/aihwkit/nn/__init__.py` & `aihwkit-0.8.0/src/aihwkit/nn/__init__.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,26 +1,32 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
 # that they have been altered from the originals.
 
 """Neural network modules."""
 
 # Convenience imports for easier access to the classes.
 
-from aihwkit.nn.modules.container import AnalogSequential
+from aihwkit.nn.modules.container import AnalogSequential, AnalogWrapper
 from aihwkit.nn.modules.conv import AnalogConv1d, AnalogConv2d, AnalogConv3d
 from aihwkit.nn.modules.linear import AnalogLinear
 from aihwkit.nn.modules.rnn.rnn import AnalogRNN
-from aihwkit.nn.modules.rnn.cells import AnalogGRUCell, AnalogLSTMCell, AnalogVanillaRNNCell, \
-    AnalogLSTMCellCombinedWeight
+from aihwkit.nn.modules.rnn.cells import (
+    AnalogGRUCell,
+    AnalogLSTMCell,
+    AnalogVanillaRNNCell,
+    AnalogLSTMCellCombinedWeight,
+)
 from aihwkit.nn.modules.linear_mapped import AnalogLinearMapped
 from aihwkit.nn.modules.conv_mapped import (
-    AnalogConv1dMapped, AnalogConv2dMapped, AnalogConv3dMapped
+    AnalogConv1dMapped,
+    AnalogConv2dMapped,
+    AnalogConv3dMapped,
 )
```

### Comparing `aihwkit-0.7.1/src/aihwkit/nn/functions.py` & `aihwkit-0.8.0/src/aihwkit/simulator/tiles/functions.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,72 +1,83 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
 # that they have been altered from the originals.
 
 """Autograd functions for aihwkit."""
 
 from typing import Any, Optional, Tuple
 
 from torch import Tensor, empty_like
-from torch.autograd import Function
+from torch.autograd import Function, no_grad
 from aihwkit.optim.context import AnalogContext
 
 
-class AnalogFunctionBase(Function):
-    """Base function for analog functions."""
+class AnalogFunction(Function):
+    """Function for analog functions."""
+
     # pylint: disable=arguments-differ, protected-access, abstract-method
 
     @staticmethod
+    @no_grad()
     def forward(
-            ctx: Any,
-            analog_ctx: AnalogContext,
-            input_: Tensor,
-            shared_weights: Optional[Tensor] = None,
-            is_test: bool = False) -> Tensor:
+        ctx: Any,
+        analog_ctx: AnalogContext,
+        analog_tile: Any,
+        input_: Tensor,
+        shared_weights: Optional[Tensor] = None,
+        is_test: bool = False,
+    ) -> Tensor:
         """Execute the forward pass in the analog tile.
-
         Note: Indexed versions can used when analog_ctx.use_indexed is
         set to True.
         """
         # Store in context for using during `backward()`.
-        analog_tile = analog_ctx.analog_tile
         ctx.analog_ctx = analog_ctx
+        ctx.analog_tile = analog_tile
         ctx.shared_weights = None
-        ctx.save_for_backward(input_)
+        ctx.saved_analog_tensors = [input_]
 
         use_indexed = analog_ctx.use_indexed
         if shared_weights is not None:
             ctx.shared_weights = shared_weights
             analog_tile.ensure_shared_weights(shared_weights)
             analog_ctx.use_torch_update = True
         else:
             analog_ctx.use_torch_update = False
 
         # Invoke the forward pass in the tile instance.
         if use_indexed:
-            return analog_tile.forward_indexed(input_, is_test, ctx)
-        return analog_tile.forward(input_, is_test, ctx)
+            out = analog_tile.joint_forward_indexed(input_, is_test, ctx)
+        else:
+            out = analog_tile.joint_forward(input_, is_test, ctx)
+
+        ctx.save_for_backward(*ctx.saved_analog_tensors)
+        ctx.saved_analog_tensors = []
+        return out
 
     @staticmethod
+    @no_grad()
     def backward(
-            ctx: Any,
-            grad_output: Tensor,
-    ) -> Tuple[Optional[Tensor], Optional[Tensor], Optional[Tensor], Optional[Tensor]]:
+        ctx: Any, grad_output: Tensor
+    ) -> Tuple[
+        Optional[Tensor], Optional[Tensor], Optional[Tensor], Optional[Tensor], Optional[Tensor]
+    ]:
         """Execute the backward pass in the analog tile."""
         analog_ctx = ctx.analog_ctx
-        analog_tile = analog_ctx.analog_tile
-        input_ = ctx.saved_tensors[0]
+        analog_tile = ctx.analog_tile
+        ctx.saved_analog_tensors = ctx.saved_tensors
+        input_ = ctx.saved_analog_tensors[0]
 
         shared_weights_grad = None
         use_indexed = analog_ctx.use_indexed
 
         if ctx.shared_weights is not None:
             analog_tile.ensure_shared_weights(ctx.shared_weights)
 
@@ -86,42 +97,9 @@
                 analog_tile.update(input_, grad_output)
             analog_tile.reset_delta_weights()
         else:
             # Store activation and errors for optimizer (for analog training)
             analog_ctx.analog_input.append(input_)
             analog_ctx.analog_grad_output.append(grad_output)
 
-        return None, grad_input, shared_weights_grad, None
-
-
-class AnalogFunction(AnalogFunctionBase):
-    """Function that delegates into a `RPU` unit."""
-    # pylint: disable=arguments-differ, abstract-method
-
-    @staticmethod
-    def forward(
-            ctx: Any,
-            analog_ctx: AnalogContext,
-            input_: Tensor,
-            shared_weights: Optional[Tensor] = None,
-            is_test: bool = False) -> Tensor:
-        """Execute the forward pass in the analog tile."""
-        analog_ctx.use_indexed = False
-        return AnalogFunctionBase.forward(
-            ctx, analog_ctx, input_, shared_weights, is_test)
-
-
-class AnalogIndexedFunction(AnalogFunctionBase):
-    """Function that delegates into a `RPU` unit to use the indexed forward/backward/update."""
-    # pylint: disable=arguments-differ, abstract-method
-
-    @staticmethod
-    def forward(
-            ctx: Any,
-            analog_ctx: AnalogContext,
-            input_: Tensor,
-            shared_weights: Optional[Tensor] = None,
-            is_test: bool = False) -> Tensor:
-        """Execute the forward pass in the analog tile."""
-        analog_ctx.use_indexed = True
-        return AnalogFunctionBase.forward(
-            ctx, analog_ctx, input_, shared_weights, is_test)
+        ctx.saved_analog_tensors = []
+        return None, None, grad_input, shared_weights_grad, None
```

### Comparing `aihwkit-0.7.1/src/aihwkit/nn/modules/__init__.py` & `aihwkit-0.8.0/src/aihwkit/nn/modules/rnn/__init__.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
 # that they have been altered from the originals.
 
-"""Neural network modules."""
+"""Analog RNN related modules."""
```

### Comparing `aihwkit-0.7.1/src/aihwkit/nn/modules/base.py` & `aihwkit-0.8.0/src/aihwkit/simulator/parameters/utils.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,650 +1,848 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
 # that they have been altered from the originals.
 
-"""Base class for analog Modules."""
-from typing import (
-    Any, Dict, List, Optional, Tuple, NamedTuple, Union,
-    Generator, TYPE_CHECKING
+# pylint: disable=too-many-instance-attributes
+# pylint: disable=too-many-lines
+
+"""Utility parameters for resistive processing units."""
+
+from dataclasses import dataclass, field
+from typing import ClassVar, Type, Any, List, Optional
+
+from aihwkit.simulator.parameters.helpers import _PrintableMixin
+from aihwkit.simulator.rpu_base import devices, tiles
+from aihwkit.simulator.parameters.enums import (
+    BoundManagementType,
+    NoiseManagementType,
+    WeightNoiseType,
+    PulseType,
+    WeightModifierType,
+    WeightClipType,
+    WeightRemapType,
+    AnalogMVType,
 )
-from copy import deepcopy
-from warnings import warn
 
-from torch import Tensor, no_grad
-from torch.nn import Module, Parameter
-from torch import device as torch_device
-
-from aihwkit.exceptions import ModuleError
-from aihwkit.simulator.configs.utils import MappingParameter
-from aihwkit.simulator.tiles import InferenceTile
-from aihwkit.optim.context import AnalogContext
-
-if TYPE_CHECKING:
-    from aihwkit.simulator.tiles import BaseTile
-    from collections import OrderedDict
-    from aihwkit.simulator.configs.configs import (
-        FloatingPointRPUConfig, InferenceRPUConfig, SingleRPUConfig,
-        UnitCellRPUConfig, DigitalRankUpdateRPUConfig
-    )
 
-RPUConfigAlias = Union['FloatingPointRPUConfig', 'SingleRPUConfig',
-                       'UnitCellRPUConfig', 'InferenceRPUConfig',
-                       'DigitalRankUpdateRPUConfig']
-
-
-class AnalogModuleBase(Module):
-    """Base class for analog Modules.
-
-    Base ``Module`` for analog layers that use analog tiles. When subclassing,
-    please note:
-
-    * the :meth:`_setup_tile()` method is expected to be called by the subclass
-      constructor, and it does not only create a tile.
-    * :meth:`register_analog_tile` needs to be called for each created analog tile
-    * this module does *not* call torch's ``Module`` init as the child is
-      likely again derived from Module
-    * the ``weight`` and ``bias`` Parameters are not guaranteed to be in
-      sync with the tile weights and biases during the lifetime of the instance,
-      for performance reasons. The canonical way of reading and writing
-      weights is via the :meth:`set_weights()` and :meth:`get_weights()` as opposed
-      to using the attributes directly.
-    * the ``BaseTile`` subclass that is created is retrieved from the
-      ``rpu_config.tile_class`` attribute.
-
-    Args:
-        in_features: input vector size (number of columns).
-        out_features: output vector size (number of rows).
-        bias: whether to use a bias row on the analog tile or not.
-        realistic_read_write: whether to enable realistic read/write
-            for setting initial weights and during reading of the weights.
-        mapping: Configuration of the hardware architecture (e.g. tile size).
-    """
-    # pylint: disable=abstract-method, too-many-instance-attributes
-    ANALOG_CTX_PREFIX: str = 'analog_ctx_'
-    ANALOG_SHARED_WEIGHT_PREFIX: str = 'analog_shared_weights_'
-    ANALOG_STATE_PREFIX: str = 'analog_tile_state_'
-    ANALOG_OUT_SCALING_ALPHA_PREFIX: str = 'analog_out_scaling_alpha_'
-    ANALOG_INPUT_RANGE_PREFIX: str = 'analog_input_range_'
-
-    def __init__(
-            self,
-            in_features: int,
-            out_features: int,
-            bias: bool,
-            realistic_read_write: bool = False,
-            mapping: Optional[MappingParameter] = None,
-    ) -> None:
-        # pylint: disable=super-init-not-called
-        self._analog_tile_counter = 0
-        self._registered_helper_parameter = []  # type: list
-        self._load_rpu_config = True
-        self._strict_rpu_config_check = True
-
-        if mapping is None:
-            mapping = MappingParameter()
-
-        self.use_bias = bias
-        self.digital_bias = bias and mapping.digital_bias
-        self.analog_bias = bias and not mapping.digital_bias
-        self.realistic_read_write = realistic_read_write
-        self.in_features = in_features
-        self.out_features = out_features
-
-    def register_helper(self, name: str) -> None:
-        """Register a helper name that is not saved to the state dict """
-
-        if name not in self._registered_helper_parameter:
-            self._registered_helper_parameter.append(name)
-
-    def register_analog_tile(self, tile: 'BaseTile', name: Optional[str] = None,
-                             update_only: bool = False) -> None:
-        """Register the analog context of the tile.
-
-        Note:
-            Needs to be called at the end init to register the tile
-            for the analog optimizers.
+@dataclass
+class IOParameters(_PrintableMixin):
+    """Parameter that define the analog-matvec (forward / backward) and
+    peripheral digital input-output behavior.
+
+    Here one can enable analog-digital conversion, dynamic input
+    scaling, and define the properties of the analog-matvec
+    computations, such as noise and non-idealities (e.g. IR-drop).
+    """
 
-        Args:
-            tile: tile to register
-            name: Optional tile name used as the parameter name.
-            update_only: Whether to re-register (does not advance tile counter)
-        """
+    bindings_class: ClassVar[Type] = devices.AnalogTileInputOutputParameter
 
-        if name is None:
-            name = str(self._analog_tile_counter)
+    is_perfect: bool = False
+    """Short-cut to compute a perfect forward pass.
 
-        if not update_only:
-            self._analog_tile_counter += 1
+    If ``True``, it assumes an ideal forward pass (e.g. no bound, ADC etc...).
+    Will disregard all other settings in this case.
+    """
 
-        ctx_name = self.ANALOG_CTX_PREFIX + name
+    mv_type: AnalogMVType = AnalogMVType.ONE_PASS
+    """Selects the type of analog mat-vec computation. See
+    :class:`AnalogMVType` for details. """
 
-        self.register_helper(ctx_name)
-        self.register_parameter(ctx_name, tile.get_analog_ctx())
-
-        if tile.shared_weights is not None:
-            if not isinstance(tile.shared_weights, Parameter):
-                tile.shared_weights = Parameter(tile.shared_weights)
-            par_name = self.ANALOG_SHARED_WEIGHT_PREFIX + name
-            self.register_parameter(par_name, tile.shared_weights)
-            self.register_helper(par_name)
-
-        if tile.out_scaling_alpha is not None:
-            if not isinstance(tile.out_scaling_alpha, Parameter):
-                tile.out_scaling_alpha = Parameter(tile.out_scaling_alpha)
-            par_name = self.ANALOG_OUT_SCALING_ALPHA_PREFIX + name
-            self.register_parameter(par_name, tile.out_scaling_alpha)
-            self.register_helper(par_name)
-
-        if tile.input_range is not None:
-            if not isinstance(tile.input_range, Parameter):
-                tile.input_range = Parameter(tile.input_range)
-            par_name = self.ANALOG_INPUT_RANGE_PREFIX + name
-            self.register_parameter(par_name, tile.input_range)
-            self.register_helper(par_name)
+    inp_bound: float = 1.0
+    """Input bound and ranges for the digital-to-analog converter (DAC)."""
 
-    def unregister_parameter(self, param_name: str) -> None:
-        """Unregister module parameter from parameters.
+    inp_noise: float = 0.0
+    r"""Std deviation of Gaussian input noise (:math:`\sigma_\text{inp}`).
 
-        Raises:
-            ModuleError: In case parameter is not found
-        """
-        param = getattr(self, param_name, None)
-        if not isinstance(param, Parameter):
-            raise ModuleError(f"Cannot find parameter {param_name} to unregister")
-        param_data = param.detach().clone()
-        delattr(self, param_name)
-        setattr(self, param_name, param_data)
-
-    def analog_tiles(self) -> Generator['BaseTile', None, None]:
-        """ Generator to loop over all registered analog tiles of the module """
-        for param in self.parameters():
-            if isinstance(param, AnalogContext):
-                yield param.analog_tile
-
-    def named_analog_tiles(self) -> Generator[Tuple[str, 'BaseTile'], None, None]:
-        """ Generator to loop over all registered analog tiles of the module with names. """
-        for name, param in self.named_parameters():
-            if isinstance(param, AnalogContext):
-                new_name = name.split(self.ANALOG_CTX_PREFIX)[-1]
-                yield (new_name, param.analog_tile)
+    i.e. noisiness of the analog input (at the stage after DAC and
+    before the multiplication).
+    """
 
-    def get_analog_tile_devices(self) -> List[Optional[Union[torch_device, str, int]]]:
-        """ Return a list of the devices used by the analog tiles.
+    inp_res: float = 1 / (2**7 - 2)
+    r"""Number of discretization steps for DAC (:math:`\le0` means infinite steps)
+    or resolution (1/steps)."""
+
+    inp_sto_round: bool = False
+    """Whether to enable stochastic rounding of DAC."""
+
+    inp_asymmetry: float = 0.0
+    """Input asymmetry :math:`a_\text{input}`.
+
+    Input of the negative input pass is scaled by :math:`(1 - a_\text{input})`.
+
+    Note:
+        This setting has only effect in case of and
+        :class:`AnalogMVType` that uses separate passes for positive
+        and negative inputs.
+    """
 
-        Returns:
-            List of torch devices
-        """
-        return [d.device for d in self.analog_tiles()]
+    out_bound: float = 12.0
+    """Output bound and ranges for analog-to-digital converter (ADC)."""
 
-    def analog_tile_count(self) -> int:
-        """Return the number of registered tiles.
+    out_noise: float = 0.06
+    r"""Output noise strength at each output of a tile.
 
-        Returns:
-           Number of registered tiles
+    This sets the std-deviation of the Gaussian output noise
+    (:math:`\sigma_\text{out}`) at each output, i.e. noisiness of
+    device summation at the output.
+    """
 
-        """
-        return getattr(self, '_analog_tile_counter', 0)
+    out_noise_std: float = 0.0
+    r"""Systematic output-to-output variation of the output noise strength.
 
-    def _setup_tile(
-            self,
-            rpu_config: RPUConfigAlias,
-    ) -> 'BaseTile':
-        """Create a single analog tile with the given RPU configuration.
-
-        Create an analog tile to be used for the basis of this layer
-        operations, while using the attributes ``(in_features,
-        out_features, bias)`` given to this instance during init.
+    In fraction of the ``out_noise`` parameter, that is 0.3 would mean a
+    30\% variation of the output noise std deviation given by ``out_noise``.
 
-        After tile creation, the tile needs to be registered using
-        :meth:`register_analog_tile`.
+    Note:
 
-        Args:
-            rpu_config: resistive processing unit configuration.
+        This variation is drawn at instantiation and kept fixed
+        thereafter. It can be adjusted, however, with
+        ``analog_tile.set_forward/backward_parameter({'out_noise_values':
+        x})`` for each analog tiles (if implemented).
 
-        Returns:
-            An analog tile with the requested parameters.
+    Caution:
 
-        """
-        # pylint: disable=protected-access
+      This is *not* simply the output noise std.-dev, but the
+      systematic variation of the noise strength across outputs. Use
+      ``out_noise`` to set the former.
+    """
 
-        # Create the tile.
-        return rpu_config.tile_class(self.out_features, self.in_features, rpu_config,
-                                     bias=self.analog_bias)
-
-    def set_weights(
-            self,
-            weight: Tensor,
-            bias: Optional[Tensor] = None,
-            force_exact: bool = False,
-            apply_weight_scaling: bool = True,
-            weight_scaling_omega: Optional[float] = None
-    ) -> None:
-        """Set the weight (and bias) values with given tensors.
-
-        This uses an realistic write if the property ``realistic_read_write``
-        of the layer is set, unless it is overwritten by ``force_exact``.
-
-        If ``weight_scaling_omega`` is larger than 0, the weights are set in a
-        scaled manner (assuming a digital output scale). See
-        :meth:`~aihwkit.simulator.tiles.base.apply_weight_scaling`
-        for details.
-
-        Note:
-            This is the recommended way for setting the weight/bias matrix of
-            the analog tile, as it will correctly store the weights into the
-            internal memory. Directly writing to ``self.weight`` and
-            ``self.bias`` might yield wrong results as they are not always in
-            sync with the analog tile Parameters, for performance reasons.
+    out_res: float = 1 / (2**9 - 2)
+    """Number of discretization steps for ADC or resolution.
 
-        Args:
-            weight: weight matrix
-            bias: bias vector
-            force_exact: forces an exact write to the analog tiles
-            apply_weight_scaling: Whether to rescale the given weight matrix
-                and populate the digital output scaling factors as
-                specified in the configuration
-                :class:`~aihwkit.configs.utils.MappingParameter`. A
-                new ``weight_scaling_omega`` can be given. Note that
-                this will overwrite the existing digital out scaling
-                factors.
-            weight_scaling_omega: The weight scaling omega factor (see
-                :class:`~aihwkit.configs.utils.MappingParameter`). If
-                given explicitly here, it will overwrite the value in
-                the mapping field.
+    Number of discretization steps for ADC (:math:`<=0` means infinite steps)
+    or resolution (1/steps).
+    """
 
-        Raises:
-            ModuleError: in case of multiple defined analog tiles in the module
+    out_sto_round: bool = False
+    """Whether to enable stochastic rounding of ADC."""
 
-        """
-        analog_tiles = list(self.analog_tiles())
-        if len(analog_tiles) != 1:
-            raise ModuleError("AnalogModuleBase.set_weights only supports a single tile.")
-
-        shape = [self.out_features, self.in_features]
-        weight = weight.clone().reshape(shape)
-
-        analog_tiles[0].set_weights(
-            weight, bias if self.analog_bias else None,
-            apply_weight_scaling,
-            weight_scaling_omega)
-
-        realistic = self.realistic_read_write and not force_exact
-        if realistic:
-            analog_tiles[0].program_weights()
-
-        if bias is not None and self.digital_bias:
-            with no_grad():
-                self.bias.data[:] = bias[:]
-
-        self._sync_weights_from_tile()
-
-    def get_weights(
-            self,
-            force_exact: bool = False,
-            apply_weight_scaling: bool = True,
-    ) -> Tuple[Tensor, Optional[Tensor]]:
-        """Get the weight (and bias) tensors.
-
-        This uses an realistic read if the property ``realistic_read_write`` of
-        the layer is set, unless it is overwritten by ``force_exact``. It
-        scales the analog weights by the digital output scales by default.
-
-        Note:
-            This is the recommended way for setting the weight/bias matrix from
-            the analog tile, as it will correctly fetch the weights from the
-            internal memory. Accessing ``self.weight`` and ``self.bias`` might
-            yield wrong results as they are not always in sync with the
-            analog tile library, for performance reasons.
+    out_scale: float = 1.0
+    """Additional fixed scalar factor."""
 
-        Args:
-            force_exact: Forces an exact read to the analog tiles
-            apply_weight_scaling: Whether to return the weights with the
-                (digital) output scaling factors applied. Note the
-                "logical" weights of the layer which the DNN is
-                effectively using are those with the output scales
-                applied. If ``apply_weight_scaling`` is set to False, then
-                only the weight values that is programmed onto the
-                crossbar array are returned, without applying the
-                digital scales. Default is True.
+    out_asymmetry: float = 0.0
+    """Output asymmetry :math:`a_\text{output}`.
 
-        Returns:
-            tuple: weight matrix, bias vector
+    Output of the negative input pass is scaled by :math:`(1 - a_\text{output})`.
 
-        Raises:
-            ModuleError: in case of multiple defined analog tiles in the module
+    Note:
+        This setting has only effect in case of and
+        :class:`AnalogMVType` that uses separate passes for positive
+        and negative inputs.
+    """
 
-        """
-        analog_tiles = list(self.analog_tiles())
-        if len(analog_tiles) != 1:
-            raise ModuleError("AnalogModuleBase.get_weights only supports a single tile.")
-
-        realistic = self.realistic_read_write and not force_exact
-        if realistic:
-            weight, analog_bias = analog_tiles[0].read_weights(
-                apply_weight_scaling)
-        else:
-            weight, analog_bias = analog_tiles[0].get_weights(
-                apply_weight_scaling)
+    bound_management: BoundManagementType = BoundManagementType.ITERATIVE
+
+    """Type of bound management, see :class:`BoundManagementType`.
+
+    Caution:
+        Bound management is **only** available for the forward pass. It
+        will be ignored when used for the backward pass.
+    """
+
+    noise_management: NoiseManagementType = NoiseManagementType.ABS_MAX
+    """Type of noise management, see :class:`NoiseManagementType`."""
+
+    w_noise: float = 0.0
+    r"""Scale of output referred weight noise (:math:`\sigma_w`) for a given
+    ``w_noise_type``."""
+
+    w_noise_type: WeightNoiseType = WeightNoiseType.NONE
+    """Type as specified in :class:`OutputWeightNoiseType`.
+
+    Note:
+        This noise us applied each time anew as it is referred to
+        the output. It will not change the conductance values of
+        the weight matrix. For the latter one can apply
+        :meth:`diffuse_weights`.
+    """
+
+    ir_drop: float = 0.0
+    """Scale of IR drop along the inputs (rows of the weight matrix).
+
+    The IR-drop is calculated assuming that the first input is
+    farthest away from the output channel. The expected drop is
+    approximating the steady-state voltage distributions and depends
+    on the input current.
+    """
+
+    ir_drop_g_ratio: float = 1.0 / 0.35 / 5e-6
+    """Physical ratio of wire conductance from one cell to the next to
+    physical max conductance of a device.
+
+    Default is compute with 5mS maximal conductance set state and 0.35
+    Ohm wire resistance.
+    """
+
+    out_nonlinearity: float = 0.0
+    """S-shaped non-linearity applied to the analog output.
+
+    Output non-linearity applies an S-shaped non-linearity to the
+    analog output (before the ADC), i.e. :math:`\frac{y_i}{1 +
+    n_i*|y_i|}` where :math:`n_i` is drawn at the instantiation time
+    by::
+        out_nonlinearity / out_bound * (1 + out_nonlinearity_std * rand)
+    """
+
+    out_nonlinearity_std: float = 0.0
+    """ Output-to-output non linearity variation. """
+
+    slope_calibration: float = 0.0
+    """Models a calibration process of the output non-linearity (and
+    r-series).
+
+    This is the relative value in the output range where the slope of
+    the non-linearity should have slope 1. E.g. 0.5 would be at half-out
+    range.
+    """
+
+    v_offset_std: float = 0.0
+    """Voltage offset variation.
+
+    The output is multiplied by a systematic factor set for each
+    output line at time of instantiation, e.g. :math:`(1 - v_i)` for
+    the coding device and :math:`(1 + v_i)` for the reference device
+    (assuming differential reads).
+
+    """
+
+    v_offset_w_min: float = -1.0
+    """ Voltage offset for an implicit reference unit. """
+
+    r_series: float = 0.0
+    """Series resistance in fraction of the total output current."""
+
+    w_read_asymmetry_dtod: float = 0.0
+    """Device polarity read dependence.
+
+    The negative inputs perceive a slightly different weight (e.g. pcm
+    polarity dependence). Each device has a different factor, and the
+    spread of this device-to-device variability can be set with
+    ``w_read_asymmetry_dtod``. A weight (given negative input) will be
+    then scaled by :math:`1 - f_{ij}` where :math:`f_{ij}` is drawn
+    from a Gaussian distribution (with zero mean and standard
+    deviation ``w_read_asymmetry_dtod``).
+    """
+
+    max_bm_factor: int = 1000
+    """Maximal bound management factor.
+
+    If this factor is reached then the iterative process is stopped.
+    """
+
+    max_bm_res: float = 0.25
+    """Limit the maximal number of iterations of the bound management.
+
+    Another way to limit the maximal number of iterations of the bound
+    management. The max effective resolution number of the inputs, e.g. use
+    :math:`1/4` for 2 bits.
+    """
+
+    bm_test_negative_bound: bool = True
+
+    nm_thres: float = 0.0
+    r"""Constant noise management value for ``type`` ``Constant``.
+
+    In other cases, this is a upper threshold :math:`\theta` above which the
+    noise management factor is saturated. E.g. for `AbsMax`:
+
+    .. math::
+        :nowrap:
+
+        \begin{equation*} \alpha=\begin{cases}\max_i|x_i|, &
+        \text{if} \max_i|x_i|<\theta \\ \theta, &
+        \text{otherwise}\end{cases} \end{equation*}
+
+    Caution:
+        If ``nm_thres`` is set (and type is not ``Constant``), the noise
+        management will clip some large input values, in favor of having a
+        better SNR for smaller input values.
+    """
+
+
+@dataclass
+class UpdateParameters(_PrintableMixin):
+    """Parameter that modify the update behaviour of a pulsed device."""
+
+    bindings_class: ClassVar[Type] = devices.AnalogTileUpdateParameter
+
+    desired_bl: int = 31
+    """Desired length of the pulse trains.
 
-        digital_bias = None
-        if self.digital_bias:
-            with no_grad():
-                digital_bias = self.bias.data.clone().detach().cpu()
-
-        if (digital_bias is not None) and (analog_bias is not None):
-            bias = digital_bias + analog_bias
-        elif digital_bias is not None:
-            bias = digital_bias
+    For update BL management, it is the maximal pulse train length.
+    """
+
+    fixed_bl: bool = True
+    """Whether to fix the length of the pulse trains.
+
+    See also ``update_bl_management``.
+
+    In case of ``True`` (where ``dw_min`` is the mean minimal weight change
+    step size) it is::
+
+        BL = desired_BL
+        A = B =  sqrt(learning_rate / (dw_min * BL))
+
+    In case of ``False``::
+
+        if dw_min * desired_BL < learning_rate:
+            A = B = 1
+            BL = ceil(learning_rate / dw_min
         else:
-            bias = analog_bias
-        return weight, bias
+            # same as for fixed_BL=True
+    """
 
-    def remap_weights(self, weight_scaling_omega: Optional[float] = 1.0) -> None:
-        """Gets and re-sets the weights in case of using the weight scaling.
+    pulse_type: PulseType = PulseType.STOCHASTIC_COMPRESSED
+    """Switching between different pulse types.
 
-        This re-sets the weights with applied mapping scales, so that
-        the weight mapping scales are updated.
+    See also :class:`PulseTypeMap` for details.
 
-        In case of hardware-aware training, this would update the
-        weight mapping scales so that the absolute max analog weights
-        are set to 1 (as specified in the ``weight_scaling``
-        configuration of
-        :class:`~aihwkit.configs.utils.MappingParameter`).
-
-        Note:
-            By default the weight scaling omega factor is set to 1
-            here (overriding any setting in the ``rpu_config``). This
-            means that the max weight value is set to 1 internally for
-            the analog weights.
-
-        Caution:
-            This should typically *not* be called for analog
-            training unless realistic_read_write is set. In this case,
-            it would perform a full re-write of the weights.
+    Important:
+        Pulsing can also be turned off in which case the update is done as if
+        in floating point and all other update related parameter are ignored.
+    """
 
-        Args:
-            weight_scaling_omega: The weight scaling omega factor (see
-                :class:`~aihwkit.configs.utils.MappingParameter`). If
-                set to None here, it will take the value in the
-                mapping parameters. Default is however 1.0.
+    res: float = 0
+    """Resolution of the update probability for the stochastic bit line
+    generation.
+
+    Resolution ie. bin width in ``0..1``) of the update probability for the
+    stochastic bit line generation. Use -1 for turning discretization off. Can
+    be given as number of steps as well.
+    """
 
-        """
-        weights, biases = self.get_weights(False, True)
-        self.set_weights(weights, biases, False, True, weight_scaling_omega)
+    x_res_implicit: float = 0
+    """Resolution of each quantization step for the inputs ``x``.
 
-    def _sync_weights_from_tile(self) -> None:
-        """Update the layer weight and bias from the values on the analog tile.
+    Resolution (ie. bin width) of each quantization step for the inputs ``x``
+    in case of ``DeterministicImplicit`` pulse trains. See
+    :class:`PulseTypeMap` for details.
+    """
 
-        Update the ``self.weight`` and ``self.bias`` Parameters with an
-        exact copy of the internal analog tile weights.
-        """
-        tile_weight, tile_bias = self.get_weights(force_exact=True)  # type: Tuple[Tensor, Tensor]
+    d_res_implicit: float = 0
+    """Resolution of each quantization step for the error ``d``.
 
-        self.weight.data[:] = tile_weight.reshape(self.weight.shape)
-        if self.analog_bias:
-            with no_grad():
-                self.bias.data[:] = tile_bias.reshape(self.bias.shape)
+    Resolution (ie. bin width) of each quantization step for the error ``d``
+    in case of `DeterministicImplicit` pulse trains. See
+    :class:`PulseTypeMap` for details.
+    """
 
-    def _sync_weights_to_tile(self) -> None:
-        """Update the tile values from the layer weights and bias.
+    sto_round: bool = False
+    """Whether to enable stochastic rounding."""
 
-        Update the internal tile weights with an exact copy of the values of
-        the ``self.weight`` and ``self.bias`` Parameters.
-        """
-        self.set_weights(self.weight, self.bias if self.analog_bias else None,
-                         force_exact=True)
+    update_bl_management: bool = True
+    """Whether to enable dynamical adjustment of ``A``,``B``,and ``BL``::
 
-    def _set_load_rpu_config_state(self, load_rpu_config: bool = True,
-                                   strict_rpu_config_check: bool = True) -> None:
-        self._load_rpu_config = load_rpu_config
-        self._strict_rpu_config_check = strict_rpu_config_check
-
-    def load_state_dict(self,  # pylint: disable=arguments-differ
-                        state_dict: 'OrderedDict[str, Tensor]',
-                        strict: bool = True,
-                        load_rpu_config: bool = True,
-                        strict_rpu_config_check: bool = True) -> NamedTuple:
-        """Specializes torch's ``load_state_dict`` to add a flag whether to
-        load the RPU config from the saved state.
+        BL = ceil(learning_rate * abs(x_j) * abs(d_i) / weight_granularity);
+        BL  = min(BL,desired_BL);
+        A = B = sqrt(learning_rate / (weight_granularity * BL));
 
-        Args:
-            state_dict: see torch's ``load_state_dict``
-            strict: see torch's ``load_state_dict``
-            load_rpu_config: Whether to load the saved RPU
-                config or use the current RPU config of the model.
-
-                Caution:
-
-                    If ``load_rpu_config=False`` the RPU config can
-                    be changed from the stored model. However, the user has to
-                    make sure that the changed RPU config makes sense.
-
-                    For instance, changing the device type might
-                    change the expected fields in the hidden
-                    parameters and result in an error.
-
-            strict_rpu_config_check: Whether to check and throw an
-                error if the current ``rpu_config`` is not of the same
-                class type when setting ``load_rpu_config`` to
-                False. In case of ``False`` the user has to make sure
-                that the ``rpu_config`` are compatible.
+    The ``weight_granularity`` is usually equal to ``dw_min``.
+    """
 
-        Returns:
-            see torch's ``load_state_dict``
+    update_management: bool = True
+    r"""Whether to apply additional scaling.
 
-        Raises:
-            ModuleError: in case the rpu_config class mismatches
-            or mapping parameter mismatch for
-            ``load_rpu_config=False``
+    After the above setting an additional scaling (always on when using
+    `update_bl_management``) is applied to account for the different input
+    strengths.
+    If
 
-        """
-        self._set_load_rpu_config_state(load_rpu_config, strict_rpu_config_check)
-        return super().load_state_dict(state_dict, strict)
+    .. math:: \gamma \equiv \max_i |x_i| / (\alpha \max_j |d_j|)
 
-    def __setstate__(self, state: Dict) -> None:
-        """Set the state after unpickling.
+    is the ratio between the two maximal inputs, then ``A`` is additionally
+    scaled by :math:`\gamma` and ``B`` is scaled by :math:`1/\gamma`.
 
-        Makes sure that the parameter in the tiles are correctly registered.
+    The gradient scale :math:`\alpha` can be set with ``um_grad_scale``
+    """
 
-        """
-        self.__dict__.update(state)
+    um_grad_scale: float = 1.0
+    r"""Scales the gradient for the update management.
 
-        # update registered parameters
-        for name, analog_tile in list(self.named_analog_tiles()):
-            self.register_analog_tile(analog_tile, name, update_only=True)
-
-    def _load_from_state_dict(
-            self,
-            state_dict: Dict,
-            prefix: str,
-            local_metadata: Dict,
-            strict: bool,
-            missing_keys: List[str],
-            unexpected_keys: List[str],
-            error_msgs: List[str]) -> None:
-        """Copy parameters and buffers from `state_dict` into only this
-        module, but not its descendants.
-
-        This method is a specialization of ``Module._load_from_state_dict``
-        that takes into account the extra ``analog_tile_state`` key used by
-        analog layers.
+    The factor :math:`\alpha` for the ``update_management``. If
+    smaller than 1 it means that the gradient will be earlier clipped
+    when learning rate is too large (ie. exceeding the maximal
+    pulse number times the weight granularity). If 1, both d and x inputs
+    are clipped for the same learning rate.
+    """
 
-        Raises:
-            ModuleError: in case the rpu_config class mismatches.
-        """
-        # pylint: disable=too-many-locals, too-many-branches
 
-        for name, analog_tile in list(self.named_analog_tiles()):
-            key = prefix + self.ANALOG_STATE_PREFIX + name
-            if key not in state_dict:  # legacy
-                key = prefix + 'analog_tile_state'
-
-            if key in state_dict:
-                analog_state = state_dict.pop(key).copy()
-
-                if not self._load_rpu_config:
-
-                    if self._strict_rpu_config_check:
-                        if not isinstance(analog_tile.rpu_config,
-                                          type(analog_state['rpu_config'])):
-                            raise ModuleError("RPU config mismatch during loading: "
-                                              "Tried to replace "
-                                              f"{analog_state['rpu_config'].__class__.__name__} "
-                                              f"with {analog_tile.rpu_config.__class__.__name__}")
-
-                    if hasattr(analog_state['rpu_config'], 'mapping'):
-                        old_mapping = analog_state['rpu_config'].mapping
-                        new_mapping = analog_tile.rpu_config.mapping
-                        if (old_mapping.max_input_size != new_mapping.max_input_size
-                                or old_mapping.max_output_size != new_mapping.max_output_size
-                                or old_mapping.digital_bias != new_mapping.digital_bias
-                                or (old_mapping.out_scaling_columnwise
-                                    != new_mapping.out_scaling_columnwise)):
-                            raise ModuleError("MappingParameter mismatch during loading: "
-                                              "Tried to replace "
-                                              f"{old_mapping} "
-                                              f"with {new_mapping}")
-
-                    analog_state['rpu_config'] = analog_tile.rpu_config
-                analog_tile.__setstate__(analog_state)
-
-                # update registered parameters
-                self.register_analog_tile(analog_tile, name, update_only=True)
-
-            elif strict:
-                missing_keys.append(key)
-
-        # update the weight / analog bias (not saved explicitly)
-        self._sync_weights_from_tile()
-
-        # remove helper parameters.
-        rm_keys = []
-        for par_name in self._registered_helper_parameter:
-            key = prefix + par_name
-            if key in state_dict:
-                rm_keys.append(key)
-
-        # legacy
-        for part in [self.ANALOG_SHARED_WEIGHT_PREFIX,
-                     self.ANALOG_OUT_SCALING_ALPHA_PREFIX]:
-            for key in state_dict:
-                if part in key:
-                    rm_keys.append(key)
-
-        for key in rm_keys:
-            if key in state_dict:
-                state_dict.pop(key)
-
-        super()._load_from_state_dict(
-           state_dict, prefix, local_metadata, strict, missing_keys,
-           unexpected_keys, error_msgs)
-
-        # legacy
-        for part in [self.ANALOG_SHARED_WEIGHT_PREFIX,
-                     self.ANALOG_OUT_SCALING_ALPHA_PREFIX]:
-            for key in missing_keys:
-                if part in key:
-                    rm_keys.append(key)
-
-        # remove the missing keys of the helper parameters
-        for key in rm_keys:
-            if key in missing_keys:
-                missing_keys.remove(key)
-
-    def state_dict(  # pylint: disable=arguments-differ
-            self,
-            destination: Any = None,
-            prefix: str = '',
-            keep_vars: bool = False
-    ) -> Dict:
-        """Return a dictionary containing a whole state of the module."""
-        self._sync_weights_from_tile()
-
-        current_state = super().state_dict(destination=destination,
-                                           prefix=prefix,
-                                           keep_vars=keep_vars)
-
-        for name, analog_tile in self.named_analog_tiles():
-            analog_state = analog_tile.__getstate__()
-            analog_state_name = prefix + self.ANALOG_STATE_PREFIX + name
-            current_state[analog_state_name] = analog_state
+@dataclass
+class WeightModifierParameter(_PrintableMixin):
+    """Parameter that modify the forward/backward weights during hardware-aware training."""
+
+    bindings_class: ClassVar[Type] = tiles.WeightModifierParameter
+
+    std_dev: float = 0.0
+    """Standard deviation of the added noise to the weight matrix.
+
+    This parameter affects the modifier types ``AddNormal``, ``MultNormal`` and
+    ``DiscretizeAddNormal``.
+
+    Note:
+        If the parameter ``rel_to_actual_wmax`` is set then the ``std_dev`` is
+        computed in relative terms to the abs max of the given weight matrix,
+        otherwise it in relative terms to the assumed max, which is set by
+        ``assumed_wmax``.
+    """
 
-        return current_state
+    per_batch_sample: bool = False
+    """Should we resample noise for each sample in the batch.
 
-    def drift_analog_weights(self, t_inference: float = 0.0) -> None:
-        """(Program) and drift the analog weights.
+    This parameter only affects is used when using the
+    ``TorchSimulatorTile``. In case of ``RPUCudaTile`` it will throw
+    an error.
+    """
 
-        Args:
-            t_inference: assumed time of inference (in sec)
+    res: float = 0.0
+    r"""Resolution of the discretization.
 
-        Raises:
-            ModuleError: if the layer is not in evaluation mode.
-        """
-        if self.training:
-            raise ModuleError('drift_analog_weights can only be applied in '
-                              'evaluation mode')
-        for analog_tile in self.analog_tiles():
-            if isinstance(analog_tile, InferenceTile):
-                analog_tile.drift_weights(t_inference)
+    The invert of ``res`` gives the number of equal sized steps in
+    :math:`-a_\text{max}\ldots,a_\text{max}` where the
+    :math:`a_\text{max}` is either given by the abs max (if
+    ``rel_to_actual_wmax`` is set) or ``assumed_wmax`` otherwise.
 
-    def program_analog_weights(self) -> None:
-        """Program the analog weights.
+    ``res`` is only used in the modifier types ``DoReFa``, ``Discretize``, and
+    ``DiscretizeAddNormal``.
+    """
 
-        Raises:
-            ModuleError: if the layer is not in evaluation mode.
-        """
-        if self.training:
-            raise ModuleError('program_analog_weights can only be applied in '
-                              'evaluation mode')
-        for analog_tile in self.analog_tiles():
-            analog_tile.program_weights()
+    sto_round: bool = False
+    """Whether the discretization is done with stochastic rounding enabled.
 
-    def extra_repr(self) -> str:
-        """Set the extra representation of the module.
+    ``sto_round`` is only used in the modifier types ``DoReFa``,
+    ``Discretize``, and ``DiscretizeAddNormal``.
+    """
 
-        Returns:
-            A string with the extra representation.
-        """
-        output = super().extra_repr()
-        if self.realistic_read_write:
-            output += ', realistic_read_write={}'.format(self.realistic_read_write)
-        if self.analog_bias:
-            output += ', analog bias'
-        if self.digital_bias:
-            output += ', digital bias'
-
-        return output
-
-    def _set_weight_scaling_omega(self, rpu_config: RPUConfigAlias,
-                                  weight_scaling_omega: Optional[float] = None
-                                  ) -> RPUConfigAlias:
-        """ Sets the weight scaling omega and raises a FutureWarning.
+    dorefa_clip: float = 0.6
+    """Parameter for DoReFa."""
+
+    pdrop: float = 0.0
+    """Drop connect probability.
+
+    Drop connect sets weights to zero with the given probability. This
+    implements drop connect.
+
+    Important:
+        Drop connect can be used with any other modifier type in combination.
+    """
+
+    enable_during_test: bool = False
+    """Whether to use the last modified weight matrix during testing.
+
+    Caution:
+        This will **not** remove drop connect or any other noise
+        during evaluation, and thus should only used with care.
+    """
+
+    rel_to_actual_wmax: bool = True
+    """Whether to calculate the abs max of the weight and apply noise relative
+    to this number.
+
+    If set to False, ``assumed_wmax`` is taken as relative units.
+    """
+
+    assumed_wmax: float = 1.0
+    """Assumed weight value that is mapped to the maximal conductance.
+
+    This is typically 1.0. This parameter will be ignored if
+    ``rel_to_actual_wmax`` is set.
+    """
+
+    copy_last_column: bool = False
+    """Whether to not apply noise to the last column (which usually contains
+    the bias values)."""
+
+    coeffs: List[float] = field(
+        default_factory=lambda: [0.0105392, 0.0768, -0.046925],
+        metadata={"hide_if": [0.0105392, 0.0768, -0.046925]},
+    )
+    """Coefficients for the ``POLY`` weight modifier type.
+
+    See :class:`WeightModifierType` for details.
+    """
+
+    type: WeightModifierType = WeightModifierType.NONE
+    """Type of the weight modification."""
+
+    g_max: float = 25.0
+    r"""PROG_NOISE parameter, :math:`g_\text{max}`
+    setting in :math:`\mu S`."""
+
+
+@dataclass
+class WeightClipParameter(_PrintableMixin):
+    """Parameter that clip the weights during hardware-aware training.
+
+    Important:
+        A clipping ``type`` has to be set before any of the parameter
+        changes take any effect.
+
+    """
+
+    bindings_class: ClassVar[Type] = tiles.WeightClipParameter
+
+    fixed_value: float = -1.0
+    """Clipping value in case of ``FixedValue`` type.
+
+    Caution:
+
+        If ``fixed_value > 0`` it will be also applied during other
+        clipping types.
+
+    """
+
+    sigma: float = 2.5
+    """Sigma value for clipping for the ``LayerGaussian`` type."""
+
+    type: WeightClipType = WeightClipType.NONE
+    """Type of clipping."""
+
+
+@dataclass
+class WeightRemapParameter(_PrintableMixin):
+    """Parameter that remap the weights during hardware-aware training.
+
+    Important:
+        A remap ``type`` has to be set before any of the parameter
+        changes take any effect.
+    """
+
+    bindings_class: ClassVar[Type] = tiles.WeightRemapParameter
+
+    remapped_wmax: float = 1.0
+    """Assumed max of weight, ie the value of the weight the maximal
+    conductance is mapped to. Typically 1.0.
+    """
+
+    max_scale_range: float = 0.0
+    """Maximal range of scale values. Use zero to turn any restrictions
+    off (default)."""
+
+    max_scale_ref: float = 0.0
+    """Reference scale that use used as minimal scale for determining the
+    scale range."""
+
+    type: WeightRemapType = WeightRemapType.NONE
+    """Type of clipping."""
+
+
+@dataclass
+class SimpleDriftParameter(_PrintableMixin):
+    r"""Parameter for a simple power law drift.
+
+    The drift as a simple power law drift without device-to-device
+    variation or conductance dependence.
+
+    It computes:
+    .. math::
+
+        w_{ij}*\left(\frac{t + \Delta t}{t_0}\right)^(-\nu)
+    """
+
+    bindings_class: ClassVar[Type] = devices.DriftParameter
+
+    nu: float = 0.0
+    r"""Average drift :math:`\nu` value.
+
+    Need to non-zero to actually use the drift.
+    """
+
+    t_0: float = 1.0
+    """Time between write and first read.
+
+    Usually assumed in milliseconds, however, it really determines the time
+    units of ``time_since_last_call`` when calling the drift.
+    """
+
+    reset_tol: float = 1e-7
+    """Reset tolerance.
+
+    This should a number smaller than the expected weight change as it is used
+    to detect any changes in the weight from the last drift call. Every change
+    to the weight above this tolerance will reset the drift time.
+
+    Caution:
+        Any write noise or diffusion on the weight might thus
+        interfere with the drift.
+   """
+
+
+@dataclass
+class DriftParameter(SimpleDriftParameter):
+    r"""Parameter for a power law drift.
+
+    The drift is based on the model described by `Oh et al (2019)`_.
+
+    It computes:
+    .. math::
+
+        w_{ij}*\left(\frac{t + \Delta t}{t_0}\right)^(-\nu^\text{actual}_{ij})
+
+    where the drift coefficient is drawn once at the beginning and
+    might depend on device. It also can depend on the actual weight
+    value.
+
+    The actual drift coefficient is computed as:
+    .. math::
+
+        \nu_{ij}^\text{actual} =  \nu_{ij} - \nu_k \log \frac{(w_{ij} - w_\text{off}) / r_\text{wg}
+        + g_\text{off}}{G_0}  + \nu\sigma_\nu\xi
+
+    here :math:`w_{ij}` is the actual weight and `\nu_{ij}` fixed for
+    each device given by the mean :math:`\nu` and the device-to-device
+    variation: :math:`\nu_{ij} = \nu + \nu_dtod\nu\xi` and are only
+    drawn once at the beginning (tile instantiation).  `\xi` is
+    Gaussian noise.
+
+    Note:
+        If the weight has changed from the last drift call (determined
+        by the ``reset_tol`` parameter), for instance due to update,
+        decay or noise, then the drift time :math:`t` will be reset and start
+        from new, however, the drift coefficients :math:`\nu_{ij}` are
+        *not* changed. On the other hand, if the weights has not
+        changed since last call, :math:`t` will accumulate the time.
+
+    Caution:
+        Note that the drift coefficient does *not* depend on the initially
+        programmed weight value at :math:`t=0` in the current
+        implementation (ie G0 is a constant for all devices), but
+        instead on the actual weight. In some materials (e.g. phase
+        changed materials), that might be not accurate.
+
+    .. _`Oh et al (2019)`: https://ieeexplore.ieee.org/document/8753712
+    """
+
+    bindings_class: ClassVar[Type] = devices.DriftParameter
+
+    nu_dtod: float = 0.0
+    r"""Device-to-device variation of the :math:`\nu` values."""
+
+    nu_std: float = 0.0
+    r"""Cycle-to-cycle variation of :math:`\nu`.
+
+    A more realistic way to add noise of the drift might be using
+    ``w_noise_std``.
+    """
+
+    wg_ratio: float = 1.0
+    """``(w_max-w_min)/(g_max-g_min)`` to convert to physical units."""
+
+    g_offset: float = 0.0
+    """``g_min`` to convert to physical units."""
+
+    w_offset: float = 0.0
+    """``w(g_min)``, i.e. to what value ``g_min`` is mapped to in w-space."""
+
+    nu_k: float = 0.0
+    r"""Variation of math:`nu` with :math:`W`.
+
+    That is :math:`\nu(R) = nu_0 - k \log(G/G_0)`.  See Oh et al. for
+    details.
+    """
+
+    log_g0: float = 0.0
+    """Log g0."""
+
+    w_noise_std: float = 0.0
+    """Additional weight noise (Gaussian diffusion) added to the weights
+    after the drift is applied."""
+
+
+@dataclass
+class MappingParameter(_PrintableMixin):
+    """Parameter related to hardware design and the mapping of logical
+    weight matrices to physical tiles.
+
+    Caution:
+
+        Some of these parameters have only an effect for modules that
+        support tile mappings.
+    """
+
+    digital_bias: bool = True
+    """Whether the bias term is handled by the analog tile or kept in
+    digital.
+
+    Note:
+        Default is having a *digital* bias so that bias values are
+        *not* stored onto the analog crossbar. This needs to be
+        supported by the chip design. Set to False if the analog bias
+        is instead situated on the the crossbar itself (as an extra
+        column)
+
+    Note:
+        ``digital_bias`` is supported by *all* analog modules.
+    """
+
+    weight_scaling_omega: float = 0.0
+    """omega_scale is a user defined parameter used to scale the weights
+    while remapping these to cover the full range of values allowed.
+    By default, no remapping is performed. If values > 0.0 are supplied
+    the abs-max of the weight is scaled to that value.
+    """
+
+    weight_scaling_columnwise: bool = False
+    """Whether the weight matrix will be remapped column-wise over
+    the maximum device allowed value."""
+
+    learn_out_scaling: bool = False
+    """Define (additional) out scales that are learnable parameter
+    used to scale the output."""
+
+    out_scaling_columnwise: bool = False
+    """Whether the learnable out scaling parameter enabled by
+    ``learn_out_scaling`` is a scalar (``False``) or learned for
+    each output (``True``).
+    """
+
+    max_input_size: int = 512
+    """Maximal input size (number of columns) of the weight matrix
+    that is handled on a single analog tile.
+
+    If the logical weight matrix size exceeds this size it will be
+    split and mapped onto multiple analog tiles.
+
+    Caution:
+        Only relevant for ``Mapped`` modules such as
+        :class:`aihwkit.nn.modules.linear_mapped.AnalogLinearMapped`.
+    """
+
+    max_output_size: int = 512
+    """Maximal output size (number of rows) of the weight matrix
+    that is handled on a single analog tile.
+
+    If the logical weight matrix size exceeds this size it will be
+    split and mapped onto multiple analog tiles.
+
+    Caution:
+        Only relevant for ``Mapped`` modules such as
+        :class:`aihwkit.nn.modules.linear_mapped.AnalogLinearMapped`.
+    """
+
+
+@dataclass
+class InputRangeParameter(_PrintableMixin):
+    """Parameter related to input range learning"""
+
+    enable: bool = False
+    """Whether to enable to learn the input range. Note that if enable is
+    ``False`` then no clip is applied.
+
+    Note:
+
+        The input bound (``forward.inp_bound``) is assumed to be 1 if
+        enabled as the input range already scales the input into to the
+        range :math:`(-1, 1)` by dividing the input to the type by
+        itself and multiplying the output accordingly.
+
+        Typically, noise and bound management should be set to `NONE`
+        for the input range learning as it replaces the dynamic
+        managements with a static but learned input bound. However, in
+        some exceptional experimental cases one might want to enable
+        the management techniques on top of the input range learning,
+        so that no error is raised if they are not set to `NONE`.
+    """
+
+    learn_input_range: bool = True
+    """Whether to learn the input range when enabled.
+
+    Note:
+
+       If not learned, the input range should in general be set
+       with some calibration method before training the DNN.
+
+    """
+
+    init_value: float = 3.0
+    """Initial setting of the input range in case of input range learning."""
+
+    init_from_data: int = 100
+    """Number of batches to use for initialization from data. Set 0 to turn off."""
+
+    init_std_alpha: float = 3.0
+    """Standard deviation multiplier for initialization from data."""
+
+    decay: float = 0.001
+    """Decay rate for input range learning."""
+
+    input_min_percentage: float = 0.95
+    """Decay is only applied if percentage of non-clipped values is above this value.
+
+    Note:
+
+        The added gradient is (in case of non-clipped input
+        percentage ``percentage > input_min_percentage``)::
+
+            grad += decay * input_range
+    """
+
+    manage_output_clipping: bool = False
+    """Whether to increase the input range when output clipping occurs.
+
+    Caution:
+
+        The output bound is taken from the ``forward.out_bound``
+        value, which has to exist. Noise and bound management have to
+        be set to NONE if this feature is enabled otherwise a
+        ``ConfigError`` is raised.
+
+    """
+
+    output_min_percentage: float = 0.95
+    """Increase of the input range is only applied if percentage of
+    non-clipped output values is below this value.
+
+    Note:
+
+        The gradient subtracted from the input range is (in case of
+        ``output_percentage < output_min_percentage``)::
+
+            grad -= (1.0 - output_percentage) * input_range
+    """
+
+    gradient_scale: float = 1.0
+    """Scale of the gradient magnitude (learning rate) for the input range learning."""
+
+    gradient_relative: bool = True
+    """Whether to make the gradient of the input range learning relative to
+    the current range value.
+    """
+
+    calibration_info: Optional[str] = None
+    """ Information field for potential post-training calibrations. """
+
+    def supports_manage_output_clipping(self, rpu_config: Any) -> bool:
+        """Checks whether rpu_config supported ``manage_output_clipping``.
 
         Args:
-            rpu_config: dtto
-            weight_scaling_omega: parameter value to set
+            rpu_config: RPUConfig to check
 
         Returns:
-            Copy of ``rpu_config`` if ``weight_scaling_omega`` is not None
-
-        Raises:
-            FutureWarning if ``weight_scaling_omega`` is not None
+            True if supported otherwise False
         """
 
-        if weight_scaling_omega is not None:
-            warn(FutureWarning("weight_scaling_omega argument to the layer module "
-                               "construction will be removed in future. "
-                               "Use aihwkit.simulator.configs.utils.MappingParameter "
-                               "instead to specify weight scaling."))
-            rpu_config = deepcopy(rpu_config)
-            rpu_config.mapping.weight_scaling_omega = weight_scaling_omega
+        if not hasattr(rpu_config, "forward") or rpu_config.forward.is_perfect:
+            return False
+        if not isinstance(rpu_config.forward, IOParameters):
+            return False
+        if rpu_config.forward.noise_management != NoiseManagementType.NONE:
+            return False
+        if rpu_config.forward.bound_management != BoundManagementType.NONE:
+            return False
+        return True
+
+
+@dataclass
+class PrePostProcessingParameter(_PrintableMixin):
+    """Parameter related to digital input and output processing, such as input clip
+    learning.
+    """
 
-        return rpu_config
+    input_range: InputRangeParameter = field(default_factory=InputRangeParameter)
```

### Comparing `aihwkit-0.7.1/src/aihwkit/nn/modules/container.py` & `aihwkit-0.8.0/src/aihwkit/simulator/tiles/analog.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,307 +1,230 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
 # that they have been altered from the originals.
 
-"""Analog Modules that contain children Modules."""
+"""High level analog tiles (analog)."""
 
-from typing import (
-    Callable, Optional, Union, Any, NamedTuple, Tuple, TYPE_CHECKING, Generator
-)
-from collections import OrderedDict
+from typing import Optional, Tuple
 
-from torch import device as torch_device
-from torch.nn import Sequential
+from torch import Tensor
 
-from aihwkit.exceptions import ModuleError, TileError
-from aihwkit.nn.modules.base import AnalogModuleBase
+from aihwkit.simulator.tiles.rpucuda import RPUCudaSimulatorTileWrapper
+from aihwkit.simulator.tiles.module import TileModule
+from aihwkit.simulator.tiles.periphery import TileWithPeriphery
+from aihwkit.simulator.tiles.functions import AnalogFunction
+from aihwkit.simulator.parameters.base import RPUConfigGeneric
+from aihwkit.simulator.rpu_base import tiles
 
-if TYPE_CHECKING:
-    from torch import Tensor  # pylint: disable=ungrouped-imports
 
+class AnalogTile(TileModule, TileWithPeriphery, RPUCudaSimulatorTileWrapper):
+    r"""Analog tile.
 
-class AnalogSequential(Sequential):
-    """An analog-aware sequential container.
+    This analog tile implements an abstract analog tile where many
+    cycle-tp-cycle non-idealities and systematic parameter-spreads
+    that can be user-defined.
 
-    Specialization of torch ``nn.Sequential`` with extra functionality for
-    handling analog layers:
+    In general stochastic bit pulse trains are generate during update
+    and device materials (or unit cells) at each cross-point are only
+    updated if a coincidence of rows and columns pulses.
 
-    * correct handling of ``.cuda()`` for children modules.
-    * apply analog-specific functions to all its children (drift and program
-      weights).
+    Here, a resistive device material is assumed that response with a
+    finite step change of its conductance value that is independent of
+    its own conductance value.
 
-    Note:
-        This class is recommended to be used in place of ``nn.Sequential`` in
-        order to correctly propagate the actions to all the children analog
-        layers. If using regular containers, please be aware that operations
-        need to be applied manually to the children analog layers when needed.
-    """
-    # pylint: disable=abstract-method
+    In its basic parameter settings it implements the analog RPU tile
+    model described in `Gokmen & Vlasov (2016)`_, but with a number of
+    enhancements that are adjustable by parameter settings.
 
-    def _apply_to_analog(self, fn: Callable) -> 'AnalogSequential':
-        """Apply a function to all the analog layers in this module.
+    All tile parameters are given in
+    :class:`~aihwkit.simulator.parameters.AnalogTileParameters`.
 
-        Args:
-            fn: function to be applied.
+    **Forward pass**:
 
-        Returns:
-            This module after the function has been applied.
-        """
-        for module in self.analog_modules():
-            fn(module)
+    In general, the following analog forward pass is computed:
 
-        return self
+    .. math::
 
-    def apply_to_analog_modules(self, fn: Callable) -> 'AnalogSequential':
-        """Apply a function to all the analog modules.
+        \mathbf{y} = f_\text{ADC}((W + \sigma_\text{w}\Xi) \otimes
+        (f_\text{DAC}( x/\alpha ) +
+        \sigma_\text{inp}\,\boldsymbol{\xi}_1 ) +
+        \sigma_\text{out}\,\boldsymbol{\xi}_2)\,s_\alpha\,
+        s_\text{out}\,\alpha
 
-        Args:
-            fn: function to be applied.
+    where :math:`W` is the weight matrix, :math:`\mathbf{x}` the input
+    vector and the :math:`\Xi,\boldsymbol{\xi}_1,\boldsymbol{\xi}_2`
+    Gaussian noise variables (with corresponding matrix and vector
+    sizes). The :math:`\alpha` is a scale from the noise management
+    (see :data:`rpu_types.NoiseManagementTypeMap`). The symbol
+    :math:`\otimes` refers to the 'analog' matrix-vector
+    multiplication, that might have additional non-linearities.
 
-        Returns:
-            This module after the function has been applied.
-        """
-        for module in self.analog_modules():
-            fn(module)
+    :math:`f_\text{Z}` (with `Z` either `ADC` or `DAC`) indicates the
+    discretization to a number of equidistant steps between a bound
+    value :math:`-b_\text{Z},\ldots,b_\text{Z}` potentially with
+    stochastic rounding (SR):
 
-        return self
+    .. math::
 
-    def apply_to_analog_tiles(self, fn: Callable) -> 'AnalogSequential':
-        """Apply a function to all the analog tiles of all layers in this module.
+        f_\text{Z}(x) = \text{round}(x\,
+        \frac{r_\text{Z}}{2\,b_\text{Z}} +
+        \zeta)\frac{2b_\text{Z}}{r_\text{Z}}
 
-        Example::
+    If SR is enabled :math:`\zeta` is an uniform random :math:`\in
+    [-0.5,0.5)`. Otherwise :math:`\zeta=0`.  Inputs are clipped below
+    :math:`-b_\text{Z}` and above :math:`b_\text{Z}`
 
-            model.apply_to_analog_tiles(lambda tile: tile.reset())
+    :math:`r_Z` is the resolution of the `ADC` or `DAC`. E.g. for 8
+    bit, it would be :math:`1/256`
 
-        This would reset each analog tile in the whole DNN looping
-        through all layers and all tiles that might exist in a
-        particular layer.
-
-        Args:
-            fn: function to be applied.
-
-        Returns:
-            This module after the function has been applied.
-
-        """
-        for module in self.analog_modules():
-            for analog_tiles in module.analog_tiles():
-                fn(analog_tiles)
-        return self
-
-    def analog_modules(self) -> Generator[AnalogModuleBase, None, None]:
-        """Generator over analog modules only"""
-        for module in self.modules():
-            if isinstance(module, AnalogModuleBase):
-                yield module
-
-    def named_analog_modules(self) -> Generator[Tuple[str, AnalogModuleBase], None, None]:
-        """Generator over analog modules only"""
-        for name, module in self.named_modules():
-            if isinstance(module, AnalogModuleBase):
-                yield name, module
-
-    def cpu(
-            self
-    ) -> 'AnalogSequential':
-        super().cpu()
-
-        self._apply_to_analog(lambda m: m.cpu())
-
-        return self
-
-    def cuda(
-            self,
-            device: Optional[Union[torch_device, str, int]] = None
-    ) -> 'AnalogSequential':
-        super().cuda(device)
-
-        self._apply_to_analog(lambda m: m.cuda(device))
-
-        return self
-
-    def to(
-            self,
-            device: Optional[Union[torch_device, str, int]] = None
-    ) -> 'AnalogSequential':
-        """Move and/or cast the parameters, buffers and analog tiles.
-
-        Note:
-            Please be aware that moving analog layers from GPU to CPU is
-            currently not supported.
+    Note:
+        Typically the resolution is reduced by 2 level, eg. in case of
+        8 bits it is set to :math:`1/254` to account for a
+        discretization mirror symmetric around zero, including the zero
+        and discarding one value.
+
+    The scalar scale :math:`s_\text{out}` can be set by
+    ``out_scale``. The scalar scale :math:`s_\alpha` is an additional
+    scale that might be use to map weight better to conductance
+    ranges.
+
+    For parameters regarding the forward pass behavior, see
+    :class:`~aihwkit.simulator.parameters.AnalogTileInputOutputParameters`.
+
+
+    **Backward pass**:
+
+    Identical to the forward direction except that the transposed
+    weight matrix is used.  Same parameters as during the forward pass
+    except that bound management is not supported.
+
+    For parameters regarding the backward pass behavior, see
+    :class:`~aihwkit.simulator.parameters.AnalogTileInputOutputParameters`.
+
+
+    **General weight update**:
+
+    The weight update that theoretically needs to be computed is
+
+    .. math:: w_{ij} = w_{ij} + \lambda d_i\,x_j
+
+    thus the outer product of error vector and input vector.
+
+    Although the update depends on the `ResistiveDevice` used, in
+    general, stochastic pulse trains of a given length are drawn,
+    where the probability of occurrence of an pulse is proportional to
+    :math:`\sqrt{\lambda}d_i` and :math:`\sqrt{\lambda}x_j`
+    respectively. Then for each cross-point, in case a coincidence of
+    column and row pulses occur, the weight is updated one `step`. For
+    details, see `Gokmen & Vlasov (2016)`_.
+
+    The amount of how the weight changes per single step might be
+    different for the different resistive devices.
+
+    In pseudo code::
+
+        # generate prob number
+        p_i  = quantize(A * d_i, res, sto_round)
+        q_j  = quantize(B * x_j, res, sto_round)
+        sign = sign(d_i)*sign(x_j)
+
+        # generate pulse trains of length BL
+        pulse_train_d = gen_pulse_train(p_i, BL) # e.g 101001001
+        pulse_train_x = gen_pulse_train(q_j, BL) # e.g 001010010
+
+        for t in range(BL):
+            if (pulse_train_x[t]==1) and (pulse_train_d[t]==1)
+                update_once(w_{ij}, direction = sign)
+
+    The probabilities are generated using scaling factors ``A`` and ``B`` that
+    are determined by the learning rate and pulse train length ``BL`` (see
+    below). ``quantize`` is an optional discretization of the resulting
+    probability, to account for limited resolution number in the stochastic
+    pulse train generation process on the chip .
+
+    The ``update_once`` functionality is in general dependent on the
+    analog tile class.  For `ConstantStep` the step width is
+    independent of the actual weight, but has cycle-to-cycle
+    variation, device-to-device variation or systematic bias for up
+    versus down direction (see below).
+
+    For parameters regarding the update behaviour, see
+    :class:`~aihwkit.simulator.parameters.AnalogTileUpdateParameters`.
+
+    Args:
+        out_size: output vector size of the tile, ie. the dimension of
+            :math:`\mathbf{y}` in case of :math:`\mathbf{y} =
+            W\mathbf{x}` (or equivalently the dimension of the
+            :math:`\boldsymbol{\delta}` of the backward pass).
+        in_size: input vector size, ie. the dimension of the vector
+            :math:`\mathbf{x}` in case of :math:`\mathbf{y} =
+            W\mathbf{x}`).
+        rpu_config: resistive processing unit configuration.
+        bias: whether to add a bias column to the tile, ie. :math:`W`
+            has an extra column to code the biases. Internally, the
+            input :math:`\mathbf{x}` will be automatically expanded by
+            an extra dimension which will be set to 1 always.
+        in_trans: Whether to assume an transposed input (batch first).
+        out_trans: Whether to assume an transposed output (batch first).
 
-        Args:
-            device: the desired device of the parameters, buffers and analog
-                tiles in this module.
+    .. _Gokmen & Vlasov (2016): https://www.frontiersin.org/articles/10.3389/fnins.2016.00333/full
+    """
 
-        Returns:
-            This module in the specified device.
-        """
+    def __init__(
+        self,
+        out_size: int,
+        in_size: int,
+        rpu_config: RPUConfigGeneric,
+        bias: bool = False,
+        in_trans: bool = False,
+        out_trans: bool = False,
+    ):
+        TileModule.__init__(self)
+        RPUCudaSimulatorTileWrapper.__init__(
+            self, out_size, in_size, rpu_config, bias, in_trans, out_trans
+        )
+        TileWithPeriphery.__init__(self)
+
+    def _create_simulator_tile(
+        self, x_size: int, d_size: int, rpu_config: RPUConfigGeneric
+    ) -> tiles.AnalogTile:
+        """Create a simulator tile.
+
+        Args:
+            x_size: input size
+            d_size: output size
+            rpu_config: resistive processing unit configuration
+
+        Returns:
+            a simulator tile based on the specified configuration.
+        """
+
+        meta_parameter = rpu_config.as_bindings()
+        device_parameter = rpu_config.device.as_bindings()
+
+        return meta_parameter.create_array(x_size, d_size, device_parameter)
+
+    def forward(
+        self, x_input: Tensor, tensor_view: Optional[Tuple] = None  # type: ignore
+    ) -> Tensor:
+        """Torch forward function that calls the analog forward"""
         # pylint: disable=arguments-differ
-        device = torch_device(device)
-
-        super().to(device)
-
-        if device.type == 'cuda':
-            self._apply_to_analog(lambda m: m.cuda(device))
-        elif device.type == 'cpu':
-            self._apply_to_analog(lambda m: m.cpu())
-
-        return self
-
-    def get_analog_tile_device(self) -> Union[torch_device, str, int]:
-        """ Return the devices used by the analog tiles.
-
-        Returns:
-            device: torch device
-
-        Raises:
-            TileError: in case the model is on non-unique devices
-        """
-
-        devices = []
-        dev_name = []
-
-        self._apply_to_analog(
-            lambda mod: devices.extend(mod.get_analog_tile_devices()))
-
-        for name in devices:
-            dev_name.append(str(name))
 
-        if len(set(dev_name)) > 1:
-            raise TileError("Torch device is not unique")
-
-        device = devices[0]
-
-        return device
-
-    def load_state_dict(self,  # pylint: disable=arguments-differ
-                        state_dict: 'OrderedDict[str, Tensor]',
-                        strict: bool = True,
-                        load_rpu_config: bool = True) -> NamedTuple:
-        """Specializes torch's ``load_state_dict`` to add a flag whether to
-        load the RPU config from the saved state.
-
-        Args:
-            state_dict: see torch's ``load_state_dict``
-            strict: see torch's ``load_state_dict``
-            load_rpu_config: Whether to load the saved RPU
-                config or use the current RPU config of the model.
-
-                Caution:
-
-                    If ``load_rpu_config=False`` the RPU config can
-                    be changed from the stored model. However, the user has to
-                    make sure that the changed RPU config makes sense.
-
-                    For instance, changing the device type might
-                    change the expected fields in the hidden
-                    parameters and result in an error.
-        Returns:
-            see torch's ``load_state_dict``
-
-        Raises: ModuleError: in case the rpu_config class mismatches
-            for ``load_rpu_config=False``.
-        """
-        # pylint: disable=protected-access
-        self._apply_to_analog(lambda m: m._set_load_rpu_config_state(load_rpu_config))
-        return super().load_state_dict(state_dict, strict)
-
-    def prepare_for_ddp(self) -> None:
-        """Adds ignores to avoid broadcasting the analog tile states in case of
-        distributed training.
-
-        Note:
-            Call this function before the mode is converted with DDP.
-
-        Important:
-            Only InferenceTile supports DDP.
-
-        Raises:
-
-            ModuleError: In case analog tiles are used that do not
-                support data-parallel model, ie. all analog training
-                tiles.
-        """
-        # pylint: disable=attribute-defined-outside-init
-        exclude_list = []
-        for module in self.modules():
-            if isinstance(module, AnalogModuleBase):
-                for analog_tile in module.analog_tiles():
-                    if analog_tile.shared_weights is None:
-                        raise ModuleError("DDP is only supported with shared weights"
-                                          "(e.g. InferenceTile)")
-                exclude_list += [module.ANALOG_CTX_PREFIX, module.ANALOG_STATE_PREFIX]
-        exclude_list = list(set(exclude_list))
-        params = self.state_dict().keys()
-        exclude_params = []
-        for param in params:
-            for word in exclude_list:
-                if word in param and word not in exclude_params:
-                    exclude_params.append(param)
-                    break
-        self._ddp_params_and_buffers_to_ignore = exclude_params
-
-    def remap_analog_weights(self, weight_scaling_omega: Optional[float] = 1.0) -> None:
-        """Remap the analog weights and set the digital out scales.
-
-        Caution:
-
-            This should typically *not* be called for analog training
-            unless realistic_read_write is set. In this case, it would
-            perform a full re-write of the weights. However,
-            typically, this this method is intended to correct the
-            mapping for hardware-aware trained models before doing the
-            inference with programmed weights.
-
-        Args:
-
-            weight_scaling_omega: The optional value to remap the
-                weight max to. If None it will take the value set
-                initially in the ``RPUConfig.mapping``. Defaults to 1.0.
-
-        """
-
-        self._apply_to_analog(lambda m: m.remap_weights(
-            weight_scaling_omega=weight_scaling_omega))
-
-    def drift_analog_weights(self, t_inference: float = 0.0) -> None:
-        """(Program) and drift all analog inference layers of a given model.
-
-        Args:
-            t_inference: assumed time of inference (in sec)
-
-        Raises:
-            ModuleError: if the layer is not in evaluation mode.
-        """
-        if self.training:
-            raise ModuleError('drift_analog_weights can only be applied in '
-                              'evaluation mode')
-
-        self._apply_to_analog(lambda m: m.drift_analog_weights(t_inference))
-
-    def program_analog_weights(self) -> None:
-        """Program all analog inference layers of a given model.
-
-        Raises:
-            ModuleError: if the layer is not in evaluation mode.
-        """
-        if self.training:
-            raise ModuleError('program_analog_weights can only be applied in '
-                              'evaluation mode')
-
-        self._apply_to_analog(lambda m: m.program_analog_weights())
-
-    @classmethod
-    def from_digital(cls, module: Sequential,  # pylint: disable=unused-argument
-                     *args: Any,
-                     **kwargs: Any) -> 'AnalogSequential':
-        """Construct AnalogSequential in-place from Sequential."""
-        return cls(OrderedDict(mod for mod in module.named_children()))
+        out = AnalogFunction.apply(
+            self.get_analog_ctx(), self, x_input, self.shared_weights, not self.training
+        )
+
+        if tensor_view is None:
+            tensor_view = self.get_tensor_view(out.dim())
+        out = self.apply_out_scaling(out, tensor_view)
+
+        if self.digital_bias:
+            return out + self.bias.view(*tensor_view)
+        return out
```

### Comparing `aihwkit-0.7.1/src/aihwkit/nn/modules/conv.py` & `aihwkit-0.8.0/src/aihwkit/nn/modules/conv.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,211 +1,196 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
 # that they have been altered from the originals.
 
 """Convolution layers."""
 
-from typing import Optional, Tuple, Union, List
+# pylint: disable=too-many-arguments, too-many-locals, too-many-instance-attributes
+
+from typing import Optional, Tuple, Union, List, Type
 
 from torch import Tensor, arange, cat, float64, int32, ones
+from torch.autograd import no_grad
 from torch.nn.functional import pad, unfold
 from torch.nn.modules.conv import _ConvNd, Conv1d, Conv2d, Conv3d
 from torch.nn.modules.utils import _single, _pair, _triple
 
-from aihwkit.nn.functions import AnalogIndexedFunction, AnalogFunction
-from aihwkit.nn.modules.base import AnalogModuleBase, RPUConfigAlias
-from aihwkit.simulator.configs import SingleRPUConfig
+from aihwkit.exceptions import ModuleError
+from aihwkit.nn.modules.base import AnalogLayerBase
+from aihwkit.simulator.parameters.base import RPUConfigBase
 
 
-class _AnalogConvNd(AnalogModuleBase, _ConvNd):
+class _AnalogConvNd(AnalogLayerBase, _ConvNd):
     """Base class for convolution layers."""
 
-    __constants__ = ['stride', 'padding', 'dilation', 'groups',
-                     'padding_mode', 'output_padding', 'in_channels',
-                     'out_channels', 'kernel_size', 'in_features', 'out_features',
-                     'realistic_read_write',
-                     'digital_bias', 'analog_bias', 'use_bias']
-    in_channels: int
-    out_channels: int
-    kernel_size: Tuple[int, ...]
-    stride: Tuple[int, ...]
-    padding: Tuple[int, ...]
-    dilation: Tuple[int, ...]
-    realistic_read_write: bool
-    transposed: bool
-    output_padding: Tuple[int, ...]
-    groups: int
-    padding_mode: str
-    fold_indices: Tensor
-    input_size: float
-    in_features: int
-    out_features: int
-    digital_bias: bool
-    analog_bias: bool
-    use_bias: bool
+    NEEDS_INDEXED = False
 
     def __init__(
-            self,
-            in_channels: int,
-            out_channels: int,
-            kernel_size: Tuple[int, ...],
-            stride: Tuple[int, ...],
-            padding: Tuple[int, ...],
-            dilation: Tuple[int, ...],
-            transposed: bool,
-            output_padding: Tuple[int, ...],
-            groups: int,
-            bias: bool,
-            padding_mode: str,
-            rpu_config: Optional[RPUConfigAlias] = None,
-            realistic_read_write: bool = False,
-            weight_scaling_omega: Optional[bool] = None,
-            use_indexed: Optional[bool] = None
+        self,
+        in_channels: int,
+        out_channels: int,
+        kernel_size: Tuple[int, ...],
+        stride: Tuple[int, ...],
+        padding: Tuple[int, ...],
+        dilation: Tuple[int, ...],
+        transposed: bool,
+        output_padding: Tuple[int, ...],
+        groups: int,
+        bias: bool,
+        padding_mode: str,
+        rpu_config: Optional[RPUConfigBase] = None,
+        tile_module_class: Optional[Type] = None,
+        use_indexed: Optional[bool] = None,
     ):
-        # pylint: disable=too-many-arguments, too-many-locals
         if groups != 1:
-            raise ValueError('Only one group is supported')
-        if padding_mode != 'zeros':
+            raise ValueError("Only one group is supported")
+        if padding_mode != "zeros":
             raise ValueError('Only "zeros" padding mode is supported')
 
-        # Call super() after tile creation, including ``reset_parameters``.
-        _ConvNd.__init__(self, in_channels, out_channels, kernel_size, stride,
-                         padding, dilation, transposed, output_padding, groups, bias,
-                         padding_mode)
-
-        # Create the tile and set the analog.
-        if rpu_config is None:
-            rpu_config = SingleRPUConfig()
-
-        rpu_config = self._set_weight_scaling_omega(rpu_config, weight_scaling_omega)
-
-        AnalogModuleBase.__init__(
+        # Call super()
+        _ConvNd.__init__(
             self,
-            self.get_tile_size(in_channels, groups, kernel_size),
+            in_channels,
             out_channels,
+            kernel_size,
+            stride,
+            padding,
+            dilation,
+            transposed,
+            output_padding,
+            groups,
             bias,
-            realistic_read_write,
-            rpu_config.mapping
+            padding_mode,
         )
-        self.analog_tile = self._setup_tile(rpu_config)
 
-        # Register analog tile
-        self.register_analog_tile(self.analog_tile)
+        # Create the tile and set the analog.
+        AnalogLayerBase.__init__(self)
+
+        if rpu_config is None:
+            # pylint: disable=import-outside-toplevel
+            from aihwkit.simulator.configs.configs import SingleRPUConfig
+
+            rpu_config = SingleRPUConfig()
 
-        # Set weights from the reset_parameters
-        self.set_weights(self.weight, self.bias)
+        if tile_module_class is None:
+            tile_module_class = rpu_config.get_default_tile_module_class()
+        self.in_features = self.get_tile_size(in_channels, groups, kernel_size)
+        self.out_features = out_channels
+        self.analog_module = tile_module_class(
+            self.out_features, self.in_features, rpu_config, bias
+        )
 
         # Set the index matrices.
         self.use_indexed = use_indexed
+        if not self.analog_module.supports_indexed:
+            self.use_indexed = False
 
         self.fold_indices = Tensor().detach()
-        self.register_helper('fold_indices')
         self.input_size = 0
-        self.register_helper('input_size')
         self.tensor_view = (-1,)  # type: Tuple[int, ...]
 
         # Unregister weight/bias as a parameter but keep it for syncs
-        self.unregister_parameter('weight')
-        if self.analog_bias:
-            self.unregister_parameter('bias')
+        self.unregister_parameter("weight")
+        if bias:
+            self.unregister_parameter("bias")
+        else:
+            # seems to be a torch bug
+            self._parameters.pop("bias", None)
+        self.bias = bias
 
-    def get_tile_size(
-            self,
-            in_channels: int,
-            groups: int,
-            kernel_size: Tuple[int, ...]
-    ) -> int:
+        self.reset_parameters()
+
+    def get_tile_size(self, in_channels: int, groups: int, kernel_size: Tuple[int, ...]) -> int:
         """Calculate the tile size."""
         raise NotImplementedError
 
     def get_image_size(self, size: int, i: int) -> int:
         """Calculate the output image sizes."""
-        nom = (size + 2 * self.padding[i] - self.dilation[i] * (self.kernel_size[i] - 1) - 1)
+        # pylint: disable=superfluous-parens
+        nom = size + 2 * self.padding[i] - self.dilation[i] * (self.kernel_size[i] - 1) - 1
         return nom // self.stride[i] + 1
 
     def reset_parameters(self) -> None:
         """Reset the parameters (weight and bias)."""
-        super().reset_parameters()
-        if self.analog_tile_count():
+        if hasattr(self, "analog_module"):
+            bias = self.bias
+            self.weight, self.bias = self.get_weights()  # type: ignore
+            super().reset_parameters()
             self.set_weights(self.weight, self.bias)
+            self.weight, self.bias = None, bias
 
+    @no_grad()
     def _recalculate_indexes(self, x_input: Tensor) -> None:
         """Calculate and set the indexes of the analog tile."""
 
-        self.fold_indices, image_sizes, self.input_size = \
-            self._calculate_indexes(x_input, self.in_channels)
-        self.analog_tile.set_indexed(self.fold_indices, image_sizes)
+        self.fold_indices, image_sizes, self.input_size = self._calculate_indexes(
+            x_input, self.in_channels
+        )
+        self.analog_module.set_indexed(self.fold_indices, image_sizes)
 
-    def _calculate_indexes(self, x_input: Tensor,
-                           in_channels: int) -> Tuple[Tensor, List[int], int]:
+    @no_grad()
+    def _calculate_indexes(
+        self, x_input: Tensor, in_channels: int
+    ) -> Tuple[Tensor, List[int], int]:
         """Calculate and return the fold indexes and sizes.
 
         Args:
             x_input: input matrix
             in_channels: number of input channel
 
         Returns:
             fold_indices: indices for the analog tile
             image_sizes: image sizes for the analog tile
             input_size: size of the current input
         """
         raise NotImplementedError
 
-    def _forward_indexed(self, x_input: Tensor) -> Tensor:
-        """Compute the forward pass in indexed fashion. This is fast and
-        memory-efficient indexed convolution (only for GPUs)"""
-
-        input_size = x_input.numel() / x_input.size(0)
-        if self.input_size != input_size or not self.analog_tile.is_indexed():
-            self._recalculate_indexes(x_input)
-
-        return AnalogIndexedFunction.apply(
-            self.analog_tile.get_analog_ctx(), x_input,
-            self.analog_tile.shared_weights, not self.training)
-
-    def _forward_unfold(self, x_input: Tensor) -> Tensor:
-        """Forward using explicit unfolding (more suitable for CPUs) """
-        im_shape = x_input.shape
-        x_input_ = unfold(x_input, kernel_size=self.kernel_size, dilation=self.dilation,
-                          padding=self.padding, stride=self.stride).transpose(1, 2)
-
-        out = AnalogFunction.apply(
-            self.analog_tile.get_analog_ctx(), x_input_,
-            self.analog_tile.shared_weights, not self.training).transpose(1, 2)
-
-        out_size = (im_shape[2] + 2 * self.padding[0]
-                    - self.dilation[0] * (self.kernel_size[0] - 1) - 1) // self.stride[0] + 1
-        return out.view(im_shape[0], self.out_channels, out_size, -1)
-
     def forward(self, x_input: Tensor) -> Tensor:
-        """Compute the forward pass."""
+        """Compute the forward pass.
 
-        if self.use_indexed is None:
-            use_indexed = self.analog_tile.device.type == 'cuda'
-        else:
-            use_indexed = self.use_indexed
+        Raises:
+            ModuleError: in case indexed convolution is needed but not supported by the TileModule.
+        """
 
+        # Use indexed only in case of cuda.
+        use_indexed = self.use_indexed
+        if use_indexed is None and not self.NEEDS_INDEXED:
+            use_indexed = self.analog_module.is_cuda
+        if not use_indexed and self.NEEDS_INDEXED:
+            raise ModuleError("Tile module does not support indexed computation.")
         if use_indexed:
-            out = self._forward_indexed(x_input)
-        else:
-            out = self._forward_unfold(x_input)
+            input_size = x_input.numel() / x_input.size(0)
+            if self.input_size != input_size or not self.analog_module.is_indexed():
+                self._recalculate_indexes(x_input)
 
-        out = self.analog_tile.apply_out_scaling(out, self.tensor_view)
+            return self.analog_module(x_input, tensor_view=self.tensor_view)
 
-        if self.digital_bias:
-            return out + self.bias.view(*self.tensor_view)
-        return out
+        # Brute-force unfold.
+        im_shape = x_input.shape
+        x_input_ = unfold(
+            x_input,
+            kernel_size=self.kernel_size,
+            dilation=self.dilation,
+            padding=self.padding,
+            stride=self.stride,
+        ).transpose(1, 2)
+
+        out = self.analog_module(x_input_).transpose(1, 2)
+        out_size = (
+            im_shape[2] + 2 * self.padding[0] - self.dilation[0] * (self.kernel_size[0] - 1) - 1
+        ) // self.stride[0] + 1
+        return out.view(im_shape[0], self.out_channels, out_size, -1)
 
 
 class AnalogConv1d(_AnalogConvNd):
     """1D convolution layer that uses an analog tile.
 
     Applies a 1D convolution over an input signal composed of several input
     planes, using an analog tile for its forward, backward and update passes.
@@ -226,105 +211,136 @@
         padding: zero-padding added to both sides of the input.
         dilation: spacing between kernel elements.
         groups: number of blocked connections from input channels to output
             channels.
         bias: whether to use a bias row on the analog tile or not.
         padding_mode: padding strategy. Only ``'zeros'`` is supported.
         rpu_config: resistive processing unit configuration.
-        realistic_read_write: whether to enable realistic read/write
-            for setting initial weights and during reading of the weights.
-        weight_scaling_omega: depreciated, use
-            :class:`aihwkit.simulator.configs.utils.MappingParameter`
-            instead to specify weight scaling
+        tile_module_class: Class for the tile module (default
+            will be specified from the ``RPUConfig``).
+
     """
+
     # pylint: disable=abstract-method
 
+    NEEDS_INDEXED = True
+
     def __init__(
-            self,
-            in_channels: int,
-            out_channels: int,
-            kernel_size: Union[int, Tuple],
-            stride: Union[int, Tuple] = 1,
-            padding: Union[int, Tuple] = 0,
-            dilation: Union[int, Tuple] = 1,
-            groups: int = 1,
-            bias: bool = True,
-            padding_mode: str = 'zeros',
-            rpu_config: Optional[RPUConfigAlias] = None,
-            realistic_read_write: bool = False,
-            weight_scaling_omega: Optional[bool] = None
+        self,
+        in_channels: int,
+        out_channels: int,
+        kernel_size: Union[int, Tuple],
+        stride: Union[int, Tuple] = 1,
+        padding: Union[int, Tuple] = 0,
+        dilation: Union[int, Tuple] = 1,
+        groups: int = 1,
+        bias: bool = True,
+        padding_mode: str = "zeros",
+        rpu_config: Optional["RPUConfigBase"] = None,
+        tile_module_class: Optional[Type] = None,
     ):
         # pylint: disable=too-many-arguments
         kernel_size = _single(kernel_size)
         stride = _single(stride)
         padding = _single(padding)
         dilation = _single(dilation)
 
         if dilation != _single(1):
-            raise ValueError('Only dilation = 1 is supported')
+            raise ValueError("Only dilation = 1 is supported")
 
         super().__init__(
-            in_channels, out_channels, kernel_size, stride, padding, dilation,  # type: ignore
-            False, _single(0), groups, bias, padding_mode,
-            rpu_config, realistic_read_write, weight_scaling_omega, True
+            in_channels,
+            out_channels,
+            kernel_size,  # type: ignore
+            stride,  # type: ignore
+            padding,  # type: ignore
+            dilation,  # type: ignore
+            False,
+            _single(0),
+            groups,
+            bias,
+            padding_mode,
+            rpu_config,
+            tile_module_class,
+            True,
         )
 
         self.tensor_view = (-1, 1)
 
     @classmethod
     def from_digital(
-            cls,
-            module: Conv1d,
-            rpu_config: Optional[RPUConfigAlias] = None,
-            realistic_read_write: bool = False,
-    ) -> 'AnalogConv1d':
+        cls, module: Conv1d, rpu_config: "RPUConfigBase", tile_module_class: Optional[Type] = None
+    ) -> "AnalogConv1d":
         """Return an AnalogConv1d layer from a torch Conv1d layer.
 
         Args:
             module: The torch module to convert. All layers that are
                 defined in the ``conversion_map``.
             rpu_config: RPU config to apply to all converted tiles.
                 Applied to all converted tiles.
-            realistic_read_write: Whether to use closed-loop programming
-                when setting the weights. Applied to all converted tiles.
-
-                Note:
-                    Make sure that the weight max and min settings of the
-                    device support the desired analog weight range.
-
+            tile_module_class: Class for the tile module (default
+                will be specified from the ``RPUConfig``).
         Returns:
             an AnalogConv1d layer based on the digital Conv1d ``module``.
         """
-        analog_module = cls(module.in_channels,
-                            module.out_channels,
-                            module.kernel_size,
-                            module.stride,
-                            module.padding,
-                            module.dilation,
-                            module.groups,
-                            module.bias is not None,
-                            module.padding_mode,
-                            rpu_config,
-                            realistic_read_write,
-                            )
+        analog_layer = cls(
+            module.in_channels,
+            module.out_channels,
+            module.kernel_size,
+            module.stride,
+            module.padding,
+            module.dilation,
+            module.groups,
+            module.bias is not None,
+            module.padding_mode,
+            rpu_config,
+            tile_module_class,
+        )
 
-        analog_module.set_weights(module.weight, module.bias)
-        return analog_module
+        analog_layer.set_weights(module.weight, module.bias)
+        return analog_layer
 
-    def get_tile_size(
-            self,
-            in_channels: int,
-            groups: int,
-            kernel_size: Tuple[int, ...]
-    ) -> int:
+    @classmethod
+    def to_digital(cls, module: "AnalogConv1d", realistic: bool = False) -> Conv1d:
+        """Return an nn.Conv1d layer from an AnalogConv1d layer.
+
+        Args:
+            module: The analog module to convert.
+            realistic: whehter to estimate the weights with the
+                non-ideal forward pass. If not set, analog weights are
+                (unrealistically) copies exactly
+
+        Returns:
+            an torch Linear layer with the same dimension and weights
+            as the analog linear layer.
+        """
+        weight, bias = module.get_weights(realistic=realistic)
+        digital_layer = Conv1d(
+            module.in_channels,
+            module.out_channels,
+            module.kernel_size,
+            module.stride,
+            module.padding,
+            module.dilation,
+            module.groups,
+            bias is not None,
+            module.padding_mode,
+        )
+        digital_layer.weight.data = weight.data.view(-1, module.in_channels, *module.kernel_size)
+        if bias is not None:
+            digital_layer.bias.data = bias.data
+        return digital_layer
+
+    def get_tile_size(self, in_channels: int, groups: int, kernel_size: Tuple[int, ...]) -> int:
         """Calculate the tile size."""
         return (in_channels // groups) * kernel_size[0]
 
-    def _calculate_indexes(self, x_input: Tensor,
-                           in_channels: int) -> Tuple[Tensor, List[int], int]:
+    def _calculate_indexes(
+        self, x_input: Tensor, in_channels: int
+    ) -> Tuple[Tensor, List[int], int]:
         """Calculate and return the fold indexes and sizes.
 
         Args:
             x_input: input matrix
             in_channels: number of input channel
 
         Returns:
@@ -332,19 +348,20 @@
             image_sizes: image sizes for the analog tile
             input_size: size of the current input
         """
         input_size = x_input.numel() / x_input.size(0)
 
         # pytorch just always uses NCHW order
         fold_indices = arange(2, x_input.size(2) + 2, dtype=float64).detach()
-        shape = [1] + [1] + list(x_input.shape[2:])
+        shape = [1, 1] + list(x_input.shape[2:])
         fold_indices = fold_indices.reshape(*shape)
         if not all(item == 0 for item in self.padding):
-            fold_indices = pad(fold_indices, pad=[self.padding[0], self.padding[0]],
-                               mode='constant', value=0)
+            fold_indices = pad(
+                fold_indices, pad=[self.padding[0], self.padding[0]], mode="constant", value=0
+            )
         unfolded = fold_indices.unfold(2, self.kernel_size[0], self.stride[0]).clone()
         fold_indices = unfolded.reshape(-1, self.kernel_size[0]).transpose(0, 1).flatten().round()
 
         # concatenate the matrix index for different channels
         fold_indices_orig = fold_indices.clone()
         for i in range(in_channels - 1):
             fold_indices_tmp = fold_indices_orig.clone()
@@ -352,15 +369,15 @@
                 if fold_indices_orig[j] != 0:
                     fold_indices_tmp[j] += (input_size / in_channels) * (i + 1)
 
             fold_indices = cat([fold_indices, fold_indices_tmp], dim=0).clone()
 
         fold_indices = fold_indices.to(dtype=int32)
 
-        if self.analog_bias:
+        if self.analog_module.analog_bias:
             out_image_size = fold_indices.numel() // (self.kernel_size[0])
             fold_indices = cat((fold_indices, ones(out_image_size, dtype=int32)), 0)
 
         fold_indices = fold_indices.to(x_input.device)
 
         x_height = x_input.size(2)
         d_height = self.get_image_size(x_height, 0)
@@ -391,106 +408,135 @@
         padding: zero-padding added to both sides of the input.
         dilation: spacing between kernel elements.
         groups: number of blocked connections from input channels to output
             channels.
         bias: whether to use a bias row on the analog tile or not.
         padding_mode: padding strategy. Only ``'zeros'`` is supported.
         rpu_config: resistive processing unit configuration.
-        realistic_read_write: whether to enable realistic read/write
-            for setting initial weights and read out of weights.
-        weight_scaling_omega: depreciated, use
-            :class:`aihwkit.simulator.configs.utils.MappingParameter`
-            instead to specify weight scaling
+        tile_module_class: Class for the tile module (default
+            will be specified from the ``RPUConfig``).
         use_indexed: Whether to use explicit unfolding or implicit indexing. If
             None (default), it will use implicit indexing for CUDA and
             explicit unfolding for CPU
     """
+
     # pylint: disable=abstract-method
 
     def __init__(
-            self,
-            in_channels: int,
-            out_channels: int,
-            kernel_size: Union[int, Tuple],
-            stride: Union[int, Tuple] = 1,
-            padding: Union[int, Tuple] = 0,
-            dilation: Union[int, Tuple] = 1,
-            groups: int = 1,
-            bias: bool = True,
-            padding_mode: str = 'zeros',
-            rpu_config: Optional[RPUConfigAlias] = None,
-            realistic_read_write: bool = False,
-            weight_scaling_omega: Optional[bool] = None,
-            use_indexed: Optional[bool] = None
+        self,
+        in_channels: int,
+        out_channels: int,
+        kernel_size: Union[int, Tuple],
+        stride: Union[int, Tuple] = 1,
+        padding: Union[int, Tuple] = 0,
+        dilation: Union[int, Tuple] = 1,
+        groups: int = 1,
+        bias: bool = True,
+        padding_mode: str = "zeros",
+        rpu_config: Optional["RPUConfigBase"] = None,
+        tile_module_class: Optional[Type] = None,
+        use_indexed: Optional[bool] = None,
     ):
         # pylint: disable=too-many-arguments
         kernel_size = _pair(kernel_size)
         stride = _pair(stride)
         padding = _pair(padding)
         dilation = _pair(dilation)
 
         super().__init__(
-            in_channels, out_channels, kernel_size, stride, padding, dilation,  # type: ignore
-            False, _pair(0), groups, bias, padding_mode,
-            rpu_config, realistic_read_write, weight_scaling_omega, use_indexed
+            in_channels,
+            out_channels,
+            kernel_size,  # type: ignore
+            stride,  # type: ignore
+            padding,  # type: ignore
+            dilation,  # type: ignore
+            False,
+            _pair(0),
+            groups,
+            bias,
+            padding_mode,
+            rpu_config,
+            tile_module_class,
+            use_indexed,
         )
 
         self.tensor_view = (-1, 1, 1)
 
     @classmethod
     def from_digital(
-            cls,
-            module: Conv2d,
-            rpu_config: Optional[RPUConfigAlias] = None,
-            realistic_read_write: bool = False,
-    ) -> 'AnalogConv2d':
+        cls, module: Conv2d, rpu_config: "RPUConfigBase", tile_module_class: Optional[Type] = None
+    ) -> "AnalogConv2d":
         """Return an AnalogConv2d layer from a torch Conv2d layer.
 
         Args:
             module: The torch module to convert. All layers that are
                 defined in the ``conversion_map``.
             rpu_config: RPU config to apply to all converted tiles.
                 Applied to all converted tiles.
-            realistic_read_write: Whether to use closed-loop programming
-                when setting the weights. Applied to all converted tiles.
-
-                Note:
-                    Make sure that the weight max and min settings of the
-                    device support the desired analog weight range.
+            tile_module_class: Class for the tile module (default
+                will be specified from the ``RPUConfig``).
 
         Returns:
             an AnalogConv2d layer based on the digital Conv2d ``module``.
         """
-        analog_module = cls(module.in_channels,
-                            module.out_channels,
-                            module.kernel_size,
-                            module.stride,
-                            module.padding,
-                            module.dilation,
-                            module.groups,
-                            module.bias is not None,
-                            module.padding_mode,
-                            rpu_config,
-                            realistic_read_write,
-                            )
+        analog_layer = cls(
+            module.in_channels,
+            module.out_channels,
+            module.kernel_size,
+            module.stride,
+            module.padding,
+            module.dilation,
+            module.groups,
+            module.bias is not None,
+            module.padding_mode,
+            rpu_config,
+            tile_module_class,
+        )
 
-        analog_module.set_weights(module.weight, module.bias)
-        return analog_module
+        analog_layer.set_weights(module.weight, module.bias)
+        return analog_layer
 
-    def get_tile_size(
-            self,
-            in_channels: int,
-            groups: int,
-            kernel_size: Tuple[int, ...]
-    ) -> int:
+    @classmethod
+    def to_digital(cls, module: "AnalogConv2d", realistic: bool = False) -> Conv2d:
+        """Return an nn.Conv2d layer from an AnalogConv2d layer.
+
+        Args:
+            module: The analog module to convert.
+            realistic: whehter to estimate the weights with the
+                non-ideal forward pass. If not set, analog weights are
+                (unrealistically) copies exactly
+
+        Returns:
+            an torch Linear layer with the same dimension and weights
+            as the analog linear layer.
+        """
+        weight, bias = module.get_weights(realistic=realistic)
+        digital_layer = Conv2d(
+            module.in_channels,
+            module.out_channels,
+            module.kernel_size,
+            module.stride,
+            module.padding,
+            module.dilation,
+            module.groups,
+            bias is not None,
+            module.padding_mode,
+        )
+        digital_layer.weight.data = weight.data.view(-1, module.in_channels, *module.kernel_size)
+        if bias is not None:
+            digital_layer.bias.data = bias.data
+        return digital_layer
+
+    def get_tile_size(self, in_channels: int, groups: int, kernel_size: Tuple[int, ...]) -> int:
         """Calculate the tile size."""
         return (in_channels // groups) * kernel_size[0] * kernel_size[1]
 
-    def _calculate_indexes(self, x_input: Tensor,
-                           in_channels: int) -> Tuple[Tensor, List[int], int]:
+    def _calculate_indexes(
+        self, x_input: Tensor, in_channels: int
+    ) -> Tuple[Tensor, List[int], int]:
         """Calculate and return the fold indexes and sizes.
 
         Args:
             x_input: input matrix
             in_channels: number of input channel
 
         Returns:
@@ -500,21 +546,28 @@
         """
         input_size = x_input.numel() / x_input.size(0)
 
         # pytorch just always uses NCHW order
         fold_indices = arange(2, input_size + 2, dtype=float64).detach()
         shape = [1] + list(x_input.shape[1:])
         fold_indices = fold_indices.reshape(*shape)
-        fold_indices = unfold(fold_indices,
-                              kernel_size=self.kernel_size,
-                              stride=self.stride,
-                              padding=self.padding,
-                              dilation=self.dilation).flatten().round().to(dtype=int32)
+        fold_indices = (
+            unfold(
+                fold_indices,
+                kernel_size=self.kernel_size,
+                stride=self.stride,
+                padding=self.padding,
+                dilation=self.dilation,
+            )
+            .flatten()
+            .round()
+            .to(dtype=int32)
+        )
 
-        if self.analog_bias:
+        if self.analog_module.analog_bias:
             out_image_size = fold_indices.numel() // (self.kernel_size[0] * self.kernel_size[1])
             fold_indices = cat((fold_indices, ones(out_image_size, dtype=int32)), 0)
 
         fold_indices = fold_indices.to(x_input.device)
 
         x_height = x_input.size(2)
         x_width = x_input.size(3)
@@ -548,106 +601,136 @@
         padding: zero-padding added to both sides of the input.
         dilation: spacing between kernel elements.
         groups: number of blocked connections from input channels to output
             channels.
         bias: whether to use a bias row on the analog tile or not.
         padding_mode: padding strategy. Only ``'zeros'`` is supported.
         rpu_config: resistive processing unit configuration.
-        realistic_read_write: whether to enable realistic read/write
-            for setting initial weights and read out of weights.
-        weight_scaling_omega: depreciated, use
-            :class:`aihwkit.simulator.configs.utils.MappingParameter`
-            instead to specify weight scaling
+        tile_module_class: Class for the tile module (default
+            will be specified from the ``RPUConfig``).
     """
+
     # pylint: disable=abstract-method
 
+    NEEDS_INDEXED = True
+
     def __init__(
-            self,
-            in_channels: int,
-            out_channels: int,
-            kernel_size: Union[int, Tuple],
-            stride: Union[int, Tuple] = 1,
-            padding: Union[int, Tuple] = 0,
-            dilation: Union[int, Tuple] = 1,
-            groups: int = 1,
-            bias: bool = True,
-            padding_mode: str = 'zeros',
-            rpu_config: Optional[RPUConfigAlias] = None,
-            realistic_read_write: bool = False,
-            weight_scaling_omega: Optional[bool] = None
+        self,
+        in_channels: int,
+        out_channels: int,
+        kernel_size: Union[int, Tuple],
+        stride: Union[int, Tuple] = 1,
+        padding: Union[int, Tuple] = 0,
+        dilation: Union[int, Tuple] = 1,
+        groups: int = 1,
+        bias: bool = True,
+        padding_mode: str = "zeros",
+        rpu_config: Optional["RPUConfigBase"] = None,
+        tile_module_class: Optional[Type] = None,
     ):
         # pylint: disable=too-many-arguments
         kernel_size = _triple(kernel_size)
         stride = _triple(stride)
         padding = _triple(padding)
         dilation = _triple(dilation)
 
         if dilation != _triple(1):
-            raise ValueError('Only dilation = 1 is supported')
+            raise ValueError("Only dilation = 1 is supported")
 
         super().__init__(
-            in_channels, out_channels, kernel_size, stride, padding, dilation,  # type: ignore
-            False, _triple(0), groups, bias, padding_mode,
-            rpu_config, realistic_read_write, weight_scaling_omega, True
+            in_channels,
+            out_channels,
+            kernel_size,  # type: ignore
+            stride,  # type: ignore
+            padding,  # type: ignore
+            dilation,  # type: ignore
+            False,
+            _triple(0),
+            groups,
+            bias,
+            padding_mode,
+            rpu_config,
+            tile_module_class,
+            True,
         )
 
         self.tensor_view = (-1, 1, 1, 1)
 
     @classmethod
     def from_digital(
-            cls,
-            module: Conv3d,
-            rpu_config: Optional[RPUConfigAlias] = None,
-            realistic_read_write: bool = False,
-    ) -> 'AnalogConv3d':
+        cls, module: Conv3d, rpu_config: "RPUConfigBase", tile_module_class: Optional[Type] = None
+    ) -> "AnalogConv3d":
         """Return an AnalogConv3d layer from a torch Conv3d layer.
 
         Args:
             module: The torch module to convert. All layers that are
                 defined in the ``conversion_map``.
             rpu_config: RPU config to apply to all converted tiles.
                 Applied to all converted tiles.
-            realistic_read_write: Whether to use closed-loop programming
-                when setting the weights. Applied to all converted tiles.
-
-                Note:
-                    Make sure that the weight max and min settings of the
-                    device support the desired analog weight range.
+            tile_module_class: Class for the tile module (default
+                will be specified from the ``RPUConfig``).
 
         Returns:
             an AnalogConv3d layer based on the digital Conv3d ``module``.
         """
-        analog_module = cls(module.in_channels,
-                            module.out_channels,
-                            module.kernel_size,
-                            module.stride,
-                            module.padding,
-                            module.dilation,
-                            module.groups,
-                            module.bias is not None,
-                            module.padding_mode,
-                            rpu_config,
-                            realistic_read_write,
-                            )
+        analog_layer = cls(
+            module.in_channels,
+            module.out_channels,
+            module.kernel_size,
+            module.stride,
+            module.padding,
+            module.dilation,
+            module.groups,
+            module.bias is not None,
+            module.padding_mode,
+            rpu_config,
+            tile_module_class,
+        )
 
-        analog_module.set_weights(module.weight, module.bias)
-        return analog_module
+        analog_layer.set_weights(module.weight, module.bias)
+        return analog_layer
 
-    def get_tile_size(
-            self,
-            in_channels: int,
-            groups: int,
-            kernel_size: Tuple[int, ...]
-    ) -> int:
+    @classmethod
+    def to_digital(cls, module: "AnalogConv3d", realistic: bool = False) -> Conv3d:
+        """Return an nn.Conv3d layer from an AnalogConv3d layer.
+
+        Args:
+            module: The analog module to convert.
+            realistic: whehter to estimate the weights with the
+                non-ideal forward pass. If not set, analog weights are
+                (unrealistically) copies exactly
+
+        Returns:
+            an torch Linear layer with the same dimension and weights
+            as the analog linear layer.
+        """
+        weight, bias = module.get_weights(realistic=realistic)
+        digital_layer = Conv3d(
+            module.in_channels,
+            module.out_channels,
+            module.kernel_size,
+            module.stride,
+            module.padding,
+            module.dilation,
+            module.groups,
+            bias is not None,
+            module.padding_mode,
+        )
+        digital_layer.weight.data = weight.data.view(-1, module.in_channels, *module.kernel_size)
+        if bias is not None:
+            digital_layer.bias.data = bias.data
+        return digital_layer
+
+    def get_tile_size(self, in_channels: int, groups: int, kernel_size: Tuple[int, ...]) -> int:
         """Calculate the tile size."""
-        return (in_channels // groups) * (
-                kernel_size[0] * kernel_size[1] * kernel_size[2])
+        return (in_channels // groups) * (kernel_size[0] * kernel_size[1] * kernel_size[2])
 
-    def _calculate_indexes(self, x_input: Tensor,
-                           in_channels: int) -> Tuple[Tensor, List[int], int]:
+    def _calculate_indexes(
+        self, x_input: Tensor, in_channels: int
+    ) -> Tuple[Tensor, List[int], int]:
         """Calculate and return the fold indexes and sizes.
 
         Args:
             x_input: input matrix
             in_channels: then number of in channels
 
         Returns:
@@ -655,46 +738,63 @@
             image_sizes: image sizes for the analog tile
             input_size: size of the current input
         """
         # pylint: disable=too-many-locals
         input_size = x_input.numel() / x_input.size(0)
 
         # pytorch just always uses NCDHW order
-        fold_indices = arange(2, x_input.size(2) * x_input.size(3) * x_input.size(4) + 2,
-                              dtype=float64).detach()
+        fold_indices = arange(
+            2, x_input.size(2) * x_input.size(3) * x_input.size(4) + 2, dtype=float64
+        ).detach()
         shape = [1] + [1] + list(x_input.shape[2:])
         fold_indices = fold_indices.reshape(*shape)
         if not all(item == 0 for item in self.padding):
-            fold_indices = pad(fold_indices, pad=[
-                self.padding[2], self.padding[2],
-                self.padding[1], self.padding[1],
-                self.padding[0], self.padding[0]], mode='constant', value=0)
-        unfolded = fold_indices.unfold(2, self.kernel_size[0], self.stride[0]). \
-            unfold(3, self.kernel_size[1], self.stride[1]). \
-            unfold(4, self.kernel_size[2], self.stride[2]).clone()
+            fold_indices = pad(
+                fold_indices,
+                pad=[
+                    self.padding[2],
+                    self.padding[2],
+                    self.padding[1],
+                    self.padding[1],
+                    self.padding[0],
+                    self.padding[0],
+                ],
+                mode="constant",
+                value=0,
+            )
+        unfolded = (
+            fold_indices.unfold(2, self.kernel_size[0], self.stride[0])
+            .unfold(3, self.kernel_size[1], self.stride[1])
+            .unfold(4, self.kernel_size[2], self.stride[2])
+            .clone()
+        )
 
-        fold_indices = unfolded.reshape(-1, self.kernel_size[0] * self.kernel_size[1] *
-                                        self.kernel_size[2]).transpose(0, 1).flatten().round()
+        fold_indices = (
+            unfolded.reshape(-1, self.kernel_size[0] * self.kernel_size[1] * self.kernel_size[2])
+            .transpose(0, 1)
+            .flatten()
+            .round()
+        )
 
         # concatenate the matrix index for different channels
         fold_indices_orig = fold_indices.clone()
         for i in range(in_channels - 1):
             fold_indices_tmp = fold_indices_orig.clone()
             for j in range(fold_indices_orig.size(0)):
                 if fold_indices_orig[j] != 0:
                     fold_indices_tmp[j] += (input_size / in_channels) * (i + 1)
 
             fold_indices = cat([fold_indices, fold_indices_tmp], dim=0).clone()
 
         fold_indices = fold_indices.to(dtype=int32)
 
-        if self.analog_bias:
-            out_image_size = fold_indices.numel() // (self.kernel_size[0] *
-                                                      self.kernel_size[1] *
-                                                      self.kernel_size[2])
+        if self.analog_module.analog_bias:
+            out_image_size = fold_indices.numel() // (
+                self.kernel_size[0] * self.kernel_size[1] * self.kernel_size[2]
+            )
             fold_indices = cat((fold_indices, ones(out_image_size, dtype=int32)), 0)
 
         fold_indices = fold_indices.to(x_input.device)
 
         x_depth = x_input.size(2)
         x_height = x_input.size(3)
         x_width = x_input.size(4)
```

### Comparing `aihwkit-0.7.1/src/aihwkit/nn/modules/conv_mapped.py` & `aihwkit-0.8.0/src/aihwkit/nn/modules/conv_mapped.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,151 +1,134 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
 # that they have been altered from the originals.
 
 """Mapped convolution layers."""
 
-from typing import Optional, Tuple, Union, List
+# pylint: disable=too-many-arguments, too-many-locals, too-many-instance-attributes, too-many-lines
 
-from torch import Tensor, arange, cat, float64, int32, ones, split, no_grad
+from typing import Optional, Tuple, Union, List, Type, Any
+
+from torch import Tensor, arange, cat, float64, int32, split, no_grad
 from torch.nn.functional import pad, unfold
 from torch.nn.modules.conv import _ConvNd, Conv1d, Conv2d, Conv3d
 from torch.nn.modules.utils import _single, _pair, _triple
+from torch.nn import ModuleList
 
-from aihwkit.nn.functions import AnalogIndexedFunction, AnalogFunction
-from aihwkit.nn.modules.base import AnalogModuleBase, RPUConfigAlias
-from aihwkit.simulator.tiles import BaseTile
-from aihwkit.exceptions import ModuleError
-from aihwkit.simulator.configs import SingleRPUConfig
-
+from aihwkit.nn.modules.base import AnalogLayerBase
+from aihwkit.simulator.tiles.module import TileModule
+from aihwkit.exceptions import AnalogBiasConfigError, ModuleError, ConfigError
+from aihwkit.simulator.parameters.base import MappableRPU, RPUConfigBase
 
-class _AnalogConvNdMapped(AnalogModuleBase, _ConvNd):
-    """Base class for convolution layers with tile mapping.
-
-    """
 
-    __constants__ = ['stride', 'padding', 'dilation', 'groups',
-                     'padding_mode', 'output_padding', 'in_channels',
-                     'out_channels', 'kernel_size', 'in_features', 'out_features',
-                     'realistic_read_write',
-                     'digital_bias', 'analog_bias', 'use_bias']
-    in_channels: int
-    out_channels: int
-    kernel_size: Tuple[int, ...]
-    stride: Tuple[int, ...]
-    padding: Tuple[int, ...]
-    dilation: Tuple[int, ...]
-    realistic_read_write: bool
-    transposed: bool
-    output_padding: Tuple[int, ...]
-    groups: int
-    padding_mode: str
-    fold_indices: Tensor
-    input_size: float
-    in_features: int
-    out_features: int
-    digital_bias: bool
-    analog_bias: bool
-    use_bias: bool
-    use_indexed: Optional[bool]
+class _AnalogConvNdMapped(AnalogLayerBase, _ConvNd):
+    """Base class for convolution layers with tile mapping."""
 
     def __init__(
-            self,
-            in_channels: int,
-            out_channels: int,
-            kernel_size: Tuple[int, ...],
-            stride: Tuple[int, ...],
-            padding: Tuple[int, ...],
-            dilation: Tuple[int, ...],
-            transposed: bool,
-            output_padding: Tuple[int, ...],
-            groups: int,
-            bias: bool,
-            padding_mode: str,
-            rpu_config: Optional[RPUConfigAlias] = None,
-            realistic_read_write: bool = False,
-            weight_scaling_omega: Optional[bool] = None,
-            use_indexed: Optional[bool] = None
+        self,
+        in_channels: int,
+        out_channels: int,
+        kernel_size: Tuple[int, ...],
+        stride: Tuple[int, ...],
+        padding: Tuple[int, ...],
+        dilation: Tuple[int, ...],
+        transposed: bool,
+        output_padding: Tuple[int, ...],
+        groups: int,
+        bias: bool,
+        padding_mode: str,
+        rpu_config: Optional[MappableRPU] = None,
+        tile_module_class: Optional[Type] = None,
+        use_indexed: Optional[bool] = None,
     ):
-        # pylint: disable=too-many-arguments, too-many-locals
         if groups != 1:
-            raise ValueError('Only one group is supported')
-        if padding_mode != 'zeros':
-            raise ValueError('Only "zeros" padding mode is supported')
-
-        # Call super() after tile creation, including ``reset_parameters``.
-        _ConvNd.__init__(self, in_channels, out_channels, kernel_size, stride,
-                         padding, dilation, transposed, output_padding, groups, bias,
-                         padding_mode)
+            raise ValueError("Only one group is supported")
+        if padding_mode != "zeros":
+            raise ValueError("Only 'zeros' padding mode is supported")
 
-        # Create tiles
-        if rpu_config is None:
-            rpu_config = SingleRPUConfig()
-
-        rpu_config = self._set_weight_scaling_omega(rpu_config, weight_scaling_omega)
-
-        AnalogModuleBase.__init__(
+        # Call super()
+        _ConvNd.__init__(
             self,
-            self.get_tile_size(in_channels, groups, kernel_size),
+            in_channels,
             out_channels,
+            kernel_size,
+            stride,
+            padding,
+            dilation,
+            transposed,
+            output_padding,
+            groups,
             bias,
-            realistic_read_write,
-            rpu_config.mapping
+            padding_mode,
         )
 
-        if self.analog_bias:
-            raise ModuleError("AnalogConvNdMapped only supports digital bias.")
+        # Create tiles
+        AnalogLayerBase.__init__(self)
+
+        if rpu_config is None:
+            # pylint: disable=import-outside-toplevel
+            from aihwkit.simulator.configs.configs import SingleRPUConfig
 
-        if not rpu_config:
             rpu_config = SingleRPUConfig()
 
-        max_input_size = rpu_config.mapping.max_input_size
-        max_output_size = rpu_config.mapping.max_output_size
+        if tile_module_class is None:
+            # Array might not make sense here since currently the
+            # logical tiling is done in this module
+            tile_module_class = rpu_config.tile_class
+
+        if bias and not rpu_config.mapping.digital_bias:
+            raise AnalogBiasConfigError("AnalogConvNdMapped only supports digital bias.")
+
+        max_input_size = rpu_config.mapping.max_input_size  # type: ignore
+        max_output_size = rpu_config.mapping.max_output_size  # type: ignore
+        self.in_features = self.get_tile_size(in_channels, groups, kernel_size)
+        self.out_features = out_channels
         kernel_elem = self.in_features // self.in_channels
         self.in_sizes = self.get_split_sizes(self.in_features, max_input_size, kernel_elem)
-        self.out_sizes = self.get_split_sizes(self.out_features, max_output_size)
 
-        self.analog_tile_array = []
-        for i, in_tile_size in enumerate(self.in_sizes):
-            in_tiles = []
-            for j, out_tile_size in enumerate(self.out_sizes):
-                tile = rpu_config.tile_class(out_tile_size,
-                                             in_tile_size * kernel_elem,
-                                             rpu_config,
-                                             bias=self.analog_bias)
-                self.register_analog_tile(tile, name=f"{i}_{j}")
-                in_tiles.append(tile)
-            self.analog_tile_array.append(in_tiles)
+        self.out_sizes = self.get_split_sizes(out_channels, max_output_size)
+        self.array = ModuleList()
+        for in_tile_size in self.in_sizes:
+            in_tiles = ModuleList()
+            for out_tile_size in self.out_sizes:
+                analog_tile = tile_module_class(
+                    out_tile_size, in_tile_size * kernel_elem, rpu_config, bias=False
+                )
+                in_tiles.append(analog_tile)
+            self.array.append(in_tiles)
 
         # Set weights from the reset_parameters (since now the
         # analog_tiles are registered)
         self.set_weights(self.weight, self.bias)
 
         # Set the index matrices.
         self.use_indexed = use_indexed
+        if use_indexed is None:
+            self.use_indexed = True
+        self.use_indexed = self.use_indexed and analog_tile.supports_indexed
+
         self.input_size = 0
-        self.register_helper('input_size')
         self.fold_indices_lst = []  # type: List[Tensor]
-        self.register_helper('fold_indices_lst')
         self.tensor_view = (-1,)  # type: Tuple[int, ...]
 
         # Unregister weight/bias as a parameter but keep it as a
         # field (needed for syncing still)
-        self.unregister_parameter('weight')
+        self.unregister_parameter("weight")
+        self.reset_parameters()
 
     def get_split_sizes(self, size: int, split_max_size: int, group_size: int = 1) -> List[int]:
-        """ Computed the split sizes across channels.
+        """Computed the split sizes across channels.
 
         Args:
             size: number of elements of the layer in one dimension
             split_max_size: max size of the split
             group_size: minimal size of features that needs to stay on one tile
 
         Returns:
@@ -156,46 +139,46 @@
                 only. If the group_size is larger than the
                 maximal tile size, mapping cannot be done
         """
         if split_max_size <= 0:
             return [size // group_size]
 
         if group_size > split_max_size:
-            raise ModuleError("Tile size too small to fit a single group (kernel): " +
-                              f"{group_size} > {split_max_size}")
+            raise ModuleError(
+                "Tile size too small to fit a single group (kernel): "
+                + f"{group_size} > {split_max_size}"
+            )
 
         size_per_group = size // group_size
         split_max_per_group = split_max_size // group_size
 
         n_splits = (size_per_group + split_max_per_group - 1) // split_max_per_group
         base, extra = divmod(size_per_group, n_splits)
         return [(base + (i < extra)) for i in range(n_splits)]
 
-    def get_tile_size(
-            self,
-            in_channels: int,
-            groups: int,
-            kernel_size: Tuple[int, ...]
-    ) -> int:
+    def get_tile_size(self, in_channels: int, groups: int, kernel_size: Tuple[int, ...]) -> int:
         """Calculate the tile size."""
         raise NotImplementedError
 
     def get_image_size(self, size: int, i: int) -> int:
         """Calculate the output image sizes."""
-        nom = (size + 2 * self.padding[i] - self.dilation[i] * (self.kernel_size[i] - 1) - 1)
+        nom = size + 2 * self.padding[i] - self.dilation[i] * (self.kernel_size[i] - 1) - 1
         return nom // self.stride[i] + 1
 
     def reset_parameters(self) -> None:
         """Reset the parameters (weight and bias)."""
-        super().reset_parameters()
-        if self.analog_tile_count():
+        if hasattr(self, "array"):
+            self.weight, _ = self.get_weights()
+            super().reset_parameters()
             self.set_weights(self.weight, self.bias)
+            self.weight = None
 
-    def _calculate_indexes(self, x_input: Tensor,
-                           in_channels: int) -> Tuple[Tensor, List[int], int]:
+    def _calculate_indexes(
+        self, x_input: Tensor, in_channels: int
+    ) -> Tuple[Tensor, List[int], int]:
         """Calculate and return the fold indexes and sizes.
 
         Args:
             x_input: input matrix
             in_channels: number of input channel
 
         Returns:
@@ -218,244 +201,162 @@
         """
         self.input_size = x_input.numel() / x_input.size(0)
         if x_input.ndim < 3:
             raise ModuleError("Expect >2-dim inputs to convolutions")
         channel_dim = 1
         self.fold_indices_lst = []
         splits = split(x_input, self.in_sizes, dim=channel_dim)
-        for x, in_channels, in_tiles in zip(splits, self.in_sizes, self.analog_tile_array):
+        for x, in_channels, in_tiles in zip(splits, self.in_sizes, self.array):
             fold_indices, image_sizes, _ = self._calculate_indexes(x, in_channels)
             self.fold_indices_lst.append(fold_indices)
 
             for analog_tile in in_tiles:
                 analog_tile.set_indexed(fold_indices, image_sizes)
 
-    def _single_forward_indexed(self, analog_tile: BaseTile, x_input: Tensor) -> Tensor:
-        """Compute the forward pass in indexed fashion. This is fast and
-        memory-efficient indexed convolution (only for GPUs)"""
-
-        return AnalogIndexedFunction.apply(
-            analog_tile.get_analog_ctx(), x_input,
-            analog_tile.shared_weights, not self.training)
-
-    def _single_forward_unfold(self, analog_tile: BaseTile, x_input: Tensor) -> Tensor:
-        """Forward using explicit unfolding (more suitable for CPUs) """
+    def _single_unfold(self, analog_tile: "TileModule", x_input: Tensor) -> Tensor:
+        """Forward using explicit unfolding (more suitable for CPUs)"""
         im_shape = x_input.shape
-        x_input_ = unfold(x_input, kernel_size=self.kernel_size, dilation=self.dilation,
-                          padding=self.padding, stride=self.stride).transpose(1, 2)
-
-        out = AnalogFunction.apply(
-            analog_tile.get_analog_ctx(), x_input_,
-            analog_tile.shared_weights, not self.training).transpose(1, 2)
 
-        out_im_size = (im_shape[2] + 2 * self.padding[0]
-                       - self.dilation[0] * (self.kernel_size[0] - 1) - 1) // self.stride[0] + 1
+        x_input_ = unfold(
+            x_input,
+            kernel_size=self.kernel_size,
+            dilation=self.dilation,
+            padding=self.padding,
+            stride=self.stride,
+        ).transpose(1, 2)
+
+        out = analog_tile(x_input_).transpose(1, 2)
+
+        out_im_size = (
+            im_shape[2] + 2 * self.padding[0] - self.dilation[0] * (self.kernel_size[0] - 1) - 1
+        ) // self.stride[0] + 1
         return out.view(im_shape[0], analog_tile.out_size, out_im_size, -1)
 
     def forward(self, x_input: Tensor) -> Tensor:
         """Compute the forward pass."""
         # pylint: disable=arguments-differ, arguments-renamed, too-many-branches
 
-        if self.use_indexed is None:
-            use_indexed = self.analog_tile_array[0][0].device.type == 'cuda'
-        else:
-            use_indexed = self.use_indexed
+        analog_tile = self.array[0][0]
+        use_indexed = self.use_indexed and analog_tile.supports_indexed
 
         if use_indexed:
             input_size = x_input.numel() / x_input.size(0)
-            if self.input_size != input_size or not self.analog_tile_array[0][0].is_indexed():
+            if self.input_size != input_size or not analog_tile.is_indexed():
                 self._recalculate_indexes(x_input)
 
         if self.analog_tile_count() == 1:
-            analog_tile = self.analog_tile_array[0][0]
-
             if use_indexed:
-                output = self._single_forward_indexed(analog_tile, x_input)
+                output = analog_tile(x_input)
             else:
-                output = self._single_forward_unfold(analog_tile, x_input)
+                output = self._single_unfold(analog_tile, x_input)
 
-            output = analog_tile.apply_out_scaling(output, self.tensor_view)
-
-            if self.digital_bias:
+            if self.bias is not None:
                 return output + self.bias.view(*self.tensor_view)
             return output
 
-        # mapped version
+        # Mapped version.
         channel_dim = 1
         splits = split(x_input, self.in_sizes, dim=channel_dim)
         result = None  # type: Tensor
-        for idx, (x, in_tiles) in enumerate(zip(splits, self.analog_tile_array)):
+        for idx, (x, in_tiles) in enumerate(zip(splits, self.array)):
             out_result = []
 
             for analog_tile in in_tiles:
                 if use_indexed:
-                    output = self._single_forward_indexed(analog_tile, x)
+                    output = analog_tile(x)
                 else:
-                    output = self._single_forward_unfold(analog_tile, x)
-
-                output = analog_tile.apply_out_scaling(output, self.tensor_view)
+                    output = self._single_unfold(analog_tile, x)
                 out_result.append(output)
 
             if idx == 0:
                 result = cat(out_result, channel_dim)
             else:
                 result.add_(cat(out_result, channel_dim))
 
-        # add bias to final result
-        if self.digital_bias:
+        # Add the bias (conditionally) to the final result.
+        if self.bias is not None:
             return result + self.bias.view(*self.tensor_view)
         return result
 
-    def set_weights(
-            self,
-            weight: Tensor,
-            bias: Optional[Tensor] = None,
-            force_exact: bool = False,
-            apply_weight_scaling: bool = True,
-            weight_scaling_omega: Optional[float] = None
-    ) -> None:
-        """Set the weight (and bias) with given Tensors.
-
-        This uses an realistic read if the property ``realistic_read_write`` of
-        the layer is set, unless it is overwritten by ``force_exact``. It
-        scales the analog weights by the digital alpha scale if
-        ``weight_scaling_omega`` is positive (see
-        :meth:`~aihwkit.simulator.tiles.base.apply_weight_scaling`).
-
-        Note:
-            This is the recommended way for setting the weight/bias matrix of
-            the analog tile, as it will correctly store the weights into the
-            internal memory. Directly writing to ``self.weight`` and
-            ``self.bias`` might yield wrong results as they are not always in
-            sync with the analog tile Parameters, for performance reasons.
+    @no_grad()
+    def set_weights(self, weight: Tensor, bias: Optional[Tensor] = None, **kwargs: Any) -> None:
+        """Set the weight (and bias) tensors to the analog crossbar.
 
         Args:
             weight: weight matrix
             bias: bias vector
-            force_exact: forces an exact write to the analog tiles
-            apply_weight_scaling: Whether to rescale the given weight matrix
-                and populate the digital output scaling factors as
-                specified in the configuration
-                :class:`~aihwkit.configs.utils.MappingParameter`. A
-                new ``weight_scaling_omega`` can be given. Note that
-                this will overwrite the existing digital out scaling
-                factors.
-
-                Note that each tile (in case of multiple mapped tiles)
-                has it separate out scaling factors.
-
-            weight_scaling_omega: The weight scaling omega factor (see
-                :class:`~aihwkit.configs.utils.MappingParameter`). If
-                given explicitly here, it will overwrite the value in
-                the mapping field.
+            **kwargs: see tile level,
+                e.g. :meth:`~aihwkit.simulator.tiles.analog.AnalogTile.set_weights`
         """
-        # pylint: disable=too-many-locals
-        realistic = self.realistic_read_write and not force_exact
 
-        shape = [self.out_features, self.in_channels, self.in_features // self.in_channels]
+        shape = [self.out_channels, self.in_channels, self.in_features // self.in_channels]
         weight = weight.clone().reshape(shape)
 
         weight_splits = split(weight, self.in_sizes, dim=1)
 
-        for in_tiles, in_weight in zip(self.analog_tile_array, weight_splits):
+        for in_tiles, in_weight in zip(self.array, weight_splits):
             out_start = out_end = 0
-            in_weight = in_weight.reshape([self.out_features, -1])
+            in_weight = in_weight.reshape([self.out_channels, -1])
             for out_size, analog_tile in zip(self.out_sizes, in_tiles):
                 out_end += out_size
                 tile_weight = in_weight[out_start:out_end, :]
                 out_start = out_end
 
-                analog_tile.set_weights(
-                    tile_weight, None,
-                    apply_weight_scaling,
-                    weight_scaling_omega
-                )
-                if realistic:
-                    analog_tile.program_weights()
-
-        self._sync_weights_from_tile()
+                analog_tile.set_weights(tile_weight, None, **kwargs)
 
-        if self.digital_bias and bias is not None:
+        if self.bias is not None and bias is not None:
             with no_grad():
                 self.bias.data[:] = bias[:]
 
-    def get_weights(self, force_exact: bool = False,
-                    apply_weight_scaling: bool = True) -> Tuple[Tensor, Optional[Tensor]]:
-        """Get the weight (and bias) tensors.
-
-        This uses an realistic read if the property ``realistic_read_write`` of
-        the layer is set, unless it is overwritten by ``force_exact``. It
-        scales the analog weights by the digital alpha scale if
-        ``weight_scaling_omega`` is positive (see
-        :meth:`~aihwkit.simulator.tiles.base.apply_weight_scaling`).
-
-        Note:
-            This is the recommended way for setting the weight/bias matrix from
-            the analog tile, as it will correctly fetch the weights from the
-            internal memory. Accessing ``self.weight`` and ``self.bias`` might
-            yield wrong results as they are not always in sync with the
-            analog tile library, for performance reasons.
-
-        Args:
-            force_exact: forces an exact read to the analog tiles
-            apply_weight_scaling: Whether to return the weights with the
-                (digital) output scaling factors applied. Note the
-                "logical" weights of the layer which the DNN is
-                effectively using are those with the output scales
-                applied. If ``apply_weight_scaling`` is set to False, then
-                only the weight values that is programmed onto the
-                crossbar array are returned, without applying the
-                digital scales.
+    @no_grad()
+    def get_weights(self, **kwargs: Any) -> Tuple[Tensor, Optional[Tensor]]:
+        """Get the (analog) weight (and bias) tensors from the crossbar(s).
+
+        Args:
+            **kwargs: see tile level,
+            e.g. :meth:`~aihwkit.simulator.tiles.analog.AnalogTile.get_weights`
 
         Returns:
             tuple: weight matrix, bias vector
         """
-        realistic = self.realistic_read_write and not force_exact
 
         weight_lst = []
-        for in_tiles in self.analog_tile_array:
+        for in_tiles in self.array:
             in_tile_weight = []
             for analog_tile in in_tiles:
-                if realistic:
-                    tile_weight, _ = analog_tile.read_weights(apply_weight_scaling)
-                else:
-                    tile_weight, _ = analog_tile.get_weights(apply_weight_scaling)
+                tile_weight, _ = analog_tile.get_weights(**kwargs)
                 in_tile_weight.append(tile_weight)
             weight_lst.append(cat(in_tile_weight, 0))
 
         weight = cat(weight_lst, 1)
 
-        if self.digital_bias:
-            with no_grad():
-                return weight, self.bias.data.clone().detach().cpu()
+        if self.bias is not None:
+            return weight, self.bias.data.clone().detach().cpu()
         return weight, None
 
     def extra_repr(self) -> str:
         """Set the extra representation of the module.
 
         Returns:
             A string with the extra representation.
         """
-        output = AnalogModuleBase.extra_repr(self)
-        output += ', mapping={}'.format((len(self.in_sizes), len(self.out_sizes)))
-
-        return output
+        return AnalogLayerBase.extra_repr(self)
 
 
 class AnalogConv1dMapped(_AnalogConvNdMapped):
     """1D convolution layer that maps to analog tiles.
 
     Applies a 1D convolution over an input signal composed of several input
     planes, using an analog tile for its forward, backward and update passes.
 
     The module will split the weight matrix onto multiple tiles if
     necessary. Physical max tile sizes are specified with
-    :class:`~aihwkit.simulator.configs.utils.MappingParameter` in the
+    :class:`~aihwkit.simulator.parameters.utils.MappingParameter` in the
     RPU configuration, see
-    :class:`~aihwkit.simulator.configs.configs.RPUConfigAlias`.
+    :class:`~aihwkit.simulator.configs.configs.RPUConfigBase`.
 
     Note:
         The tensor parameters of this layer (``.weight`` and ``.bias``) are not
         guaranteed to contain the same values as the internal weights and biases
         stored in the analog tile. Please use ``set_weights`` and
         ``get_weights`` when attempting to read or modify the weight/bias. This
         read/write process can simulate the (noisy and inexact) analog writing
@@ -469,105 +370,141 @@
         padding: zero-padding added to both sides of the input.
         dilation: spacing between kernel elements.
         groups: number of blocked connections from input channels to output
             channels.
         bias: whether to use a bias row on the analog tile or not.
         padding_mode: padding strategy. Only ``'zeros'`` is supported.
         rpu_config: resistive processing unit configuration.
-        realistic_read_write: whether to enable realistic read/write for
-            setting initial weights and read out of weights.
-        weight_scaling_omega: depreciated, use
-            :class:`aihwkit.simulator.configs.utils.MappingParameter`
-            instead to specify weight scaling
+        tile_module_class: Class for the tile module (default
+            will be specified from the ``RPUConfig``).
     """
+
     # pylint: disable=abstract-method
 
     def __init__(
-            self,
-            in_channels: int,
-            out_channels: int,
-            kernel_size: Union[int, Tuple],
-            stride: Union[int, Tuple] = 1,
-            padding: Union[int, Tuple] = 0,
-            dilation: Union[int, Tuple] = 1,
-            groups: int = 1,
-            bias: bool = True,
-            padding_mode: str = 'zeros',
-            rpu_config: Optional[RPUConfigAlias] = None,
-            realistic_read_write: bool = False,
-            weight_scaling_omega: Optional[bool] = None
+        self,
+        in_channels: int,
+        out_channels: int,
+        kernel_size: Union[int, Tuple[int]],
+        stride: Union[int, Tuple[int]] = 1,
+        padding: Union[int, Tuple[int]] = 0,
+        dilation: Union[int, Tuple[int]] = 1,
+        groups: int = 1,
+        bias: bool = True,
+        padding_mode: str = "zeros",
+        rpu_config: Optional[MappableRPU] = None,
+        tile_module_class: Optional[Type] = None,
     ):
         # pylint: disable=too-many-arguments
-        kernel_size = _single(kernel_size)
-        stride = _single(stride)
-        padding = _single(padding)
-        dilation = _single(dilation)
+        kernel_size = _single(kernel_size)  # type: ignore
+        stride = _single(stride)  # type: ignore
+        padding = _single(padding)  # type: ignore
+        dilation = _single(dilation)  # type: ignore
 
         if dilation != _single(1):
-            raise ValueError('Only dilation = 1 is supported')
+            raise ValueError("Only dilation = 1 is supported")
 
         super().__init__(
-            in_channels, out_channels, kernel_size, stride, padding, dilation,  # type: ignore
-            False, _single(0), groups, bias, padding_mode,
-            rpu_config, realistic_read_write, weight_scaling_omega, True
+            in_channels,
+            out_channels,
+            kernel_size,  # type: ignore
+            stride,  # type: ignore
+            padding,  # type: ignore
+            dilation,  # type: ignore
+            False,
+            _single(0),
+            groups,
+            bias,
+            padding_mode,
+            rpu_config,
+            tile_module_class,
+            True,
         )
 
         self.tensor_view = (-1, 1)
 
     @classmethod
     def from_digital(
-            cls,
-            module: Conv1d,
-            rpu_config: Optional[RPUConfigAlias] = None,
-            realistic_read_write: bool = False,
-    ) -> 'AnalogConv1dMapped':
+        cls, module: Conv1d, rpu_config: RPUConfigBase, tile_module_class: Optional[Type] = None
+    ) -> "AnalogConv1dMapped":
         """Return an AnalogConv1dMapped layer from a torch Conv1d layer.
 
         Args:
             module: The torch module to convert. All layers that are
                 defined in the ``conversion_map``.
             rpu_config: RPU config to apply to all converted tiles.
                 Applied to all converted tiles.
-            realistic_read_write: Whether to use closed-loop programming
-                when setting the weights. Applied to all converted tiles.
-
-                Note:
-                    Make sure that the weight max and min settings of the
-                    device support the desired analog weight range.
+            tile_module_class: Class for the tile module (default
+                will be specified from the ``RPUConfig``).
 
         Returns:
             an AnalogConv1d layer based on the digital Conv1d ``module``.
+
+        Raises:
+            ConfigError: In case the ``RPUConfig`` is not of type ``MappableRPU``
         """
-        analog_module = cls(module.in_channels,
-                            module.out_channels,
-                            module.kernel_size,
-                            module.stride,
-                            module.padding,
-                            module.dilation,
-                            module.groups,
-                            module.bias is not None,
-                            module.padding_mode,
-                            rpu_config,
-                            realistic_read_write,
-                            )
 
-        analog_module.set_weights(module.weight, module.bias)
-        return analog_module
+        if not isinstance(rpu_config, MappableRPU):
+            raise ConfigError("Only mappable RPUConfigs are supported.")
 
-    def get_tile_size(
-            self,
-            in_channels: int,
-            groups: int,
-            kernel_size: Tuple[int, ...]
-    ) -> int:
+        analog_layer = cls(
+            module.in_channels,
+            module.out_channels,
+            module.kernel_size,
+            module.stride,
+            module.padding,
+            module.dilation,
+            module.groups,
+            module.bias is not None,
+            module.padding_mode,
+            rpu_config,
+            tile_module_class,
+        )
+
+        analog_layer.set_weights(module.weight, module.bias)
+        return analog_layer
+
+    @classmethod
+    def to_digital(cls, module: "AnalogConv1dMapped", realistic: bool = False) -> Conv1d:
+        """Return an nn.Conv1d layer from an AnalogConv1dMapped layer.
+
+        Args:
+            module: The analog module to convert.
+            realistic: whehter to estimate the weights with the
+                non-ideal forward pass. If not set, analog weights are
+                (unrealistically) copies exactly
+
+        Returns:
+            an torch Linear layer with the same dimension and weights
+            as the analog linear layer.
+        """
+        weight, bias = module.get_weights(realistic=realistic)
+        digital_layer = Conv1d(
+            module.in_channels,
+            module.out_channels,
+            module.kernel_size,
+            module.stride,
+            module.padding,
+            module.dilation,
+            module.groups,
+            bias is not None,
+            module.padding_mode,
+        )
+        digital_layer.weight.data = weight.data.view(-1, module.in_channels, *module.kernel_size)
+        if bias is not None:
+            digital_layer.bias.data = bias.data
+        return digital_layer
+
+    def get_tile_size(self, in_channels: int, groups: int, kernel_size: Tuple[int, ...]) -> int:
         """Calculate the tile size."""
         return (in_channels // groups) * kernel_size[0]
 
-    def _calculate_indexes(self, x_input: Tensor,
-                           in_channels: int) -> Tuple[Tensor, List[int], int]:
+    def _calculate_indexes(
+        self, x_input: Tensor, in_channels: int
+    ) -> Tuple[Tensor, List[int], int]:
         """Calculate and return the fold indexes and sizes.
 
         Args:
             x_input: input matrix
             in_channels: number of input channel
 
         Returns:
@@ -578,38 +515,32 @@
         input_size = x_input.numel() / x_input.size(0)
 
         # pytorch just always uses NCHW order?
         fold_indices = arange(2, x_input.size(2) + 2, dtype=float64).detach()
         shape = [1] + [1] + list(x_input.shape[2:])
         fold_indices = fold_indices.reshape(*shape)
         if not all(item == 0 for item in self.padding):
-            fold_indices = pad(fold_indices, pad=[self.padding[0], self.padding[0]],
-                               mode='constant', value=0)
+            fold_indices = pad(
+                fold_indices, pad=[self.padding[0], self.padding[0]], mode="constant", value=0
+            )
         unfolded = fold_indices.unfold(2, self.kernel_size[0], self.stride[0]).clone()
 
         fold_indices = unfolded.reshape(-1, self.kernel_size[0]).transpose(0, 1).flatten().round()
 
         # concatenate the matrix index for different channels
         fold_indices_orig = fold_indices.clone()
         for i in range(in_channels - 1):
             fold_indices_tmp = fold_indices_orig.clone()
             for j in range(fold_indices_orig.size(0)):
                 if fold_indices_orig[j] != 0:
                     fold_indices_tmp[j] += (input_size / in_channels) * (i + 1)
 
             fold_indices = cat([fold_indices, fold_indices_tmp], dim=0).clone()
 
-        fold_indices = fold_indices.to(dtype=int32)
-
-        if self.analog_bias:
-            out_image_size = fold_indices.numel() // (self.kernel_size[0])
-            fold_indices = cat((fold_indices, ones(out_image_size, dtype=int32)), 0)
-
-        fold_indices = fold_indices.to(x_input.device)
-
+        fold_indices = fold_indices.to(dtype=int32, device=x_input.device)
         x_height = x_input.size(2)
         d_height = self.get_image_size(x_height, 0)
 
         image_sizes = [in_channels, x_height, d_height]
         return (fold_indices, image_sizes, input_size)
 
 
@@ -617,17 +548,17 @@
     """2D convolution layer that maps to analog tiles.
 
     Applies a 2D convolution over an input signal composed of several input
     planes, using an analog tile for its forward, backward and update passes.
 
     The module will split the weight matrix onto multiple tiles if
     necessary. Physical max tile sizes are specified with
-    :class:`~aihwkit.simulator.configs.utils.MappingParameter` in the
+    :class:`~aihwkit.simulator.parameters.utils.MappingParameter` in the
     RPU configuration, see
-    :class:`~aihwkit.simulator.configs.configs.RPUConfigAlias`.
+    :class:`~aihwkit.simulator.configs.configs.RPUConfigBase`.
 
     Note:
         The tensor parameters of this layer (``.weight`` and ``.bias``) are not
         guaranteed to contain the same values as the internal weights and biases
         stored in the analog tile. Please use ``set_weights`` and
         ``get_weights`` when attempting to read or modify the weight/bias. This
         read/write process can simulate the (noisy and inexact) analog writing
@@ -641,105 +572,142 @@
         padding: zero-padding added to both sides of the input.
         dilation: spacing between kernel elements.
         groups: number of blocked connections from input channels to output
             channels.
         bias: whether to use a bias row on the analog tile or not.
         padding_mode: padding strategy. Only ``'zeros'`` is supported.
         rpu_config: resistive processing unit configuration.
-        realistic_read_write: whether to enable realistic read/write
-            for setting initial weights and read out of weights.
-        weight_scaling_omega: depreciated, use
-            :class:`aihwkit.simulator.configs.utils.MappingParameter`
-            instead to specify weight scaling
+        tile_module_class: Class for the tile module (default
+            will be specified from the ``RPUConfig``).
         use_indexed: Whether to use explicit unfolding or implicit indexing. If
             None (default), it will use implicit indexing for CUDA and
             explicit unfolding for CPU
     """
+
     # pylint: disable=abstract-method
 
     def __init__(
-            self,
-            in_channels: int,
-            out_channels: int,
-            kernel_size: Union[int, Tuple],
-            stride: Union[int, Tuple] = 1,
-            padding: Union[int, Tuple] = 0,
-            dilation: Union[int, Tuple] = 1,
-            groups: int = 1,
-            bias: bool = True,
-            padding_mode: str = 'zeros',
-            rpu_config: Optional[RPUConfigAlias] = None,
-            realistic_read_write: bool = False,
-            weight_scaling_omega: Optional[bool] = None,
-            use_indexed: Optional[bool] = None,
+        self,
+        in_channels: int,
+        out_channels: int,
+        kernel_size: Union[int, Tuple[int, int]],
+        stride: Union[int, Tuple[int, int]] = 1,
+        padding: Union[int, Tuple[int, int]] = 0,
+        dilation: Union[int, Tuple[int, int]] = 1,
+        groups: int = 1,
+        bias: bool = True,
+        padding_mode: str = "zeros",
+        rpu_config: Optional[MappableRPU] = None,
+        tile_module_class: Optional[Type] = None,
+        use_indexed: Optional[bool] = None,
     ):
         # pylint: disable=too-many-arguments
         kernel_size = _pair(kernel_size)
         stride = _pair(stride)
         padding = _pair(padding)
         dilation = _pair(dilation)
 
         super().__init__(
-            in_channels, out_channels, kernel_size, stride, padding, dilation,  # type: ignore
-            False, _pair(0), groups, bias, padding_mode,
-            rpu_config, realistic_read_write, weight_scaling_omega, use_indexed
+            in_channels,
+            out_channels,
+            kernel_size,  # type: ignore
+            stride,  # type: ignore
+            padding,  # type: ignore
+            dilation,  # type: ignore
+            False,
+            _pair(0),
+            groups,
+            bias,
+            padding_mode,
+            rpu_config,
+            tile_module_class,
+            use_indexed,
         )
 
         self.tensor_view = (-1, 1, 1)
 
     @classmethod
     def from_digital(
-            cls,
-            module: Conv2d,
-            rpu_config: Optional[RPUConfigAlias] = None,
-            realistic_read_write: bool = False,
-    ) -> 'AnalogConv2dMapped':
+        cls, module: Conv2d, rpu_config: RPUConfigBase, tile_module_class: Optional[Type] = None
+    ) -> "AnalogConv2dMapped":
         """Return an AnalogConv2dMapped layer from a torch Conv2d layer.
 
         Args:
             module: The torch module to convert. All layers that are
                 defined in the ``conversion_map``.
             rpu_config: RPU config to apply to all converted tiles.
                 Applied to all converted tiles.
-            realistic_read_write: Whether to use closed-loop programming
-                when setting the weights. Applied to all converted tiles.
-
-                Note:
-                    Make sure that the weight max and min settings of the
-                    device support the desired analog weight range.
+            tile_module_class: Class for the tile module (default
+                will be specified from the ``RPUConfig``).
 
         Returns:
             an AnalogConv2dMapped layer based on the digital Conv2d ``module``.
+
+        Raises:
+            ConfigError: In case the ``RPUConfig`` is not of type ``MappableRPU``
         """
-        analog_module = cls(module.in_channels,
-                            module.out_channels,
-                            module.kernel_size,
-                            module.stride,
-                            module.padding,
-                            module.dilation,
-                            module.groups,
-                            module.bias is not None,
-                            module.padding_mode,
-                            rpu_config,
-                            realistic_read_write)
 
-        analog_module.set_weights(module.weight, module.bias)
-        return analog_module
+        if not isinstance(rpu_config, MappableRPU):
+            raise ConfigError("Only mappable RPUConfigs are supported.")
 
-    def get_tile_size(
-            self,
-            in_channels: int,
-            groups: int,
-            kernel_size: Tuple[int, ...]
-    ) -> int:
+        analog_layer = cls(
+            module.in_channels,
+            module.out_channels,
+            module.kernel_size,
+            module.stride,
+            module.padding,
+            module.dilation,
+            module.groups,
+            module.bias is not None,
+            module.padding_mode,
+            rpu_config,
+            tile_module_class,
+        )
+
+        analog_layer.set_weights(module.weight, module.bias)
+        return analog_layer
+
+    @classmethod
+    def to_digital(cls, module: "AnalogConv2dMapped", realistic: bool = False) -> Conv2d:
+        """Return an nn.Conv2d layer from an AnalogConv2dMapped layer.
+
+        Args:
+            module: The analog module to convert.
+            realistic: whehter to estimate the weights with the
+                non-ideal forward pass. If not set, analog weights are
+                (unrealistically) copies exactly
+
+        Returns:
+            an torch Linear layer with the same dimension and weights
+            as the analog linear layer.
+        """
+        weight, bias = module.get_weights(realistic=realistic)
+        digital_layer = Conv2d(
+            module.in_channels,
+            module.out_channels,
+            module.kernel_size,
+            module.stride,
+            module.padding,
+            module.dilation,
+            module.groups,
+            bias is not None,
+            module.padding_mode,
+        )
+        digital_layer.weight.data = weight.data.view(-1, module.in_channels, *module.kernel_size)
+        if bias is not None:
+            digital_layer.bias.data = bias.data
+        return digital_layer
+
+    def get_tile_size(self, in_channels: int, groups: int, kernel_size: Tuple[int, ...]) -> int:
         """Calculate the tile size."""
         return (in_channels // groups) * kernel_size[0] * kernel_size[1]
 
-    def _calculate_indexes(self, x_input: Tensor,
-                           in_channels: int) -> Tuple[Tensor, List[int], int]:
+    def _calculate_indexes(
+        self, x_input: Tensor, in_channels: int
+    ) -> Tuple[Tensor, List[int], int]:
         """Calculate and return the fold indexes and sizes.
 
         Args:
             x_input: input matrix
             in_channels: number of input channel
 
         Returns:
@@ -749,25 +717,27 @@
         """
         input_size = x_input.numel() / x_input.size(0)
 
         # pytorch just always uses NCHW order
         fold_indices = arange(2, input_size + 2, dtype=float64).detach()
         shape = [1] + list(x_input.shape[1:])
         fold_indices = fold_indices.reshape(*shape)
-        fold_indices = unfold(fold_indices,
-                              kernel_size=self.kernel_size,
-                              stride=self.stride,
-                              padding=self.padding,
-                              dilation=self.dilation).flatten().round().to(dtype=int32)
-
-        if self.analog_bias:
-            out_image_size = fold_indices.numel() // (self.kernel_size[0] * self.kernel_size[1])
-            fold_indices = cat((fold_indices, ones(out_image_size, dtype=int32)), 0)
-
-        fold_indices = fold_indices.to(x_input.device)
+        fold_indices = (
+            unfold(
+                fold_indices,
+                kernel_size=self.kernel_size,
+                stride=self.stride,
+                padding=self.padding,
+                dilation=self.dilation,
+            )
+            .flatten()
+            .round()
+            .to(dtype=int32)
+            .to(device=x_input.device)
+        )
 
         x_height = x_input.size(2)
         x_width = x_input.size(3)
 
         d_height = self.get_image_size(x_height, 0)
         d_width = self.get_image_size(x_width, 1)
 
@@ -779,17 +749,17 @@
     """3D convolution layer that maps to analog tiles.
 
     Applies a 3D convolution over an input signal composed of several input
     planes, using an analog tile for its forward, backward and update passes.
 
     The module will split the weight matrix onto multiple tiles if
     necessary. Physical max tile sizes are specified with
-    :class:`~aihwkit.simulator.configs.utils.MappingParameter` in the
+    :class:`~aihwkit.simulator.parameters.utils.MappingParameter` in the
     RPU configuration, see
-    :class:`~aihwkit.simulator.configs.configs.RPUConfigAlias`.
+    :class:`~aihwkit.simulator.configs.configs.RPUConfigBase`.
 
     Note:
         The tensor parameters of this layer (``.weight`` and ``.bias``) are not
         guaranteed to contain the same values as the internal weights and biases
         stored in the analog tile. Please use ``set_weights`` and
         ``get_weights`` when attempting to read or modify the weight/bias. This
         read/write process can simulate the (noisy and inexact) analog writing
@@ -803,111 +773,148 @@
         padding: zero-padding added to both sides of the input.
         dilation: spacing between kernel elements.
         groups: number of blocked connections from input channels to output
             channels.
         bias: whether to use a bias row on the analog tile or not.
         padding_mode: padding strategy. Only ``'zeros'`` is supported.
         rpu_config: resistive processing unit configuration.
-        realistic_read_write: whether to enable realistic read/write
-            for setting initial weights and read out of weights.
-        weight_scaling_omega: depreciated, use
-            :class:`aihwkit.simulator.configs.utils.MappingParameter`
-            instead to specify weight scaling
+        tile_module_class: Class for the tile module (default
+            will be specified from the ``RPUConfig``).
 
     Raises:
         ModuleError: Tiling weight matrices is always done across channels
             only. If the kernel number of elements is larger than the
             maximal tile size, mapping cannot be done
 
     """
+
     # pylint: disable=abstract-method
 
     def __init__(
-            self,
-            in_channels: int,
-            out_channels: int,
-            kernel_size: Union[int, Tuple],
-            stride: Union[int, Tuple] = 1,
-            padding: Union[int, Tuple] = 0,
-            dilation: Union[int, Tuple] = 1,
-            groups: int = 1,
-            bias: bool = True,
-            padding_mode: str = 'zeros',
-            rpu_config: Optional[RPUConfigAlias] = None,
-            realistic_read_write: bool = False,
-            weight_scaling_omega: Optional[bool] = None,
+        self,
+        in_channels: int,
+        out_channels: int,
+        kernel_size: Union[int, Tuple[int, int, int]],
+        stride: Union[int, Tuple[int, int, int]] = 1,
+        padding: Union[int, Tuple[int, int, int]] = 0,
+        dilation: Union[int, Tuple[int, int, int]] = 1,
+        groups: int = 1,
+        bias: bool = True,
+        padding_mode: str = "zeros",
+        rpu_config: Optional[MappableRPU] = None,
+        tile_module_class: Optional[Type] = None,
     ):
         # pylint: disable=too-many-arguments
         kernel_size = _triple(kernel_size)
         stride = _triple(stride)
         padding = _triple(padding)
         dilation = _triple(dilation)
 
         if dilation != _triple(1):
-            raise ValueError('Only dilation = 1 is supported')
+            raise ValueError("Only dilation = 1 is supported")
 
         super().__init__(
-            in_channels, out_channels, kernel_size, stride, padding, dilation,  # type: ignore
-            False, _triple(0), groups, bias, padding_mode,
-            rpu_config, realistic_read_write, weight_scaling_omega, True
+            in_channels,
+            out_channels,
+            kernel_size,  # type: ignore
+            stride,  # type: ignore
+            padding,  # type: ignore
+            dilation,  # type: ignore
+            False,
+            _triple(0),
+            groups,
+            bias,
+            padding_mode,
+            rpu_config,
+            tile_module_class,
+            True,
         )
 
         self.tensor_view = (-1, 1, 1, 1)
 
     @classmethod
     def from_digital(
-            cls,
-            module: Conv3d,
-            rpu_config: Optional[RPUConfigAlias] = None,
-            realistic_read_write: bool = False,
-    ) -> 'AnalogConv3dMapped':
+        cls, module: Conv3d, rpu_config: RPUConfigBase, tile_module_class: Optional[Type] = None
+    ) -> "AnalogConv3dMapped":
         """Return an AnalogConv3dMapped layer from a torch Conv3d layer.
 
         Args:
             module: The torch module to convert. All layers that are
                 defined in the ``conversion_map``.
             rpu_config: RPU config to apply to all converted tiles.
                 Applied to all converted tiles.
-            realistic_read_write: Whether to use closed-loop programming
-                when setting the weights. Applied to all converted tiles.
-
-                Note:
-                    Make sure that the weight max and min settings of the
-                    device support the desired analog weight range.
+            tile_module_class: Class for the tile module (default
+                will be specified from the ``RPUConfig``).
 
         Returns:
             an AnalogConv3d layer based on the digital Conv3d ``module``.
+
+        Raises:
+            ConfigError: In case the ``RPUConfig`` is not of type ``MappableRPU``
+
         """
-        analog_module = cls(module.in_channels,
-                            module.out_channels,
-                            module.kernel_size,
-                            module.stride,
-                            module.padding,
-                            module.dilation,
-                            module.groups,
-                            module.bias is not None,
-                            module.padding_mode,
-                            rpu_config,
-                            realistic_read_write)
 
-        analog_module.set_weights(module.weight, module.bias)
-        return analog_module
+        if not isinstance(rpu_config, MappableRPU):
+            raise ConfigError("Only mappable RPUConfigs are supported.")
 
-    def get_tile_size(
-            self,
-            in_channels: int,
-            groups: int,
-            kernel_size: Tuple[int, ...]
-    ) -> int:
+        analog_layer = cls(
+            module.in_channels,
+            module.out_channels,
+            module.kernel_size,
+            module.stride,
+            module.padding,
+            module.dilation,
+            module.groups,
+            module.bias is not None,
+            module.padding_mode,
+            rpu_config,
+            tile_module_class,
+        )
+
+        analog_layer.set_weights(module.weight, module.bias)
+        return analog_layer
+
+    @classmethod
+    def to_digital(cls, module: "AnalogConv3dMapped", realistic: bool = False) -> Conv3d:
+        """Return an nn.Conv3d layer from an AnalogConv3dMapped layer.
+
+        Args:
+            module: The analog module to convert.
+            realistic: whehter to estimate the weights with the
+                non-ideal forward pass. If not set, analog weights are
+                (unrealistically) copies exactly
+
+        Returns:
+            an torch Linear layer with the same dimension and weights
+            as the analog linear layer.
+        """
+        weight, bias = module.get_weights(realistic=realistic)
+        digital_layer = Conv3d(
+            module.in_channels,
+            module.out_channels,
+            module.kernel_size,
+            module.stride,
+            module.padding,
+            module.dilation,
+            module.groups,
+            bias is not None,
+            module.padding_mode,
+        )
+        digital_layer.weight.data = weight.data.view(-1, module.in_channels, *module.kernel_size)
+        if bias is not None:
+            digital_layer.bias.data = bias.data
+        return digital_layer
+
+    def get_tile_size(self, in_channels: int, groups: int, kernel_size: Tuple[int, ...]) -> int:
         """Calculate the tile size."""
-        return (in_channels // groups) * (
-                kernel_size[0] * kernel_size[1] * kernel_size[2])
+        return (in_channels // groups) * (kernel_size[0] * kernel_size[1] * kernel_size[2])
 
-    def _calculate_indexes(self, x_input: Tensor,
-                           in_channels: int) -> Tuple[Tensor, List[int], int]:
+    def _calculate_indexes(
+        self, x_input: Tensor, in_channels: int
+    ) -> Tuple[Tensor, List[int], int]:
         """Calculate and return the fold indexes and sizes.
 
         Args:
             x_input: input matrix
             in_channels: number of input channel
 
         Returns:
@@ -915,49 +922,58 @@
             image_sizes: image sizes for the analog tile
             input_size: size of the current input
         """
         # pylint: disable=too-many-locals
         input_size = x_input.numel() / x_input.size(0)
 
         # pytorch just always uses NCDHW order
-        fold_indices = arange(2, x_input.size(2) * x_input.size(3) * x_input.size(4) + 2,
-                              dtype=float64).detach()
+        fold_indices = arange(
+            2, x_input.size(2) * x_input.size(3) * x_input.size(4) + 2, dtype=float64
+        ).detach()
         shape = [1] + [1] + list(x_input.shape[2:])
         fold_indices = fold_indices.reshape(*shape)
         if not all(item == 0 for item in self.padding):
-            fold_indices = pad(fold_indices, pad=[
-                self.padding[2], self.padding[2],
-                self.padding[1], self.padding[1],
-                self.padding[0], self.padding[0]], mode='constant', value=0)
-        unfolded = fold_indices.unfold(2, self.kernel_size[0], self.stride[0]). \
-            unfold(3, self.kernel_size[1], self.stride[1]). \
-            unfold(4, self.kernel_size[2], self.stride[2]).clone()
+            fold_indices = pad(
+                fold_indices,
+                pad=[
+                    self.padding[2],
+                    self.padding[2],
+                    self.padding[1],
+                    self.padding[1],
+                    self.padding[0],
+                    self.padding[0],
+                ],
+                mode="constant",
+                value=0,
+            )
+        unfolded = (
+            fold_indices.unfold(2, self.kernel_size[0], self.stride[0])
+            .unfold(3, self.kernel_size[1], self.stride[1])
+            .unfold(4, self.kernel_size[2], self.stride[2])
+            .clone()
+        )
 
-        fold_indices = unfolded.reshape(-1, self.kernel_size[0] * self.kernel_size[1] *
-                                        self.kernel_size[2]).transpose(0, 1).flatten().round()
+        fold_indices = (
+            unfolded.reshape(-1, self.kernel_size[0] * self.kernel_size[1] * self.kernel_size[2])
+            .transpose(0, 1)
+            .flatten()
+            .round()
+        )
 
         # concatenate the matrix index for different channels
         fold_indices_orig = fold_indices.clone()
         for i in range(in_channels - 1):
             fold_indices_tmp = fold_indices_orig.clone()
             for j in range(fold_indices_orig.size(0)):
                 if fold_indices_orig[j] != 0:
                     fold_indices_tmp[j] += (input_size / in_channels) * (i + 1)
 
             fold_indices = cat([fold_indices, fold_indices_tmp], dim=0).clone()
 
-        fold_indices = fold_indices.to(dtype=int32)
-
-        if self.analog_bias:
-            out_image_size = fold_indices.numel() // (self.kernel_size[0] *
-                                                      self.kernel_size[1] *
-                                                      self.kernel_size[2])
-            fold_indices = cat((fold_indices, ones(out_image_size, dtype=int32)), 0)
-
-        fold_indices = fold_indices.to(x_input.device)
+        fold_indices = fold_indices.to(device=x_input.device, dtype=int32)
 
         x_depth = x_input.size(2)
         x_height = x_input.size(3)
         x_width = x_input.size(4)
 
         d_depth = self.get_image_size(x_depth, 0)
         d_height = self.get_image_size(x_height, 1)
```

### Comparing `aihwkit-0.7.1/src/aihwkit/nn/modules/linear.py` & `aihwkit-0.8.0/src/aihwkit/nn/modules/linear.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,31 +1,30 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
 # that they have been altered from the originals.
 
 """Analog layers."""
-from typing import Optional
+from typing import Optional, Type
 
 from torch import Tensor
 from torch.nn import Linear
 
-from aihwkit.nn.functions import AnalogFunction
-from aihwkit.nn.modules.base import AnalogModuleBase, RPUConfigAlias
-from aihwkit.simulator.configs import SingleRPUConfig
+from aihwkit.nn.modules.base import AnalogLayerBase
+from aihwkit.simulator.parameters.base import RPUConfigBase
 
 
-class AnalogLinear(AnalogModuleBase, Linear):
+class AnalogLinear(AnalogLayerBase, Linear):
     """Linear layer that uses an analog tile.
 
     Linear layer that uses an analog tile during its forward, backward and
     update passes.
 
     Note:
         The tensor parameters of this layer (``.weight`` and ``.bias``) are not
@@ -34,118 +33,117 @@
         ``get_weights`` when attempting to read or modify the weight/bias. This
         read/write process can simulate the (noisy and inexact) analog writing
         and reading of the resistive elements.
 
     Args:
         in_features: input vector size (number of columns).
         out_features: output vector size (number of rows).
-        rpu_config: resistive processing unit configuration.
         bias: whether to use a bias row on the analog tile or not.
-        realistic_read_write: whether to enable realistic read/write
             for setting initial weights and during reading of the weights.
-        weight_scaling_omega: depreciated, use
-            :class:`aihwkit.simulator.configs.utils.MappingParameter`
-            instead to specify weight scaling
+        rpu_config: resistive processing unit configuration.
+        tile_module_class: Class for the tile module (default
+            will be specified from the ``RPUConfig``).
     """
-    # pylint: disable=abstract-method
 
-    __constants__ = ['in_features', 'out_features', 'realistic_read_write',
-                     'digital_bias', 'analog_bias', 'use_bias']
-    in_features: int
-    out_features: int
-    realistic_read_write: bool
-    digital_bias: bool
-    analog_bias: bool
-    use_bias: bool
+    # pylint: disable=abstract-method
 
     def __init__(
-            self,
-            in_features: int,
-            out_features: int,
-            bias: bool = True,
-            rpu_config: Optional[RPUConfigAlias] = None,
-            realistic_read_write: bool = False,
-            weight_scaling_omega: Optional[bool] = None
-              ):
-        # Call super() after tile creation, including ``reset_parameters``.
+        self,
+        in_features: int,
+        out_features: int,
+        bias: bool = True,
+        rpu_config: Optional[RPUConfigBase] = None,
+        tile_module_class: Optional[Type] = None,
+    ):
+        # Call super()
         Linear.__init__(self, in_features, out_features, bias=bias)
 
         # Create tile
         if rpu_config is None:
+            # pylint: disable=import-outside-toplevel
+            from aihwkit.simulator.configs.configs import SingleRPUConfig
+
             rpu_config = SingleRPUConfig()
 
-        rpu_config = self._set_weight_scaling_omega(rpu_config, weight_scaling_omega)
+        AnalogLayerBase.__init__(self)
 
-        AnalogModuleBase.__init__(
-            self,
-            in_features,
-            out_features,
-            bias,
-            realistic_read_write,
-            rpu_config.mapping
-        )
-        self.analog_tile = self._setup_tile(rpu_config)
+        if tile_module_class is None:
+            tile_module_class = rpu_config.get_default_tile_module_class(out_features, in_features)
 
-        # Register tile
-        self.register_analog_tile(self.analog_tile)
+        self.analog_module = tile_module_class(out_features, in_features, rpu_config, bias)
+        # Unregister weight/bias as a parameter.
+        self.unregister_parameter("weight")
+        if bias:
+            self.unregister_parameter("bias")
+        else:
+            # Seems to be a torch bug.
+            self._parameters.pop("bias", None)
+        self.bias = bias
 
-        # Set weights from the reset_parameters call
-        self.set_weights(self.weight, self.bias)
+        self.reset_parameters()
+
+    def reset_parameters(self) -> None:
+        """Reset the parameters (weight and bias)."""
+        if hasattr(self, "analog_module"):
+            bias = self.bias
+            self.weight, self.bias = self.get_weights()  # type: ignore
+            super().reset_parameters()
+            self.set_weights(self.weight, self.bias)  # type: ignore
+            self.weight, self.bias = None, bias
+
+    def forward(self, x_input: Tensor) -> Tensor:
+        """Compute the forward pass."""
+        # pylint: disable=arguments-differ, arguments-renamed
 
-        # Unregister weight/bias as a parameter but keep it as a
-        # field (needed for syncing still)
-        self.unregister_parameter('weight')
-        if self.analog_bias:
-            self.unregister_parameter('bias')
+        return self.analog_module(x_input)  # type: ignore
 
     @classmethod
     def from_digital(
-            cls,
-            module: Linear,
-            rpu_config: Optional[RPUConfigAlias] = None,
-            realistic_read_write: bool = False,
-    ) -> 'AnalogLinear':
+        cls, module: Linear, rpu_config: RPUConfigBase, tile_module_class: Optional[Type] = None
+    ) -> "AnalogLinear":
         """Return an AnalogLinear layer from a torch Linear layer.
 
         Args:
             module: The torch module to convert. All layers that are
                 defined in the ``conversion_map``.
             rpu_config: RPU config to apply to all converted tiles.
                 Applied to all converted tiles.
-            realistic_read_write: Whether to use closed-loop programming
-                when setting the weights. Applied to all converted tiles.
-
-                Note:
-                    Make sure that the weight max and min settings of the
-                    device support the desired analog weight range.
+            tile_module_class: Class of the underlying
+                `TileModule`. If not given, will select based on
+                the `MappingParameter` setting either
+                :class:`~aihwkit.simulator.tiles.base.TileModule` or
+                :class:`~aihwkit.simulator.tiles.array.TileModuleArray`
 
         Returns:
             an AnalogLinear layer based on the digital Linear ``module``.
         """
-        analog_module = cls(module.in_features,
-                            module.out_features,
-                            module.bias is not None,
-                            rpu_config,
-                            realistic_read_write)
+        analog_layer = cls(
+            module.in_features,
+            module.out_features,
+            module.bias is not None,
+            rpu_config,
+            tile_module_class,
+        )
 
-        analog_module.set_weights(module.weight, module.bias)
-        return analog_module
+        analog_layer.set_weights(module.weight, module.bias)
+        return analog_layer
 
-    def reset_parameters(self) -> None:
-        """Reset the parameters (weight and bias)."""
-        super().reset_parameters()
-        if self.analog_tile_count():
-            self.set_weights(self.weight, self.bias)
+    @classmethod
+    def to_digital(cls, module: "AnalogLinear", realistic: bool = False) -> "Linear":
+        """Return an nn.Linear layer from an AnalogLinear layer.
 
-    def forward(self, x_input: Tensor) -> Tensor:
-        """Compute the forward pass."""
-        # pylint: disable=arguments-differ, arguments-renamed
+        Args:
+            module: The analog module to convert.
+            realistic: whehter to estimate the weights with the
+                non-ideal forward pass. If not set, analog weights are
+                (unrealistically) copies exactly
 
-        out = AnalogFunction.apply(
-                self.analog_tile.get_analog_ctx(), x_input,
-                self.analog_tile.shared_weights, not self.training)
-
-        out = self.analog_tile.apply_out_scaling(out, (-1, ))
-
-        if self.digital_bias:
-            return out + self.bias
-        return out
+        Returns:
+            an torch Linear layer with the same dimension and weights
+            as the analog linear layer.
+        """
+        weight, bias = module.get_weights(realistic=realistic)
+        digital_layer = Linear(module.in_features, module.out_features, bias is not None)
+        digital_layer.weight.data = weight.data
+        if bias is not None:
+            digital_layer.bias.data = bias.data
+        return digital_layer
```

### Comparing `aihwkit-0.7.1/src/aihwkit/nn/modules/linear_mapped.py` & `aihwkit-0.8.0/src/aihwkit/nn/modules/base.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,360 +1,363 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
 # that they have been altered from the originals.
 
-"""Analog mapped layers."""
+"""Base class for adding functionality to analog layers."""
+from typing import Any, List, Optional, Tuple, NamedTuple, Union, Generator, Callable
+from collections import OrderedDict
+
+from torch import Tensor
+from torch.nn import Parameter
+from torch import device as torch_device
 
-from typing import Optional, Tuple, List
-
-from torch import Tensor, cat, split, no_grad
-from torch.nn import Linear
-
-from aihwkit.nn.functions import AnalogFunction
-from aihwkit.nn.modules.base import AnalogModuleBase, RPUConfigAlias
-from aihwkit.simulator.configs import SingleRPUConfig
 from aihwkit.exceptions import ModuleError
+from aihwkit.simulator.tiles.module import TileModule
+from aihwkit.simulator.tiles.inference import InferenceTileWithPeriphery
+from aihwkit.simulator.tiles.base import AnalogTileStateNames
 
 
-class AnalogLinearMapped(AnalogModuleBase, Linear):
-    """Linear layer that uses an analog tile.
+class AnalogLayerBase:
+    """Mixin that adds functionality on the layer level.
 
-    Linear layer that uses an analog tile during its forward, backward
-    and update passes. In contrast to
-    :class:`~aihwkit.bb.modules.linear.Linear` the maximal in and/or
-    out dimension can be restricted, in which case the linear layer is
-    split into multiple parts and computed on multiple tiles of given
-    max sizes.
-
-    In contrast to :class:`~aihwkit.bb.modules.linear.Linear`, the
-    bias vector (if requested) is always handled in digital (floating
-    point).
-
-    Note:
-        Mapping is controlled by the :class:`aihwkit.simulator.configs.utils.MappingParameter`.
-
-    Note:
-        The tensor parameters of this layer (``.weight`` and ``.bias``) are not
-        guaranteed to contain the same values as the internal weights and biases
-        stored in the analog tile. Please use ``set_weights`` and
-        ``get_weights`` when attempting to read or modify the weight/bias. This
-        read/write process can simulate the (noisy and inexact) analog writing
-        and reading of the resistive elements.
-
-    Args:
-        in_features: input vector size (number of columns).
-        out_features: output vector size (number of rows).
-        rpu_config: resistive processing unit configuration.
-        bias: whether to use a bias row on the analog tile or not
-        realistic_read_write: whether to enable realistic read/write
-            for setting initial weights and read out of weights
-        weight_scaling_omega: depreciated, use
-            :class:`aihwkit.simulator.configs.utils.MappingParameter`
-            instead to specify weight scaling
+    In general, the defined methods will be looped for all analog tile
+    modules and delegate the function.
     """
-    # pylint: disable=abstract-method, too-many-locals, too-many-instance-attributes
 
-    __constants__ = ['in_features', 'out_features', 'realistic_read_write',
-                     'digital_bias', 'analog_bias', 'use_bias']
-    in_features: int
-    out_features: int
-    realistic_read_write: bool
-    digital_bias: bool
-    analog_bias: bool
-    use_bias: bool
-    in_sizes: List[int]
-    out_sizes: List[int]
-
-    def __init__(
-            self,
-            in_features: int,
-            out_features: int,
-            bias: bool = True,
-            rpu_config: Optional[RPUConfigAlias] = None,
-            realistic_read_write: bool = False,
-            weight_scaling_omega: Optional[bool] = None,
-    ):
-
-        # Call super() after tile creation, including ``reset_parameters``.
-        Linear.__init__(self, in_features, out_features, bias=bias)
-
-        # Create tiles
-        if rpu_config is None:
-            rpu_config = SingleRPUConfig()
-
-        rpu_config = self._set_weight_scaling_omega(rpu_config, weight_scaling_omega)
-
-        AnalogModuleBase.__init__(
-            self,
-            in_features,
-            out_features,
-            bias,
-            realistic_read_write,
-            rpu_config.mapping
-        )
-        if self.analog_bias:
-            raise ModuleError("AnalogLinearMapped only supports digital bias.")
-
-        # More than one tile may need to be created. If so, divide
-        # weight matrix into equal pieces along input dimension with
-        # as many tiles as needed
-        max_input_size = rpu_config.mapping.max_input_size
-        max_output_size = rpu_config.mapping.max_output_size
-
-        self.in_sizes = self.get_split_sizes(in_features, max_input_size)
-        self.out_sizes = self.get_split_sizes(out_features, max_output_size)
-
-        self.analog_tile_array = []
-        for i, in_tile_size in enumerate(self.in_sizes):
-            in_tiles = []
-            for j, out_tile_size in enumerate(self.out_sizes):
-                tile = rpu_config.tile_class(out_tile_size,
-                                             in_tile_size,
-                                             rpu_config,
-                                             bias=self.analog_bias)
-                self.register_analog_tile(tile, name=f"{i}_{j}")
-                in_tiles.append(tile)
-            self.analog_tile_array.append(in_tiles)
+    IS_CONTAINER: bool = False
+    """Class constant indicating whether sub-layers exist or whether
+    this layer is a leave node (that is only having tile modules)"""
 
-        # Set weights from the reset_parameters
-        self.set_weights(self.weight, self.bias)
+    # pylint: disable=no-member
 
-        # Unregister weight/bias as a parameter but keep for sync
-        self.unregister_parameter('weight')
+    def apply_to_analog_layers(self, fn: Callable) -> "AnalogLayerBase":
+        """Apply a function to all the analog layers.
 
-    def get_split_sizes(self, size: int, split_max_size: int) -> List[int]:
-        """ Computed the split sizes.
+        Note:
+            Here analog layers are all sub modules of the current
+            module that derive from ``AnalogLayerBase`` (such as
+            ``AnalogLinear``) _except_ ``AnalogSequential``.
 
         Args:
-            size: number of elements of the layer in one dimension
-            split_max_size: max size of the split
+            fn: function to be applied.
 
         Returns:
-            List of split sizes
+            This module after the function has been applied.
+
         """
-        if split_max_size <= 0:
-            return [size]
+        for _, module in self.named_analog_layers():
+            fn(module)
 
-        n_splits = (size + split_max_size - 1) // split_max_size
-        base, extra = divmod(size, n_splits)
-        return [base + (i < extra) for i in range(n_splits)]
-
-    def set_weights(
-            self,
-            weight: Tensor,
-            bias: Optional[Tensor] = None,
-            force_exact: bool = False,
-            apply_weight_scaling: bool = True,
-            weight_scaling_omega: Optional[float] = None
-    ) -> None:
-        """Set the weight (and bias) with given Tensors.
-
-        This uses an realistic read if the property ``realistic_read_write`` of
-        the layer is set, unless it is overwritten by ``force_exact``. It
-        scales the analog weights by the digital alpha scale if
-        ``weight_scaling_omega`` is positive (see
-        :meth:`~aihwkit.simulator.tiles.base.apply_weight_scaling`).
+        return self
 
-        Note:
-            This is the recommended way for setting the weight/bias matrix of
-            the analog tile, as it will correctly store the weights into the
-            internal memory. Directly writing to ``self.weight`` and
-            ``self.bias`` might yield wrong results as they are not always in
-            sync with the analog tile Parameters, for performance reasons.
+    def apply_to_analog_tiles(self, fn: Callable) -> "AnalogLayerBase":
+        """Apply a function to all the analog tiles of all layers in this module.
+
+        Example::
+
+            model.apply_to_analog_tiles(lambda tile: tile.reset())
+
+        This would reset each analog tile in the whole DNN looping
+        through all layers and all tiles that might exist in a
+        particular layer.
 
         Args:
-            weight: weight matrix
-            bias: bias vector
-            force_exact: forces an exact write to the analog tiles
-            apply_weight_scaling: Whether to rescale the given weight matrix
-                and populate the digital output scaling factors as
-                specified in the configuration
-                :class:`~aihwkit.configs.utils.MappingParameter`. A
-                new ``weight_scaling_omega`` can be given. Note that
-                this will overwrite the existing digital out scaling
-                factors.
-            weight_scaling_omega: The weight scaling omega factor (see
-                :class:`~aihwkit.configs.utils.MappingParameter`). If
-                given explicitly here, it will overwrite the value in
-                the mapping field.
-        """
-        shape = [self.out_features, self.in_features]
-        weight = weight.clone().reshape(shape)
-
-        realistic = self.realistic_read_write and not force_exact
-        in_start = in_end = 0
-        for in_size, in_tiles in zip(self.in_sizes, self.analog_tile_array):
-            in_end += in_size
-            out_start = out_end = 0
-            for out_size, analog_tile in zip(self.out_sizes, in_tiles):
-                out_end += out_size
-
-                tile_weight = weight[out_start:out_end, in_start:in_end]
-
-                analog_tile.set_weights(
-                    tile_weight, None,
-                    apply_weight_scaling,
-                    weight_scaling_omega)
-
-                if realistic:
-                    analog_tile.program_weights()
-
-                out_start = out_end
-            in_start = in_end
-
-        if self.digital_bias and bias is not None:
-            with no_grad():
-                self.bias.data[:] = bias[:]
-
-        self._sync_weights_from_tile()
-
-    def get_weights(self, force_exact: bool = False,
-                    apply_weight_scaling: bool = True) -> Tuple[Tensor, Optional[Tensor]]:
-        """Get the weight (and bias) tensors.
-
-        This uses an realistic read if the property ``realistic_read_write`` of
-        the layer is set, unless it is overwritten by ``force_exact``. It
-        scales the analog weights by the digital alpha scale if
-        ``weight_scaling_omega`` is positive (see
-        :meth:`~aihwkit.simulator.tiles.base.apply_weight_scaling`).
+            fn: function to be applied.
+
+        Returns:
+            This module after the function has been applied.
+
+        """
+        for _, analog_tile in self.named_analog_tiles():
+            fn(analog_tile)
+        return self
+
+    def analog_layers(self) -> Generator["AnalogLayerBase", None, None]:
+        """Generator over analog layers only.
 
         Note:
-            This is the recommended way for setting the weight/bias matrix from
-            the analog tile, as it will correctly fetch the weights from the
-            internal memory. Accessing ``self.weight`` and ``self.bias`` might
-            yield wrong results as they are not always in sync with the
-            analog tile library, for performance reasons.
+            Here analog layers are all sub modules of the current module that
+            derive from ``AnalogLayerBase`` (such as ``AnalogLinear``)
+            _except_ ``AnalogSequential``.
+        """
+        for _, layer in self.named_analog_layers():  # type: ignore
+            yield layer
+
+    def named_analog_layers(self) -> Generator[Tuple[str, "AnalogLayerBase"], None, None]:
+        """Generator over analog layers only.
+
+        Note:
+            Here analog layers are all sub-modules of the current
+            module that derive from ``AnalogLayerBase`` (such as
+            ``AnalogLinear``) _except_ those that are containers
+            (`IS_CONTAINER=True`) such as ``AnalogSequential``.
+
+        """
+        for name, layer in self.named_modules():  # type: ignore
+            if isinstance(layer, AnalogLayerBase) and not layer.IS_CONTAINER:
+                yield name, layer
+
+    def analog_modules(self) -> Generator["AnalogLayerBase", None, None]:
+        """Generator over analog layers and containers.
+
+        Note:
+            Similar to :meth:`analog_layers` but also returning all
+            analog containers
+        """
+        for layer in self.modules():  # type: ignore
+            if isinstance(layer, AnalogLayerBase):
+                yield layer
+
+    def named_analog_modules(self) -> Generator[Tuple[str, "AnalogLayerBase"], None, None]:
+        """Generator over analog layers.
+
+        Note:
+            Similar to :meth:`named_analog_layers` but also returning all
+            analog containers
+        """
+        for name, layer in self.named_modules():  # type: ignore
+            if isinstance(layer, AnalogLayerBase):
+                yield name, layer
+
+    def analog_tile_count(self) -> int:
+        """Returns the number of tiles.
+
+        Caution:
+
+             This is a static number only counted when first called.
+
+        Returns:
+             Number of AnalogTileModules in this layer.
+        """
+        # pylint: disable=attribute-defined-outside-init
+        if not hasattr(self, "_analog_tile_counter"):
+            self._analog_tile_counter = len(list(self.analog_tiles()))
+        return self._analog_tile_counter
+
+    def analog_tiles(self) -> Generator["TileModule", None, None]:
+        """Generator to loop over all registered analog tiles of the module"""
+        for _, tile in self.named_analog_tiles():
+            yield tile
+
+    def named_analog_tiles(self) -> Generator[Tuple[str, "TileModule"], None, None]:
+        """Generator to loop over all registered analog tiles of the module with names."""
+        for name, module in self.named_modules():  # type: ignore
+            if isinstance(module, TileModule):
+                yield (name, module)
+
+    def unregister_parameter(self, param_name: str) -> None:
+        """Unregister module parameter from parameters.
+
+        Raises:
+            ModuleError: In case parameter is not found.
+        """
+        param = getattr(self, param_name, None)
+        if not isinstance(param, Parameter):
+            raise ModuleError(f"Cannot find parameter {param_name} to unregister")
+        delattr(self, param_name)
+        setattr(self, param_name, None)
+
+    def get_analog_tile_devices(self) -> List[Optional[Union[torch_device, str, int]]]:
+        """Return a list of the devices used by the analog tiles.
+
+        Returns:
+            List of torch devices.
+        """
+        return [d.device for d in self.analog_tiles()]
+
+    def set_weights(self, weight: Tensor, bias: Optional[Tensor] = None, **kwargs: Any) -> None:
+        """Set the weight (and bias) tensors to the analog crossbar.
 
         Args:
-            force_exact: forces an exact read to the analog tiles
+            weight: the weight tensor
+            bias: the bias tensor is available
+            **kwargs: see tile level,
+                e.g. :meth:`~aihwkit.simulator.tiles.analog.AnalogTile.set_weights`
 
-            apply_weight_scaling: Whether to return the weights with the
-                (digital) output scaling factors applied. Note the
-                "logical" weights of the layer which the DNN is
-                effectively using are those with the output scales
-                applied. If ``apply_weight_scaling`` is set to False, then
-                only the weight values that is programmed onto the
-                crossbar array are returned, without applying the
-                digital scales. Default is True.
+        Raises:
+            ModuleError: if not of type TileModule.
+        """
+        if hasattr(self, "analog_module"):
+            return self.analog_module.set_weights(weight, bias, **kwargs)
+        raise ModuleError(f"set_weights not implemented for {type(self).__name__} ")
+
+    def get_weights(self, **kwargs: Any) -> Tuple[Tensor, Optional[Tensor]]:
+        """Get the weight (and bias) tensors from the analog crossbar.
+
+        Args:
+            **kwargs: see tile level,
+                e.g. :meth:`~aihwkit.simulator.tiles.analog.AnalogTile.get_weights`.
 
         Returns:
             tuple: weight matrix, bias vector
-        """
-        realistic = self.realistic_read_write and not force_exact
 
-        weight_lst = []
-        for in_tiles in self.analog_tile_array:
-            in_tile_weight = []
-            for analog_tile in in_tiles:
-                if realistic:
-                    tile_weight, _ = analog_tile.read_weights(apply_weight_scaling)
-                else:
-                    tile_weight, _ = analog_tile.get_weights(apply_weight_scaling)
-                in_tile_weight.append(tile_weight)
-            weight_lst.append(cat(in_tile_weight, 0))
-
-        weight = cat(weight_lst, 1)
-
-        if self.digital_bias:
-            with no_grad():
-                return weight, self.bias.data.clone().detach().cpu()
-        return weight, None
-
-    def reset_parameters(self) -> None:
-        """Reset the parameters (weight and bias)."""
-        super().reset_parameters()
-        if self.analog_tile_count():
-            self.set_weights(self.weight, self.bias)
-
-    def forward(self, x_input: Tensor) -> Tensor:
-        """Compute the forward pass."""
-        # pylint: disable=arguments-differ,arguments-renamed
-
-        if self.analog_tile_count() == 1:
-            analog_tile = self.analog_tile_array[0][0]
-            out = AnalogFunction.apply(
-                analog_tile.get_analog_ctx(), x_input,
-                analog_tile.shared_weights, not self.training)
-
-            out = analog_tile.apply_out_scaling(out, (-1, ))
-
-            if self.digital_bias:
-                return out + self.bias
-            return out
-
-        # mapped version
-        last_dim = x_input.ndim - 1
-        splits = split(x_input, self.in_sizes, dim=last_dim)
-        result = None  # type: Tensor
-        for idx, (x, in_tiles) in enumerate(zip(splits, self.analog_tile_array)):
-            out_result = []
-
-            for analog_tile in in_tiles:
-                output = AnalogFunction.apply(
-                    analog_tile.get_analog_ctx(), x,
-                    analog_tile.shared_weights, not self.training)
-
-                output = analog_tile.apply_out_scaling(output, (-1, ))
-                out_result.append(output)
-
-            if idx == 0:
-                result = cat(out_result, last_dim)
-            else:
-                result.add_(cat(out_result, last_dim))
-
-        # add bias to final result
-        if self.digital_bias:
-            return result.add_(self.bias)
-        return result
+        Raises:
+            ModuleError: if not of type TileModule.
+        """
+        if hasattr(self, "analog_module"):
+            return self.analog_module.get_weights(**kwargs)
+        raise ModuleError(f"get_weights not implemented for {type(self).__name__} ")
+
+    def load_state_dict(
+        self,  # pylint: disable=arguments-differ
+        state_dict: "OrderedDict[str, Tensor]",
+        strict: bool = True,
+        load_rpu_config: Optional[bool] = None,
+        strict_rpu_config_check: Optional[bool] = None,
+    ) -> NamedTuple:
+        """Specializes torch's ``load_state_dict`` to add a flag whether to
+        load the RPU config from the saved state.
 
-    def extra_repr(self) -> str:
-        """Set the extra representation of the module.
+        Args:
+            state_dict: see torch's ``load_state_dict``
+            strict: see torch's ``load_state_dict``
+            load_rpu_config: Whether to load the saved RPU
+                config or use the current RPU config of the model.
+
+                Caution:
+
+                    If ``load_rpu_config=False`` the RPU config can
+                    be changed from the stored model. However, the user has to
+                    make sure that the changed RPU config makes sense.
+
+                    For instance, changing the device type might
+                    change the expected fields in the hidden
+                    parameters and result in an error.
+
+            strict_rpu_config_check: Whether to check and throw an
+                error if the current ``rpu_config`` is not of the same
+                class type when setting ``load_rpu_config`` to
+                False. In case of ``False`` the user has to make sure
+                that the ``rpu_config`` are compatible.
 
         Returns:
-            A string with the extra representation.
+            see torch's ``load_state_dict``
+
+        Raises:
+            ModuleError: in case the rpu_config class mismatches
+            or mapping parameter mismatch for
+            ``load_rpu_config=False``.
+
         """
-        output = AnalogModuleBase.extra_repr(self)
-        output += ', mapping={}'.format((len(self.in_sizes), len(self.out_sizes)))
+        for analog_tile in self.analog_tiles():
+            analog_tile.set_load_rpu_config_state(load_rpu_config, strict_rpu_config_check)
+        return super().load_state_dict(state_dict, strict)  # type: ignore
+
+    def prepare_for_ddp(self) -> None:
+        """Adds ignores to avoid broadcasting the analog tile states in case of
+        distributed training.
+
+        Note:
+            Call this function before the mode is converted with DDP.
+
+        Important:
+            Only InferenceTile supports DDP.
 
-        return output
+        Raises:
 
-    @classmethod
-    def from_digital(
-            cls,
-            module: Linear,
-            rpu_config: Optional[RPUConfigAlias] = None,
-            realistic_read_write: bool = False,
-    ) -> 'AnalogLinearMapped':
-        """Return an AnalogLinearMapped layer from a torch Linear layer.
+            ModuleError: In case analog tiles are used that do not
+                support data-parallel model, ie. all analog training
+                tiles.
+        """
+        # pylint: disable=attribute-defined-outside-init
+        exclude_list = []
+        for module in self.modules():  # type: ignore
+            if isinstance(module, AnalogLayerBase):
+                for analog_tile in module.analog_tiles():
+                    if analog_tile.shared_weights is None:
+                        raise ModuleError(
+                            "DDP is only supported with shared weights (e.g. InferenceTile)"
+                        )
+                exclude_list += [
+                    AnalogTileStateNames.CONTEXT,
+                    AnalogTileStateNames.ANALOG_STATE_NAME,
+                ]
+        exclude_list = list(set(exclude_list))
+        params = self.state_dict().keys()  # type: ignore
+        exclude_params = []
+        for param in params:
+            for word in exclude_list:
+                if word in param and word not in exclude_params:
+                    exclude_params.append(param)
+                    break
+        self._ddp_params_and_buffers_to_ignore = exclude_params
+
+    def drift_analog_weights(self, t_inference: float = 0.0) -> None:
+        """(Program) and drift the analog weights.
 
         Args:
-            module: The torch module to convert. All layers that are
-                defined in the ``conversion_map``.
-            rpu_config: RPU config to apply to all converted tiles.
-                Applied to all converted tiles.
-            realistic_read_write: Whether to use closed-loop programming
-                when setting the weights. Applied to all converted tiles.
-
-                Note:
-                    Make sure that the weight max and min settings of the
-                    device support the desired analog weight range.
+            t_inference: assumed time of inference (in sec).
+
+        Raises:
+            ModuleError: if the layer is not in evaluation mode.
+        """
+        if self.training:  # type: ignore
+            raise ModuleError("drift_analog_weights can only be applied in evaluation mode")
+        for analog_tile in self.analog_tiles():
+            if isinstance(analog_tile, InferenceTileWithPeriphery):
+                analog_tile.drift_weights(t_inference)
+
+    def program_analog_weights(self) -> None:
+        """Program the analog weights.
+
+        Raises:
+            ModuleError: if the layer is not in evaluation mode.
+        """
+        if self.training:  # type: ignore
+            raise ModuleError("program_analog_weights can only be applied in evaluation mode")
+        for analog_tile in self.analog_tiles():
+            analog_tile.program_weights()
+
+    def extra_repr(self) -> str:
+        """Set the extra representation of the module.
 
         Returns:
-            an AnalogLinearMapped layer based on the digital Linear ``module``.
+            A string with the extra representation.
         """
-        analog_module = cls(module.in_features,
-                            module.out_features,
-                            module.bias is not None,
-                            rpu_config,
-                            realistic_read_write,
-                            )
+        output = super().extra_repr()  # type: ignore
+        if len(output) == 0:
+            # likely Sequential. Keep also silent
+            return output
+        if not hasattr(self, "_extra_repr_save"):
+            # pylint: disable=attribute-defined-outside-init
+            self._extra_repr_save = next(self.analog_tiles()).rpu_config.__class__.__name__
+        output += ", " + self._extra_repr_save
+        return output.rstrip()
+
+    def remap_analog_weights(self, weight_scaling_omega: Optional[float] = 1.0) -> None:
+        """Gets and re-sets the weights in case of using the weight scaling.
+
+        This re-sets the weights with applied mapping scales, so that
+        the weight mapping scales are updated.
+
+        In case of hardware-aware training, this would update the
+        weight mapping scales so that the absolute max analog weights
+        are set to 1 (as specified in the ``weight_scaling``
+        configuration of
+        :class:`~aihwkit.parameters.utils.MappingParameter`).
 
-        analog_module.set_weights(module.weight, module.bias)
-        return analog_module
+        Note:
+            By default the weight scaling omega factor is set to 1
+            here (overriding any setting in the ``rpu_config``). This
+            means that the max weight value is set to 1 internally for
+            the analog weights.
+
+        Caution:
+            This should typically *not* be called for analog. Use
+            ``program_weights`` to re-program.
+
+        Args:
+            weight_scaling_omega: The weight scaling omega factor (see
+                :class:`~aihwkit.parameters.utils.MappingParameter`). If
+                set to None here, it will take the value in the
+                mapping parameters. Default is however 1.0.
+        """
+        for analog_tile in self.analog_tiles():
+            analog_tile.remap_weights(weight_scaling_omega=weight_scaling_omega)
```

### Comparing `aihwkit-0.7.1/src/aihwkit/nn/modules/rnn/__init__.py` & `aihwkit-0.8.0/src/aihwkit/cloud/__init__.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
 # that they have been altered from the originals.
 
-"""Analog RNN related modules."""
+"""Functionality related to the cloud client for AIHW Composer API."""
```

### Comparing `aihwkit-0.7.1/src/aihwkit/nn/modules/rnn/cells.py` & `aihwkit-0.8.0/src/aihwkit/nn/modules/rnn/cells.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,86 +1,80 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
 # that they have been altered from the originals.
 
 """ Analog cells for RNNs. """
 
-from typing import Optional, Tuple
+from typing import Optional, Tuple, Type
 from collections import namedtuple
 
 from torch import Tensor, sigmoid, tanh, zeros, cat
 
 from aihwkit.nn.modules.container import AnalogSequential
+from aihwkit.nn.modules.linear import AnalogLinear
 
-from aihwkit.simulator.configs import InferenceRPUConfig
-from aihwkit.nn.modules.base import RPUConfigAlias
+from aihwkit.simulator.configs.configs import InferenceRPUConfig
+from aihwkit.simulator.parameters.base import RPUConfigBase
 
-LSTMState = namedtuple('LSTMState', ['hx', 'cx'])
+LSTMState = namedtuple("LSTMState", ["hx", "cx"])
 
 
 class AnalogVanillaRNNCell(AnalogSequential):
     """Analog Vanilla RNN Cell.
 
     Args:
         input_size: in_features size for W_ih matrix
         hidden_size: in_features and out_features size for W_hh matrix
         bias: whether to use a bias row on the analog tile or not
         rpu_config: configuration for an analog resistive processing unit
-        realistic_read_write: whether to enable realistic read/write
-            for setting initial weights and read out of weights
+        tile_module_class: Class for the analog tile module (default
+            will be specified from the ``RPUConfig``).
     """
+
     # pylint: disable=abstract-method
     def __init__(
-            self,
-            input_size: int,
-            hidden_size: int,
-            bias: bool,
-            rpu_config: Optional[RPUConfigAlias] = None,
-            realistic_read_write: bool = False,
+        self,
+        input_size: int,
+        hidden_size: int,
+        bias: bool,
+        rpu_config: Optional[RPUConfigBase] = None,
+        tile_module_class: Optional[Type] = None,
     ):
         super().__init__()
 
         # Default to InferenceRPUConfig
         if not rpu_config:
             rpu_config = InferenceRPUConfig()
 
         self.input_size = input_size
         self.hidden_size = hidden_size
-        self.weight_ih = rpu_config.get_linear()(input_size, hidden_size, bias=bias,
-                                                 rpu_config=rpu_config,
-                                                 realistic_read_write=realistic_read_write)
-        self.weight_hh = rpu_config.get_linear()(hidden_size, hidden_size, bias=bias,
-                                                 rpu_config=rpu_config,
-                                                 realistic_read_write=realistic_read_write)
+        self.weight_ih = AnalogLinear(input_size, hidden_size, bias, rpu_config, tile_module_class)
+        self.weight_hh = AnalogLinear(hidden_size, hidden_size, bias, rpu_config, tile_module_class)
 
     def get_zero_state(self, batch_size: int) -> Tensor:
         """Returns a zeroed state.
 
         Args:
             batch_size: batch size of the input
 
         Returns:
            Zeroed state tensor
         """
         device = self.weight_ih.get_analog_tile_devices()[0]
         return zeros(batch_size, self.hidden_size, device=device)
 
-    def forward(
-            self,
-            input_: Tensor,
-            state: Tensor
-    ) -> Tuple[Tensor, Tensor]:
+    def forward(self, input_: Tensor, state: Tensor) -> Tuple[Tensor, Tensor]:
         # pylint: disable=arguments-differ
         igates = self.weight_ih(input_)
         hgates = self.weight_hh(state)
 
         out = tanh(igates + hgates)
 
         return out, out  # output will also be hidden state
@@ -90,58 +84,61 @@
     """Analog LSTM Cell.
 
     Args:
         input_size: in_features size for W_ih matrix
         hidden_size: in_features and out_features size for W_hh matrix
         bias: whether to use a bias row on the analog tile or not
         rpu_config: configuration for an analog resistive processing unit
-        realistic_read_write: whether to enable realistic read/write
-            for setting initial weights and read out of weights
+        tile_module_class: Class for the analog tile module (default
+            will be specified from the ``RPUConfig``).
     """
+
     # pylint: disable=abstract-method
 
     def __init__(
-            self,
-            input_size: int,
-            hidden_size: int,
-            bias: bool,
-            rpu_config: Optional[RPUConfigAlias] = None,
-            realistic_read_write: bool = False,
+        self,
+        input_size: int,
+        hidden_size: int,
+        bias: bool,
+        rpu_config: Optional[RPUConfigBase] = None,
+        tile_module_class: Optional[Type] = None,
     ):
         super().__init__()
 
         # Default to InferenceRPUConfig
         if not rpu_config:
             rpu_config = InferenceRPUConfig()
 
         self.input_size = input_size
         self.hidden_size = hidden_size
-        self.weight_ih = rpu_config.get_linear()(input_size, 4 * hidden_size, bias=bias,
-                                                 rpu_config=rpu_config,
-                                                 realistic_read_write=realistic_read_write)
-        self.weight_hh = rpu_config.get_linear()(hidden_size, 4 * hidden_size, bias=bias,
-                                                 rpu_config=rpu_config,
-                                                 realistic_read_write=realistic_read_write)
+        self.weight_ih = AnalogLinear(
+            input_size, 4 * hidden_size, bias, rpu_config, tile_module_class
+        )
+        self.weight_hh = AnalogLinear(
+            hidden_size, 4 * hidden_size, bias, rpu_config, tile_module_class
+        )
 
     def get_zero_state(self, batch_size: int) -> Tensor:
         """Returns a zeroed state.
 
         Args:
             batch_size: batch size of the input
 
         Returns:
            Zeroed state tensor
         """
         device = self.weight_ih.get_analog_tile_devices()[0]
-        return LSTMState(zeros(batch_size, self.hidden_size, device=device),
-                         zeros(batch_size, self.hidden_size, device=device))
-
-    def forward(self, input_: Tensor,
-                state: Tuple[Tensor, Tensor]) -> Tuple[Tensor, Tuple[Tensor, Tensor]]:
+        return LSTMState(
+            zeros(batch_size, self.hidden_size, device=device),
+            zeros(batch_size, self.hidden_size, device=device),
+        )
 
+    def forward(
+        self, input_: Tensor, state: Tuple[Tensor, Tensor]
+    ) -> Tuple[Tensor, Tuple[Tensor, Tensor]]:
         # pylint: disable=arguments-differ
         h_x, c_x = state
         gates = self.weight_ih(input_) + self.weight_hh(h_x)
         in_gate, forget_gate, cell_gate, out_gate = gates.chunk(4, 1)
 
         in_gate = sigmoid(in_gate)
         forget_gate = sigmoid(forget_gate)
@@ -158,57 +155,58 @@
     """Analog LSTM Cell that use a combined weight for storing gates and inputs.
 
     Args:
         input_size: The number of expected features in the input `x`
         hidden_size: The number of features in the hidden state `h`
         bias: whether to use a bias row on the analog tile or not.
         rpu_config: resistive processing unit configuration.
-        realistic_read_write: whether to enable realistic read/write
-            for setting initial weights and during reading of the weights.
+        tile_module_class: Class for the analog tile module (default
+            will be specified from the ``RPUConfig``).
     """
+
     # pylint: disable=abstract-method
 
     def __init__(
-            self,
-            input_size: int,
-            hidden_size: int,
-            bias: bool,
-            rpu_config: Optional[RPUConfigAlias] = None,
-            realistic_read_write: bool = False,
+        self,
+        input_size: int,
+        hidden_size: int,
+        bias: bool,
+        rpu_config: Optional[RPUConfigBase] = None,
+        tile_module_class: Optional[Type] = None,
     ):
         super().__init__()
 
         # Default to InferenceRPUConfig
         if not rpu_config:
             rpu_config = InferenceRPUConfig()
 
         self.input_size = input_size
         self.hidden_size = hidden_size
-        self.weight = rpu_config.get_linear()(input_size + hidden_size,
-                                              4 * hidden_size,
-                                              bias=bias,
-                                              rpu_config=rpu_config,
-                                              realistic_read_write=realistic_read_write)
+        self.weight = AnalogLinear(
+            input_size + hidden_size, 4 * hidden_size, bias, rpu_config, tile_module_class
+        )
 
     def get_zero_state(self, batch_size: int) -> Tensor:
         """Returns a zeroed state.
 
         Args:
             batch_size: batch size of the input
 
         Returns:
            Zeroed state tensor
         """
         device = self.weight.get_analog_tile_devices()[0]
-        return LSTMState(zeros(batch_size, self.hidden_size, device=device),
-                         zeros(batch_size, self.hidden_size, device=device))
-
-    def forward(self, input_: Tensor,
-                state: Tuple[Tensor, Tensor]) -> Tuple[Tensor, Tuple[Tensor, Tensor]]:
+        return LSTMState(
+            zeros(batch_size, self.hidden_size, device=device),
+            zeros(batch_size, self.hidden_size, device=device),
+        )
 
+    def forward(
+        self, input_: Tensor, state: Tuple[Tensor, Tensor]
+    ) -> Tuple[Tensor, Tuple[Tensor, Tensor]]:
         # pylint: disable=arguments-differ
         h_x, c_x = state
         x_input = cat((input_, h_x), 1)
         gates = self.weight(x_input)
         in_gate, forget_gate, cell_gate, out_gate = gates.chunk(4, 1)
 
         in_gate = sigmoid(in_gate)
@@ -226,56 +224,56 @@
     """Analog GRU Cell.
 
     Args:
         input_size: in_features size for W_ih matrix
         hidden_size: in_features and out_features size for W_hh matrix
         bias: whether to use a bias row on the analog tile or not
         rpu_config: configuration for an analog resistive processing unit
-        realistic_read_write: whether to enable realistic read/write
-            for setting initial weights and read out of weights
+        tile_module_class: Class for the analog tile module (default
+            will be specified from the ``RPUConfig``).
     """
+
     # pylint: disable=abstract-method
 
     def __init__(
-            self,
-            input_size: int,
-            hidden_size: int,
-            bias: bool,
-            rpu_config: Optional[RPUConfigAlias] = None,
-            realistic_read_write: bool = False,
+        self,
+        input_size: int,
+        hidden_size: int,
+        bias: bool,
+        rpu_config: Optional[RPUConfigBase] = None,
+        tile_module_class: Optional[Type] = None,
     ):
         super().__init__()
 
         # Default to InferenceRPUConfig
         if not rpu_config:
             rpu_config = InferenceRPUConfig()
 
         self.input_size = input_size
         self.hidden_size = hidden_size
-        self.weight_ih = rpu_config.get_linear()(input_size, 3 * hidden_size, bias=bias,
-                                                 rpu_config=rpu_config,
-                                                 realistic_read_write=realistic_read_write)
-        self.weight_hh = rpu_config.get_linear()(hidden_size, 3 * hidden_size, bias=bias,
-                                                 rpu_config=rpu_config,
-                                                 realistic_read_write=realistic_read_write)
+        self.weight_ih = AnalogLinear(
+            input_size, 3 * hidden_size, bias, rpu_config, tile_module_class
+        )
+        self.weight_hh = AnalogLinear(
+            hidden_size, 3 * hidden_size, bias, rpu_config, tile_module_class
+        )
 
     def get_zero_state(self, batch_size: int) -> Tensor:
         """Returns a zeroed state.
 
         Args:
             batch_size: batch size of the input
 
         Returns:
            Zeroed state tensor
         """
         device = self.weight_ih.get_analog_tile_devices()[0]
         return zeros(batch_size, self.hidden_size, device=device)
 
     def forward(self, input_: Tensor, state: Tensor) -> Tuple[Tensor, Tensor]:
-
         # pylint: disable=arguments-differ
 
         g_i = self.weight_ih(input_)
         g_h = self.weight_hh(state)
         i_r, i_i, i_n = g_i.chunk(3, 1)
         h_r, h_i, h_n = g_h.chunk(3, 1)
```

### Comparing `aihwkit-0.7.1/src/aihwkit/nn/modules/rnn/layers.py` & `aihwkit-0.8.0/src/aihwkit/nn/modules/rnn/layers.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -22,14 +22,15 @@
     """Analog RNN Layer.
 
     Args:
         cell: RNNCell type (AnalogLSTMCell/AnalogGRUCell/AnalogVanillaRNNCell/
               AnalogLSTMCellSingleRPU)
         cell_args: arguments to RNNCell (e.g. input_size, hidden_size, rpu_configs)
     """
+
     # pylint: disable=abstract-method
 
     def __init__(self, cell: Type, *cell_args: Any):
         super().__init__()
         self.cell = cell(*cell_args)
 
     def get_zero_state(self, batch_size: int) -> Tensor:
@@ -40,98 +41,99 @@
 
         Returns:
            Zeroed state tensor
         """
         return self.cell.get_zero_state(batch_size)
 
     def forward(
-            self, input_: Tensor,
-            state: Union[Tuple[Tensor, Tensor], Tensor]
+        self, input_: Tensor, state: Union[Tuple[Tensor, Tensor], Tensor]
     ) -> Tuple[Tensor, Tuple[Tensor, Tensor]]:
         # pylint: disable=arguments-differ
         inputs = input_.unbind(0)
         outputs = jit.annotate(List[Tensor], [])
         for input_item in inputs:
             out, state = self.cell(input_item, state)
             outputs += [out]
         return stack(outputs), state
 
 
 class AnalogReverseRNNLayer(AnalogSequential):
-    """ Analog RNN layer for direction.
+    """Analog RNN layer for direction.
 
     Args:
         cell: RNNCell type (AnalogLSTMCell/AnalogGRUCell/AnalogVanillaRNNCell)
         cell_args: arguments to RNNCell (e.g. input_size, hidden_size, rpu_configs)
     """
+
     def __init__(self, cell: Type, *cell_args: Any):
         super().__init__()
         self.cell = cell(*cell_args)
 
     @staticmethod
     def reverse(lst: List[Tensor]) -> List[Tensor]:
-        """ Reverses the list of input tensors. """
+        """Reverses the list of input tensors."""
         return lst[::-1]
 
     def get_zero_state(self, batch_size: int) -> Tensor:
         """Returns a zeroed state.
 
         Args:
             batch_size: batch size of the input
 
         Returns:
            Zeroed state tensor
         """
         return self.cell.get_zero_state(batch_size)
 
-    def forward(self, input_: Tensor,
-                state: Union[Tuple[Tensor, Tensor], Tensor]
-                ) -> Tuple[Tensor, Union[Tuple[Tensor, Tensor], Tensor]]:
+    def forward(
+        self, input_: Tensor, state: Union[Tuple[Tensor, Tensor], Tensor]
+    ) -> Tuple[Tensor, Union[Tuple[Tensor, Tensor], Tensor]]:
         # pylint: disable=arguments-differ
         inputs = self.reverse(input_.unbind(0))
         outputs = jit.annotate(List[Tensor], [])
         for input_values in inputs:
             out, state = self.cell(input_values, state)
             outputs += [out]
         return stack(self.reverse(outputs)), state
 
 
 class AnalogBidirRNNLayer(AnalogSequential):
-    """ Bi-directional analog RNN layer.
+    """Bi-directional analog RNN layer.
 
     Args:
         cell: RNNCell type (AnalogLSTMCell/AnalogGRUCell/AnalogVanillaRNNCell)
         cell_args: arguments to RNNCell (e.g. input_size, hidden_size, rpu_configs)
     """
 
-    __constants__ = ['directions']
+    __constants__ = ["directions"]
 
     def __init__(self, cell: Type, *cell_args: Any):
         super().__init__()
 
-        self.directions = ModuleList([
-            AnalogRNNLayer(cell, *cell_args),
-            AnalogReverseRNNLayer(cell, *cell_args),
-        ])
+        self.directions = ModuleList(
+            [AnalogRNNLayer(cell, *cell_args), AnalogReverseRNNLayer(cell, *cell_args)]
+        )
 
     def get_zero_state(self, batch_size: int) -> Tensor:
         """Returns a zeroed state.
 
         Args:
             batch_size: batch size of the input
 
         Returns:
            Zeroed state tensor
         """
-        return [self.directions[0].get_zero_state(batch_size),
-                self.directions[1].get_zero_state(batch_size)]
+        return [
+            self.directions[0].get_zero_state(batch_size),
+            self.directions[1].get_zero_state(batch_size),
+        ]
 
-    def forward(self, input_: Tensor,
-                states: List[Union[Tuple[Tensor, Tensor], Tensor]]
-                ) -> Tuple[Tensor, List[Union[Tuple[Tensor, Tensor], Tensor]]]:
+    def forward(
+        self, input_: Tensor, states: List[Union[Tuple[Tensor, Tensor], Tensor]]
+    ) -> Tuple[Tensor, List[Union[Tuple[Tensor, Tensor], Tensor]]]:
         # pylint: disable=arguments-differ
         # List[RNNState]: [forward RNNState, backward RNNState]
         outputs = jit.annotate(List[Tensor], [])
         output_states = jit.annotate(List[Tuple[Tensor, Tensor]], [])
 
         for direction, state in zip(self.directions, states):
             out, out_state = direction(input_, state)
```

### Comparing `aihwkit-0.7.1/src/aihwkit/nn/modules/rnn/rnn.py` & `aihwkit-0.8.0/src/aihwkit/nn/modules/rnn/rnn.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,107 +1,110 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
 # that they have been altered from the originals.
 
 """ Analog RNN modules. """
 
 import warnings
 import math
+
 from typing import Any, List, Optional, Tuple, Type, Callable
 from torch import Tensor, jit
 from torch.nn import Dropout, ModuleList, init
+from torch.autograd import no_grad
 
 from aihwkit.nn.modules.container import AnalogSequential
 from aihwkit.nn.modules.rnn.layers import AnalogRNNLayer, AnalogBidirRNNLayer
-from aihwkit.nn.modules.base import AnalogModuleBase, RPUConfigAlias
+from aihwkit.nn.modules.base import AnalogLayerBase
+from aihwkit.simulator.parameters.base import RPUConfigBase
 
 
 class ModularRNN(AnalogSequential):
     """Helper class to create a Modular RNN
 
     Args:
         num_layers: number of serially connected RNN layers
         layer: RNN layer type (e.g. AnalogLSTMLayer)
         dropout: dropout applied to output of all RNN layers except last
         first_layer_args: RNNCell type, input_size, hidden_size, rpu_config, etc.
         other_layer_args: RNNCell type, hidden_size, hidden_size, rpu_config, etc.
     """
+
     # pylint: disable=abstract-method
 
     # Necessary for iterating through self.layers and dropout support
-    __constants__ = ['layers', 'num_layers']
+    __constants__ = ["layers", "num_layers"]
 
     def __init__(
-            self,
-            num_layers: int,
-            layer: Type,
-            dropout: float,
-            first_layer_args: Any,
-            other_layer_args: Any):
+        self,
+        num_layers: int,
+        layer: Type,
+        dropout: float,
+        first_layer_args: Any,
+        other_layer_args: Any,
+    ):
         super().__init__()
-        self.layers = self.init_stacked_analog_lstm(num_layers, layer, first_layer_args,
-                                                    other_layer_args)
+        self.layers = self.init_stacked_analog_lstm(
+            num_layers, layer, first_layer_args, other_layer_args
+        )
 
         # Introduce a Dropout layer on the outputs of each RNN layer except
         # the last layer.
         self.num_layers = num_layers
         if num_layers == 1 and dropout > 0:
-            warnings.warn('dropout lstm adds dropout layers after all but last '
-                          'recurrent layer, it expects num_layers greater than '
-                          '1, but got num_layers = 1')
+            warnings.warn(
+                "dropout lstm adds dropout layers after all but last "
+                "recurrent layer, it expects num_layers greater than "
+                "1, but got num_layers = 1"
+            )
         self.dropout_layer = Dropout(dropout) if dropout else None
 
     @staticmethod
     def init_stacked_analog_lstm(
-            num_layers: int,
-            layer: Type,
-            first_layer_args: Any,
-            other_layer_args: Any
+        num_layers: int, layer: Type, first_layer_args: Any, other_layer_args: Any
     ) -> ModuleList:
         """Construct a list of LSTMLayers over which to iterate.
 
         Args:
             num_layers: number of serially connected LSTM layers
             layer: RNN layer type (e.g. AnalogLSTMLayer)
             first_layer_args: RNNCell type, input_size, hidden_size, rpu_config, etc.
             other_layer_args: RNNCell type, hidden_size, hidden_size, rpu_config, etc.
 
         Returns:
             torch.nn.ModuleList, which is similar to a regular Python list,
             but where torch.nn.Module methods can be applied
         """
-        layers = [layer(*first_layer_args)] \
-            + [layer(*other_layer_args) for _ in range(num_layers - 1)]
+        layers = [layer(*first_layer_args)] + [
+            layer(*other_layer_args) for _ in range(num_layers - 1)
+        ]
         return ModuleList(layers)
 
     def get_zero_state(self, batch_size: int) -> List[Tensor]:
         """Returns a zeroed state.
 
         Args:
             batch_size: batch size of the input
 
         Returns:
            List of zeroed state tensors for each layer
         """
         return [lay.get_zero_state(batch_size) for lay in self.layers]
 
     def forward(  # pylint: disable=arguments-differ
-            self,
-            input: Tensor,  # pylint: disable=redefined-builtin
-            states: List
+        self, input: Tensor, states: List  # pylint: disable=redefined-builtin
     ) -> Tuple[Tensor, List]:
-
         # List[RNNState]: One state per layer.
         output_states = jit.annotate(List, [])
         output = input
 
         for i, rnn_layer in enumerate(self.layers):
             state = states[i]
             output, out_state = rnn_layer(output, state)
@@ -118,97 +121,107 @@
 
     Args:
         cell: type of Analog RNN cell (AnalogLSTMCell/AnalogGRUCell/AnalogVanillaRNNCell)
         input_size: in_features to W_{ih} matrix of first layer
         hidden_size: in_features and out_features for W_{hh} matrices
         bias: whether to use a bias row on the analog tile or not
         rpu_config: resistive processing unit configuration.
-        realistic_read_write: whether to enable realistic read/write
-            for setting initial weights and read out of weights
+        tile_module_class: Class for the analog tile module (default
+            will be specified from the ``RPUConfig``).
         xavier: whether standard PyTorch LSTM weight
             initialization (default) or Xavier initialization
         num_layers: number of serially connected RNN layers
         bidir: if True, becomes a bidirectional RNN
         dropout: dropout applied to output of all RNN layers except last
     """
+
     # pylint: disable=abstract-method, too-many-arguments
 
     def __init__(
-            self,
-            cell: Type,
-            input_size: int,
-            hidden_size: int,
-            bias: bool = True,
-            rpu_config: Optional[RPUConfigAlias] = None,
-            realistic_read_write: bool = False,
-            xavier: bool = False,
-            num_layers: int = 1,
-            bidir: bool = False,
-            dropout: float = 0.0
-            ):
+        self,
+        cell: Type,
+        input_size: int,
+        hidden_size: int,
+        bias: bool = True,
+        rpu_config: Optional[RPUConfigBase] = None,
+        tile_module_class: Optional[Type] = None,
+        xavier: bool = False,
+        num_layers: int = 1,
+        bidir: bool = False,
+        dropout: float = 0.0,
+    ):
         super().__init__()
 
         if bidir:
             layer = AnalogBidirRNNLayer
             num_dirs = 2
         else:
             layer = AnalogRNNLayer
             num_dirs = 1
 
         self.rnn = ModularRNN(
-            num_layers, layer, dropout,
-            first_layer_args=[cell, input_size, hidden_size, bias,
-                              rpu_config, realistic_read_write],
-            other_layer_args=[cell, num_dirs*hidden_size, hidden_size, bias,
-                              rpu_config, realistic_read_write])
+            num_layers,
+            layer,
+            dropout,
+            first_layer_args=[cell, input_size, hidden_size, bias, rpu_config, tile_module_class],
+            other_layer_args=[
+                cell,
+                num_dirs * hidden_size,
+                hidden_size,
+                bias,
+                rpu_config,
+                tile_module_class,
+            ],
+        )
         self.hidden_size = hidden_size
         self.num_layers = num_layers
         self.reset_parameters(xavier)
 
+    @no_grad()
     def init_layers(
-            self,
-            weight_init_fn: Callable,
-            bias_init_fn: Optional[Callable] = None
+        self, weight_init_fn: Callable, bias_init_fn: Optional[Callable] = None
     ) -> None:
         """Init the analog layers with custom functions.
 
         Args:
             weight_init_fn: in-place tensor function applied to weight of
                 ``AnalogLinear`` layers
             bias_init_fn: in-place tensor function applied to bias of
                 ``AnalogLinear`` layers
 
         Note:
             If no bias init function is provided the weight init
             function is taken for the bias as well.
         """
-        def init_analog_layer(layer: AnalogModuleBase) -> None:
+
+        def init_analog_layer(layer: AnalogLayerBase) -> None:
             """Init the weights and bias of an analog linear layer."""
-            weight_init_fn(layer.weight.data)
-            if layer.use_bias:
+            weight, bias = layer.get_weights()
+            weight_init_fn(weight.data)
+            if bias is not None:
                 if bias_init_fn is None:
-                    weight_init_fn(layer.bias.data)
+                    weight_init_fn(bias.data)
                 else:
-                    bias_init_fn(layer.bias.data)
+                    bias_init_fn(bias.data)
 
-            layer.set_weights(layer.weight, layer.bias)
+            layer.set_weights(weight, bias)
 
-        self._apply_to_analog(init_analog_layer)  # pylint: disable=protected-access
+        self.apply_to_analog_tiles(init_analog_layer)
 
     def reset_parameters(self, xavier: bool = False) -> None:
         """Weight and bias initialization.
 
         Args:
             xavier: whether standard PyTorch LSTM weight
                initialization (default) or Xavier initialization
         """
         if xavier:
             self.init_layers(init.xavier_uniform_, init.zeros_)
         else:
-            stdv = 1. / math.sqrt(self.hidden_size)
+            stdv = 1.0 / math.sqrt(self.hidden_size)
             self.init_layers(lambda x: x.uniform_(-stdv, stdv))
 
     def get_zero_state(self, batch_size: int) -> List[Tensor]:
         """Returns a zeroed RNN state based on cell type and layer type
 
         Args:
             batch_size: batch size of the input
@@ -216,16 +229,14 @@
         Returns:
            List of zeroed state tensors for each layer
 
         """
         return self.rnn.get_zero_state(batch_size)
 
     def forward(
-            self,
-            input: Tensor,  # pylint: disable=redefined-builtin
-            states: Optional[List] = None
+        self, input: Tensor, states: Optional[List] = None  # pylint: disable=redefined-builtin
     ) -> Tuple[Tensor, List]:
         if states is None:
             # TODO: batch_first.
             states = self.get_zero_state(input.shape[1])
 
         return self.rnn(input, states)
```

### Comparing `aihwkit-0.7.1/src/aihwkit/optim/__init__.py` & `aihwkit-0.8.0/src/aihwkit/optim/__init__.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,17 +1,17 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
 # that they have been altered from the originals.
 
 """Analog Optimizers."""
 
 # Convenience imports for easier access to the classes.
 
-from aihwkit.optim.analog_optimizer import AnalogOptimizer, AnalogSGD
+from aihwkit.optim.analog_optimizer import AnalogOptimizer, AnalogSGD, AnalogAdam
```

### Comparing `aihwkit-0.7.1/src/aihwkit/optim/analog_optimizer.py` & `aihwkit-0.8.0/src/aihwkit/optim/analog_optimizer.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -12,15 +12,15 @@
 
 """Analog-aware inference optimizer."""
 
 from types import new_class
 from typing import Any, Callable, Dict, Optional, Type
 
 from torch import cat
-from torch.optim import Optimizer, SGD
+from torch.optim import Optimizer, SGD, Adam
 from torch.autograd import no_grad
 
 from aihwkit.optim.context import AnalogContext
 
 
 class AnalogOptimizerMixin:
     """Mixin for analog optimizers.
@@ -37,30 +37,30 @@
         each analog layer to a new single group.
         """
         # Create the new param groups.
         analog_param_groups = []
         rm_group_lst = []
         for group in self.param_groups:  # type: ignore[has-type]
             rm_lst = []
-            for param in group['params']:
+            for param in group["params"]:
                 if isinstance(param, AnalogContext):
                     param.analog_tile.set_learning_rate(
-                        self.defaults['lr'])  # type: ignore[attr-defined]
-                    analog_param_groups.append({
-                        'params': [param],
-                    })
+                        self.defaults["lr"]  # type: ignore[attr-defined]
+                    )
+                    analog_param_groups.append({"params": [param]})
                     rm_lst.append(id(param))
 
-            group['params'] = [p for p in group['params'] if id(p) not in rm_lst]
+            group["params"] = [p for p in group["params"] if id(p) not in rm_lst]
 
-            if len(group['params']) == 0:
+            if len(group["params"]) == 0:
                 rm_group_lst.append(id(group))
 
-        self.param_groups = [g for g in self.param_groups  # type: ignore[has-type]
-                             if id(g) not in rm_group_lst]
+        self.param_groups = [
+            g for g in self.param_groups if id(g) not in rm_group_lst  # type: ignore[has-type]
+        ]
 
         # Add analog groups.
         for group in analog_param_groups:
             self.add_param_group(group)  # type: ignore[attr-defined]
 
     @no_grad()
     def step(self, closure: Optional[Callable] = None) -> Optional[float]:
@@ -79,69 +79,76 @@
         """
         # pylint: disable=too-many-branches
         # Update non-analog parameters using the given optimizer
         ret = super().step(closure)  # type: ignore[misc]
 
         # Update analog parameters
         for group in self.param_groups:
-            learning_rate = group.get('lr')
+            learning_rate = group.get("lr")
 
             # Use analog_tile object.
-            for param in group['params']:
+            for param in group["params"]:
                 if isinstance(param, AnalogContext):
-
                     # Handle internal analog update.
                     analog_ctx = param
                     analog_tile = analog_ctx.analog_tile
 
                     if analog_ctx.use_torch_update:
                         # In this case a separate weight parameter exists: do nothing.
                         continue
 
-                    # Update learning rate.
-                    if learning_rate is not None:
-                        analog_tile.set_learning_rate(learning_rate)
-
                     # Call `update` in the tile.
                     if not analog_ctx.has_gradient():
                         # Forward never used.
                         continue
 
+                    # Update learning rate.
+                    if learning_rate == 0.0:
+                        analog_ctx.reset()
+                        continue
+
+                    if learning_rate is not None:
+                        analog_tile.set_learning_rate(learning_rate)
+
                     if analog_ctx.use_indexed:
-                        for x_input, d_input in zip(analog_ctx.analog_input,
-                                                    analog_ctx.analog_grad_output):
+                        for x_input, d_input in zip(
+                            analog_ctx.analog_input, analog_ctx.analog_grad_output
+                        ):
                             analog_tile.update_indexed(x_input, d_input)
                     else:
-                        x_input = cat(analog_ctx.analog_input,
-                                      axis=-1 if analog_tile.in_trans else 0)
-                        d_input = cat(analog_ctx.analog_grad_output,
-                                      axis=-1 if analog_tile.out_trans else 0)
+                        x_input = cat(
+                            analog_ctx.analog_input, axis=-1 if analog_tile.in_trans else 0
+                        )
+                        d_input = cat(
+                            analog_ctx.analog_grad_output, axis=-1 if analog_tile.out_trans else 0
+                        )
                         analog_tile.update(x_input, d_input)
 
                     analog_ctx.reset()
+
         # Apply post-update step operations (diffuse, decay, etc).
         # (only here because of unknown params order and shared weights)
         for group in self.param_groups:
-            for param in group['params']:
+            for param in group["params"]:
                 if isinstance(param, AnalogContext):
                     param.analog_tile.post_update_step()
         return ret
 
     def set_learning_rate(self, learning_rate: float = 0.1) -> None:
         """Update the learning rate to a new value.
 
         Update the learning rate of the optimizer, propagating the changes
         to the analog tiles accordingly.
 
         Args:
             learning_rate: learning rate for the optimizer.
         """
         for param_group in self.param_groups:
-            param_group['lr'] = learning_rate
-            for param in param_group['params']:
+            param_group["lr"] = learning_rate
+            for param in param_group["params"]:
                 if isinstance(param, AnalogContext):
                     # Update learning rate on the tile
                     param.analog_tile.set_learning_rate(learning_rate)
 
 
 class AnalogOptimizer(AnalogOptimizerMixin, Optimizer):
     """Generic optimizer that wraps an existing ``Optimizer`` for analog inference.
@@ -175,34 +182,30 @@
         >>> model = AnalogLinear(3, 4, rpu_config=InferenceRPUConfig)
         >>> optimizer = AnalogOptimizer(SGD, model.parameters(), lr=0.02)
     """
 
     SUBCLASSES = {}  # type: Dict[str, Type]
     """Registry of the created subclasses."""
 
-    def __new__(
-            cls,
-            optimizer_cls: Type,
-            *_: Any,
-            **__: Any
-    ) -> 'AnalogOptimizer':
-        subclass_name = '{}{}'.format(cls.__name__, optimizer_cls.__name__)
+    def __new__(cls, optimizer_cls: Type, *_: Any, **__: Any) -> "AnalogOptimizer":
+        subclass_name = "{}{}".format(cls.__name__, optimizer_cls.__name__)
 
         # Retrieve or create a new subclass, that inherits both from
         # `AnalogOptimizer` and for the specific torch optimizer
         # (`optimizer_cls`).
         if subclass_name not in cls.SUBCLASSES:
             cls.SUBCLASSES[subclass_name] = new_class(subclass_name, (cls, optimizer_cls), {})
 
         return super().__new__(cls.SUBCLASSES[subclass_name])
 
     def __init__(
-            self,
-            optimizer_cls: Type,  # pylint: disable=unused-argument
-            *args: Any,
-            **kwargs: Any
+        self, optimizer_cls: Type, *args: Any, **kwargs: Any  # pylint: disable=unused-argument
     ):
         super().__init__(*args, **kwargs)
 
 
 class AnalogSGD(AnalogOptimizerMixin, SGD):
     """Implements analog-aware stochastic gradient descent."""
+
+
+class AnalogAdam(AnalogOptimizerMixin, Adam):
+    """Implements analog-aware Adam."""
```

### Comparing `aihwkit-0.7.1/src/aihwkit/optim/context.py` & `aihwkit-0.8.0/src/aihwkit/optim/context.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,65 +1,74 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
 # that they have been altered from the originals.
 
 """Parameter context for analog tiles."""
 
+# pylint: disable=attribute-defined-outside-init
+
 from typing import Optional, Type, Union, Any, TYPE_CHECKING
 
 from torch import ones, dtype, Tensor, no_grad
 from torch.nn import Parameter
 from torch import device as torch_device
 
 if TYPE_CHECKING:
-    from aihwkit.simulator.tiles.base import BaseTile
+    from aihwkit.simulator.tiles.base import SimulatorTileWrapper
 
 
 class AnalogContext(Parameter):
     """Context for analog optimizer."""
 
-    def __new__(cls: Type['AnalogContext'], analog_tile: 'BaseTile',
-                parameter: Optional[Parameter] = None) -> 'AnalogContext':
+    def __new__(
+        cls: Type["AnalogContext"],
+        analog_tile: "SimulatorTileWrapper",
+        parameter: Optional[Parameter] = None,
+    ) -> "AnalogContext":
         # pylint: disable=signature-differs
         if parameter is None:
-            return Parameter.__new__(cls, data=ones((), device=analog_tile.device),
-                                     requires_grad=True)
+            return Parameter.__new__(
+                cls, data=ones((), device=analog_tile.device), requires_grad=True
+            )
         parameter.__class__ = cls
         return parameter
 
-    def __init__(self, analog_tile: 'BaseTile',
-                 parameter: Optional[Parameter] = None):  # pylint: disable=unused-argument
+    def __init__(
+        self, analog_tile: "SimulatorTileWrapper", parameter: Optional[Parameter] = None
+    ):  # pylint: disable=unused-argument
         super().__init__()
         self.analog_tile = analog_tile
         self.use_torch_update = False
         self.use_indexed = False
         self.analog_input = []  # type: list
         self.analog_grad_output = []  # type: list
         self.reset(analog_tile)
 
+    def set_indexed(self, value: bool = True) -> None:
+        """Set the context to forward_indexed."""
+        self.use_indexed = value
+
     def set_data(self, data: Tensor) -> None:
         """Set the data value of the Tensor."""
-        # pylint: disable=attribute-defined-outside-init
         with no_grad():
             self.data.copy_(data)
 
     def get_data(self) -> Tensor:
         """Get the data value of the underlying Tensor."""
-        # pylint: disable=attribute-defined-outside-init
         return self.data.detach()
 
-    def reset(self, analog_tile: Optional['BaseTile'] = None) -> None:
+    def reset(self, analog_tile: Optional["SimulatorTileWrapper"] = None) -> None:
         """Reset the gradient trace and optionally sets the tile pointer."""
 
         if analog_tile is not None:
             self.analog_tile = analog_tile
             self.analog_tile.analog_ctx = self
 
         self.analog_input = []
@@ -71,79 +80,73 @@
 
     def __copy__(self) -> Parameter:
         """Turn off copying of the pointers. Context will be re-created
         when tile is created"""
         return Parameter(self.data)
 
     def __deepcopy__(self, memo: Any) -> Parameter:
-        """ Turn off deep copying. Context will be re-created when tile is created """
+        """Turn off deep copying. Context will be re-created when tile is created"""
         return Parameter(self.data)
 
-    def cuda(
-            self,
-            device: Optional[Union[torch_device, str, int]] = None
-    ) -> 'AnalogContext':
+    def cuda(self, device: Optional[Union[torch_device, str, int]] = None) -> "AnalogContext":
         """Move the context to a cuda device.
 
         Args:
              device: the desired device of the tile.
 
         Returns:
             This context in the specified device.
         """
-        # pylint: disable=attribute-defined-outside-init
         self.data = self.data.cuda(device)  # type: Tensor
-
         if not self.analog_tile.is_cuda:
             self.analog_tile = self.analog_tile.cuda(device)
-        self.reset(self.analog_tile)
-
+            self.reset(self.analog_tile)
         return self
 
-    def cpu(self) -> 'AnalogContext':
+    def cpu(self) -> "AnalogContext":
         """Move the context to CPU.
 
         Note:
             This is a no-op for CPU context.
 
         Returns:
             self
         """
-        super().cpu()
-        if self.analog_tile is not None:
-            self.analog_tile = self.analog_tile.cpu()  # will raise an error if not possile
+        self.data = self.data.cpu()
+        if self.analog_tile is not None and self.analog_tile.is_cuda:
+            self.analog_tile = self.analog_tile.cpu()
+            self.reset(self.analog_tile)
         return self
 
-    def to(self, *args: Any, **kwargs: Any) -> 'AnalogContext':
+    def to(self, *args: Any, **kwargs: Any) -> "AnalogContext":
         """Move analog tiles of the current context to a device.
 
         Note:
             Please be aware that moving analog tiles from GPU to CPU is
             currently not supported.
 
         Caution:
             Other tensor conversions than moving the device to CUDA,
             such as changing the data type are not supported for analog
             tiles and will be simply ignored.
 
         Returns:
             This module in the specified device.
         """
-        # pylint: disable=invalid-name, attribute-defined-outside-init
+        # pylint: disable=invalid-name
         self.data = self.data.to(*args, **kwargs)
-
         device = None
-        if 'device' in kwargs:
-            device = kwargs['device']
+        if "device" in kwargs:
+            device = kwargs["device"]
         elif len(args) > 0 and not isinstance(args[0], (Tensor, dtype)):
             device = torch_device(args[0])
 
         if device is not None:
             device = torch_device(device)
-            if device.type == 'cuda' and not self.analog_tile.is_cuda:
-                self.analog_tile = self.analog_tile.cuda(device)
-            self.reset(self.analog_tile)
-
+            if device.type == "cuda" and not self.analog_tile.is_cuda:
+                self.cuda(device)
+            elif device.type == "cpu" and self.analog_tile.is_cuda:
+                self.cpu()
         return self
 
     def __repr__(self) -> str:
-        return 'AnalogContext of ' + self.analog_tile.get_brief_info()
+        return "AnalogContext of " + self.analog_tile.get_brief_info()
```

### Comparing `aihwkit-0.7.1/src/aihwkit/simulator/CMakeLists.txt` & `aihwkit-0.8.0/src/aihwkit/simulator/CMakeLists.txt`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/aihwkit/simulator/__init__.py` & `aihwkit-0.8.0/src/aihwkit/simulator/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/aihwkit/simulator/configs/__init__.py` & `aihwkit-0.8.0/src/aihwkit/simulator/configs/__init__.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,40 +1,71 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
 # that they have been altered from the originals.
 
 """Configurations for resistive processing units."""
 
-from .configs import (
-    FloatingPointRPUConfig, InferenceRPUConfig, SingleRPUConfig,
-    UnitCellRPUConfig, DigitalRankUpdateRPUConfig
-)
-from .enums import (
-    BoundManagementType, NoiseManagementType,
-    WeightNoiseType, PulseType, WeightModifierType, WeightClipType,
-    WeightRemapType, VectorUnitCellUpdatePolicy, AnalogMVType
+from aihwkit.simulator.parameters.utils import (
+    IOParameters,
+    UpdateParameters,
+    WeightModifierParameter,
+    WeightClipParameter,
+    WeightRemapParameter,
+    SimpleDriftParameter,
+    DriftParameter,
+    MappingParameter,
+    InputRangeParameter,
+    PrePostProcessingParameter,
 )
-from .utils import (
-    IOParameters, UpdateParameters,
-    WeightModifierParameter, WeightClipParameter,
-    WeightRemapParameter, SimpleDriftParameter, DriftParameter,
-    MappingParameter, InputRangeParameter, PrePostProcessingParameter
+from aihwkit.simulator.parameters.enums import (
+    BoundManagementType,
+    NoiseManagementType,
+    WeightNoiseType,
+    PulseType,
+    WeightModifierType,
+    WeightClipType,
+    WeightRemapType,
+    VectorUnitCellUpdatePolicy,
+    AnalogMVType,
 )
 from .devices import (
-    FloatingPointDevice, IdealDevice, ConstantStepDevice,
-    LinearStepDevice, SoftBoundsDevice, SoftBoundsPmaxDevice,
-    SoftBoundsReferenceDevice, ExpStepDevice, PowStepDevice,
-    PowStepReferenceDevice, PiecewiseStepDevice
+    FloatingPointDevice,
+    IdealDevice,
+    ConstantStepDevice,
+    LinearStepDevice,
+    SoftBoundsDevice,
+    SoftBoundsPmaxDevice,
+    SoftBoundsReferenceDevice,
+    ExpStepDevice,
+    PowStepDevice,
+    PowStepReferenceDevice,
+    PiecewiseStepDevice,
 )
 from .compounds import (
-    VectorUnitCell, ReferenceUnitCell,
-    OneSidedUnitCell, DifferenceUnitCell, TransferCompound,
-    BufferedTransferCompound, MixedPrecisionCompound
+    VectorUnitCell,
+    ReferenceUnitCell,
+    OneSidedUnitCell,
+    DifferenceUnitCell,
+    TransferCompound,
+    BufferedTransferCompound,
+    ChoppedTransferCompound,
+    DynamicTransferCompound,
+    MixedPrecisionCompound,
 )
+from .configs import (
+    FloatingPointRPUConfig,
+    InferenceRPUConfig,
+    SingleRPUConfig,
+    UnitCellRPUConfig,
+    DigitalRankUpdateRPUConfig,
+    TorchInferenceRPUConfig,
+)
+
+from .helpers import build_config
```

### Comparing `aihwkit-0.7.1/src/aihwkit/simulator/configs/compounds.py` & `aihwkit-0.8.0/src/aihwkit/simulator/configs/compounds.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -16,34 +16,30 @@
 
 from copy import deepcopy
 from dataclasses import dataclass, field
 from typing import ClassVar, List, Type, Union, TYPE_CHECKING
 from warnings import warn
 
 from aihwkit.exceptions import ConfigError
-from aihwkit.simulator.configs.helpers import (
-    _PrintableMixin, parameters_to_bindings
-)
-from aihwkit.simulator.configs.utils import (
-    IOParameters, UpdateParameters
-)
-from aihwkit.simulator.configs.enums import VectorUnitCellUpdatePolicy
+from aihwkit.simulator.parameters.helpers import _PrintableMixin, parameters_to_bindings
+from aihwkit.simulator.parameters.utils import IOParameters, UpdateParameters
+from aihwkit.simulator.parameters.enums import VectorUnitCellUpdatePolicy
 from aihwkit.simulator.rpu_base import devices
 
 if TYPE_CHECKING:
     from aihwkit.simulator.configs.devices import PulsedDevice
 
 
 @dataclass
 class UnitCell(_PrintableMixin):
     """Parameters that modify the behaviour of a unit cell."""
 
     bindings_class: ClassVar[Type] = devices.VectorResistiveDeviceParameter
 
-    bindings_ignore: ClassVar[List] = ['diffusion', 'lifetime']
+    bindings_ignore: ClassVar[List] = ["diffusion", "lifetime"]
 
     unit_cell_devices: List = field(default_factory=list)
     """Devices that compose this unit cell."""
 
     construction_seed: int = 0
     """If not ``0``, set a unique seed for hidden parameters during
     construction.
@@ -64,14 +60,15 @@
         return any(dev.requires_decay() for dev in self.unit_cell_devices)
 
 
 ###############################################################################
 # Specific devices based on ``unit cell``.
 ###############################################################################
 
+
 @dataclass
 class VectorUnitCell(UnitCell):
     """Abstract resistive device that combines multiple pulsed resistive
     devices in a single 'unit cell'.
 
     For instance, a vector device can consist of 2 resistive devices
     where the sum of the two resistive values are coded for each
@@ -86,33 +83,33 @@
 
     first_update_idx: int = 0
     """Device that receives the first mini-batch.
 
     Useful only for ``VectorUnitCellUpdatePolicy.SINGLE_FIXED``.
     """
 
-    gamma_vec: List[float] = field(default_factory=list, metadata={'hide_if': []})
+    gamma_vec: List[float] = field(default_factory=list, metadata={"hide_if": []})
     """Weighting of the unit cell devices to reduce to final weight.
 
     User-defined weightening can be given as a list if factors. If not
     given, each device index of the unit cell is weighted by equal
     amounts (:math:`1/n`).
     """
 
     def as_bindings(self) -> devices.VectorResistiveDeviceParameter:
         """Return a representation of this instance as a simulator bindings object."""
         vector_parameters = parameters_to_bindings(self)
 
         if not isinstance(self.unit_cell_devices, list):
-            raise ConfigError('unit_cell_devices should be a list of devices')
+            raise ConfigError("unit_cell_devices should be a list of devices")
 
         for param in self.unit_cell_devices:
             device_parameters = param.as_bindings()
             if not vector_parameters.append_parameter(device_parameters):
-                raise ConfigError('Could not add unit cell device parameter')
+                raise ConfigError("Could not add unit cell device parameter")
 
         return vector_parameters
 
 
 @dataclass
 class ReferenceUnitCell(UnitCell):
     """Abstract device model takes two arbitrary device per cross-point and
@@ -145,42 +142,45 @@
     Caution:
         This parameter should be kept to SINGLE_FIXED for this device.
     """
 
     first_update_idx: int = 0
     """Device that receives the update."""
 
-    gamma_vec: List[float] = field(default_factory=lambda: [1., -1.],
-                                   metadata={'hide_if': [1., -1.]})
+    gamma_vec: List[float] = field(
+        default_factory=lambda: [1.0, -1.0], metadata={"hide_if": [1.0, -1.0]}
+    )
     """Weighting of the unit cell devices to reduce to final weight.
 
     Note:
         While user-defined weighting can be given it is suggested to keep it to
         the default ``[1, -1]`` to implement the reference device subtraction.
     """
 
     def as_bindings(self) -> devices.VectorResistiveDeviceParameter:
         """Return a representation of this instance as a simulator bindings object."""
         vector_parameters = parameters_to_bindings(self)
 
         if not isinstance(self.unit_cell_devices, list):
-            raise ConfigError('unit_cell_devices should be a list of devices')
+            raise ConfigError("unit_cell_devices should be a list of devices")
 
         if len(self.unit_cell_devices) > 2:
             self.unit_cell_devices = self.unit_cell_devices[:2]
         elif len(self.unit_cell_devices) == 1:
-            self.unit_cell_devices = [self.unit_cell_devices[0],
-                                      deepcopy(self.unit_cell_devices[0])]
+            self.unit_cell_devices = [
+                self.unit_cell_devices[0],
+                deepcopy(self.unit_cell_devices[0]),
+            ]
         elif len(self.unit_cell_devices) != 2:
-            raise ConfigError('ReferenceUnitCell expects two unit_cell_devices')
+            raise ConfigError("ReferenceUnitCell expects two unit_cell_devices")
 
         for param in self.unit_cell_devices:
             device_parameters = param.as_bindings()
             if not vector_parameters.append_parameter(device_parameters):
-                raise ConfigError('Could not add unit cell device parameter')
+                raise ConfigError("Could not add unit cell device parameter")
 
         return vector_parameters
 
 
 @dataclass
 class OneSidedUnitCell(UnitCell):
     """Abstract device model takes an arbitrary device per crosspoint and
@@ -245,74 +245,76 @@
 
     refresh_upper_thres: float = 0.75
     """Upper threshold for determining the refresh, see above."""
 
     refresh_lower_thres: float = 0.25
     """Lower threshold for determining the refresh, see above."""
 
-    refresh_forward: IOParameters = field(
-        default_factory=IOParameters)
+    refresh_forward: IOParameters = field(default_factory=IOParameters)
     """Input-output parameters that define the read during a refresh event.
 
-    :class:`~aihwkit.simulator.config.utils.AnalogTileInputOutputParameters`
+    :class:`~aihwkit.simulator.parameters.utils.IOParameters`
     that define the read (forward) of an refresh event. For instance
     the amount of noise or whether refresh is done using a ADC/DAC
     etc.
     """
 
     refresh_update: UpdateParameters = field(default_factory=UpdateParameters)
     """Update parameters that define the type of update used for each refresh
     event.
 
     Update parameters
-    :class:`~aihwkit.simulator.config.utils.AnalogTileUpdateParameters`
+    :class:`~aihwkit.simulator.parameters.utils.UpdateParameters`
     that define the type of update used for each refresh event.
     """
 
     copy_inverted: bool = False
     """Whether the use the "down" update behavior of the first device for
     the negative updates instead of the positive half of the second
     device."""
 
     def as_bindings(self) -> devices.OneSidedResistiveDeviceParameter:
         """Return a representation of this instance as a simulator
         bindings object."""
         if not isinstance(self.unit_cell_devices, list):
-            raise ConfigError('unit_cell_devices should be a list of devices')
+            raise ConfigError("unit_cell_devices should be a list of devices")
 
         onesided_parameters = parameters_to_bindings(self)
         device_parameter0 = self.unit_cell_devices[0].as_bindings()
 
         if len(self.unit_cell_devices) == 0 or len(self.unit_cell_devices) > 2:
-            raise ConfigError('Need 1 or 2 unit_cell_devices')
+            raise ConfigError("Need 1 or 2 unit_cell_devices")
 
         if len(self.unit_cell_devices) == 1:
             device_parameter1 = device_parameter0
         else:
             device_parameter1 = self.unit_cell_devices[1].as_bindings()
 
         # need to be exactly 2 and same parameters
         if not onesided_parameters.append_parameter(device_parameter0):
-            raise ConfigError('Could not add unit cell device parameter')
+            raise ConfigError("Could not add unit cell device parameter")
 
         if not onesided_parameters.append_parameter(device_parameter1):
-            raise ConfigError('Could not add unit cell device parameter ' +
-                              '(both devices need to be of the same type)')
+            raise ConfigError(
+                "Could not add unit cell device parameter "
+                + "(both devices need to be of the same type)"
+            )
 
         return onesided_parameters
 
 
 @dataclass
 class DifferenceUnitCell(OneSidedUnitCell):
     """Deprecated alias to ``OneSidedUnitCell``."""
 
     def __post__init__(self) -> None:
-        warn('The DifferenceUnitCell class is deprecated. Please use '
-             'OneSidedUnitCell instead.',
-             DeprecationWarning)
+        warn(
+            "The DifferenceUnitCell class is deprecated. Please use OneSidedUnitCell instead.",
+            DeprecationWarning,
+        )
 
 
 @dataclass
 class TransferCompound(UnitCell):
     r"""Abstract device model that takes 2 or more devices and
     implements a transfer-based learning rule.
 
@@ -370,16 +372,15 @@
     matrices.
 
     The default scheme is:
 
     .. math:: g^{n-1} W_0 + g^{n-2} W_1 + \ldots + g^0  W_{n-1}
     """
 
-    gamma_vec: List[float] = field(default_factory=list,
-                                   metadata={'hide_if': []})
+    gamma_vec: List[float] = field(default_factory=list, metadata={"hide_if": []})
     """User-defined weightening.
 
     User-defined weightening can be given as a list if weights in which case
     the default weightening scheme with ``gamma`` is not used.
     """
 
     transfer_every: float = 1.0
@@ -399,16 +400,15 @@
     / n_reads_per_transfer``.
     """
 
     no_self_transfer: bool = True
     """Whether to set the transfer rate of the last device (which is applied to
     itself) to zero."""
 
-    transfer_every_vec: List[float] = field(default_factory=list,
-                                            metadata={'hide_if': []})
+    transfer_every_vec: List[float] = field(default_factory=list, metadata={"hide_if": []})
     """Transfer cycles lengths.
 
     A list of :math:`n` entries, to explicitly set the transfer cycles lengths.
     In this case, the above defaults are ignored.
     """
 
     units_in_mbatch: bool = True
@@ -452,15 +452,15 @@
 
     Whether to select a random starting column or row for each
     transfer event and not take the next column or row that was
     previously not transferred as a starting column or row (the
     default).
     """
 
-    fast_lr: float = 0.0
+    fast_lr: float = 1.0
     """Whether to set the `fast` tile's learning rate.
 
     If set, then the SGD gradient update onto the first (fast) tile is
     set to this learning rate and is kept constant even when the SGD
     learning rate is scheduled. The SGD learning rate is then only
     used to scale the transfer LR (see ``scale_transfer_lr``).
     """
@@ -472,61 +472,58 @@
     set, the transfer LR is scaled by current learning rate of the SGD.
 
     Note:
         LR is always a positive number, sign will be correctly
         applied internally.
     """
 
-    transfer_lr_vec: List[float] = field(default_factory=list,
-                                         metadata={'hide_if': []})
+    transfer_lr_vec: List[float] = field(default_factory=list, metadata={"hide_if": []})
     """Transfer LR for each individual transfer in the device chain can be
     given."""
 
     scale_transfer_lr: bool = True
     """Whether to give the transfer_lr in relative units.
 
     ie. whether to scale the transfer LR with the current LR of the SGD.
     """
 
-    transfer_forward: IOParameters = field(
-        default_factory=IOParameters)
+    transfer_forward: IOParameters = field(default_factory=IOParameters)
     """Input-output parameters that define the read of a transfer event.
 
-    :class:`~aihwkit.simulator.config.utils.AnalogTileInputOutputParameters` that define the read
+    :class:`~aihwkit.simulator.parameters.utils.IOParameters` that define the read
     (forward or backward) of an transfer event. For instance the amount of noise
     or whether transfer is done using a ADC/DAC etc.
     """
 
-    transfer_update: UpdateParameters = field(
-        default_factory=UpdateParameters)
+    transfer_update: UpdateParameters = field(default_factory=UpdateParameters)
     """Update parameters that define the type of update used for each transfer
     event.
 
-    Update parameters :class:`~aihwkit.simulator.config.utils.AnalogTileUpdateParameters` that
+    Update parameters :class:`~aihwkit.simulator.parameters.utils.UpdateParameters` that
     define the type of update used for each transfer event.
     """
 
     def as_bindings(self) -> devices.TransferResistiveDeviceParameter:
         """Return a representation of this instance as a simulator bindings object."""
         if not isinstance(self.unit_cell_devices, list):
-            raise ConfigError('unit_cell_devices should be a list of devices')
+            raise ConfigError("unit_cell_devices should be a list of devices")
 
         n_devices = len(self.unit_cell_devices)
 
         transfer_parameters = parameters_to_bindings(self)
 
         param_fast = self.unit_cell_devices[0].as_bindings()
         param_slow = self.unit_cell_devices[1].as_bindings()
 
         if not transfer_parameters.append_parameter(param_fast):
-            raise ConfigError('Could not add unit cell device parameter')
+            raise ConfigError("Could not add unit cell device parameter")
 
         for _ in range(n_devices - 1):
             if not transfer_parameters.append_parameter(param_slow):
-                raise ConfigError('Could not add unit cell device parameter')
+                raise ConfigError("Could not add unit cell device parameter")
 
         return transfer_parameters
 
 
 @dataclass
 class BufferedTransferCompound(TransferCompound):
     r"""Abstract device model that takes 2 or more devices and
@@ -578,46 +575,428 @@
     e.g. 1 (``desired_BL`` in the ``transfer_update``) the transfer
     might be clipped and the potentially larger buffer values are
     forgotten. If disabled, then the buffer values are faithfully
     subtracted by the amount transferred (times one minus momentum).
     """
 
     transfer_update: UpdateParameters = field(
-        default_factory=lambda: UpdateParameters(desired_bl=1, update_bl_management=False,
-                                                 update_management=False))
+        default_factory=lambda: UpdateParameters(
+            desired_bl=1, update_bl_management=False, update_management=False
+        )
+    )
     """Update parameters that define the type of update used for each transfer
     event.
 
-    Update parameters :class:`~aihwkit.simulator.config.utils.AnalogTileUpdateParameters` that
+    Update parameters :class:`~aihwkit.simulator.parameters.utils.UpdateParameters` that
     define the type of update used for each transfer event.
     """
 
 
+@dataclass
+class ChoppedTransferCompound(TransferCompound):
+    r"""Abstract device model that takes exactly two devices and
+    implements a chopped and buffered transfer-based learning rule.
+
+    Similar to :class:`BufferedTransferCompound`, however, the
+    gradient update onto the fast tile is done with `choppers`, that
+    is random sign changes. These sign changes reduce any potential
+    bias or long-term correlation that might be present on the fast
+    device, as gradients are written in both directions and signs
+    recovered after readout (averaging out any existing correlations
+    during the update on the devices).
+
+    Here the choices of transfer are more restricted, to enable a fast
+    CUDA optimization of the transfer. In particular, only 2 devices
+    are supported, transfer has to be sequential with exactly one read
+    at each transfer event (that is the settings
+    ``random_selection=False``, ``with_reset_prob=0.0``,
+    ``n_reads_per_transfer=1``).
+
+    Note:
+        This device is identical to :class:`BufferedTransferCompound` if
+        the chopper probabilities are set to 0 (with the above
+        restrictions), but will run up to 40x faster on GPU for larger
+        batches and small settings of ``transfer_every``, because of a
+        fused CUDA kernel design.
+    """
+
+    bindings_class: ClassVar[Type] = devices.ChoppedTransferResistiveDeviceParameter
+
+    in_chop_prob: float = 0.1
+    """Switching probability of the input choppers. The chopper will be
+    switched with the given probability once after the corresponding
+    vector read (column or row).
+    """
+
+    in_chop_random: bool = True
+    """Whether to switch randomly (default) or regular.
+
+    If regular, then the ``in_chop_prob`` sets the frequency of
+    switching, ie. ``MIN(1/in_chop_prob, 2)`` is the period of
+    switching in terms of number of reads of a particular row /
+    col. All rows/cols will switch at the same matrix update cycle.
+    """
+
+    out_chop_prob: float = 0.0
+    """Switching probability of the output choppers. The chopper will be
+    switched with the given probability once after a full matrix
+    update has been accomplished.
+    """
+
+    buffer_granularity: float = 1.0
+    """ Granularity if the buffer. """
+
+    auto_granularity: float = 0.0
+    """If set, scales the ``buffer_granularity`` based on the expected
+    number of MVMs needed to cross the buffer, ie::
+
+         buffer_granularity *= auto_granularity / (in_size * transfer_every) *
+                               weight_granularity
+
+    Typical value would be e.g. 20000.
+    """
+
+    step: float = 1.0
+    """Value to fill the ``d`` vector for the update if buffered value is
+    above threshold.
+    """
+
+    momentum: float = 0.0
+    """Momentum of the buffer.
+
+    After transfer, this momentum fraction stays on the buffer instead
+    of subtracting all of what was transferred.
+    """
+
+    forget_buffer: bool = True
+    """Whether to forget the value of the buffer after transfer.
+
+    If enabled, the buffer is reset to the momentum times the
+    transferred value. Thus, if the number of pulses is limited to
+    e.g. 1 (``desired_BL`` in the ``transfer_update``) the transfer
+    might be clipped and the potentially larger buffer values are
+    forgotten. If disabled, then the buffer values are faithfully
+    subtracted by the amount transferred (times one minus momentum).
+    """
+
+    units_in_mbatch: bool = False
+    """Units for ``transfer_every``.
+
+    If set, then the cycle length units of ``transfer_every`` are in
+    ``m_batch`` instead of mat-vecs, which is equal to the overall of the
+    weight re-use during a while mini-batch.
+    """
+
+    auto_scale: bool = False
+    """Scaling the weight gradient onto the fast matrix by the averaged
+    recent past of the maximum gradient.
+
+    This will dynamically compute a reasonable update strength onto
+    the fast matrix. ``fast_lr`` can be used to scale the gradient
+    update further.
+    """
+
+    auto_momentum: float = 0.99
+    """Momentum of the gradient when using auto scale """
+
+    transfer_columns: bool = True
+    """Whether to read and transfer columns or rows.
+
+    If set, read is done with an additional forward pass
+    determined by the ``transfer_forward`` settings. If not set, rows
+    are transferred instead, that is, the read is done internally
+    with a backward pass instead. However, the parameters defining the
+    backward are still given by setting the ``transfer_forward`` field for
+    convenience.
+    """
+
+    fast_lr: float = 1.0
+    """Whether to set the `fast` tile's learning rate.
+
+    If set, then the SGD gradient update onto the first (fast) tile is
+    set to this learning rate and is kept constant even when the SGD
+    learning rate is scheduled. The SGD learning rate is then only
+    used to scale the transfer LR (see ``scale_transfer_lr``).
+    """
+
+    transfer_lr: float = 1.0
+    """Learning rate (LR) for the update step of the transfer event.
+
+    Per default all learning rates are identical. If ``scale_transfer_lr`` is
+    set, the transfer LR is scaled by current learning rate of the SGD.
+
+    Note:
+        LR is always a positive number, sign will be correctly
+        applied internally.
+    """
+
+    scale_transfer_lr: bool = True
+    """Whether to give the transfer_lr in relative units.
+
+    ie. whether to scale the transfer LR with the current LR of the SGD.
+    """
+    experimental_adjust_auto_scale_with_transfer_every: bool = False
+    """ (EXPERIMENTAL). Adjusts the auto scale with transfer every ratio """
+
+    transfer_forward: IOParameters = field(default_factory=IOParameters)
+    """Input-output parameters that define the read of a transfer event.
+
+    :class:`~aihwkit.simulator.parameters.utils.IOParameters` that define the read
+    (forward or backward) of an transfer event. For instance the amount of noise
+    or whether transfer is done using a ADC/DAC etc.
+    """
+
+    transfer_update: UpdateParameters = field(
+        default_factory=lambda: UpdateParameters(
+            desired_bl=1, update_bl_management=False, update_management=False
+        )
+    )
+    """Update parameters that define the type of update used for each transfer
+    event.
+
+    Update parameters :class:`~aihwkit.simulator.parameters.utils.UpdateParameters` that
+    define the type of update used for each transfer event.
+    """
+
+    def as_bindings(self) -> devices.ChoppedTransferResistiveDeviceParameter:
+        """Return a representation of this instance as a simulator bindings object."""
+        if not isinstance(self.unit_cell_devices, list):
+            raise ConfigError("unit_cell_devices should be a list of devices")
+
+        n_devices = len(self.unit_cell_devices)
+        if n_devices != 2:
+            raise ConfigError("Only 2 devices supported for ChoppedTransferCompound")
+
+        transfer_parameters = parameters_to_bindings(self)
+
+        param_fast = self.unit_cell_devices[0].as_bindings()
+        param_slow = self.unit_cell_devices[1].as_bindings()
+
+        if not transfer_parameters.append_parameter(param_fast):
+            raise ConfigError("Could not add unit cell device parameter")
+
+        for _ in range(n_devices - 1):
+            if not transfer_parameters.append_parameter(param_slow):
+                raise ConfigError("Could not add unit cell device parameter")
+
+        return transfer_parameters
+
+
+@dataclass
+class DynamicTransferCompound(ChoppedTransferCompound):
+    r"""Abstract device model that takes exactly two devices and
+    implements a chopped and buffered transfer-based learning rule.
+
+    Similar to :class:`ChoppedTransferCompound`, however, the gradient
+    update onto the fast tile is done with a statistically motivated
+    gradient computation: The mean of the reads during the last
+    chopper period is compared with the current (switched) chopper
+    period and the update (transfer) onto the slow matrix is
+    proportionally to the difference. In addition, no update is done
+    if the difference is not significantly different from zero, judged
+    by the running std estimation (thus computing the standard error
+    of the mean).
+
+    Note that the choices of transfer are similarly restricted as in
+    the :class:`ChoppedTransferCompound`, to enable a fast CUDA
+    optimization of the transfer. In particular, only 2 devices are
+    supported, transfer has to be sequential with exactly one read at
+    each transfer event (that is the settings
+    ``random_selection=False``, ``with_reset_prob=0.0``,
+    ``n_reads_per_transfer=1``).
+
+    """
+
+    bindings_class: ClassVar[Type] = devices.DynamicTransferResistiveDeviceParameter
+
+    in_chop_prob: float = 0.1
+    """Switching probability of the input choppers. The chopper will be
+    switched with the given frequency once after the corresponding
+    vector read (column or row).
+
+    Note:
+       In contrast to :class:`ChoppedTransferCompound` here the
+       chopping periods are regular with the switches occurring with
+       frequency of the given value
+    """
+
+    in_chop_random: bool = False
+    """Whether to switch randomly or regular (default).
+
+    If regular, then the ``in_chop_prob`` sets the frequency of
+    switching, ie. ``MIN(1/in_chop_prob, 2)`` is the period of
+    switching in terms of number of reads of a particular row /
+    col. All rows/cols will switch at the same matrix update cycle.
+    """
+
+    out_chop_prob: float = 0.0
+    """Switching probability of the output choppers. The chopper will be
+    switched with the given probability once after a full matrix
+    update has been accomplished.
+    """
+
+    always_write: bool = True
+    """Whether to always write (to the weight matrix), or only write after
+    each chopper period."""
+
+    thres_scale: float = 0.0
+    """Threshold scale for buffer to determine whether to transfer to next
+    device. This is multiplied with the running standard deviation.
+
+    Setting the this to 0.0 mean that the standard deviation
+    estimation and threshold computation is omitted and the difference
+    is updated onto the buffer directly without scaling.
+    """
+
+    step: float = 1.0
+    """Value to fill the ``d`` vector for the update if buffered value is
+    above threshold.
+    """
+
+    momentum: float = 0.0
+    """Momentum.
+
+    If enabled an additional momentum matrix is used that is filtering
+    the computed weight update in the usual manner.
+    """
+
+    transfer_columns: bool = True
+    """Whether to read and transfer columns or rows.
+
+    If set, read is done with an additional forward pass
+    determined by the ``transfer_forward`` settings. If not set, rows
+    are transferred instead, that is, the read is done internally
+    with a backward pass instead. However, the parameters defining the
+    backward are still given by setting the ``transfer_forward`` field for
+    convenience.
+    """
+
+    buffer_granularity: float = 1.0
+    """ Granularity if the buffer. """
+
+    buffer_cap: float = 0.0
+    """Capacity of buffer.
+
+    Capacity in times of max steps
+    (``transfer_update.desired_bl``). Only applied in case of
+    ``forget_buffer=False``
+    """
+
+    forget_buffer: bool = True
+    """Whether to forget the value of the buffer after transfer.
+
+    If enabled, the buffer is reset to the momentum times the
+    transferred value. Thus, if the number of pulses is limited to
+    e.g. 1 (``desired_BL`` in the ``transfer_update``) the transfer
+    might be clipped and the potentially larger buffer values are
+    forgotten. If disabled, then the buffer values are faithfully
+    subtracted by the amount transferred (times one minus momentum).
+    """
+
+    fast_lr: float = 1.0
+    """Whether to set the `fast` tile's learning rate.
+
+    If set, then the SGD gradient update onto the first (fast) tile is
+    set to this learning rate and is kept constant even when the SGD
+    learning rate is scheduled. The SGD learning rate is then only
+    used to scale the transfer LR (see ``scale_transfer_lr``).
+    """
+
+    transfer_lr: float = 1.0
+    """Learning rate (LR) for the update step of the transfer event.
+
+    Per default all learning rates are identical. If ``scale_transfer_lr`` is
+    set, the transfer LR is scaled by current learning rate of the SGD.
+
+    Note:
+        LR is always a positive number, sign will be correctly
+        applied internally.
+    """
+
+    scale_transfer_lr: bool = True
+    """Whether to give the transfer_lr in relative units.
+
+    ie. whether to scale the transfer LR with the current LR of the SGD.
+    """
+
+    scale_thres_with_samples: bool = True
+    """Whether to divide the threshold with the square of the number of
+    samples in the current chopper period."""
+
+    tail_weightening: float = 5.0
+    """Weight the tail of the chopper period more (if larger than 1). This
+    helps to reduce the impact of the transient period"""
+
+    transfer_forward: IOParameters = field(default_factory=IOParameters)
+
+    """Input-output parameters that define the read of a transfer event.
+
+    :class:`~aihwkit.simulator.parameters.utils.IOParameters`
+    that define the read (forward or backward) of an transfer
+    event. For instance the amount of noise or whether transfer is
+    done using a ADC/DAC etc.
+    """
+
+    transfer_update: UpdateParameters = field(
+        default_factory=lambda: UpdateParameters(
+            desired_bl=1, update_bl_management=False, update_management=False
+        )
+    )
+    """Update parameters that define the type of update used for each transfer
+    event.
+
+    Update parameters :class:`~aihwkit.simulator.parameters.utils.UpdateParameters` that
+    define the type of update used for each transfer event.
+    """
+
+    def as_bindings(self) -> devices.ChoppedTransferResistiveDeviceParameter:
+        """Return a representation of this instance as a simulator bindings object."""
+        if not isinstance(self.unit_cell_devices, list):
+            raise ConfigError("unit_cell_devices should be a list of devices")
+
+        n_devices = len(self.unit_cell_devices)
+        if n_devices != 2:
+            raise ConfigError("Only 2 devices supported for ChoppedTransferCompound")
+
+        transfer_parameters = parameters_to_bindings(self)
+
+        param_fast = self.unit_cell_devices[0].as_bindings()
+        param_slow = self.unit_cell_devices[1].as_bindings()
+
+        if not transfer_parameters.append_parameter(param_fast):
+            raise ConfigError("Could not add unit cell device parameter")
+
+        for _ in range(n_devices - 1):
+            if not transfer_parameters.append_parameter(param_slow):
+                raise ConfigError("Could not add unit cell device parameter")
+
+        return transfer_parameters
+
+
 ###############################################################################
 # Specific compound-devices with digital rank update
 ###############################################################################
 
+
 @dataclass
 class DigitalRankUpdateCell(_PrintableMixin):
     """Parameters that modify the behavior of the digital rank update cell.
 
     This is the base class for devices that compute the rank update in
     digital and then (occasionally) transfer the information to the
     (analog) crossbar array that is used during forward and backward.
     """
 
     bindings_class: ClassVar[Type] = devices.AbstractResistiveDeviceParameter
 
-    bindings_ignore: ClassVar[List] = ['diffusion', 'lifetime']
+    bindings_ignore: ClassVar[List] = ["diffusion", "lifetime"]
 
-    device: Union['PulsedDevice',
-                  OneSidedUnitCell,
-                  VectorUnitCell,
-                  ReferenceUnitCell] = field(
-                      default_factory=VectorUnitCell)
+    device: Union["PulsedDevice", OneSidedUnitCell, VectorUnitCell, ReferenceUnitCell] = field(
+        default_factory=VectorUnitCell
+    )
     """(Analog) device that are used for forward and backward."""
 
     construction_seed: int = 0
     """If not ``0``, set a unique seed for hidden parameters during
     construction.
 
     Applies to ``device``.
@@ -735,10 +1114,10 @@
 
     def as_bindings(self) -> devices.MixedPrecResistiveDeviceParameter:
         """Return a representation of this instance as a simulator bindings object."""
         mixed_prec_parameter = parameters_to_bindings(self)
         param_device = self.device.as_bindings()
 
         if not mixed_prec_parameter.set_device_parameter(param_device):
-            raise ConfigError('Could not add device parameter')
+            raise ConfigError("Could not add device parameter")
 
         return mixed_prec_parameter
```

### Comparing `aihwkit-0.7.1/src/aihwkit/simulator/configs/configs.py` & `aihwkit-0.8.0/src/aihwkit/simulator/configs/configs.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,153 +1,198 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
 # that they have been altered from the originals.
 
 """Configurations for resistive processing units."""
 
+# pylint: disable=too-few-public-methods
+
 from dataclasses import dataclass, field
-from typing import ClassVar, Type, Optional
+from typing import ClassVar, Type, Optional, Union
+
+from aihwkit.simulator.parameters.base import PrePostProcessingRPU, MappableRPU
 
 from aihwkit.simulator.configs.devices import (
-    ConstantStepDevice, FloatingPointDevice, IdealDevice, PulsedDevice
-)
-from aihwkit.simulator.configs.compounds import (
-    DigitalRankUpdateCell, UnitCell,
-)
-from aihwkit.simulator.configs.helpers import (
-    _PrintableMixin, tile_parameters_to_bindings
+    ConstantStepDevice,
+    FloatingPointDevice,
+    IdealDevice,
+    PulsedDevice,
 )
-from aihwkit.simulator.configs.utils import (
-    IOParameters, PulseType, UpdateParameters, WeightClipParameter,
-    WeightModifierParameter, WeightRemapParameter,
-    MapableRPU, PrePostProcessingRPU
+from aihwkit.simulator.configs.compounds import DigitalRankUpdateCell, UnitCell, TransferCompound
+from aihwkit.simulator.parameters.utils import (
+    IOParameters,
+    PulseType,
+    UpdateParameters,
+    WeightClipParameter,
+    WeightModifierParameter,
+    WeightRemapParameter,
 )
 from aihwkit.inference import (
-    BaseDriftCompensation, BaseNoiseModel, GlobalDriftCompensation,
-    PCMLikeNoiseModel
+    BaseDriftCompensation,
+    BaseNoiseModel,
+    GlobalDriftCompensation,
+    PCMLikeNoiseModel,
 )
 from aihwkit.simulator.rpu_base import devices
-from aihwkit.simulator.tiles import AnalogTile, FloatingPointTile, InferenceTile
+from aihwkit.simulator.tiles import AnalogTile, FloatingPointTile, InferenceTile, TorchInferenceTile
+from aihwkit.simulator.tiles.torch_tile import TorchSimulatorTile
+from aihwkit.simulator.tiles.array import TileModuleArray
 
 
 @dataclass
-class FloatingPointRPUConfig(MapableRPU, PrePostProcessingRPU, _PrintableMixin):
+class FloatingPointRPUConfig(MappableRPU, PrePostProcessingRPU):
     """Configuration for a floating point resistive processing unit."""
 
     tile_class: ClassVar[Type] = FloatingPointTile
     """Tile class that correspond to this RPUConfig."""
 
+    tile_array_class: ClassVar[Type] = TileModuleArray
+    """Tile class used for mapped logical tile arrays."""
+
     device: FloatingPointDevice = field(default_factory=FloatingPointDevice)
     """Parameter that modify the behavior of the pulsed device."""
 
 
 @dataclass
-class SingleRPUConfig(MapableRPU, PrePostProcessingRPU, _PrintableMixin):
+class SingleRPUConfig(MappableRPU, PrePostProcessingRPU):
     """Configuration for an analog (pulsed device) resistive processing unit."""
 
     tile_class: ClassVar[Type] = AnalogTile
     """Tile class that correspond to this RPUConfig."""
 
+    tile_array_class: ClassVar[Type] = TileModuleArray
+    """Tile class used for mapped logical tile arrays."""
+
     bindings_class: ClassVar[Type] = devices.AnalogTileParameter
 
     device: PulsedDevice = field(default_factory=ConstantStepDevice)
     """Parameter that modify the behavior of the pulsed device."""
 
     forward: IOParameters = field(default_factory=IOParameters)
     """Input-output parameter setting for the forward direction."""
 
     backward: IOParameters = field(default_factory=IOParameters)
     """Input-output parameter setting for the backward direction."""
 
     update: UpdateParameters = field(default_factory=UpdateParameters)
     """Parameter for the update behavior."""
 
-    def as_bindings(self) -> devices.AnalogTileParameter:
-        """Return a representation of this instance as a simulator bindings object."""
-        return tile_parameters_to_bindings(self)
-
 
 @dataclass
-class UnitCellRPUConfig(MapableRPU, PrePostProcessingRPU, _PrintableMixin):
+class UnitCellRPUConfig(MappableRPU, PrePostProcessingRPU):
     """Configuration for an analog (unit cell) resistive processing unit."""
 
     tile_class: ClassVar[Type] = AnalogTile
     """Tile class that correspond to this RPUConfig."""
 
+    tile_array_class: ClassVar[Type] = TileModuleArray
+    """Tile class used for mapped logical tile arrays."""
+
     bindings_class: ClassVar[Type] = devices.AnalogTileParameter
 
-    device: UnitCell = field(default_factory=UnitCell)
+    device: Union[UnitCell, TransferCompound] = field(default_factory=UnitCell)
     """Parameter that modify the behavior of the pulsed device."""
 
     forward: IOParameters = field(default_factory=IOParameters)
     """Input-output parameter setting for the forward direction."""
 
     backward: IOParameters = field(default_factory=IOParameters)
     """Input-output parameter setting for the backward direction."""
 
     update: UpdateParameters = field(default_factory=UpdateParameters)
     """Parameter for the parallel analog update behavior."""
 
-    def as_bindings(self) -> devices.AnalogTileParameter:
-        """Return a representation of this instance as a simulator bindings object."""
-        return tile_parameters_to_bindings(self)
+
+@dataclass
+class DigitalRankUpdateRPUConfig(MappableRPU, PrePostProcessingRPU):
+    """Configuration for an analog (unit cell) resistive processing unit
+    where the rank update is done in digital.
+
+    Note that for forward and backward, an analog crossbar is still
+    used, and during update the digitally computed rank update is
+    transferred to the analog crossbar using pulses.
+    """
+
+    tile_class: ClassVar[Type] = AnalogTile
+    """Tile class that correspond to this RPUConfig."""
+
+    tile_array_class: ClassVar[Type] = TileModuleArray
+    """Tile class used for mapped logical tile arrays."""
+
+    bindings_class: ClassVar[Type] = devices.AnalogTileParameter
+
+    device: DigitalRankUpdateCell = field(default_factory=DigitalRankUpdateCell)
+    """Parameter that modify the behavior of the pulsed device."""
+
+    forward: IOParameters = field(default_factory=IOParameters)
+    """Input-output parameter setting for the forward direction."""
+
+    backward: IOParameters = field(default_factory=IOParameters)
+    """Input-output parameter setting for the backward direction."""
+
+    update: UpdateParameters = field(default_factory=UpdateParameters)
+    """Parameter for the analog part of the update, that is the transfer
+    from the digital buffer to the devices."""
 
 
 @dataclass
-class InferenceRPUConfig(MapableRPU, PrePostProcessingRPU, _PrintableMixin):
+class InferenceRPUConfig(MappableRPU, PrePostProcessingRPU):
     """Configuration for an analog tile that is used only for inference.
 
     Training is done in *hardware-aware* manner, thus using only the
     non-idealities of the forward-pass, but backward and update passes
     are ideal.
 
     During inference, statistical models of programming, drift
     and read noise can be used.
     """
+
     # pylint: disable=too-many-instance-attributes
 
     tile_class: ClassVar[Type] = InferenceTile
     """Tile class that correspond to this RPUConfig."""
 
+    tile_array_class: ClassVar[Type] = TileModuleArray
+    """Tile class used for mapped logical tile arrays."""
+
     bindings_class: ClassVar[Type] = devices.AnalogTileParameter
 
     forward: IOParameters = field(default_factory=IOParameters)
     """Input-output parameter setting for the forward direction.
 
     This parameters govern the hardware definitions specifying analog
     MVM non-idealities.
 
     Note:
 
         This forward pass is applied equally in training and
         inference. In addition, materials effects such as drift and
         programming noise can be enabled during inference by
         specifying the ``noise_model``
-
     """
 
     noise_model: BaseNoiseModel = field(default_factory=PCMLikeNoiseModel)
     """Statistical noise model to be used during (realistic) inference.
 
     This noise models establishes a phenomenological model of the
     material which is applied to the weights during inference only, when
     ``program_analog_weights`` or ``drift_analog_weights`` is called.
-
     """
 
     drift_compensation: Optional[BaseDriftCompensation] = field(
-        default_factory=GlobalDriftCompensation)
+        default_factory=GlobalDriftCompensation
+    )
     """For compensating the drift during inference only."""
 
     clip: WeightClipParameter = field(default_factory=WeightClipParameter)
     """Parameter for weight clip.
 
     If a clipping type is set, the weights are clipped according to
     the type specified.
@@ -181,59 +226,52 @@
     model becomes more noise robust during inference (e.g. when the
     ``noise_model`` is employed).
     """
 
     # The following fields are not included in `__init__`, and should be
     # treated as read-only.
 
-    device: IdealDevice = field(default_factory=IdealDevice,
-                                init=False)
+    device: IdealDevice = field(default_factory=IdealDevice, init=False)
     """Parameter that modify the behavior of the pulsed device: ideal device."""
 
     backward: IOParameters = field(
-        default_factory=lambda: IOParameters(is_perfect=True),
-        init=False
+        default_factory=lambda: IOParameters(is_perfect=True), init=False
     )
     """Input-output parameter setting for the backward direction: perfect."""
 
     update: UpdateParameters = field(
-        default_factory=lambda: UpdateParameters(pulse_type=PulseType.NONE),
-        init=False
+        default_factory=lambda: UpdateParameters(pulse_type=PulseType.NONE), init=False
     )
     """Parameter for the update behavior: ``NONE`` pulse type."""
 
-    def as_bindings(self) -> devices.AnalogTileParameter:
-        """Return a representation of this instance as a simulator bindings object."""
-        return tile_parameters_to_bindings(self)
+    def compatible_with(self, tile_class_name: str) -> bool:
+        if tile_class_name in ["TorchInferenceTile"]:
+            return True
+        return tile_class_name == self.tile_class.__name__
 
 
 @dataclass
-class DigitalRankUpdateRPUConfig(MapableRPU, PrePostProcessingRPU, _PrintableMixin):
-    """Configuration for an analog (unit cell) resistive processing unit
-    where the rank update is done in digital.
-
-    Note that for forward and backward, an analog crossbar is still
-    used, and during update the digitally computed rank update is
-    transferred to the analog crossbar using pulses.
-    """
+class TorchInferenceRPUConfig(InferenceRPUConfig):
+    """TorchInference configuration.
 
-    tile_class: ClassVar[Type] = AnalogTile
-    """Tile class that correspond to this RPUConfig."""
-
-    bindings_class: ClassVar[Type] = devices.AnalogTileParameter
+    This configuration defaults to a tile module implementation that
+    supported a subset of functions of the ``InferenceRPUConfig`` but
+    uses native torch instead of the RPUCuda library for simulating
+    the analog MVM.
+
+    The advantage is that autograd is more fully supported and
+    hardware aware training is more flexible to be modified. However,
+    some nonidealities are not supported.
 
-    device: DigitalRankUpdateCell = field(default_factory=DigitalRankUpdateCell)
-    """Parameter that modify the behavior of the pulsed device."""
+    Note:
 
-    forward: IOParameters = field(default_factory=IOParameters)
-    """Input-output parameter setting for the forward direction."""
+        For features that are not supported a ``NotImplementedError`` or a
+        ``TorchTileConfigError`` is raised.
+    """
 
-    backward: IOParameters = field(default_factory=IOParameters)
-    """Input-output parameter setting for the backward direction."""
+    simulator_tile_class: ClassVar[Type] = TorchSimulatorTile
 
-    update: UpdateParameters = field(default_factory=UpdateParameters)
-    """Parameter for the analog part of the update, that is the transfer
-    from the digital buffer to the devices."""
+    tile_class: ClassVar[Type] = TorchInferenceTile
+    """Tile class that correspond to this RPUConfig."""
 
-    def as_bindings(self) -> devices.AnalogTileParameter:
-        """Return a representation of this instance as a simulator bindings object."""
-        return tile_parameters_to_bindings(self)
+    tile_array_class: ClassVar[Type] = TileModuleArray
+    """Tile class used for mapped logical tile arrays."""
```

### Comparing `aihwkit-0.7.1/src/aihwkit/simulator/configs/devices.py` & `aihwkit-0.8.0/src/aihwkit/simulator/configs/devices.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -14,27 +14,29 @@
 
 # pylint: disable=too-many-instance-attributes, too-many-lines
 
 from dataclasses import dataclass, field
 from typing import ClassVar, List, Type
 from numpy import exp
 
-from aihwkit.simulator.configs.helpers import (
-    _PrintableMixin, parameters_to_bindings
-)
-from aihwkit.simulator.configs.utils import (
-    DriftParameter, SimpleDriftParameter
-)
+from aihwkit.simulator.parameters.helpers import _PrintableMixin, parameters_to_bindings
+from aihwkit.simulator.parameters.utils import DriftParameter, SimpleDriftParameter
 from aihwkit.simulator.rpu_base import devices
 
 # legacy
 from aihwkit.simulator.configs.compounds import (  # pylint: disable=unused-import
-    VectorUnitCell, ReferenceUnitCell,
-    OneSidedUnitCell, DifferenceUnitCell, TransferCompound,
-    BufferedTransferCompound, MixedPrecisionCompound
+    VectorUnitCell,
+    ReferenceUnitCell,
+    OneSidedUnitCell,
+    DifferenceUnitCell,
+    TransferCompound,
+    BufferedTransferCompound,
+    MixedPrecisionCompound,
+    DynamicTransferCompound,
+    ChoppedTransferCompound,
 )
 
 
 @dataclass
 class FloatingPointDevice(_PrintableMixin):
     """Floating point reference.
 
@@ -127,15 +129,15 @@
     Note:
         If diffusion happens to move the weight beyond the hard bounds of the
         weight it is ensured to be clipped appropriately.
 
     **Drift**:
 
     Optional power-law drift setting, as described in
-    :class:`~aihwkit.similar.configs.utils.DriftParameter`.
+    :class:`~aihwkit.similar.parameters.utils.DriftParameter`.
 
     Important:
         Similar to reset, drift is *not* applied automatically each
         mini-batch but requires an explicit call to
         :meth:`~aihwkit.simulator.tiles.base.Base.drift_weights` each
         time the drift should be applied.
 
@@ -156,16 +158,17 @@
 
     diffusion: float = 0.0
     """Standard deviation of diffusion process."""
 
     diffusion_dtod: float = 0.0
     """Device-to device variation of diffusion rate in relative units."""
 
-    drift: DriftParameter = field(default_factory=DriftParameter,
-                                  metadata={'hide_if': DriftParameter()})
+    drift: DriftParameter = field(
+        default_factory=DriftParameter, metadata={"hide_if": DriftParameter()}
+    )
     """Parameter governing a power-law drift."""
 
     dw_min: float = 0.001
     """Mean of the minimal update step sizes across devices and directions."""
 
     dw_min_dtod: float = 0.3
     """Device-to-device std deviation of ``dw_min`` (in relative units to
@@ -266,14 +269,22 @@
     """Device-to-device variation of the hard bounds.
 
     Device-to-device variation of the hard bounds, of min and max value,
     respectively. All are given in relative units to ``w_min``, or ``w_max``,
     respectively.
     """
 
+    count_pulses: bool = False
+    """Whether to count the positive and negative pulses that were applied.
+
+    Only for GPU devices currently implemented. Some runtime penalty expected.
+
+    Pulses can be obtained by ``analog_tile.tile.get_pulse_counters()``
+    """
+
     def as_bindings(self) -> devices.PulsedResistiveDeviceParameter:
         """Return a representation of this instance as a simulator bindings object."""
         return parameters_to_bindings(self)
 
     def requires_diffusion(self) -> bool:
         """Return whether device has diffusion enabled."""
         return self.diffusion > 0.0
@@ -283,14 +294,15 @@
         return self.lifetime > 0.0
 
 
 ###############################################################################
 # Specific devices based on ``pulsed``.
 ###############################################################################
 
+
 @dataclass
 class IdealDevice(_PrintableMixin):
     """Ideal update behavior (using floating point), but forward/backward
     might be non-ideal.
 
     Ideal update behavior (using floating point), however,
     forward/backward might still have a non-ideal ADC or noise added.
@@ -304,14 +316,17 @@
 
     diffusion: float = 0.0
     """Standard deviation of diffusion process."""
 
     lifetime: float = 0.0
     r"""One over `decay_rate`, ie :math:`1/r_\text{decay}`."""
 
+    reset_std: float = 0.01
+    """Standard deviation around zero mean in case reset is called."""
+
     def as_bindings(self) -> devices.IdealResistiveDeviceParameter:
         """Return a representation of this instance as a simulator bindings object."""
         return parameters_to_bindings(self)
 
     def requires_diffusion(self) -> bool:
         """Return whether device has diffusion enabled."""
         return self.diffusion > 0.0
@@ -485,14 +500,22 @@
     .. math::
         w_\text{apparent}{ij} = w_{ij} + \sigma_\text{write_noise} \Delta w_\text{min}\xi
 
     and the update is done on :math:`w_{ij}` but the forward sees the
     :math:`w_\text{apparent}`.
     """
 
+    apply_write_noise_on_set: bool = True
+    r"""Whether setting the weights with ``set_weights`` will add
+    write noise to the apparent weight state or not.
+
+    If ``False`` the persistent weight state will be equal to the
+    apparent state initially.
+    """
+
     reverse_up: bool = False
     """Whether to increase the step size in up direction with increasing
     weights (default decreases).
 
     Note:
         If set, ``mult_noise`` needs to be also set.
     """
@@ -543,14 +566,22 @@
     .. math::
         w_\text{apparent}{ij} = w_{ij} + \sigma_\text{write_noise} \Delta w_\text{min}\xi
 
     and the update is done on :math:`w_{ij}` but the forward sees the
     :math:`w_\text{apparent}`.
     """
 
+    apply_write_noise_on_set: bool = True
+    r"""Whether setting the weights with ``set_weights`` will add
+    write noise to the apparent weight state or not.
+
+    If ``False`` the persistent weight state will be equal to the
+    apparent state initially.
+    """
+
     reverse_up: bool = False
     """Whether to increase the step size in up direction with increasing
     weights (default decreases).
 
     Note:
         If set, ``mult_noise`` needs to be also set.
     """
@@ -614,39 +645,39 @@
         which will vary ``w_min`` and ``w_max`` across devices,
         respectively.
     """
 
     p_max: int = 1000
     """Number of pulses to drive the synapse from ``range_min`` to ``range_max``."""
 
-    alpha: float = 0.001/2
+    alpha: float = 0.001 / 2
     r"""The slope of the soft bounds model :math:`dw \propto \alpha w` for both
     up and down direction."""
 
     range_min: float = -1.0
     """Setting of the weight when starting the :math:`P_max` up pulse
     experiment."""
 
     range_max: float = 1.0
     """Value of the weight for :math:`P_max` number of up pulses."""
 
     #  these values will be set from the above, so we hide it.
-    w_min: float = field(default_factory=lambda: None, metadata={'hide_if': None})  # type: ignore
-    w_max: float = field(default_factory=lambda: None, metadata={'hide_if': None})  # type: ignore
-    dw_min: float = field(default_factory=lambda: None, metadata={'hide_if': None})  # type: ignore
-    up_down: float = field(default_factory=lambda: None, metadata={'hide_if': None})  # type: ignore
+    w_min: float = field(default_factory=lambda: None, metadata={"hide_if": None})  # type: ignore
+    w_max: float = field(default_factory=lambda: None, metadata={"hide_if": None})  # type: ignore
+    dw_min: float = field(default_factory=lambda: None, metadata={"hide_if": None})  # type: ignore
+    up_down: float = field(default_factory=lambda: None, metadata={"hide_if": None})  # type: ignore
 
     def as_bindings(self) -> devices.PulsedResistiveDeviceParameter:
         """Return a representation of this instance as a simulator bindings object."""
         params = SoftBoundsDevice()
         for key, value in self.__dict__.items():
-            if key not in ['range_min', 'range_max', 'alpha', 'p_max']:
+            if key not in ["range_min", "range_max", "alpha", "p_max"]:
                 setattr(params, key, value)
 
-        b_factor = (self.range_max - self.range_min)/(1 - exp(-self.p_max * self.alpha))
+        b_factor = (self.range_max - self.range_min) / (1 - exp(-self.p_max * self.alpha))
         params.w_min = self.range_min
         params.w_max = self.range_min + b_factor
         params.dw_min = b_factor * self.alpha
         params.up_down = 1 + 2 * self.range_min / b_factor
 
         return parameters_to_bindings(params)
 
@@ -693,14 +724,22 @@
     .. math::
         w_\text{apparent}{ij} = w_{ij} + \sigma_\text{write_noise} \Delta w_\text{min}\xi
 
     and the update is done on :math:`w_{ij}` but the forward sees the
     :math:`w_\text{apparent}`.
     """
 
+    apply_write_noise_on_set: bool = True
+    r"""Whether setting the weights with ``set_weights`` will add
+    write noise to the apparent weight state or not.
+
+    If ``False`` the persistent weight state will be equal to the
+    apparent state initially.
+    """
+
     slope_up_dtod: float = 0.0
     r"""Device-to-device variation on the up-pulse slope.
 
     Note:
 
         Since the up slope is proportional to
         :math:`\propto\frac{1}{b_\text{max}}` the device-to-device variation
@@ -831,14 +870,22 @@
     .. math::
         w_\text{apparent}{ij} = w_{ij} + \sigma_\text{write_noise}\xi
 
     and the update is done on :math:`w_{ij}` but the forward sees the
     :math:`w_\text{apparent}`.
     """
 
+    apply_write_noise_on_set: bool = True
+    r"""Whether setting the weights with ``set_weights`` will add
+    write noise to the apparent weight state or not.
+
+    If ``False`` the persistent weight state will be equal to the
+    apparent state initially.
+    """
+
 
 @dataclass
 class PowStepDevice(PulsedDevice):
     r"""Pulsed update behavioral model: power-dependent step.
 
     Pulsed update behavioral model, where the update step response
     size of the material has a power-dependent with resistance. This
@@ -945,14 +992,22 @@
     .. math::
         w_\text{apparent}{ij} = w_{ij} + \sigma_\text{write_noise}\xi
 
     and the update is done on :math:`w_{ij}` but the forward sees the
     :math:`w_\text{apparent}`.
     """
 
+    apply_write_noise_on_set: bool = True
+    r"""Whether setting the weights with ``set_weights`` will add
+    write noise to the apparent weight state or not.
+
+    If ``False`` the persistent weight state will be equal to the
+    apparent state initially.
+    """
+
 
 @dataclass
 class PowStepReferenceDevice(PulsedDevice):
     r"""Pulsed update behavioral model: power-dependent step.
 
     Pulsed update behavioral model, where the update step response
     size of the material has a power-dependent with resistance. This
@@ -1168,7 +1223,15 @@
 
     .. math::
         w_\text{apparent}{ij} = w_{ij} + \sigma_\text{write_noise}\xi
 
     and the update is done on :math:`w_{ij}` but the forward sees the
     :math:`w_\text{apparent}`.
     """
+
+    apply_write_noise_on_set: bool = True
+    r"""Whether setting the weights with ``set_weights`` will add
+    write noise to the apparent weight state or not.
+
+    If ``False`` the persistent weight state will be equal to the
+    apparent state initially.
+    """
```

### Comparing `aihwkit-0.7.1/src/aihwkit/simulator/configs/enums.py` & `aihwkit-0.8.0/src/aihwkit/simulator/parameters/enums.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -24,33 +24,33 @@
     inputs iteratively halved, when the output bound was hit.
 
     Caution:
         Bound management is **only** available for the forward pass. It
         will be ignored when used for the backward pass.
     """
 
-    NONE = 'None'
+    NONE = "None"
     """No bound management."""
 
-    ITERATIVE = 'Iterative'
+    ITERATIVE = "Iterative"
     r"""Iteratively recomputes input scale set to :math:`\alpha\leftarrow\alpha/2`.
 
     It iteratively recomputes the bounds up to limit of passes (given by
     ``max_bm_factor`` or ``max_bm_res``).
     """
 
-    ITERATIVE_WORST_CASE = 'IterativeWorstCase'
+    ITERATIVE_WORST_CASE = "IterativeWorstCase"
     """Worst case bound management.
 
     Uses ``AbsMax`` noise management for the first pass and only when output
     bound is hit, the ``AbsMaxNPSum`` for the second. Thus, at most 2 passes
     are computed.
     """
 
-    SHIFT = 'Shift'
+    SHIFT = "Shift"
     """Shift bound management.
 
     Shifts the output by adding the difference ``output_bound - max_output`` to
     the analog output value. This is only useful to increase the dynamic range
     before the softmax, where the max can be safely.
 
     Note:
@@ -62,35 +62,35 @@
     r"""Noise management type.
 
     Noise management determines a factor :math:`\alpha` how the input is reduced:
 
     .. math:: \mathbf{y} = \alpha\;F_\text{analog-mac}\left(\mathbf{x}/\alpha\right)
     """
 
-    NONE = 'None'
+    NONE = "None"
     """No noise management."""
 
-    ABS_MAX = 'AbsMax'
+    ABS_MAX = "AbsMax"
     r"""Use :math:`\alpha\equiv\max{|\mathbf{x}|}`."""
 
-    ABS_MAX_NP_SUM = 'AbsMaxNPSum'
+    ABS_MAX_NP_SUM = "AbsMaxNPSum"
     """Assume weight value is constant and given by ``nm_assumed_wmax``.
 
     Takes a worst case scenario of the weight matrix to calculate the input
     scale to ensure that output is not clipping. Assumed weight value is
     constant and given by ``nm_assumed_wmax``.
     """
 
-    MAX = 'Max'
+    MAX = "Max"
     r"""Use :math:`\alpha\equiv\max{\mathbf{x}}`."""
 
-    CONSTANT = 'Constant'
+    CONSTANT = "Constant"
     r"""A constant value (given by parameter ``nm_thres``)."""
 
-    AVERAGE_ABS_MAX = 'AverageAbsMax'
+    AVERAGE_ABS_MAX = "AverageAbsMax"
     """Moment-based scale input scale estimation.
 
     Computes the average abs max over the mini-batch and applies ``nm_decay``
     to update the value with the history.
 
     Note:
         ``nm_decay`` is ``1-momentum`` and always given in mini-batches.
@@ -104,24 +104,24 @@
 
     The weight noise is applied for each MAC computation, while not
     touching the actual weight matrix but referring it to the output.
 
     .. math:: y_i = \sum_j w_{ij}+\xi_{ij}
     """
 
-    NONE = 'None'
+    NONE = "None"
     """No weight noise."""
 
-    ADDITIVE_CONSTANT = 'AdditiveConstant'
+    ADDITIVE_CONSTANT = "AdditiveConstant"
     r"""The :math:`\xi\sim{\cal N}(0,\sigma)` thus all are Gaussian distributed.
 
     :math:`\sigma` is determined by ``w_noise``.
     """
 
-    PCM_READ = 'PCMRead'
+    PCM_READ = "PCMRead"
     """Output-referred PCM-like read noise.
 
     Output-referred PCM-like read noise that scales with the amount of current
     generated for each output line and thus scales with both conductance values
     and input strength.
 
     The same general for is taken as for PCM-like statistical model of the 1/f
@@ -129,142 +129,161 @@
     :class:`aihwkit.inference.noise.pcm.PCMLikeNoiseModel`.
     """
 
 
 class PulseType(Enum):
     """Pulse type."""
 
-    NONE = 'None'
+    NONE = "None"
     """Floating point update instead of pulses."""
 
-    STOCHASTIC_COMPRESSED = 'StochasticCompressed'
+    STOCHASTIC_COMPRESSED = "StochasticCompressed"
     """Generates actual stochastic bit lines.
 
     Plus and minus pulses are taken in the same pass.
     """
 
-    STOCHASTIC = 'Stochastic'
+    STOCHASTIC = "Stochastic"
     """Two passes for plus and minus (only CPU)."""
 
-    NONE_WITH_DEVICE = 'NoneWithDevice'
+    NONE_WITH_DEVICE = "NoneWithDevice"
     """Floating point like ``None``, but with analog devices (e.g. weight
     clipping)."""
 
-    MEAN_COUNT = 'MeanCount'
+    MEAN_COUNT = "MeanCount"
     """Coincidence based in prob (:math:`p_a p_b`)."""
 
-    DETERMINISTIC_IMPLICIT = 'DeterministicImplicit'
+    DETERMINISTIC_IMPLICIT = "DeterministicImplicit"
     r"""Coincidences are computed in deterministic manner.
 
     Coincidences are calculated by :math:`b_l x_q d_q` where ``BL`` is the
     desired bit length (possibly subject to dynamic adjustments using
     ``update_bl_management``) and :math:`x_q` and :math:`d_q` are the quantized
     input and error values, respectively, normalized to the range
     :math:`0,\ldots,1`. It can be shown that explicit bit lines exist that
     generate these coincidences.
     """
 
 
 class WeightModifierType(Enum):
     """Weight modifier type."""
 
-    COPY = 'Copy'
-    """Just copy, however, could also drop."""
+    NONE = "None"
+    """No weight modifier. Nothing happens to the weight. """
 
-    DISCRETIZE = 'Discretize'
+    DISCRETIZE = "Discretize"
     """Quantize the weights."""
 
-    MULT_NORMAL = 'MultNormal'
+    MULT_NORMAL = "MultNormal"
     """Multiplicative Gaussian noise."""
 
-    ADD_NORMAL = 'AddNormal'
+    ADD_NORMAL = "AddNormal"
     """Additive Gaussian noise."""
 
-    DISCRETIZE_ADD_NORMAL = 'DiscretizeAddNormal'
+    DISCRETIZE_ADD_NORMAL = "DiscretizeAddNormal"
     """First discretize and then additive Gaussian noise."""
 
-    DOREFA = 'DoReFa'
+    DOREFA = "DoReFa"
     """DoReFa discretization."""
 
-    POLY = 'Poly'
+    POLY = "Poly"
     r"""Nth order Polynomial noise model (in terms of the weight value).
 
     In detail, for the duration of a mini-batch, each weight will be
     added a Gaussian random number with the standard deviation of
     :math:`\sigma_\text{wnoise} (c_0 + c_1 w_{ij}/\omega +
     c_N w_{ij}^N/\omega^N)`, where :math:`omega` is either the actual
     max weight (if ``rel_to_actual_wmax`` is set) or the value
     ``assumed_wmax``.
     """
 
+    PROG_NOISE = "ProgNoise"
+    """Programming noise model added to the weight matrix during
+    training. Same as "POLY", except that weights are ensured to keep
+    the sign with noise mirrored at zero.
+    """
+
+    DROP_CONNECT = "DropConnect"
+    """Drop connect.
+
+    Note that if "pdrop > 0" this is applied. It drop connect can be
+    applied in conjunction to other modifiers as well.
+    """
+
+    COPY = "Copy"
+    """Legacy. No explicit weight modifier, however, pdrop is still observed.
+
+    Use DROP_CONNECT or NONE instead.
+    """
+
 
 class WeightClipType(Enum):
     """Weight clipper type."""
 
-    NONE = 'None'
+    NONE = "None"
     """None."""
 
-    FIXED_VALUE = 'FixedValue'
+    FIXED_VALUE = "FixedValue"
     """Clip to fixed value give, symmetrical around zero."""
 
-    LAYER_GAUSSIAN = 'LayerGaussian'
+    LAYER_GAUSSIAN = "LayerGaussian"
     """Calculates the second moment of the whole weight matrix and clips
     at ``sigma`` times the result symmetrically around zero."""
 
-    AVERAGE_CHANNEL_MAX = 'AverageChannelMax'
+    AVERAGE_CHANNEL_MAX = "AverageChannelMax"
     """Calculates the abs max of each output channel (row of the weight
     matrix) and takes the average as clipping value for all."""
 
 
 class WeightRemapType(Enum):
     """Weight clipper type."""
 
-    NONE = 'None'
+    NONE = "None"
     """None."""
 
-    LAYERWISE_SYMMETRIC = 'LayerwiseSymmetric'
+    LAYERWISE_SYMMETRIC = "LayerwiseSymmetric"
     """Remap according to the absolute max of the full weight matrix."""
 
-    CHANNELWISE_SYMMETRIC = 'ChannelwiseSymmetric'
+    CHANNELWISE_SYMMETRIC = "ChannelwiseSymmetric"
     """Remap each column (output channel) in respect to the absolute max."""
 
 
 class VectorUnitCellUpdatePolicy(Enum):
     """Vector unit cell update policy."""
 
-    ALL = 'All'
+    ALL = "All"
     """All devices updated simultaneously."""
 
-    SINGLE_FIXED = 'SingleFixed'
+    SINGLE_FIXED = "SingleFixed"
     """Device index is not changed. Can be set initially and/or updated on
     the fly."""
 
-    SINGLE_SEQUENTIAL = 'SingleSequential'
+    SINGLE_SEQUENTIAL = "SingleSequential"
     """Each device one at a time in sequence."""
 
-    SINGLE_RANDOM = 'SingleRandom'
+    SINGLE_RANDOM = "SingleRandom"
     """A single device is selected by random choice each mini-batch."""
 
 
 class AnalogMVType(Enum):
     """Type of the analog matrix-vector product."""
 
-    IDEAL = 'Ideal'
+    IDEAL = "Ideal"
     """FP mat-vec without any non-idealities. Same as setting
     ``is_perfect=True``."""
 
-    ONE_PASS = 'OnePass'
+    ONE_PASS = "OnePass"
     """One pass through the crossbar array for positive and negative inputs."""
 
-    POS_NEG_SEPARATE = 'PosNegSeparate'
+    POS_NEG_SEPARATE = "PosNegSeparate"
     """Two passes through the crossbar array for positive and negative
     inputs separately. The output of the two passes are added in
     analog and then passed once through ADC stage (which also applies
     the output noise, range clipping, output non-linearity etc.).
     """
 
-    POS_NEG_SEPARATE_DIGITAL_SUM = 'PosNegSeparateDigitalSum'
+    POS_NEG_SEPARATE_DIGITAL_SUM = "PosNegSeparateDigitalSum"
     """Two passes through the crossbar array for positive and negative
     inputs separately. The ADC output stage is applied to each pass
     separately and the results are summed in full precision (i.e. in
     digital).
     """
```

### Comparing `aihwkit-0.7.1/src/aihwkit/simulator/configs/helpers.py` & `aihwkit-0.8.0/src/aihwkit/simulator/parameters/helpers.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -19,79 +19,95 @@
 
 from aihwkit.simulator.rpu_base import devices, tiles
 from aihwkit.exceptions import ConfigError
 
 if version_info[0] >= 3 and version_info[1] > 7:
     # pylint: disable=no-name-in-module
     from typing import get_origin  # type: ignore
+
     HAS_ORIGIN = True
 else:
     HAS_ORIGIN = False
 
+ALL_SKIP_FIELD = "is_perfect"
+
 
 def parameters_to_bindings(params: Any, check_fields: bool = True) -> Any:
     """Convert a dataclass parameter into a bindings class.
 
     Args:
         params: parameter dataclass
         check_fields: whether to check for the correct attributes
 
     Returns:
         the C++ bindings
 
     Raises:
         ConfigError: if the field type mismatches (int to float conversion is ignored)
     """
+    # pylint: disable=no-name-in-module, too-many-branches
     result = params.bindings_class()
 
-    field_dict = {field.name: (field, getattr(params, field.name))
-                  for field in fields(params)}
+    field_dict = {field.name: (field, getattr(params, field.name)) for field in fields(params)}
     if check_fields:
-        ignore_fields = getattr(params, 'bindings_ignore', [])
+        ignore_fields = getattr(params, "bindings_ignore", [])
         for key in params.__dict__.keys():
             if key not in field_dict and key not in ignore_fields:
-                raise ConfigError(f"Cannot find '{key}' in params "
-                                  f"'{params.__class__.__name__}'. "
-                                  "Wrong attribute name?")
+                raise ConfigError(
+                    f"Cannot find '{key}' in params "
+                    f"'{params.__class__.__name__}'. "
+                    "Wrong attribute name?"
+                )
 
     for field, (dataclass_field, value) in field_dict.items():
-
         # Convert enums to the bindings enums.
-        if field in ('unit_cell_devices', 'device'):
+        if field in ("unit_cell_devices", "device"):
             # Exclude special fields that are not present in the bindings.
             continue
 
         if isinstance(value, Enum):
             if hasattr(tiles, value.__class__.__name__):
                 enum_class = getattr(tiles, value.__class__.__name__)
             else:
                 enum_class = getattr(devices, value.__class__.__name__)
             enum_value = getattr(enum_class, value.value)
             setattr(result, field, enum_value)
         elif is_dataclass(value):
-            setattr(result, field, parameters_to_bindings(value))
+            if hasattr(value, "bindings_class"):
+                setattr(result, field, parameters_to_bindings(value))
         else:
             if HAS_ORIGIN:
                 expected_type = get_origin(dataclass_field.type) or dataclass_field.type
-                if ((not isinstance(value, expected_type))
-                    and not (expected_type == float and isinstance(value, int)
-                             and not isinstance(value, bool))):
+                if (not isinstance(value, expected_type)) and not (
+                    expected_type == float
+                    and isinstance(value, int)
+                    and not isinstance(value, bool)
+                ):
                     raise ConfigError(f"Expected type {expected_type} for field {field}")
 
             setattr(result, field, value)
 
     return result
 
 
 def tile_parameters_to_bindings(params: Any) -> Any:
     """Convert a tile dataclass parameter into a bindings class."""
-    field_map = {'forward': 'forward_io',
-                 'backward': 'backward_io'}
-    excluded_fields = ('device', 'noise_model', 'drift_compensation',
-                       'clip', 'modifier', 'mapping', 'remap', 'pre_post')
+    field_map = {"forward": "forward_io", "backward": "backward_io"}
+    excluded_fields = (
+        "device",
+        "noise_model",
+        "drift_compensation",
+        "clip",
+        "modifier",
+        "mapping",
+        "remap",
+        "pre_post",
+        "tile_class",
+        "tile_array_class",
+    )
 
     result = params.bindings_class()
     for field, value in params.__dict__.items():
         # Get the mapped field name, if needed.
         field = field_map.get(field, field)
 
         # Convert enums to the bindings enums.
@@ -100,34 +116,36 @@
             continue
 
         if isinstance(value, Enum):
             enum_class = getattr(devices, value.__class__.__name__)
             enum_value = getattr(enum_class, value.value)
             setattr(result, field, enum_value)
         elif is_dataclass(value):
-            setattr(result, field, parameters_to_bindings(value))
+            if hasattr(value, "bindings_class"):
+                setattr(result, field, parameters_to_bindings(value))
         else:
             setattr(result, field, value)
 
     return result
 
 
 class _PrintableMixin:
     """Helper class for pretty-printing of config dataclasses."""
+
     # pylint: disable=too-few-public-methods
 
     def __str__(self) -> str:
         """Return a pretty-print representation."""
 
         def lines_list_to_str(
-                lines_list: List[str],
-                prefix: str = '',
-                suffix: str = '',
-                indent_: int = 0,
-                force_multiline: bool = False
+            lines_list: List[str],
+            prefix: str = "",
+            suffix: str = "",
+            indent_: int = 0,
+            force_multiline: bool = False,
         ) -> str:
             """Convert a list of lines into a string.
 
             Args:
                 lines_list: the list of lines to be converted.
                 prefix: an optional prefix to be appended at the beginning of
                     the string.
@@ -135,25 +153,24 @@
                 indent_: the optional number of spaces to indent the code.
                 force_multiline: force the output to be multiline.
 
             Returns:
                 The lines collapsed into a single string (potentially with line
                 breaks).
             """
-            if force_multiline or len(lines_list) > 3 or any(
-                    '\n' in line for line in lines_list):
+            if force_multiline or len(lines_list) > 3 or any("\n" in line for line in lines_list):
                 # Return a multi-line string.
-                lines_str = indent(',\n'.join(lines_list), ' '*indent_)
-                prefix = '{}\n'.format(prefix) if prefix else prefix
-                suffix = '\n{}'.format(suffix) if suffix else suffix
+                lines_str = indent(",\n".join(lines_list), " " * indent_)
+                prefix = "{}\n".format(prefix) if prefix else prefix
+                suffix = "\n{}".format(suffix) if suffix else suffix
             else:
                 # Return an inline string.
-                lines_str = ', '.join(lines_list)
+                lines_str = ", ".join(lines_list)
 
-            return '{}{}{}'.format(prefix, lines_str, suffix)
+            return "{}{}{}".format(prefix, lines_str, suffix)
 
         def field_to_str(field_value: Any) -> str:
             """Return a string representation of the value of a field.
 
             Args:
                 field_value: the object that contains a field value.
 
@@ -162,54 +179,56 @@
                 breaks).
             """
             field_lines = []
             force_multiline = False
 
             # Handle special cases.
             if isinstance(field_value, list) and len(value) > 0:
-                # For non-emtpy lists, always use multiline, with one item per line.
+                # For non-empty lists, always use multiline, with one item per line.
                 for item in field_value:
-                    field_lines.append(indent('{}'.format(str(item)), ' '*4))
+                    field_lines.append(indent("{}".format(str(item)), " " * 4))
                 force_multiline = True
             else:
                 field_lines.append(str(field_value))
 
-            prefix = '[' if force_multiline else ''
-            suffix = ']' if force_multiline else ''
-            return lines_list_to_str(
-                field_lines, prefix, suffix, force_multiline=force_multiline)
+            prefix = "[" if force_multiline else ""
+            suffix = "]" if force_multiline else ""
+            return lines_list_to_str(field_lines, prefix, suffix, force_multiline=force_multiline)
 
         def is_skippable(field: Field, value: Any) -> bool:
             """Return whether a field should be skipped."""
             if value == field.default:
                 # Skip fields with the default value.
                 return True
 
-            if 'hide_if' in field.metadata and field.metadata.get('hide_if') == value:
+            if "hide_if" in field.metadata and field.metadata.get("hide_if") == value:
                 return True
 
             return False
 
         # Main loop.
 
         # Build the list of lines.
         fields_lines = []
+
+        # special case for global skip:
+        all_skip = hasattr(self, ALL_SKIP_FIELD) and getattr(self, ALL_SKIP_FIELD)
+
         for field in fields(self):
             value = getattr(self, field.name)
 
             # Exclude fields.
-            if is_skippable(field, value):
+            if (all_skip and field.name != ALL_SKIP_FIELD) or is_skippable(field, value):
                 continue
 
             # Convert the value into a string, falling back to repr if needed.
             try:
                 value_str = field_to_str(value)
             except Exception:  # pylint: disable=broad-except
                 value_str = str(value)
 
-            fields_lines.append('{}={}'.format(field.name, value_str))
+            fields_lines.append("{}={}".format(field.name, value_str))
 
         # Convert the full object to str.
-        output = lines_list_to_str(
-            fields_lines, '{}('.format(self.__class__.__name__), ')', 4)
+        output = lines_list_to_str(fields_lines, "{}(".format(self.__class__.__name__), ")", 4)
 
         return output
```

### Comparing `aihwkit-0.7.1/src/aihwkit/simulator/noise_models.py` & `aihwkit-0.8.0/src/aihwkit/simulator/noise_models.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -12,11 +12,14 @@
 
 """Legacy class for import the noise models for inference"""
 
 import warnings
 from aihwkit.inference import *  # pylint: disable=unused-wildcard-import, wildcard-import
 
 warnings.warn(
-    FutureWarning("\n\nThe `aihwkit.simulator.noise_models` module has been superseded "
-                  "by the `aihwkit.inference` module. "
-                  "Please replace `from aihwkit.simulator.noise_models import ...` "
-                  "with `from aihwkit.inference import ...` in your import statement.\n"))
+    FutureWarning(
+        "\n\nThe `aihwkit.simulator.noise_models` module has been superseded "
+        "by the `aihwkit.inference` module. "
+        "Please replace `from aihwkit.simulator.noise_models import ...` "
+        "with `from aihwkit.inference import ...` in your import statement.\n"
+    )
+)
```

### Comparing `aihwkit-0.7.1/src/aihwkit/simulator/presets/__init__.py` & `aihwkit-0.8.0/src/aihwkit/simulator/presets/__init__.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,46 +1,92 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
 # that they have been altered from the originals.
 
 """Configurations presets for resistive processing units."""
 
 from .configs import (
     # Single device configs.
-    ReRamESPreset, ReRamSBPreset, CapacitorPreset, EcRamPreset, EcRamMOPreset, IdealizedPreset,
-    GokmenVlasovPreset, PCMPreset,
+    ReRamESPreset,
+    ReRamSBPreset,
+    CapacitorPreset,
+    EcRamPreset,
+    EcRamMOPreset,
+    IdealizedPreset,
+    GokmenVlasovPreset,
+    PCMPreset,
     # 2-device configs.
-    ReRamES2Preset, ReRamSB2Preset, Capacitor2Preset, EcRam2Preset, EcRamMO2Preset,
+    ReRamES2Preset,
+    ReRamSB2Preset,
+    Capacitor2Preset,
+    EcRam2Preset,
+    EcRamMO2Preset,
     Idealized2Preset,
     # 4-device configs.
-    ReRamES4Preset, ReRamSB4Preset, Capacitor4Preset, EcRam4Preset, EcRamMO4Preset,
+    ReRamES4Preset,
+    ReRamSB4Preset,
+    Capacitor4Preset,
+    EcRam4Preset,
+    EcRamMO4Preset,
     Idealized4Preset,
     # Tiki-taka configs.
-    TikiTakaReRamESPreset, TikiTakaReRamSBPreset, TikiTakaCapacitorPreset,
-    TikiTakaEcRamPreset, TikiTakaEcRamMOPreset, TikiTakaIdealizedPreset,
+    TikiTakaReRamESPreset,
+    TikiTakaReRamSBPreset,
+    TikiTakaCapacitorPreset,
+    TikiTakaEcRamPreset,
+    TikiTakaEcRamMOPreset,
+    TikiTakaIdealizedPreset,
     # TTv2 configs.
-    TTv2ReRamESPreset, TTv2ReRamSBPreset, TTv2CapacitorPreset,
-    TTv2EcRamPreset, TTv2EcRamMOPreset, TTv2IdealizedPreset,
+    TTv2ReRamESPreset,
+    TTv2ReRamSBPreset,
+    TTv2CapacitorPreset,
+    TTv2EcRamPreset,
+    TTv2EcRamMOPreset,
+    TTv2IdealizedPreset,
+    # c-TTv2 configs.
+    ChoppedTTv2ReRamESPreset,
+    ChoppedTTv2ReRamSBPreset,
+    ChoppedTTv2CapacitorPreset,
+    ChoppedTTv2EcRamPreset,
+    ChoppedTTv2EcRamMOPreset,
+    ChoppedTTv2IdealizedPreset,
+    # AGAD configs.
+    AGADReRamESPreset,
+    AGADReRamSBPreset,
+    AGADCapacitorPreset,
+    AGADEcRamPreset,
+    AGADEcRamMOPreset,
+    AGADIdealizedPreset,
     # MixedPrecision configs.
-    MixedPrecisionReRamESPreset, MixedPrecisionReRamSBPreset, MixedPrecisionCapacitorPreset,
-    MixedPrecisionEcRamPreset, MixedPrecisionEcRamMOPreset, MixedPrecisionIdealizedPreset,
-    MixedPrecisionGokmenVlasovPreset, MixedPrecisionPCMPreset
+    MixedPrecisionReRamESPreset,
+    MixedPrecisionReRamSBPreset,
+    MixedPrecisionCapacitorPreset,
+    MixedPrecisionEcRamPreset,
+    MixedPrecisionEcRamMOPreset,
+    MixedPrecisionIdealizedPreset,
+    MixedPrecisionGokmenVlasovPreset,
+    MixedPrecisionPCMPreset,
 )
+from .inference import StandardHWATrainingPreset
+
 from .devices import (
-    ReRamESPresetDevice, ReRamSBPresetDevice,
-    CapacitorPresetDevice, EcRamPresetDevice, EcRamMOPresetDevice,
-    IdealizedPresetDevice, GokmenVlasovPresetDevice, PCMPresetDevice
-)
-from .compounds import (
-    PCMPresetUnitCell,
-)
-from .utils import (
-    PresetIOParameters, PresetUpdateParameters
+    ReRamESPresetDevice,
+    ReRamSBPresetDevice,
+    CapacitorPresetDevice,
+    EcRamPresetDevice,
+    EcRamMOPresetDevice,
+    IdealizedPresetDevice,
+    GokmenVlasovPresetDevice,
+    PCMPresetDevice,
+    ReRamArrayOMPresetDevice,
+    ReRamArrayHfO2PresetDevice,
 )
+from .compounds import PCMPresetUnitCell
+from .utils import PresetIOParameters, StandardIOParameters, PresetUpdateParameters
```

### Comparing `aihwkit-0.7.1/src/aihwkit/simulator/presets/compounds.py` & `aihwkit-0.8.0/src/aihwkit/simulator/presets/compounds.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -13,32 +13,30 @@
 """Compound configurations presets for resistive processing units."""
 
 # pylint: disable=too-many-instance-attributes
 from typing import List
 from dataclasses import dataclass, field
 
 from aihwkit.simulator.configs.compounds import OneSidedUnitCell
-from aihwkit.simulator.configs.utils import IOParameters, UpdateParameters
+from aihwkit.simulator.parameters.utils import IOParameters, UpdateParameters
 from aihwkit.simulator.presets.devices import PCMPresetDevice
-from aihwkit.simulator.presets.utils import (
-    PresetIOParameters, PresetUpdateParameters
-)
+from aihwkit.simulator.presets.utils import PresetIOParameters, PresetUpdateParameters
 
 
 @dataclass
 class PCMPresetUnitCell(OneSidedUnitCell):
     """A unit cell that is comprised of two uni-directional PCM devices of
     opposite sign (see :class:`~PCMPresetDevice`).
 
     Check for refresh is performed after each mini-batch update. See
     :class:`~aihwkit.simulator.configs.device.OneSidedUnitCell` for
     details on the refresh implementation.
     """
 
-    unit_cell_devices: List = field(
-        default_factory=lambda: [PCMPresetDevice(), PCMPresetDevice()])
+    unit_cell_devices: List = field(default_factory=lambda: [PCMPresetDevice(), PCMPresetDevice()])
 
     refresh_every: int = 1
     units_in_mbatch: bool = True
     refresh_forward: IOParameters = field(default_factory=PresetIOParameters)
     refresh_update: UpdateParameters = field(
-        default_factory=lambda: PresetUpdateParameters(desired_bl=31))
+        default_factory=lambda: PresetUpdateParameters(desired_bl=31)
+    )
```

### Comparing `aihwkit-0.7.1/src/aihwkit/simulator/presets/configs.py` & `aihwkit-0.8.0/src/aihwkit/simulator/presets/configs.py`

 * *Files 21% similar despite different names*

```diff
@@ -13,37 +13,50 @@
 # pylint: disable=too-many-lines
 
 """RPU configurations presets for resistive processing units."""
 
 from dataclasses import dataclass, field
 
 from aihwkit.simulator.configs.configs import (
-    SingleRPUConfig, UnitCellRPUConfig, DigitalRankUpdateRPUConfig
+    SingleRPUConfig,
+    UnitCellRPUConfig,
+    DigitalRankUpdateRPUConfig,
 )
 from aihwkit.simulator.configs.devices import PulsedDevice
 from aihwkit.simulator.configs.compounds import (
-    TransferCompound, UnitCell, VectorUnitCell, BufferedTransferCompound,
-    DigitalRankUpdateCell, MixedPrecisionCompound,
+    TransferCompound,
+    UnitCell,
+    VectorUnitCell,
+    DynamicTransferCompound,
+    DigitalRankUpdateCell,
+    MixedPrecisionCompound,
+    ChoppedTransferCompound,
 )
-from aihwkit.simulator.configs.enums import VectorUnitCellUpdatePolicy
-from aihwkit.simulator.configs.utils import (
-    IOParameters, UpdateParameters
+from aihwkit.simulator.parameters.enums import (
+    VectorUnitCellUpdatePolicy,
+    NoiseManagementType,
+    BoundManagementType,
 )
+from aihwkit.simulator.parameters.utils import IOParameters, UpdateParameters
 from aihwkit.simulator.presets.devices import (
-    CapacitorPresetDevice, EcRamPresetDevice, EcRamMOPresetDevice, IdealizedPresetDevice,
-    ReRamESPresetDevice, ReRamSBPresetDevice, GokmenVlasovPresetDevice,
+    CapacitorPresetDevice,
+    EcRamPresetDevice,
+    EcRamMOPresetDevice,
+    IdealizedPresetDevice,
+    ReRamESPresetDevice,
+    ReRamSBPresetDevice,
+    GokmenVlasovPresetDevice,
 )
 from aihwkit.simulator.presets.compounds import PCMPresetUnitCell
-from aihwkit.simulator.presets.utils import (
-    PresetIOParameters, PresetUpdateParameters
-)
+from aihwkit.simulator.presets.utils import PresetIOParameters, PresetUpdateParameters
 
 
 # Single device configs.
 
+
 @dataclass
 class ReRamESPreset(SingleRPUConfig):
     """Preset configuration using a single ReRam device (based on ExpStep
     model, see :class:`~aihwkit.simulator.presets.devices.ReRamESPresetDevice`).
 
     This preset uses standard SGD with fully parallel update on analog
     with stochastic pulses.
@@ -208,14 +221,15 @@
     forward: IOParameters = field(default_factory=PresetIOParameters)
     backward: IOParameters = field(default_factory=PresetIOParameters)
     update: UpdateParameters = field(default_factory=PresetUpdateParameters)
 
 
 # 2-device configs.
 
+
 @dataclass
 class ReRamES2Preset(UnitCellRPUConfig):
     """Preset configuration using two ReRam devices per cross-point
     (:class:`~aihwkit.simulator.presets.devices.ReRamESPresetDevice`),
     where both are updated with random selection policy for update.
 
     See :class:`~aihwkit.simulator.configs.devices.VectorUnitCell` for
@@ -224,18 +238,20 @@
     The default peripheral hardware
     (:class:`~aihwkit.simulator.presets.utils.PresetIOParameters`) and
     analog update
     (:class:`~aihwkit.simulator.presets.utils.PresetUpdateParameters`)
     configuration is used otherwise.
     """
 
-    device: UnitCell = field(default_factory=lambda: VectorUnitCell(
-        unit_cell_devices=[ReRamESPresetDevice(), ReRamESPresetDevice()],
-        update_policy=VectorUnitCellUpdatePolicy.SINGLE_RANDOM
-    ))
+    device: UnitCell = field(
+        default_factory=lambda: VectorUnitCell(
+            unit_cell_devices=[ReRamESPresetDevice(), ReRamESPresetDevice()],
+            update_policy=VectorUnitCellUpdatePolicy.SINGLE_RANDOM,
+        )
+    )
     forward: IOParameters = field(default_factory=PresetIOParameters)
     backward: IOParameters = field(default_factory=PresetIOParameters)
     update: UpdateParameters = field(default_factory=PresetUpdateParameters)
 
 
 @dataclass
 class ReRamSB2Preset(UnitCellRPUConfig):
@@ -249,18 +265,20 @@
     The default peripheral hardware
     (:class:`~aihwkit.simulator.presets.utils.PresetIOParameters`) and
     analog update
     (:class:`~aihwkit.simulator.presets.utils.PresetUpdateParameters`)
     configuration is used otherwise.
     """
 
-    device: UnitCell = field(default_factory=lambda: VectorUnitCell(
-        unit_cell_devices=[ReRamSBPresetDevice(), ReRamSBPresetDevice()],
-        update_policy=VectorUnitCellUpdatePolicy.SINGLE_RANDOM
-    ))
+    device: UnitCell = field(
+        default_factory=lambda: VectorUnitCell(
+            unit_cell_devices=[ReRamSBPresetDevice(), ReRamSBPresetDevice()],
+            update_policy=VectorUnitCellUpdatePolicy.SINGLE_RANDOM,
+        )
+    )
     forward: IOParameters = field(default_factory=PresetIOParameters)
     backward: IOParameters = field(default_factory=PresetIOParameters)
     update: UpdateParameters = field(default_factory=PresetUpdateParameters)
 
 
 @dataclass
 class Capacitor2Preset(UnitCellRPUConfig):
@@ -274,18 +292,20 @@
     The default peripheral hardware
     (:class:`~aihwkit.simulator.presets.utils.PresetIOParameters`) and
     analog update
     (:class:`~aihwkit.simulator.presets.utils.PresetUpdateParameters`)
     configuration is used otherwise.
     """
 
-    device: UnitCell = field(default_factory=lambda: VectorUnitCell(
-        unit_cell_devices=[CapacitorPresetDevice(), CapacitorPresetDevice()],
-        update_policy=VectorUnitCellUpdatePolicy.SINGLE_RANDOM
-    ))
+    device: UnitCell = field(
+        default_factory=lambda: VectorUnitCell(
+            unit_cell_devices=[CapacitorPresetDevice(), CapacitorPresetDevice()],
+            update_policy=VectorUnitCellUpdatePolicy.SINGLE_RANDOM,
+        )
+    )
     forward: IOParameters = field(default_factory=PresetIOParameters)
     backward: IOParameters = field(default_factory=PresetIOParameters)
     update: UpdateParameters = field(default_factory=PresetUpdateParameters)
 
 
 @dataclass
 class EcRam2Preset(UnitCellRPUConfig):
@@ -299,18 +319,20 @@
     The default peripheral hardware
     (:class:`~aihwkit.simulator.presets.utils.PresetIOParameters`) and
     analog update
     (:class:`~aihwkit.simulator.presets.utils.PresetUpdateParameters`)
     configuration is used otherwise.
     """
 
-    device: UnitCell = field(default_factory=lambda: VectorUnitCell(
-        unit_cell_devices=[EcRamPresetDevice(), EcRamPresetDevice()],
-        update_policy=VectorUnitCellUpdatePolicy.SINGLE_RANDOM
-    ))
+    device: UnitCell = field(
+        default_factory=lambda: VectorUnitCell(
+            unit_cell_devices=[EcRamPresetDevice(), EcRamPresetDevice()],
+            update_policy=VectorUnitCellUpdatePolicy.SINGLE_RANDOM,
+        )
+    )
     forward: IOParameters = field(default_factory=PresetIOParameters)
     backward: IOParameters = field(default_factory=PresetIOParameters)
     update: UpdateParameters = field(default_factory=PresetUpdateParameters)
 
 
 @dataclass
 class EcRamMO2Preset(UnitCellRPUConfig):
@@ -324,18 +346,20 @@
     The default peripheral hardware
     (:class:`~aihwkit.simulator.presets.utils.PresetIOParameters`) and
     analog update
     (:class:`~aihwkit.simulator.presets.utils.PresetUpdateParameters`)
     configuration is used otherwise.
     """
 
-    device: UnitCell = field(default_factory=lambda: VectorUnitCell(
-        unit_cell_devices=[EcRamMOPresetDevice(), EcRamMOPresetDevice()],
-        update_policy=VectorUnitCellUpdatePolicy.SINGLE_RANDOM
-    ))
+    device: UnitCell = field(
+        default_factory=lambda: VectorUnitCell(
+            unit_cell_devices=[EcRamMOPresetDevice(), EcRamMOPresetDevice()],
+            update_policy=VectorUnitCellUpdatePolicy.SINGLE_RANDOM,
+        )
+    )
     forward: IOParameters = field(default_factory=PresetIOParameters)
     backward: IOParameters = field(default_factory=PresetIOParameters)
     update: UpdateParameters = field(default_factory=PresetUpdateParameters)
 
 
 @dataclass
 class Idealized2Preset(UnitCellRPUConfig):
@@ -349,25 +373,28 @@
     The default peripheral hardware
     (:class:`~aihwkit.simulator.presets.utils.PresetIOParameters`) and
     analog update
     (:class:`~aihwkit.simulator.presets.utils.PresetUpdateParameters`)
     configuration is used otherwise.
     """
 
-    device: UnitCell = field(default_factory=lambda: VectorUnitCell(
-        unit_cell_devices=[IdealizedPresetDevice(), IdealizedPresetDevice()],
-        update_policy=VectorUnitCellUpdatePolicy.SINGLE_RANDOM
-    ))
+    device: UnitCell = field(
+        default_factory=lambda: VectorUnitCell(
+            unit_cell_devices=[IdealizedPresetDevice(), IdealizedPresetDevice()],
+            update_policy=VectorUnitCellUpdatePolicy.SINGLE_RANDOM,
+        )
+    )
     forward: IOParameters = field(default_factory=PresetIOParameters)
     backward: IOParameters = field(default_factory=PresetIOParameters)
     update: UpdateParameters = field(default_factory=PresetUpdateParameters)
 
 
 # 4-device configs.
 
+
 @dataclass
 class ReRamES4Preset(UnitCellRPUConfig):
     """Preset configuration using four ReRam devices per cross-point
     (:class:`~aihwkit.simulator.presets.devices.ReRamESPresetDevice`),
     where both are updated with random selection policy for update.
 
     See :class:`~aihwkit.simulator.configs.devices.VectorUnitCell` for
@@ -376,19 +403,25 @@
     The default peripheral hardware
     (:class:`~aihwkit.simulator.presets.utils.PresetIOParameters`) and
     analog update
     (:class:`~aihwkit.simulator.presets.utils.PresetUpdateParameters`)
     configuration is used otherwise.
     """
 
-    device: UnitCell = field(default_factory=lambda: VectorUnitCell(
-        unit_cell_devices=[ReRamESPresetDevice(), ReRamESPresetDevice(),
-                           ReRamESPresetDevice(), ReRamESPresetDevice()],
-        update_policy=VectorUnitCellUpdatePolicy.SINGLE_RANDOM
-    ))
+    device: UnitCell = field(
+        default_factory=lambda: VectorUnitCell(
+            unit_cell_devices=[
+                ReRamESPresetDevice(),
+                ReRamESPresetDevice(),
+                ReRamESPresetDevice(),
+                ReRamESPresetDevice(),
+            ],
+            update_policy=VectorUnitCellUpdatePolicy.SINGLE_RANDOM,
+        )
+    )
     forward: IOParameters = field(default_factory=PresetIOParameters)
     backward: IOParameters = field(default_factory=PresetIOParameters)
     update: UpdateParameters = field(default_factory=PresetUpdateParameters)
 
 
 @dataclass
 class ReRamSB4Preset(UnitCellRPUConfig):
@@ -402,19 +435,25 @@
     The default peripheral hardware
     (:class:`~aihwkit.simulator.presets.utils.PresetIOParameters`) and
     analog update
     (:class:`~aihwkit.simulator.presets.utils.PresetUpdateParameters`)
     configuration is used otherwise.
     """
 
-    device: UnitCell = field(default_factory=lambda: VectorUnitCell(
-        unit_cell_devices=[ReRamSBPresetDevice(), ReRamSBPresetDevice(),
-                           ReRamSBPresetDevice(), ReRamSBPresetDevice()],
-        update_policy=VectorUnitCellUpdatePolicy.SINGLE_RANDOM
-    ))
+    device: UnitCell = field(
+        default_factory=lambda: VectorUnitCell(
+            unit_cell_devices=[
+                ReRamSBPresetDevice(),
+                ReRamSBPresetDevice(),
+                ReRamSBPresetDevice(),
+                ReRamSBPresetDevice(),
+            ],
+            update_policy=VectorUnitCellUpdatePolicy.SINGLE_RANDOM,
+        )
+    )
     forward: IOParameters = field(default_factory=PresetIOParameters)
     backward: IOParameters = field(default_factory=PresetIOParameters)
     update: UpdateParameters = field(default_factory=PresetUpdateParameters)
 
 
 @dataclass
 class Capacitor4Preset(UnitCellRPUConfig):
@@ -428,19 +467,25 @@
     The default peripheral hardware
     (:class:`~aihwkit.simulator.presets.utils.PresetIOParameters`) and
     analog update
     (:class:`~aihwkit.simulator.presets.utils.PresetUpdateParameters`)
     configuration is used otherwise.
     """
 
-    device: UnitCell = field(default_factory=lambda: VectorUnitCell(
-        unit_cell_devices=[CapacitorPresetDevice(), CapacitorPresetDevice(),
-                           CapacitorPresetDevice(), CapacitorPresetDevice()],
-        update_policy=VectorUnitCellUpdatePolicy.SINGLE_RANDOM
-    ))
+    device: UnitCell = field(
+        default_factory=lambda: VectorUnitCell(
+            unit_cell_devices=[
+                CapacitorPresetDevice(),
+                CapacitorPresetDevice(),
+                CapacitorPresetDevice(),
+                CapacitorPresetDevice(),
+            ],
+            update_policy=VectorUnitCellUpdatePolicy.SINGLE_RANDOM,
+        )
+    )
     forward: IOParameters = field(default_factory=PresetIOParameters)
     backward: IOParameters = field(default_factory=PresetIOParameters)
     update: UpdateParameters = field(default_factory=PresetUpdateParameters)
 
 
 @dataclass
 class EcRam4Preset(UnitCellRPUConfig):
@@ -454,19 +499,25 @@
     The default peripheral hardware
     (:class:`~aihwkit.simulator.presets.utils.PresetIOParameters`) and
     analog update
     (:class:`~aihwkit.simulator.presets.utils.PresetUpdateParameters`)
     configuration is used otherwise.
     """
 
-    device: UnitCell = field(default_factory=lambda: VectorUnitCell(
-        unit_cell_devices=[EcRamPresetDevice(), EcRamPresetDevice(),
-                           EcRamPresetDevice(), EcRamPresetDevice()],
-        update_policy=VectorUnitCellUpdatePolicy.SINGLE_RANDOM
-    ))
+    device: UnitCell = field(
+        default_factory=lambda: VectorUnitCell(
+            unit_cell_devices=[
+                EcRamPresetDevice(),
+                EcRamPresetDevice(),
+                EcRamPresetDevice(),
+                EcRamPresetDevice(),
+            ],
+            update_policy=VectorUnitCellUpdatePolicy.SINGLE_RANDOM,
+        )
+    )
     forward: IOParameters = field(default_factory=PresetIOParameters)
     backward: IOParameters = field(default_factory=PresetIOParameters)
     update: UpdateParameters = field(default_factory=PresetUpdateParameters)
 
 
 @dataclass
 class EcRamMO4Preset(UnitCellRPUConfig):
@@ -480,19 +531,25 @@
     The default peripheral hardware
     (:class:`~aihwkit.simulator.presets.utils.PresetIOParameters`) and
     analog update
     (:class:`~aihwkit.simulator.presets.utils.PresetUpdateParameters`)
     configuration is used otherwise.
     """
 
-    device: UnitCell = field(default_factory=lambda: VectorUnitCell(
-        unit_cell_devices=[EcRamMOPresetDevice(), EcRamMOPresetDevice(),
-                           EcRamMOPresetDevice(), EcRamMOPresetDevice()],
-        update_policy=VectorUnitCellUpdatePolicy.SINGLE_RANDOM
-    ))
+    device: UnitCell = field(
+        default_factory=lambda: VectorUnitCell(
+            unit_cell_devices=[
+                EcRamMOPresetDevice(),
+                EcRamMOPresetDevice(),
+                EcRamMOPresetDevice(),
+                EcRamMOPresetDevice(),
+            ],
+            update_policy=VectorUnitCellUpdatePolicy.SINGLE_RANDOM,
+        )
+    )
     forward: IOParameters = field(default_factory=PresetIOParameters)
     backward: IOParameters = field(default_factory=PresetIOParameters)
     update: UpdateParameters = field(default_factory=PresetUpdateParameters)
 
 
 @dataclass
 class Idealized4Preset(UnitCellRPUConfig):
@@ -506,26 +563,33 @@
     The default peripheral hardware
     (:class:`~aihwkit.simulator.presets.utils.PresetIOParameters`) and
     analog update
     (:class:`~aihwkit.simulator.presets.utils.PresetUpdateParameters`)
     configuration is used otherwise.
     """
 
-    device: UnitCell = field(default_factory=lambda: VectorUnitCell(
-        unit_cell_devices=[IdealizedPresetDevice(), IdealizedPresetDevice(),
-                           IdealizedPresetDevice(), IdealizedPresetDevice()],
-        update_policy=VectorUnitCellUpdatePolicy.SINGLE_RANDOM
-    ))
+    device: UnitCell = field(
+        default_factory=lambda: VectorUnitCell(
+            unit_cell_devices=[
+                IdealizedPresetDevice(),
+                IdealizedPresetDevice(),
+                IdealizedPresetDevice(),
+                IdealizedPresetDevice(),
+            ],
+            update_policy=VectorUnitCellUpdatePolicy.SINGLE_RANDOM,
+        )
+    )
     forward: IOParameters = field(default_factory=PresetIOParameters)
     backward: IOParameters = field(default_factory=PresetIOParameters)
     update: UpdateParameters = field(default_factory=PresetUpdateParameters)
 
 
 # Tiki-taka configs.
 
+
 @dataclass
 class TikiTakaReRamESPreset(UnitCellRPUConfig):
     """Configuration using Tiki-taka with
     :class:`~aihwkit.simulator.presets.devices.ReRamESPresetDevice`.
 
     See :class:`~aihwkit.simulator.configs.devices.TransferCompound`
     for details on Tiki-taka-like optimizers.
@@ -536,19 +600,21 @@
     (:class:`~aihwkit.simulator.presets.utils.PresetUpdateParameters`)
     configuration is used otherwise.
     """
 
     device: UnitCell = field(
         default_factory=lambda: TransferCompound(
             unit_cell_devices=[ReRamESPresetDevice(), ReRamESPresetDevice()],
-            transfer_forward=PresetIOParameters(),
+            transfer_forward=PresetIOParameters(
+                noise_management=NoiseManagementType.NONE, bound_management=BoundManagementType.NONE
+            ),
             transfer_update=PresetUpdateParameters(),
-            transfer_every=1.0,
             units_in_mbatch=True,
-            ))
+        )
+    )
     forward: IOParameters = field(default_factory=PresetIOParameters)
     backward: IOParameters = field(default_factory=PresetIOParameters)
     update: UpdateParameters = field(default_factory=PresetUpdateParameters)
 
 
 @dataclass
 class TikiTakaReRamSBPreset(UnitCellRPUConfig):
@@ -563,20 +629,26 @@
     analog update
     (:class:`~aihwkit.simulator.presets.utils.PresetUpdateParameters`)
     configuration is used otherwise.
     """
 
     device: UnitCell = field(
         default_factory=lambda: TransferCompound(
-            unit_cell_devices=[ReRamSBPresetDevice(), ReRamSBPresetDevice()],
-            transfer_forward=PresetIOParameters(),
+            unit_cell_devices=[
+                ReRamSBPresetDevice(subtract_symmetry_point=True),
+                ReRamSBPresetDevice(subtract_symmetry_point=True),
+            ],
+            transfer_forward=PresetIOParameters(
+                noise_management=NoiseManagementType.NONE, bound_management=BoundManagementType.NONE
+            ),
             transfer_update=PresetUpdateParameters(),
             transfer_every=1.0,
             units_in_mbatch=True,
-            ))
+        )
+    )
     forward: IOParameters = field(default_factory=PresetIOParameters)
     backward: IOParameters = field(default_factory=PresetIOParameters)
     update: UpdateParameters = field(default_factory=PresetUpdateParameters)
 
 
 @dataclass
 class TikiTakaCapacitorPreset(UnitCellRPUConfig):
@@ -592,19 +664,21 @@
     (:class:`~aihwkit.simulator.presets.utils.PresetUpdateParameters`)
     configuration is used otherwise.
     """
 
     device: UnitCell = field(
         default_factory=lambda: TransferCompound(
             unit_cell_devices=[CapacitorPresetDevice(), CapacitorPresetDevice()],
-            transfer_forward=PresetIOParameters(),
+            transfer_forward=PresetIOParameters(
+                noise_management=NoiseManagementType.NONE, bound_management=BoundManagementType.NONE
+            ),
             transfer_update=PresetUpdateParameters(),
-            transfer_every=1.0,
             units_in_mbatch=True,
-            ))
+        )
+    )
     forward: IOParameters = field(default_factory=PresetIOParameters)
     backward: IOParameters = field(default_factory=PresetIOParameters)
     update: UpdateParameters = field(default_factory=PresetUpdateParameters)
 
 
 @dataclass
 class TikiTakaEcRamPreset(UnitCellRPUConfig):
@@ -620,19 +694,21 @@
     (:class:`~aihwkit.simulator.presets.utils.PresetUpdateParameters`)
     configuration is used otherwise.
     """
 
     device: UnitCell = field(
         default_factory=lambda: TransferCompound(
             unit_cell_devices=[EcRamPresetDevice(), EcRamPresetDevice()],
-            transfer_forward=PresetIOParameters(),
+            transfer_forward=PresetIOParameters(
+                noise_management=NoiseManagementType.NONE, bound_management=BoundManagementType.NONE
+            ),
             transfer_update=PresetUpdateParameters(),
-            transfer_every=1.0,
             units_in_mbatch=True,
-            ))
+        )
+    )
     forward: IOParameters = field(default_factory=PresetIOParameters)
     backward: IOParameters = field(default_factory=PresetIOParameters)
     update: UpdateParameters = field(default_factory=PresetUpdateParameters)
 
 
 @dataclass
 class TikiTakaEcRamMOPreset(UnitCellRPUConfig):
@@ -648,19 +724,21 @@
     (:class:`~aihwkit.simulator.presets.utils.PresetUpdateParameters`)
     configuration is used otherwise.
     """
 
     device: UnitCell = field(
         default_factory=lambda: TransferCompound(
             unit_cell_devices=[EcRamMOPresetDevice(), EcRamMOPresetDevice()],
-            transfer_forward=PresetIOParameters(),
+            transfer_forward=PresetIOParameters(
+                noise_management=NoiseManagementType.NONE, bound_management=BoundManagementType.NONE
+            ),
             transfer_update=PresetUpdateParameters(),
-            transfer_every=1.0,
             units_in_mbatch=True,
-            ))
+        )
+    )
     forward: IOParameters = field(default_factory=PresetIOParameters)
     backward: IOParameters = field(default_factory=PresetIOParameters)
     update: UpdateParameters = field(default_factory=PresetUpdateParameters)
 
 
 @dataclass
 class TikiTakaIdealizedPreset(UnitCellRPUConfig):
@@ -676,192 +754,694 @@
     (:class:`~aihwkit.simulator.presets.utils.PresetUpdateParameters`)
     configuration is used otherwise.
     """
 
     device: UnitCell = field(
         default_factory=lambda: TransferCompound(
             unit_cell_devices=[IdealizedPresetDevice(), IdealizedPresetDevice()],
-            transfer_forward=PresetIOParameters(),
+            transfer_forward=PresetIOParameters(
+                noise_management=NoiseManagementType.NONE, bound_management=BoundManagementType.NONE
+            ),
             transfer_update=PresetUpdateParameters(),
-            transfer_every=1.0,
             units_in_mbatch=True,
-            ))
+        )
+    )
     forward: IOParameters = field(default_factory=PresetIOParameters)
     backward: IOParameters = field(default_factory=PresetIOParameters)
     update: UpdateParameters = field(default_factory=PresetUpdateParameters)
 
+
 # TTv2 configs.
 
 
 @dataclass
 class TTv2ReRamESPreset(UnitCellRPUConfig):
     """Configuration using TTv2 with
     :class:`~aihwkit.simulator.presets.devices.ReRamESPresetDevice`.
 
-    See :class:`~aihwkit.simulator.configs.devices.BufferedTransferCompound`
+    See :class:`~aihwkit.simulator.configs.devices.ChoppedTransferCompound`
     for details on TTv2-like optimizers.
 
     The default peripheral hardware
     (:class:`~aihwkit.simulator.presets.utils.PresetIOParameters`) and
     analog update
     (:class:`~aihwkit.simulator.presets.utils.PresetUpdateParameters`)
     configuration is used otherwise.
     """
 
     device: UnitCell = field(
-        default_factory=lambda: BufferedTransferCompound(
+        default_factory=lambda: ChoppedTransferCompound(
             unit_cell_devices=[ReRamESPresetDevice(), ReRamESPresetDevice()],
-            transfer_forward=PresetIOParameters(),
-            transfer_update=PresetUpdateParameters(),
-            transfer_every=1.0,
-            units_in_mbatch=True,
-            ))
+            transfer_forward=PresetIOParameters(
+                noise_management=NoiseManagementType.NONE, bound_management=BoundManagementType.NONE
+            ),
+            transfer_update=PresetUpdateParameters(
+                desired_bl=1, update_bl_management=False, update_management=False
+            ),
+            units_in_mbatch=False,
+            in_chop_prob=0.0,
+            fast_lr=0.5,
+            auto_scale=True,
+            auto_granularity=1000,
+        )
+    )
     forward: IOParameters = field(default_factory=PresetIOParameters)
     backward: IOParameters = field(default_factory=PresetIOParameters)
-    update: UpdateParameters = field(default_factory=PresetUpdateParameters)
+    update: UpdateParameters = field(default_factory=lambda: PresetUpdateParameters(desired_bl=31))
 
 
 @dataclass
 class TTv2ReRamSBPreset(UnitCellRPUConfig):
     """Configuration using TTv2 with
     :class:`~aihwkit.simulator.presets.devices.ReRamSBPresetDevice`.
 
-    See :class:`~aihwkit.simulator.configs.devices.BufferedTransferCompound`
+    See :class:`~aihwkit.simulator.configs.devices.ChoppedTransferCompound`
     for details on TTv2-like optimizers.
 
     The default peripheral hardware
     (:class:`~aihwkit.simulator.presets.utils.PresetIOParameters`) and
     analog update
     (:class:`~aihwkit.simulator.presets.utils.PresetUpdateParameters`)
     configuration is used otherwise.
     """
 
     device: UnitCell = field(
-        default_factory=lambda: BufferedTransferCompound(
-            unit_cell_devices=[ReRamSBPresetDevice(), ReRamSBPresetDevice()],
-            transfer_forward=PresetIOParameters(),
-            transfer_update=PresetUpdateParameters(),
-            transfer_every=1.0,
-            units_in_mbatch=True,
-            ))
+        default_factory=lambda: ChoppedTransferCompound(
+            unit_cell_devices=[
+                ReRamSBPresetDevice(subtract_symmetry_point=True),
+                ReRamSBPresetDevice(subtract_symmetry_point=True),
+            ],
+            transfer_forward=PresetIOParameters(
+                noise_management=NoiseManagementType.NONE, bound_management=BoundManagementType.NONE
+            ),
+            transfer_update=PresetUpdateParameters(
+                desired_bl=1, update_bl_management=False, update_management=False
+            ),
+            units_in_mbatch=False,
+            in_chop_prob=0.0,
+            fast_lr=0.5,
+            auto_scale=True,
+            auto_granularity=1000,
+        )
+    )
     forward: IOParameters = field(default_factory=PresetIOParameters)
     backward: IOParameters = field(default_factory=PresetIOParameters)
-    update: UpdateParameters = field(default_factory=PresetUpdateParameters)
+    update: UpdateParameters = field(default_factory=lambda: PresetUpdateParameters(desired_bl=31))
 
 
 @dataclass
 class TTv2CapacitorPreset(UnitCellRPUConfig):
     """Configuration using TTv2 with
     :class:`~aihwkit.simulator.presets.devices.CapacitorPresetDevice`.
 
-    See :class:`~aihwkit.simulator.configs.devices.BufferedTransferCompound`
+    See :class:`~aihwkit.simulator.configs.devices.ChoppedTransferCompound`
     for details on TTv2-like optimizers.
 
     The default peripheral hardware
     (:class:`~aihwkit.simulator.presets.utils.PresetIOParameters`) and
     analog update
     (:class:`~aihwkit.simulator.presets.utils.PresetUpdateParameters`)
     configuration is used otherwise.
     """
 
     device: UnitCell = field(
-        default_factory=lambda: BufferedTransferCompound(
+        default_factory=lambda: ChoppedTransferCompound(
             unit_cell_devices=[CapacitorPresetDevice(), CapacitorPresetDevice()],
-            transfer_forward=PresetIOParameters(),
-            transfer_update=PresetUpdateParameters(),
-            transfer_every=1.0,
-            units_in_mbatch=True,
-            ))
+            transfer_forward=PresetIOParameters(
+                noise_management=NoiseManagementType.NONE, bound_management=BoundManagementType.NONE
+            ),
+            transfer_update=PresetUpdateParameters(
+                desired_bl=1, update_bl_management=False, update_management=False
+            ),
+            units_in_mbatch=False,
+            in_chop_prob=0.0,
+            fast_lr=0.1,
+            auto_scale=True,
+            auto_granularity=1000,
+        )
+    )
     forward: IOParameters = field(default_factory=PresetIOParameters)
     backward: IOParameters = field(default_factory=PresetIOParameters)
-    update: UpdateParameters = field(default_factory=PresetUpdateParameters)
+    update: UpdateParameters = field(default_factory=lambda: PresetUpdateParameters(desired_bl=31))
 
 
 @dataclass
 class TTv2EcRamPreset(UnitCellRPUConfig):
     """Configuration using TTv2 with
     :class:`~aihwkit.simulator.presets.devices.EcRamPresetDevice`.
 
-    See :class:`~aihwkit.simulator.configs.devices.BufferedTransferCompound`
+    See :class:`~aihwkit.simulator.configs.devices.ChoppedTransferCompound`
     for details on TTv2-like optimizers.
 
     The default peripheral hardware
     (:class:`~aihwkit.simulator.presets.utils.PresetIOParameters`) and
     analog update
     (:class:`~aihwkit.simulator.presets.utils.PresetUpdateParameters`)
     configuration is used otherwise.
     """
 
     device: UnitCell = field(
-        default_factory=lambda: BufferedTransferCompound(
+        default_factory=lambda: ChoppedTransferCompound(
             unit_cell_devices=[EcRamPresetDevice(), EcRamPresetDevice()],
-            transfer_forward=PresetIOParameters(),
-            transfer_update=PresetUpdateParameters(),
-            transfer_every=1.0,
-            units_in_mbatch=True,
-            ))
+            transfer_forward=PresetIOParameters(
+                noise_management=NoiseManagementType.NONE, bound_management=BoundManagementType.NONE
+            ),
+            transfer_update=PresetUpdateParameters(
+                desired_bl=1, update_bl_management=False, update_management=False
+            ),
+            units_in_mbatch=False,
+            in_chop_prob=0.0,
+            fast_lr=0.1,
+            auto_scale=True,
+            auto_granularity=500,
+        )
+    )
     forward: IOParameters = field(default_factory=PresetIOParameters)
     backward: IOParameters = field(default_factory=PresetIOParameters)
-    update: UpdateParameters = field(default_factory=PresetUpdateParameters)
+    update: UpdateParameters = field(default_factory=lambda: PresetUpdateParameters(desired_bl=100))
 
 
 @dataclass
 class TTv2EcRamMOPreset(UnitCellRPUConfig):
     """Configuration using TTv2 with
     :class:`~aihwkit.simulator.presets.devices.EcRamMOPresetDevice`.
 
-    See :class:`~aihwkit.simulator.configs.devices.BufferedTransferCompound`
+    See :class:`~aihwkit.simulator.configs.devices.ChoppedTransferCompound`
     for details on TTv2-like optimizers.
 
     The default peripheral hardware
     (:class:`~aihwkit.simulator.presets.utils.PresetIOParameters`) and
     analog update
     (:class:`~aihwkit.simulator.presets.utils.PresetUpdateParameters`)
     configuration is used otherwise.
     """
 
     device: UnitCell = field(
-        default_factory=lambda: BufferedTransferCompound(
+        default_factory=lambda: ChoppedTransferCompound(
             unit_cell_devices=[EcRamMOPresetDevice(), EcRamMOPresetDevice()],
-            transfer_forward=PresetIOParameters(),
-            transfer_update=PresetUpdateParameters(),
-            transfer_every=1.0,
-            units_in_mbatch=True,
-            ))
+            transfer_forward=PresetIOParameters(
+                noise_management=NoiseManagementType.NONE, bound_management=BoundManagementType.NONE
+            ),
+            transfer_update=PresetUpdateParameters(
+                desired_bl=1, update_bl_management=False, update_management=False
+            ),
+            units_in_mbatch=False,
+            in_chop_prob=0.0,
+            fast_lr=0.1,
+            auto_scale=True,
+            auto_granularity=500,
+        )
+    )
     forward: IOParameters = field(default_factory=PresetIOParameters)
     backward: IOParameters = field(default_factory=PresetIOParameters)
-    update: UpdateParameters = field(default_factory=PresetUpdateParameters)
+    update: UpdateParameters = field(default_factory=lambda: PresetUpdateParameters(desired_bl=100))
 
 
 @dataclass
 class TTv2IdealizedPreset(UnitCellRPUConfig):
     """Configuration using TTv2 with
     :class:`~aihwkit.simulator.presets.devices.IdealizedPresetDevice`.
 
-    See :class:`~aihwkit.simulator.configs.devices.BufferedTransferCompound`
+    See :class:`~aihwkit.simulator.configs.devices.ChoppedTransferCompound`
     for details on TTv2-like optimizers.
 
     The default peripheral hardware
     (:class:`~aihwkit.simulator.presets.utils.PresetIOParameters`) and
     analog update
     (:class:`~aihwkit.simulator.presets.utils.PresetUpdateParameters`)
     configuration is used otherwise.
     """
 
     device: UnitCell = field(
-        default_factory=lambda: BufferedTransferCompound(
+        default_factory=lambda: ChoppedTransferCompound(
             unit_cell_devices=[IdealizedPresetDevice(), IdealizedPresetDevice()],
-            transfer_forward=PresetIOParameters(),
-            transfer_update=PresetUpdateParameters(),
-            transfer_every=1.0,
-            units_in_mbatch=True,
-            ))
+            transfer_forward=PresetIOParameters(
+                noise_management=NoiseManagementType.NONE, bound_management=BoundManagementType.NONE
+            ),
+            transfer_update=PresetUpdateParameters(
+                desired_bl=1, update_bl_management=False, update_management=False
+            ),
+            units_in_mbatch=False,
+            in_chop_prob=0.0,
+            fast_lr=0.1,
+            auto_scale=True,
+            auto_granularity=500,
+        )
+    )
     forward: IOParameters = field(default_factory=PresetIOParameters)
     backward: IOParameters = field(default_factory=PresetIOParameters)
-    update: UpdateParameters = field(default_factory=PresetUpdateParameters)
+    update: UpdateParameters = field(default_factory=lambda: PresetUpdateParameters(desired_bl=100))
+
+
+# Chopped-TTv2 configs.
+
+
+@dataclass
+class ChoppedTTv2ReRamESPreset(UnitCellRPUConfig):
+    """Configuration using ChoppedTTv2 with
+    :class:`~aihwkit.simulator.presets.devices.ReRamESPresetDevice`.
+
+    See :class:`~aihwkit.simulator.configs.devices.ChoppedTransferCompound`
+    for details on TTv2-like optimizers.
+
+    The default peripheral hardware
+    (:class:`~aihwkit.simulator.presets.utils.PresetIOParameters`) and
+    analog update
+    (:class:`~aihwkit.simulator.presets.utils.PresetUpdateParameters`)
+    configuration is used otherwise.
+    """
+
+    device: UnitCell = field(
+        default_factory=lambda: ChoppedTransferCompound(
+            unit_cell_devices=[ReRamESPresetDevice(), ReRamESPresetDevice()],
+            transfer_forward=PresetIOParameters(
+                noise_management=NoiseManagementType.NONE, bound_management=BoundManagementType.NONE
+            ),
+            transfer_update=PresetUpdateParameters(
+                desired_bl=1, update_bl_management=False, update_management=False
+            ),
+            units_in_mbatch=False,
+            in_chop_prob=0.01,
+            fast_lr=0.5,
+            auto_scale=True,
+            auto_granularity=1000,
+        )
+    )
+    forward: IOParameters = field(default_factory=PresetIOParameters)
+    backward: IOParameters = field(default_factory=PresetIOParameters)
+    update: UpdateParameters = field(default_factory=lambda: PresetUpdateParameters(desired_bl=31))
+
+
+@dataclass
+class ChoppedTTv2ReRamSBPreset(UnitCellRPUConfig):
+    """Configuration using ChoppedTTv2 with
+    :class:`~aihwkit.simulator.presets.devices.ReRamSBPresetDevice`.
+
+    See :class:`~aihwkit.simulator.configs.devices.ChoppedTransferCompound`
+    for details on ChoppedTTv2-like optimizers.
+
+    The default peripheral hardware
+    (:class:`~aihwkit.simulator.presets.utils.PresetIOParameters`) and
+    analog update
+    (:class:`~aihwkit.simulator.presets.utils.PresetUpdateParameters`)
+    configuration is used otherwise.
+    """
+
+    device: UnitCell = field(
+        default_factory=lambda: ChoppedTransferCompound(
+            unit_cell_devices=[
+                ReRamSBPresetDevice(subtract_symmetry_point=True),
+                ReRamSBPresetDevice(subtract_symmetry_point=True),
+            ],
+            transfer_forward=PresetIOParameters(
+                noise_management=NoiseManagementType.NONE, bound_management=BoundManagementType.NONE
+            ),
+            transfer_update=PresetUpdateParameters(
+                desired_bl=1, update_bl_management=False, update_management=False
+            ),
+            units_in_mbatch=False,
+            in_chop_prob=0.01,
+            fast_lr=0.5,
+            auto_scale=True,
+            auto_granularity=1000,
+        )
+    )
+    forward: IOParameters = field(default_factory=PresetIOParameters)
+    backward: IOParameters = field(default_factory=PresetIOParameters)
+    update: UpdateParameters = field(default_factory=lambda: PresetUpdateParameters(desired_bl=31))
+
+
+@dataclass
+class ChoppedTTv2CapacitorPreset(UnitCellRPUConfig):
+    """Configuration using ChoppedTTv2 with
+    :class:`~aihwkit.simulator.presets.devices.CapacitorPresetDevice`.
+
+    See :class:`~aihwkit.simulator.configs.devices.ChoppedTransferCompound`
+    for details on ChoppedTTv2-like optimizers.
+
+    The default peripheral hardware
+    (:class:`~aihwkit.simulator.presets.utils.PresetIOParameters`) and
+    analog update
+    (:class:`~aihwkit.simulator.presets.utils.PresetUpdateParameters`)
+    configuration is used otherwise.
+    """
+
+    device: UnitCell = field(
+        default_factory=lambda: ChoppedTransferCompound(
+            unit_cell_devices=[CapacitorPresetDevice(), CapacitorPresetDevice()],
+            transfer_forward=PresetIOParameters(
+                noise_management=NoiseManagementType.NONE, bound_management=BoundManagementType.NONE
+            ),
+            transfer_update=PresetUpdateParameters(
+                desired_bl=1, update_bl_management=False, update_management=False
+            ),
+            units_in_mbatch=False,
+            in_chop_prob=0.01,
+            fast_lr=0.1,
+            auto_scale=True,
+            auto_granularity=1000,
+        )
+    )
+    forward: IOParameters = field(default_factory=PresetIOParameters)
+    backward: IOParameters = field(default_factory=PresetIOParameters)
+    update: UpdateParameters = field(default_factory=lambda: PresetUpdateParameters(desired_bl=31))
+
+
+@dataclass
+class ChoppedTTv2EcRamPreset(UnitCellRPUConfig):
+    """Configuration using ChoppedTTv2 with
+    :class:`~aihwkit.simulator.presets.devices.EcRamPresetDevice`.
+
+    See :class:`~aihwkit.simulator.configs.devices.ChoppedTransferCompound`
+    for details on TTv2-like optimizers.
+
+    The default peripheral hardware
+    (:class:`~aihwkit.simulator.presets.utils.PresetIOParameters`) and
+    analog update
+    (:class:`~aihwkit.simulator.presets.utils.PresetUpdateParameters`)
+    configuration is used otherwise.
+    """
+
+    device: UnitCell = field(
+        default_factory=lambda: ChoppedTransferCompound(
+            unit_cell_devices=[EcRamPresetDevice(), EcRamPresetDevice()],
+            transfer_forward=PresetIOParameters(
+                noise_management=NoiseManagementType.NONE, bound_management=BoundManagementType.NONE
+            ),
+            transfer_update=PresetUpdateParameters(
+                desired_bl=1, update_bl_management=False, update_management=False
+            ),
+            auto_granularity=500,
+            in_chop_prob=0.01,
+            units_in_mbatch=False,
+            auto_scale=True,
+        )
+    )
+    forward: IOParameters = field(default_factory=PresetIOParameters)
+    backward: IOParameters = field(default_factory=PresetIOParameters)
+    update: UpdateParameters = field(default_factory=lambda: PresetUpdateParameters(desired_bl=100))
+
+
+@dataclass
+class ChoppedTTv2EcRamMOPreset(UnitCellRPUConfig):
+    """Configuration using ChoppedTTv2 with
+    :class:`~aihwkit.simulator.presets.devices.EcRamMOPresetDevice`.
+
+    See :class:`~aihwkit.simulator.configs.devices.ChoppedTransferCompound`
+    for details on TTv2-like optimizers.
+
+    The default peripheral hardware
+    (:class:`~aihwkit.simulator.presets.utils.PresetIOParameters`) and
+    analog update
+    (:class:`~aihwkit.simulator.presets.utils.PresetUpdateParameters`)
+    configuration is used otherwise.
+    """
+
+    device: UnitCell = field(
+        default_factory=lambda: ChoppedTransferCompound(
+            unit_cell_devices=[EcRamMOPresetDevice(), EcRamMOPresetDevice()],
+            transfer_forward=PresetIOParameters(
+                noise_management=NoiseManagementType.NONE, bound_management=BoundManagementType.NONE
+            ),
+            transfer_update=PresetUpdateParameters(
+                desired_bl=1, update_bl_management=False, update_management=False
+            ),
+            units_in_mbatch=False,
+            in_chop_prob=0.001,
+            fast_lr=0.1,
+            auto_scale=True,
+            auto_granularity=500,
+        )
+    )
+    forward: IOParameters = field(default_factory=PresetIOParameters)
+    backward: IOParameters = field(default_factory=PresetIOParameters)
+    update: UpdateParameters = field(default_factory=lambda: PresetUpdateParameters(desired_bl=100))
+
+
+@dataclass
+class ChoppedTTv2IdealizedPreset(UnitCellRPUConfig):
+    """Configuration using ChoppedTTv2 with
+    :class:`~aihwkit.simulator.presets.devices.IdealizedPresetDevice`.
+
+    See :class:`~aihwkit.simulator.configs.devices.ChoppedTransferCompound`
+    for details on TTv2-like optimizers.
+
+    The default peripheral hardware
+    (:class:`~aihwkit.simulator.presets.utils.PresetIOParameters`) and
+    analog update
+    (:class:`~aihwkit.simulator.presets.utils.PresetUpdateParameters`)
+    configuration is used otherwise.
+    """
+
+    device: UnitCell = field(
+        default_factory=lambda: ChoppedTransferCompound(
+            unit_cell_devices=[IdealizedPresetDevice(), IdealizedPresetDevice()],
+            transfer_forward=PresetIOParameters(
+                noise_management=NoiseManagementType.NONE, bound_management=BoundManagementType.NONE
+            ),
+            transfer_update=PresetUpdateParameters(
+                desired_bl=1, update_bl_management=False, update_management=False
+            ),
+            units_in_mbatch=False,
+            in_chop_prob=0.001,
+            fast_lr=0.1,
+            auto_scale=True,
+            auto_granularity=500,
+        )
+    )
+    forward: IOParameters = field(default_factory=PresetIOParameters)
+    backward: IOParameters = field(default_factory=PresetIOParameters)
+    update: UpdateParameters = field(default_factory=lambda: PresetUpdateParameters(desired_bl=100))
+
+
+# AGAD configs.
+
+
+@dataclass
+class AGADReRamESPreset(UnitCellRPUConfig):
+    """Configuration using AGAD with
+    :class:`~aihwkit.simulator.presets.devices.ReRamESPresetDevice`.
+
+    See :class:`~aihwkit.simulator.configs.devices.DynamicTransferCompound`
+    for details on TTv2-like optimizers.
+
+    The default peripheral hardware
+    (:class:`~aihwkit.simulator.presets.utils.PresetIOParameters`) and
+    analog update
+    (:class:`~aihwkit.simulator.presets.utils.PresetUpdateParameters`)
+    configuration is used otherwise.
+    """
+
+    device: UnitCell = field(
+        default_factory=lambda: DynamicTransferCompound(
+            unit_cell_devices=[ReRamESPresetDevice(), ReRamESPresetDevice()],
+            transfer_forward=PresetIOParameters(
+                noise_management=NoiseManagementType.NONE, bound_management=BoundManagementType.NONE
+            ),
+            transfer_update=PresetUpdateParameters(
+                desired_bl=1, update_bl_management=False, update_management=False
+            ),
+            fast_lr=0.5,
+            auto_granularity=1000,
+            tail_weightening=5.0,
+            in_chop_prob=0.02,
+            auto_scale=True,
+        )
+    )
+    forward: IOParameters = field(default_factory=PresetIOParameters)
+    backward: IOParameters = field(default_factory=PresetIOParameters)
+    update: UpdateParameters = field(default_factory=lambda: PresetUpdateParameters(desired_bl=31))
+
+
+@dataclass
+class AGADReRamSBPreset(UnitCellRPUConfig):
+    """Configuration using AGAD with
+    :class:`~aihwkit.simulator.presets.devices.ReRamSBPresetDevice`.
+
+    See :class:`~aihwkit.simulator.configs.devices.DynamicTransferCompound`
+    for details on AGAD-like optimizers.
+
+    The default peripheral hardware
+    (:class:`~aihwkit.simulator.presets.utils.PresetIOParameters`) and
+    analog update
+    (:class:`~aihwkit.simulator.presets.utils.PresetUpdateParameters`)
+    configuration is used otherwise.
+    """
+
+    device: UnitCell = field(
+        default_factory=lambda: DynamicTransferCompound(
+            unit_cell_devices=[
+                ReRamSBPresetDevice(subtract_symmetry_point=True),
+                ReRamSBPresetDevice(subtract_symmetry_point=True),
+            ],
+            transfer_forward=PresetIOParameters(
+                noise_management=NoiseManagementType.NONE, bound_management=BoundManagementType.NONE
+            ),
+            transfer_update=PresetUpdateParameters(
+                desired_bl=1, update_bl_management=False, update_management=False
+            ),
+            fast_lr=0.5,
+            auto_granularity=1000,
+            tail_weightening=5.0,
+            in_chop_prob=0.02,
+            auto_scale=True,
+        )
+    )
+    forward: IOParameters = field(default_factory=PresetIOParameters)
+    backward: IOParameters = field(default_factory=PresetIOParameters)
+    update: UpdateParameters = field(default_factory=lambda: PresetUpdateParameters(desired_bl=31))
+
+
+@dataclass
+class AGADCapacitorPreset(UnitCellRPUConfig):
+    """Configuration using AGAD with
+    :class:`~aihwkit.simulator.presets.devices.CapacitorPresetDevice`.
+
+    See :class:`~aihwkit.simulator.configs.devices.DynamicTransferCompound`
+    for details on AGAD-like optimizers.
+
+    The default peripheral hardware
+    (:class:`~aihwkit.simulator.presets.utils.PresetIOParameters`) and
+    analog update
+    (:class:`~aihwkit.simulator.presets.utils.PresetUpdateParameters`)
+    configuration is used otherwise.
+    """
+
+    device: UnitCell = field(
+        default_factory=lambda: DynamicTransferCompound(
+            unit_cell_devices=[CapacitorPresetDevice(), CapacitorPresetDevice()],
+            transfer_forward=PresetIOParameters(
+                noise_management=NoiseManagementType.NONE, bound_management=BoundManagementType.NONE
+            ),
+            transfer_update=PresetUpdateParameters(
+                desired_bl=1, update_bl_management=False, update_management=False
+            ),
+            units_in_mbatch=False,
+            fast_lr=0.1,
+            auto_granularity=1000,
+            tail_weightening=20.0,
+            in_chop_prob=0.01,
+            auto_scale=True,
+        )
+    )
+    forward: IOParameters = field(default_factory=PresetIOParameters)
+    backward: IOParameters = field(default_factory=PresetIOParameters)
+    update: UpdateParameters = field(default_factory=lambda: PresetUpdateParameters(desired_bl=31))
+
+
+@dataclass
+class AGADEcRamPreset(UnitCellRPUConfig):
+    """Configuration using AGAD with
+    :class:`~aihwkit.simulator.presets.devices.EcRamPresetDevice`.
+
+    See :class:`~aihwkit.simulator.configs.devices.DynamicTransferCompound`
+    for details on TTv2-like optimizers.
+
+    The default peripheral hardware
+    (:class:`~aihwkit.simulator.presets.utils.PresetIOParameters`) and
+    analog update
+    (:class:`~aihwkit.simulator.presets.utils.PresetUpdateParameters`)
+    configuration is used otherwise.
+    """
+
+    device: UnitCell = field(
+        default_factory=lambda: DynamicTransferCompound(
+            unit_cell_devices=[EcRamPresetDevice(), EcRamPresetDevice()],
+            transfer_forward=PresetIOParameters(
+                noise_management=NoiseManagementType.NONE, bound_management=BoundManagementType.NONE
+            ),
+            transfer_update=PresetUpdateParameters(
+                desired_bl=1, update_bl_management=False, update_management=False
+            ),
+            fast_lr=0.1,
+            auto_granularity=750,
+            tail_weightening=50.0,
+            in_chop_prob=0.005,
+            units_in_mbatch=False,
+            auto_scale=True,
+        )
+    )
+    forward: IOParameters = field(default_factory=PresetIOParameters)
+    backward: IOParameters = field(default_factory=PresetIOParameters)
+    update: UpdateParameters = field(default_factory=lambda: PresetUpdateParameters(desired_bl=31))
+
+
+@dataclass
+class AGADEcRamMOPreset(UnitCellRPUConfig):
+    """Configuration using AGAD with
+    :class:`~aihwkit.simulator.presets.devices.EcRamMOPresetDevice`.
+
+    See :class:`~aihwkit.simulator.configs.devices.DynamicTransferCompound`
+    for details on TTv2-like optimizers.
+
+    The default peripheral hardware
+    (:class:`~aihwkit.simulator.presets.utils.PresetIOParameters`) and
+    analog update
+    (:class:`~aihwkit.simulator.presets.utils.PresetUpdateParameters`)
+    configuration is used otherwise.
+    """
+
+    device: UnitCell = field(
+        default_factory=lambda: DynamicTransferCompound(
+            unit_cell_devices=[EcRamMOPresetDevice(), EcRamMOPresetDevice()],
+            transfer_forward=PresetIOParameters(
+                noise_management=NoiseManagementType.NONE, bound_management=BoundManagementType.NONE
+            ),
+            transfer_update=PresetUpdateParameters(
+                desired_bl=1, update_bl_management=False, update_management=False
+            ),
+            fast_lr=0.1,
+            auto_granularity=500,
+            tail_weightening=50.0,
+            in_chop_prob=0.005,
+            units_in_mbatch=False,
+            auto_scale=True,
+        )
+    )
+    forward: IOParameters = field(default_factory=PresetIOParameters)
+    backward: IOParameters = field(default_factory=PresetIOParameters)
+    update: UpdateParameters = field(default_factory=lambda: PresetUpdateParameters(desired_bl=100))
+
+
+@dataclass
+class AGADIdealizedPreset(UnitCellRPUConfig):
+    """Configuration using AGAD with
+    :class:`~aihwkit.simulator.presets.devices.IdealizedPresetDevice`.
+
+    See :class:`~aihwkit.simulator.configs.devices.DynamicTransferCompound`
+    for details on TTv2-like optimizers.
+
+    The default peripheral hardware
+    (:class:`~aihwkit.simulator.presets.utils.PresetIOParameters`) and
+    analog update
+    (:class:`~aihwkit.simulator.presets.utils.PresetUpdateParameters`)
+    configuration is used otherwise.
+    """
+
+    device: UnitCell = field(
+        default_factory=lambda: DynamicTransferCompound(
+            unit_cell_devices=[IdealizedPresetDevice(), IdealizedPresetDevice()],
+            transfer_forward=PresetIOParameters(
+                noise_management=NoiseManagementType.NONE, bound_management=BoundManagementType.NONE
+            ),
+            transfer_update=PresetUpdateParameters(
+                desired_bl=1, update_bl_management=False, update_management=False
+            ),
+            fast_lr=0.1,
+            auto_granularity=500,
+            tail_weightening=50.0,
+            in_chop_prob=0.005,
+            units_in_mbatch=False,
+            auto_scale=True,
+        )
+    )
+    forward: IOParameters = field(default_factory=PresetIOParameters)
+    backward: IOParameters = field(default_factory=PresetIOParameters)
+    update: UpdateParameters = field(default_factory=lambda: PresetUpdateParameters(desired_bl=100))
+
 
 # Mixed precision presets
 
 
 @dataclass
 class MixedPrecisionReRamESPreset(DigitalRankUpdateRPUConfig):
     """Configuration using Mixed-precision with
@@ -875,17 +1455,16 @@
     (:class:`~aihwkit.simulator.presets.utils.PresetIOParameters`) and
     analog update
     (:class:`~aihwkit.simulator.presets.utils.PresetUpdateParameters`)
     configuration is used otherwise.
     """
 
     device: DigitalRankUpdateCell = field(
-        default_factory=lambda: MixedPrecisionCompound(
-            device=ReRamESPresetDevice(),
-        ))
+        default_factory=lambda: MixedPrecisionCompound(device=ReRamESPresetDevice())
+    )
     forward: IOParameters = field(default_factory=PresetIOParameters)
     backward: IOParameters = field(default_factory=PresetIOParameters)
     update: UpdateParameters = field(default_factory=PresetUpdateParameters)
 
 
 @dataclass
 class MixedPrecisionReRamSBPreset(DigitalRankUpdateRPUConfig):
@@ -899,17 +1478,16 @@
     (:class:`~aihwkit.simulator.presets.utils.PresetIOParameters`) and
     analog update
     (:class:`~aihwkit.simulator.presets.utils.PresetUpdateParameters`)
     configuration is used otherwise.
     """
 
     device: DigitalRankUpdateCell = field(
-        default_factory=lambda: MixedPrecisionCompound(
-            device=ReRamSBPresetDevice(),
-        ))
+        default_factory=lambda: MixedPrecisionCompound(device=ReRamSBPresetDevice())
+    )
     forward: IOParameters = field(default_factory=PresetIOParameters)
     backward: IOParameters = field(default_factory=PresetIOParameters)
     update: UpdateParameters = field(default_factory=PresetUpdateParameters)
 
 
 @dataclass
 class MixedPrecisionCapacitorPreset(DigitalRankUpdateRPUConfig):
@@ -923,17 +1501,16 @@
     (:class:`~aihwkit.simulator.presets.utils.PresetIOParameters`) and
     analog update
     (:class:`~aihwkit.simulator.presets.utils.PresetUpdateParameters`)
     configuration is used otherwise.
     """
 
     device: DigitalRankUpdateCell = field(
-        default_factory=lambda: MixedPrecisionCompound(
-            device=CapacitorPresetDevice(),
-        ))
+        default_factory=lambda: MixedPrecisionCompound(device=CapacitorPresetDevice())
+    )
     forward: IOParameters = field(default_factory=PresetIOParameters)
     backward: IOParameters = field(default_factory=PresetIOParameters)
     update: UpdateParameters = field(default_factory=PresetUpdateParameters)
 
 
 @dataclass
 class MixedPrecisionEcRamPreset(DigitalRankUpdateRPUConfig):
@@ -947,17 +1524,16 @@
     (:class:`~aihwkit.simulator.presets.utils.PresetIOParameters`) and
     analog update
     (:class:`~aihwkit.simulator.presets.utils.PresetUpdateParameters`)
     configuration is used otherwise.
     """
 
     device: DigitalRankUpdateCell = field(
-        default_factory=lambda: MixedPrecisionCompound(
-            device=EcRamPresetDevice(),
-        ))
+        default_factory=lambda: MixedPrecisionCompound(device=EcRamPresetDevice())
+    )
     forward: IOParameters = field(default_factory=PresetIOParameters)
     backward: IOParameters = field(default_factory=PresetIOParameters)
     update: UpdateParameters = field(default_factory=PresetUpdateParameters)
 
 
 @dataclass
 class MixedPrecisionEcRamMOPreset(DigitalRankUpdateRPUConfig):
@@ -971,17 +1547,16 @@
     (:class:`~aihwkit.simulator.presets.utils.PresetIOParameters`) and
     analog update
     (:class:`~aihwkit.simulator.presets.utils.PresetUpdateParameters`)
     configuration is used otherwise.
     """
 
     device: DigitalRankUpdateCell = field(
-        default_factory=lambda: MixedPrecisionCompound(
-            device=EcRamMOPresetDevice(),
-        ))
+        default_factory=lambda: MixedPrecisionCompound(device=EcRamMOPresetDevice())
+    )
     forward: IOParameters = field(default_factory=PresetIOParameters)
     backward: IOParameters = field(default_factory=PresetIOParameters)
     update: UpdateParameters = field(default_factory=PresetUpdateParameters)
 
 
 @dataclass
 class MixedPrecisionIdealizedPreset(DigitalRankUpdateRPUConfig):
@@ -995,17 +1570,16 @@
     (:class:`~aihwkit.simulator.presets.utils.PresetIOParameters`) and
     analog update
     (:class:`~aihwkit.simulator.presets.utils.PresetUpdateParameters`)
     configuration is used otherwise.
     """
 
     device: DigitalRankUpdateCell = field(
-        default_factory=lambda: MixedPrecisionCompound(
-            device=IdealizedPresetDevice(),
-        ))
+        default_factory=lambda: MixedPrecisionCompound(device=IdealizedPresetDevice())
+    )
     forward: IOParameters = field(default_factory=PresetIOParameters)
     backward: IOParameters = field(default_factory=PresetIOParameters)
     update: UpdateParameters = field(default_factory=PresetUpdateParameters)
 
 
 @dataclass
 class MixedPrecisionGokmenVlasovPreset(DigitalRankUpdateRPUConfig):
@@ -1019,17 +1593,16 @@
     (:class:`~aihwkit.simulator.presets.utils.PresetIOParameters`) and
     analog update
     (:class:`~aihwkit.simulator.presets.utils.PresetUpdateParameters`)
     configuration is used otherwise.
     """
 
     device: DigitalRankUpdateCell = field(
-        default_factory=lambda: MixedPrecisionCompound(
-            device=GokmenVlasovPresetDevice(),
-        ))
+        default_factory=lambda: MixedPrecisionCompound(device=GokmenVlasovPresetDevice())
+    )
     forward: IOParameters = field(default_factory=PresetIOParameters)
     backward: IOParameters = field(default_factory=PresetIOParameters)
     update: UpdateParameters = field(default_factory=PresetUpdateParameters)
 
 
 @dataclass
 class MixedPrecisionPCMPreset(DigitalRankUpdateRPUConfig):
@@ -1043,13 +1616,12 @@
     (:class:`~aihwkit.simulator.presets.utils.PresetIOParameters`) and
     analog update
     (:class:`~aihwkit.simulator.presets.utils.PresetUpdateParameters`)
     configuration is used otherwise.
     """
 
     device: DigitalRankUpdateCell = field(
-        default_factory=lambda: MixedPrecisionCompound(
-            device=PCMPresetUnitCell(),
-        ))
+        default_factory=lambda: MixedPrecisionCompound(device=PCMPresetUnitCell())
+    )
     forward: IOParameters = field(default_factory=PresetIOParameters)
     backward: IOParameters = field(default_factory=PresetIOParameters)
     update: UpdateParameters = field(default_factory=PresetUpdateParameters)
```

### Comparing `aihwkit-0.7.1/src/aihwkit/simulator/presets/devices.py` & `aihwkit-0.8.0/src/aihwkit/simulator/presets/devices.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -12,39 +12,43 @@
 
 """Device configurations presets for resistive processing units."""
 
 # pylint: disable=too-many-instance-attributes
 
 from dataclasses import dataclass
 from aihwkit.simulator.configs.devices import (
-    ConstantStepDevice, ExpStepDevice, LinearStepDevice, SoftBoundsDevice
+    ConstantStepDevice,
+    ExpStepDevice,
+    LinearStepDevice,
+    SoftBoundsReferenceDevice,
 )
 
 
 @dataclass
 class ReRamESPresetDevice(ExpStepDevice):
     """Preset configuration for a single RRAM analog resistive processing
     unit based on exp. step device.
 
     Fit of the model :class:`ExpStepDevice` to  `Gong & al., Nat. Commun., 2018`_.
 
     .. _`Gong & al., Nat. Commun., 2018`: https://www.nature.com/articles/s41467-018-04485-1
     """
+
     # pylint: disable=invalid-name
 
     dw_min: float = 0.00135
     up_down: float = 0.259359
 
     w_max: float = 1.0
     w_min: float = -1.0
 
     a: float = -0.5
     b: float = -0.5
-    gamma_up: float = 5.
-    gamma_down: float = 5.
+    gamma_up: float = 5.0
+    gamma_down: float = 5.0
     A_up: float = -1.18445
     A_down: float = -0.081404
 
     # Device-to-device var.
     dw_min_dtod: float = 0.2  # a little reduced compared to SB because of non-linearity
     up_down_dtod: float = 0.05
 
@@ -53,15 +57,15 @@
 
     # Cycle-to_cycle.
     dw_min_std: float = 5.0
     write_noise_std: float = 75.0
 
 
 @dataclass
-class ReRamSBPresetDevice(SoftBoundsDevice):
+class ReRamSBPresetDevice(SoftBoundsReferenceDevice):
     """Preset configuration for a single ReRAM analog resistive processing
     unit based on soft bounds device.
 
     Loose fit of the model :class:`SoftBoundsDevice` to  `Gong & al., Nat. Commun., 2018`_.
 
     Note:
         Here it is assumed that the devices have been calibrated to the symmetry
@@ -79,42 +83,42 @@
 
     mult_noise: bool = False
 
     # Device-to-device var.
     dw_min_dtod: float = 0.3
     up_down_dtod: float = 0.01  # assumes symmetry point corrected.
 
-    w_max_dtod: float = 0.3/1.25
-    w_min_dtod: float = 0.3/0.75
+    w_max_dtod: float = 0.3 / 1.25
+    w_min_dtod: float = 0.3 / 0.75
 
     # Cycle-to_cycle.
     dw_min_std: float = 3.75
     write_noise_std: float = 56
 
 
 @dataclass
 class CapacitorPresetDevice(LinearStepDevice):
     """Preset configuration for a single capacitor resistive processing
     unit based on linear step device.
 
-    Fit of the model :class:`LinearStepDevice` to  `Li & al., VLSI, 2018`_
+    Fit of the model :class:`LinearStepDevice` to  `Li et al., VLSI, 2018`_
 
     Here some capacitor leakage is assumed as well.
 
     Caution:
         Capacitor leakage is applied only once per mini-batch and this
         the size of the leakage has to be adapted by the user as it
         depends not only on the size of the leak of the physical
         capacitor but also on the assumptions how much physical time
         is required for a full forward and backward cycle through the
         network (which depends on whether one assumes pipelining or not).
 
         The parameter ``lifetime`` needs to be adjusted accordingly.
 
-    .. _`Li & al., VLSI, 2018`: https://ieeexplore.ieee.org/abstract/document/8510648
+    .. _`Li et al., VLSI, 2018`: https://ieeexplore.ieee.org/abstract/document/8510648
     """
 
     dw_min: float = 0.005
     up_down: float = 0.0
 
     w_max: float = 1.0
     w_min: float = -1.0
@@ -313,21 +317,22 @@
 
         When the device is reset, device-to-device and cycle-to-cycle
         variation is assumed.
 
     .. _`Nandakumar et al., Front. Neurosci. 2020`: \
         https://www.frontiersin.org/articles/10.3389/fnins.2020.00406/full
     """
+
     # pylint: disable=invalid-name
 
     dw_min: float = 0.01
     up_down: float = 0.0
 
-    w_max: float = 2.
-    w_min: float = 0.
+    w_max: float = 2.0
+    w_min: float = 0.0
 
     a: float = -1.0  # scales with w_max
     b: float = 0.0
     gamma_up: float = 2.5
     gamma_down: float = 2.5
     A_up: float = -27.235
     A_down: float = -2.235
@@ -345,7 +350,125 @@
     dw_min_std_slope: float = 0.108  # dw_min_std/0.6*1.3/20
 
     write_noise_std: float = 0.0
 
     # reset behavior
     reset: float = 0.01
     reset_dtod: float = 0.02
+
+
+@dataclass
+class ReRamArrayOMPresetDevice(SoftBoundsReferenceDevice):
+    r"""Preset configuration for a single ReRAM analog resistive processing
+    unit based on soft bounds reference device.
+
+    This parameter setting was obtained from ReRAM device array measurements
+    as described in `Gong & Rasch et al., IEDM., 2022`_.
+
+    This setting is a fit to the "Optimized Material" ReRAM device
+    array in the article.
+
+    Here the :class:`SoftBoundsReferenceDevice` is used as device
+    model class, so that the symmetry point can be easily
+    subtracted. The subtraction is by default on, assuming 5 \% of
+    :math:`w_\max` error (adjustable with ``reference_std``).
+
+    Note:
+
+        Here the weight range is compliance adjusted as described in
+        the article.
+
+        The corrupt device probability (which is 13.5 \% for the array
+        measured) is by default set to zero. It can be set with the
+        ``corrupt_devices_prob`` parameter.
+
+    .. _`Gong & Rasch et al., IEDM., 2022`: \
+        https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10019569
+
+    """
+
+    enforce_consistency: bool = True
+    dw_min_dtod_log_normal: bool = True
+
+    dw_min: float = 0.0949
+    up_down: float = 0.0
+
+    w_max: float = 1.0  # 1.4839
+    w_min: float = -1.0  # -0.6192
+
+    mult_noise: bool = False
+
+    # Device-to-device var.
+    dw_min_dtod: float = 0.7829
+    up_down_dtod: float = 0.01
+
+    w_max_dtod: float = 0.3499
+    w_min_dtod: float = 0.5695
+
+    # Cycle-to_cycle.
+    dw_min_std: float = 0.4158
+    write_noise_std: float = 1.4113
+
+    corrupt_devices_range: float = 0.0100
+    corrupt_devices_prob: float = 0.0  # 0.1348
+
+    subtract_symmetry_point: bool = True
+    reference_std: float = 0.05
+
+
+@dataclass
+class ReRamArrayHfO2PresetDevice(SoftBoundsReferenceDevice):
+    r"""Preset configuration for a single ReRAM analog resistive processing
+    unit based on soft bounds reference device.
+
+    This parameter setting was obtained from ReRAM device array measurements
+    as described in `Gong & Rasch et al., IEDM., 2022`_.
+
+    This setting is a fit to the "Baseline HfO2" ReRAM device
+    array in the article.
+
+    Here the :class:`SoftBoundsReferenceDevice` is used as device
+    model class, so that the symmetry point can be easily
+    subtracted. The subtraction is by default on, assuming 5 \% of
+    :math:`w_\max` error (adjustable with ``reference_std``).
+
+    Note:
+
+        Here the weight range is compliance adjusted as described in
+        the article.
+
+        The corrupt device probability (which is 10 \% for the array
+        measured) is by default set to zero. It can be set with the
+        ``corrupt_devices_prob`` parameter.
+
+    .. _`Gong & Rasch et al., IEDM., 2022`: \
+        https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10019569
+
+    """
+
+    enforce_consistency: bool = True
+    dw_min_dtod_log_normal: bool = True
+
+    dw_min: float = 0.4622
+    up_down: float = 0.0
+
+    w_max: float = 1.0  # 1.1490
+    w_min: float = -1.0  # -1.0284
+
+    mult_noise: bool = False
+
+    # Device-to-device var.
+    dw_min_dtod: float = 0.7125
+    up_down_dtod: float = 0.01
+
+    w_max_dtod: float = 0.4295
+    w_min_dtod: float = 0.5990
+
+    # Cycle-to_cycle.
+    dw_min_std: float = 0.2174
+    write_noise_std: float = 0.5841
+
+    corrupt_devices_range: float = 0.0100
+    corrupt_devices_prob: float = 0.0  # 0.0977
+
+    subtract_symmetry_point: bool = True
+    reference_std: float = 0.05
```

### Comparing `aihwkit-0.7.1/src/aihwkit/simulator/presets/utils.py` & `aihwkit-0.8.0/src/aihwkit/simulator/presets/utils.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -12,17 +12,21 @@
 
 """Utils for configurations presets for resistive processing units."""
 
 # pylint: disable=too-many-instance-attributes
 
 from dataclasses import dataclass
 
-from aihwkit.simulator.configs.utils import (
-    BoundManagementType, IOParameters, NoiseManagementType, PulseType,
-    UpdateParameters, WeightNoiseType
+from aihwkit.simulator.parameters.utils import (
+    BoundManagementType,
+    IOParameters,
+    NoiseManagementType,
+    PulseType,
+    UpdateParameters,
+    WeightNoiseType,
 )
 
 
 @dataclass
 class PresetIOParameters(IOParameters):
     r"""Preset for the forward and backward pass parameters.
 
@@ -40,17 +44,17 @@
     which is on the order of 1 LSB of the ADC.
 
     By default, we turned additional weight noise off, however, some
     presets might turn it on as required by the device specification.
 
     Finally, we assume by default that the device is run with bound
     management (see
-    :class:`~aihwkit.simulator.config.utils.BoundManagementType`) and
+    :class:`~aihwkit.simulator.parameters.enums.BoundManagementType`) and
     noise management (see
-    :class:`~aihwkit.simulator.config.utils.NoiseManagementType`)
+    :class:`~aihwkit.simulator.parameters.enums.NoiseManagementType`)
     turned on to `ITERATIVE` and `ABS_MAX`, respectively.
     """
 
     bound_management: BoundManagementType = BoundManagementType.ITERATIVE
     noise_management: NoiseManagementType = NoiseManagementType.ABS_MAX
 
     inp_res: float = 1.0 / (2**7 - 2)  # 7 bit DAC.
@@ -84,7 +88,38 @@
         https://www.frontiersin.org/articles/10.3389/fnins.2016.00333/full
     """
 
     desired_bl: int = 31  # Less than 32 preferable (faster implementation).
     pulse_type: PulseType = PulseType.STOCHASTIC_COMPRESSED
     update_bl_management: bool = True  # Dynamically adjusts pulse train length (max 31).
     update_management: bool = True
+
+
+@dataclass
+class StandardIOParameters(IOParameters):
+    r"""Preset for the forward and backward pass parameters.
+
+    Preset that is more aligned with the the forward pass of
+    :class:`~aihwkit.simulator.presets.configs.StandardHWATrainingPreset`,
+    as it assumes the same DAC/ ADC resolution, output bound and
+    output noise (see also `Rasch et al. ArXiv 2023`_ for a discussion)
+
+    However, here, noise and bound mangement is turned on by default,
+    and IR-drop as well as short-term weight noise is set to 0 by
+    default.
+
+    .. _`Rasch et al. ArXiv 2023`: https://arxiv.org/abs/2302.08469
+    """
+
+    bound_management: BoundManagementType = BoundManagementType.ITERATIVE
+    noise_management: NoiseManagementType = NoiseManagementType.ABS_MAX
+
+    inp_res: float = 1.0 / (2**8 - 2)  # 8 bit DAC.
+    inp_sto_round: bool = False
+
+    out_bound: float = 10.0
+    out_noise: float = 0.04
+    out_res: float = 1.0 / (2**8 - 2)  # 8 bit ADC.
+
+    # No read noise by default.
+    w_noise: float = 0.0
+    w_noise_type: WeightNoiseType = WeightNoiseType.NONE
```

### Comparing `aihwkit-0.7.1/src/aihwkit/simulator/presets/web.py` & `aihwkit-0.8.0/src/aihwkit/simulator/presets/web.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,36 +1,42 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
 # that they have been altered from the originals.
 
 """RPU configurations presets used for the composer interface."""
 
 # pylint: disable=too-many-instance-attributes
 
 from dataclasses import dataclass, field
-from aihwkit.simulator.configs.configs import (
-    InferenceRPUConfig
-)
+from aihwkit.simulator.configs.configs import InferenceRPUConfig
 
-from aihwkit.simulator.configs.utils import (
-    IOParameters, WeightClipParameter,  WeightModifierParameter, MappingParameter,
-    BoundManagementType, NoiseManagementType, WeightNoiseType, WeightModifierType,
-    WeightClipType
+from aihwkit.simulator.parameters.utils import (
+    IOParameters,
+    WeightClipParameter,
+    WeightModifierParameter,
+    MappingParameter,
+    BoundManagementType,
+    NoiseManagementType,
+    WeightNoiseType,
+    WeightModifierType,
+    WeightClipType,
 )
 from aihwkit.inference import (
-    BaseDriftCompensation, BaseNoiseModel, GlobalDriftCompensation,
-    PCMLikeNoiseModel
+    BaseDriftCompensation,
+    BaseNoiseModel,
+    GlobalDriftCompensation,
+    PCMLikeNoiseModel,
 )
 
 
 @dataclass
 class WebComposerIOParameters(IOParameters):
     r"""Preset for the forward and backward pass parameters.
 
@@ -90,16 +96,15 @@
 
     max_input_size: int = 512
     max_output_size: int = 512
 
 
 @dataclass
 class WebComposerInferenceRPUConfig(InferenceRPUConfig):
-    """Preset configuration used as default for the Inference Composer
-    """
+    """Preset configuration used as default for the Inference Composer"""
 
     forward: IOParameters = field(default_factory=WebComposerIOParameters)
     """Input-output parameter setting for the forward direction."""
 
     noise_model: BaseNoiseModel = field(default_factory=PCMLikeNoiseModel)
     """Statistical noise model to be used during (realistic) inference."""
 
@@ -126,16 +131,17 @@
 
     digital_bias: bool = False
 
     weight_scaling_omega: float = 1.0
     weight_scaling_columnwise: bool = True
     learn_out_scaling: bool = False
     out_scaling_columnwise: bool = False
+    max_input_size: int = 0
+    max_output_size: int = 0
 
 
 @dataclass
 class OldWebComposerInferenceRPUConfig(WebComposerInferenceRPUConfig):
-    """Preset configuration used as default for the Inference Composer
-    """
+    """Preset configuration used as default for the Inference Composer"""
 
     mapping: MappingParameter = field(default_factory=OldWebComposerMappingParameter)
     """Parameter related to mapping weights to tiles for supporting modules."""
```

### Comparing `aihwkit-0.7.1/src/aihwkit/simulator/rpu_base_src/rpu_base.cpp` & `aihwkit-0.8.0/src/aihwkit/simulator/rpu_base_src/rpu_base.cpp`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/aihwkit/simulator/rpu_base_src/rpu_base.h` & `aihwkit-0.8.0/src/aihwkit/simulator/rpu_base_src/rpu_base.h`

 * *Files 6% similar despite different names*

```diff
@@ -1,22 +1,24 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
  * that they have been altered from the originals.
  */
 #pragma once
 #include "rpu.h"
 #include "rpu_buffered_transfer_device.h"
+#include "rpu_chopped_transfer_device.h"
 #include "rpu_constantstep_device.h"
+#include "rpu_dynamic_transfer_device.h"
 #include "rpu_expstep_device.h"
 #include "rpu_linearstep_device.h"
 #include "rpu_mixedprec_device.h"
 #include "rpu_mixedprec_device_base.h"
 #include "rpu_onesided_device.h"
 #include "rpu_piecewisestep_device.h"
 #include "rpu_powstep_device.h"
```

### Comparing `aihwkit-0.7.1/src/aihwkit/simulator/rpu_base_src/rpu_base_devices.cpp` & `aihwkit-0.8.0/src/aihwkit/simulator/rpu_base_src/rpu_base_devices.cpp`

 * *Files 3% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -26,14 +26,16 @@
   using OneSidedParam = RPU::OneSidedRPUDeviceMetaParameter<T>;
   using TransferParam = RPU::TransferRPUDeviceMetaParameter<T>;
   using MixedPrecParam = RPU::MixedPrecRPUDeviceMetaParameter<T>;
   using PowStepParam = RPU::PowStepRPUDeviceMetaParameter<T>;
   using PowStepReferenceParam = RPU::PowStepReferenceRPUDeviceMetaParameter<T>;
   using PiecewiseStepParam = RPU::PiecewiseStepRPUDeviceMetaParameter<T>;
   using BufferedTransferParam = RPU::BufferedTransferRPUDeviceMetaParameter<T>;
+  using ChoppedTransferParam = RPU::ChoppedTransferRPUDeviceMetaParameter<T>;
+  using DynamicTransferParam = RPU::DynamicTransferRPUDeviceMetaParameter<T>;
   using SoftBoundsReferenceParam = RPU::SoftBoundsReferenceRPUDeviceMetaParameter<T>;
 
   /*
    * Trampoline classes for allowing inheritance.
    */
   class PyAbstractParam : public AbstractParam {
   public:
@@ -344,14 +346,58 @@
           rng);
     }
     T calcWeightGranularity() const override {
       PYBIND11_OVERLOAD(T, BufferedTransferParam, calcWeightGranularity, );
     }
   };
 
+  class PyChoppedTransferParam : public ChoppedTransferParam {
+  public:
+    std::string getName() const override {
+      PYBIND11_OVERLOAD(std::string, ChoppedTransferParam, getName, );
+    }
+    ChoppedTransferParam *clone() const override {
+      PYBIND11_OVERLOAD(ChoppedTransferParam *, ChoppedTransferParam, clone, );
+    }
+    RPU::DeviceUpdateType implements() const override {
+      PYBIND11_OVERLOAD(RPU::DeviceUpdateType, ChoppedTransferParam, implements, );
+    }
+    RPU::ChoppedTransferRPUDevice<T> *
+    createDevice(int x_size, int d_size, RPU::RealWorldRNG<T> *rng) override {
+      PYBIND11_OVERLOAD(
+          RPU::ChoppedTransferRPUDevice<T> *, ChoppedTransferParam, createDevice, x_size, d_size,
+          rng);
+    }
+    T calcWeightGranularity() const override {
+      PYBIND11_OVERLOAD(T, ChoppedTransferParam, calcWeightGranularity, );
+    }
+  };
+
+  class PyDynamicTransferParam : public DynamicTransferParam {
+  public:
+    std::string getName() const override {
+      PYBIND11_OVERLOAD(std::string, DynamicTransferParam, getName, );
+    }
+    DynamicTransferParam *clone() const override {
+      PYBIND11_OVERLOAD(DynamicTransferParam *, DynamicTransferParam, clone, );
+    }
+    RPU::DeviceUpdateType implements() const override {
+      PYBIND11_OVERLOAD(RPU::DeviceUpdateType, DynamicTransferParam, implements, );
+    }
+    RPU::DynamicTransferRPUDevice<T> *
+    createDevice(int x_size, int d_size, RPU::RealWorldRNG<T> *rng) override {
+      PYBIND11_OVERLOAD(
+          RPU::DynamicTransferRPUDevice<T> *, DynamicTransferParam, createDevice, x_size, d_size,
+          rng);
+    }
+    T calcWeightGranularity() const override {
+      PYBIND11_OVERLOAD(T, DynamicTransferParam, calcWeightGranularity, );
+    }
+  };
+
   class PySoftBoundsReferenceParam : public SoftBoundsReferenceParam {
   public:
     std::string getName() const override {
       PYBIND11_OVERLOAD(std::string, SoftBoundsReferenceParam, getName, );
     }
     SoftBoundsReferenceParam *clone() const override {
       PYBIND11_OVERLOAD(SoftBoundsReferenceParam *, SoftBoundsReferenceParam, clone, );
@@ -434,14 +480,15 @@
       .def_readwrite("max_bm_res", &RPU::IOMetaParameter<T>::max_bm_res)
       .def_readwrite("nm_assumed_wmax", &RPU::IOMetaParameter<T>::nm_assumed_wmax)
       .def_readwrite("nm_decay", &RPU::IOMetaParameter<T>::nm_decay)
       .def_readwrite("nm_thres", &RPU::IOMetaParameter<T>::nm_thres)
       .def_readwrite("noise_management", &RPU::IOMetaParameter<T>::noise_management)
       .def_readwrite("out_bound", &RPU::IOMetaParameter<T>::out_bound)
       .def_readwrite("out_noise", &RPU::IOMetaParameter<T>::out_noise)
+      .def_readwrite("out_noise_std", &RPU::IOMetaParameter<T>::out_noise_std)
       .def_readwrite("out_res", &RPU::IOMetaParameter<T>::out_res)
       .def_readwrite("out_scale", &RPU::IOMetaParameter<T>::out_scale)
       .def_readwrite("out_sto_round", &RPU::IOMetaParameter<T>::out_sto_round)
       .def_readwrite("w_noise", &RPU::IOMetaParameter<T>::w_noise)
       .def_readwrite("w_noise_type", &RPU::IOMetaParameter<T>::w_noise_type)
       .def_readwrite("ir_drop", &RPU::IOMetaParameter<T>::ir_drop)
       .def_readwrite("ir_drop_g_ratio", &RPU::IOMetaParameter<T>::ir_drop_Gw_div_gmax)
@@ -478,14 +525,15 @@
 
   py::class_<PulsedBaseParam, PyPulsedBaseParam, AbstractParam>(
       m, "PulsedBaseResistiveDeviceParameter")
       .def(py::init<>());
 
   py::class_<SimpleParam, PySimpleParam, AbstractParam>(m, "IdealResistiveDeviceParameter")
       .def(py::init<>())
+      .def_readwrite("reset_std", &SimpleParam::reset_std)
       .def("__str__", [](SimpleParam &self) {
         std::stringstream ss;
         self.printToStream(ss);
         return ss.str();
       });
 
   py::class_<PulsedParam, PyPulsedParam, AbstractParam>(m, "PulsedResistiveDeviceParameter")
@@ -498,22 +546,23 @@
       .def_readwrite("dw_min_dtod", &PulsedParam::dw_min_dtod)
       .def_readwrite("dw_min_dtod_log_normal", &PulsedParam::dw_min_dtod_log_normal)
       .def_readwrite("dw_min_std", &PulsedParam::dw_min_std)
       .def_readwrite("enforce_consistency", &PulsedParam::enforce_consistency)
       .def_readwrite("lifetime_dtod", &PulsedParam::lifetime_dtod)
       .def_readwrite("perfect_bias", &PulsedParam::perfect_bias)
       .def_readwrite("reset", &PulsedParam::reset)
-      .def_readwrite("reset_dtod", &PulsedParam::reset_dtod)
       .def_readwrite("reset_std", &PulsedParam::reset_std)
+      .def_readwrite("reset_dtod", &PulsedParam::reset_dtod)
       .def_readwrite("up_down", &PulsedParam::up_down)
       .def_readwrite("up_down_dtod", &PulsedParam::up_down_dtod)
       .def_readwrite("w_max", &PulsedParam::w_max)
       .def_readwrite("w_max_dtod", &PulsedParam::w_max_dtod)
       .def_readwrite("w_min", &PulsedParam::w_min)
       .def_readwrite("w_min_dtod", &PulsedParam::w_min_dtod)
+      .def_readwrite("count_pulses", &PulsedParam::count_pulses)
       .def("__str__", [](PulsedParam &self) {
         std::stringstream ss;
         self.printToStream(ss);
         return ss.str();
       });
 
   py::class_<ConstantStepParam, PyConstantStepParam, PulsedParam>(
@@ -541,14 +590,15 @@
       .def_readwrite("gamma_up", &LinearStepParam::ls_decrease_up)
       .def_readwrite("gamma_down", &LinearStepParam::ls_decrease_down)
       .def_readwrite("gamma_up_dtod", &LinearStepParam::ls_decrease_up_dtod)
       .def_readwrite("gamma_down_dtod", &LinearStepParam::ls_decrease_down_dtod)
       .def_readwrite("allow_increasing", &LinearStepParam::ls_allow_increasing_slope)
       .def_readwrite("mean_bound_reference", &LinearStepParam::ls_mean_bound_reference)
       .def_readwrite("write_noise_std", &LinearStepParam::write_noise_std)
+      .def_readwrite("apply_write_noise_on_set", &LinearStepParam::apply_write_noise_on_set)
       .def_readwrite("mult_noise", &LinearStepParam::ls_mult_noise)
       .def_readwrite("reverse_up", &LinearStepParam::ls_reverse_up)
       .def_readwrite("reverse_down", &LinearStepParam::ls_reverse_down)
       .def_readwrite("reverse_offset", &LinearStepParam::ls_reverse_offset)
       .def(
           "__str__",
           [](LinearStepParam &self) {
@@ -565,14 +615,15 @@
            float: weight granularity
         )pbdoc");
 
   py::class_<SoftBoundsParam, PySoftBoundsParam, LinearStepParam>(
       m, "SoftBoundsResistiveDeviceParameter")
       .def_readwrite("mult_noise", &SoftBoundsParam::ls_mult_noise)
       .def_readwrite("write_noise_std", &SoftBoundsParam::write_noise_std)
+      .def_readwrite("apply_write_noise_on_set", &SoftBoundsParam::apply_write_noise_on_set)
       .def_readwrite("reverse_up", &SoftBoundsParam::ls_reverse_up)
       .def_readwrite("reverse_down", &SoftBoundsParam::ls_reverse_down)
       .def_readwrite("reverse_offset", &SoftBoundsParam::ls_reverse_offset)
       .def(py::init<>())
       .def(
           "__str__",
           [](SoftBoundsParam &self) {
@@ -594,14 +645,15 @@
       .def_readwrite("A_up", &ExpStepParam::es_A_up)
       .def_readwrite("A_down", &ExpStepParam::es_A_down)
       .def_readwrite("gamma_up", &ExpStepParam::es_gamma_up)
       .def_readwrite("gamma_down", &ExpStepParam::es_gamma_down)
       .def_readwrite("a", &ExpStepParam::es_a)
       .def_readwrite("b", &ExpStepParam::es_b)
       .def_readwrite("write_noise_std", &ExpStepParam::write_noise_std)
+      .def_readwrite("apply_write_noise_on_set", &ExpStepParam::apply_write_noise_on_set)
       .def_readwrite("dw_min_std_add", &ExpStepParam::dw_min_std_add)
       .def_readwrite("dw_min_std_slope", &ExpStepParam::dw_min_std_slope)
       .def(
           "__str__",
           [](ExpStepParam &self) {
             std::stringstream ss;
             self.printToStream(ss);
@@ -735,14 +787,15 @@
   py::class_<PowStepParam, PyPowStepParam, PulsedParam>(m, "PowStepResistiveDeviceParameter")
       .def(py::init<>())
       .def_readwrite("pow_gamma", &PowStepParam::ps_gamma)
       .def_readwrite("pow_gamma_dtod", &PowStepParam::ps_gamma_dtod)
       .def_readwrite("pow_up_down", &PowStepParam::ps_gamma_up_down)
       .def_readwrite("pow_up_down_dtod", &PowStepParam::ps_gamma_up_down_dtod)
       .def_readwrite("write_noise_std", &PowStepParam::write_noise_std)
+      .def_readwrite("apply_write_noise_on_set", &PowStepParam::apply_write_noise_on_set)
       .def(
           "__str__",
           [](PowStepParam &self) {
             std::stringstream ss;
             self.printToStream(ss);
             return ss.str();
           })
@@ -784,14 +837,15 @@
 
   py::class_<PiecewiseStepParam, PyPiecewiseStepParam, PulsedParam>(
       m, "PiecewiseStepResistiveDeviceParameter")
       .def(py::init<>())
       .def_readwrite("piecewise_up", &PiecewiseStepParam::piecewise_up_vec)
       .def_readwrite("piecewise_down", &PiecewiseStepParam::piecewise_down_vec)
       .def_readwrite("write_noise_std", &PiecewiseStepParam::write_noise_std)
+      .def_readwrite("apply_write_noise_on_set", &PiecewiseStepParam::apply_write_noise_on_set)
       .def(
           "__str__",
           [](PiecewiseStepParam &self) {
             std::stringstream ss;
             self.printToStream(ss);
             return ss.str();
           })
@@ -813,23 +867,57 @@
       .def_readwrite("forget_buffer", &BufferedTransferParam::forget_buffer)
       .def("__str__", [](BufferedTransferParam &self) {
         std::stringstream ss;
         self.printToStream(ss);
         return ss.str();
       });
 
+  py::class_<ChoppedTransferParam, PyChoppedTransferParam, BufferedTransferParam>(
+      m, "ChoppedTransferResistiveDeviceParameter")
+      .def(py::init<>())
+      .def_readwrite("in_chop_prob", &ChoppedTransferParam::in_chop_prob)
+      .def_readwrite("in_chop_random", &ChoppedTransferParam::in_chop_random)
+      .def_readwrite("out_chop_prob", &ChoppedTransferParam::out_chop_prob)
+      .def_readwrite("auto_scale", &ChoppedTransferParam::auto_scale)
+      .def_readwrite("auto_momentum", &ChoppedTransferParam::auto_momentum)
+      .def_readwrite("auto_granularity", &ChoppedTransferParam::auto_granularity)
+      .def_readwrite("buffer_granularity", &ChoppedTransferParam::buffer_granularity)
+      .def_readwrite(
+          "experimental_adjust_auto_scale_with_transfer_every",
+          &ChoppedTransferParam::experimental_adjust_auto_scale_with_transfer_every)
+      .def("__str__", [](ChoppedTransferParam &self) {
+        std::stringstream ss;
+        self.printToStream(ss);
+        return ss.str();
+      });
+
+  py::class_<DynamicTransferParam, PyDynamicTransferParam, ChoppedTransferParam>(
+      m, "DynamicTransferResistiveDeviceParameter")
+      .def(py::init<>())
+      .def_readwrite("scale_thres_with_samples", &DynamicTransferParam::scale_thres_with_samples)
+      .def_readwrite("tail_weightening", &DynamicTransferParam::tail_weightening)
+      .def_readwrite("buffer_cap", &DynamicTransferParam::buffer_cap)
+      .def_readwrite("always_write", &DynamicTransferParam::always_write)
+      .def("__str__", [](DynamicTransferParam &self) {
+        std::stringstream ss;
+        self.printToStream(ss);
+        return ss.str();
+      });
+
   py::class_<SoftBoundsReferenceParam, PySoftBoundsReferenceParam, PulsedParam>(
       m, "SoftBoundsReferenceResistiveDeviceParameter")
       .def(py::init<>())
       .def_readwrite("slope_up_dtod", &SoftBoundsReferenceParam::slope_up_dtod)
       .def_readwrite("slope_down_dtod", &SoftBoundsReferenceParam::slope_down_dtod)
       .def_readwrite("reference_mean", &SoftBoundsReferenceParam::reference_mean)
       .def_readwrite("reference_std", &SoftBoundsReferenceParam::reference_std)
       .def_readwrite("subtract_symmetry_point", &SoftBoundsReferenceParam::subtract_symmetry_point)
       .def_readwrite("write_noise_std", &SoftBoundsReferenceParam::write_noise_std)
+      .def_readwrite(
+          "apply_write_noise_on_set", &SoftBoundsReferenceParam::apply_write_noise_on_set)
       .def_readwrite("mult_noise", &SoftBoundsReferenceParam::mult_noise)
       .def(
           "__str__",
           [](SoftBoundsReferenceParam &self) {
             std::stringstream ss;
             self.printToStream(ss);
             return ss.str();
```

### Comparing `aihwkit-0.7.1/src/aihwkit/simulator/rpu_base_src/rpu_base_tiles.cpp` & `aihwkit-0.8.0/src/aihwkit/simulator/rpu_base_src/rpu_base_tiles.cpp`

 * *Files 3% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -21,33 +21,38 @@
 void declare_rpu_tiles(py::module &m) {
   using Class = RPU::RPUSimple<T>;
   using ClassPulsed = RPU::RPUPulsed<T>;
 
   py::class_<RPU::WeightModifierParameter<T>>(m, "WeightModifierParameter")
       .def(py::init<>())
       .def_readwrite("std_dev", &RPU::WeightModifierParameter<T>::std_dev)
+      .def_readwrite("per_batch_sample", &RPU::WeightModifierParameter<T>::per_batch_sample)
       .def_readwrite("res", &RPU::WeightModifierParameter<T>::res)
       .def_readwrite("sto_round", &RPU::WeightModifierParameter<T>::sto_round)
       .def_readwrite("dorefa_clip", &RPU::WeightModifierParameter<T>::dorefa_clip)
       .def_readwrite("pdrop", &RPU::WeightModifierParameter<T>::pdrop)
       .def_readwrite("enable_during_test", &RPU::WeightModifierParameter<T>::enable_during_test)
       .def_readwrite("copy_last_column", &RPU::WeightModifierParameter<T>::copy_last_column)
       .def_readwrite("rel_to_actual_wmax", &RPU::WeightModifierParameter<T>::rel_to_actual_wmax)
       .def_readwrite("assumed_wmax", &RPU::WeightModifierParameter<T>::assumed_wmax)
       .def_readwrite("type", &RPU::WeightModifierParameter<T>::type)
-      .def_readwrite("coeffs", &RPU::WeightModifierParameter<T>::coeffs);
+      .def_readwrite("coeffs", &RPU::WeightModifierParameter<T>::coeffs)
+      .def_readwrite("g_max", &RPU::WeightModifierParameter<T>::g_max);
 
   py::enum_<RPU::WeightModifierType>(m, "WeightModifierType")
       .value("Copy", RPU::WeightModifierType::Copy)
       .value("Discretize", RPU::WeightModifierType::Discretize)
       .value("MultNormal", RPU::WeightModifierType::MultNormal)
       .value("AddNormal", RPU::WeightModifierType::AddNormal)
       .value("DiscretizeAddNormal", RPU::WeightModifierType::DiscretizeAddNormal)
       .value("DoReFa", RPU::WeightModifierType::DoReFa)
-      .value("Poly", RPU::WeightModifierType::Poly);
+      .value("Poly", RPU::WeightModifierType::Poly)
+      .value("ProgNoise", RPU::WeightModifierType::ProgNoise)
+      .value("DropConnect", RPU::WeightModifierType::DropConnect)
+      .value("None", RPU::WeightModifierType::Copy);
 
   py::class_<RPU::WeightClipParameter>(m, "WeightClipParameter")
       .def(py::init<>())
       .def_readwrite("fixed_value", &RPU::WeightClipParameter::fixed_value)
       .def_readwrite("sigma", &RPU::WeightClipParameter::sigma)
       .def_readwrite("type", &RPU::WeightClipParameter::type);
 
@@ -98,14 +103,52 @@
       .def(
           "get_brief_info",
           [](Class &self) {
             std::stringstream ss;
             self.printToStream(ss);
             return ss.str();
           })
+      .def("__copy__", [](const Class &self) { return Class(self); })
+      .def(
+          "__deepcopy__", [](const Class &self, py::dict) { return Class(self); }, py::arg("memo"))
+      .def(
+          "dump_extra",
+          [](Class &self) {
+            RPU::state_t state;
+            self.dumpExtra(state, "rpu");
+            return state;
+          },
+          R"pbdoc(
+           Return additional state vraiables for pickling. 
+ 
+           Returns:
+               state: dictionary of extra variables states
+           )pbdoc")
+      .def(
+          "load_extra",
+          [](Class &self, RPU::state_t state, bool strict) {
+            self.loadExtra(state, "rpu", strict);
+          },
+          py::arg("state"), py::arg("strict"),
+          R"pbdoc(
+           Load the state dictionary generated by dump_extra.
+
+           Args:
+               strict: Whether to throw a runtime error when a field is not found. 
+           )pbdoc")
+      .def(
+          "set_verbosity_level", [](Class &self, int verbose) { self.setVerbosityLevel(verbose); },
+          py::arg("verbose"),
+          R"pbdoc(
+           Sets the verbosity level for debugging. 
+
+           Args:
+               verbose: verbosity level
+
+           )pbdoc")
       .def(
           "get_learning_rate", &Class::getLearningRate,
           R"pbdoc(
            Return the tile learning rate.
 
            Returns:
                float: the tile learning rate.
@@ -219,24 +262,25 @@
             std::lock_guard<std::mutex> lock(self.mutex_);
             return self.setDeltaWeights(delta_weights.data_ptr<T>());
           },
           py::arg("delta_weights"))
       .def(
           "reset_delta_weights",
           [](Class &self) {
+            self.finishUpdateCalculations();
             std::lock_guard<std::mutex> lock(self.mutex_);
             return self.setDeltaWeights(nullptr);
           })
       .def(
           "get_shared_weights_if", &Class::getSharedWeightsIf,
           R"pbdoc(
            Returns whether weight is shared.
            )pbdoc")
       .def(
-          "get_parameters", &Class::getPar,
+          "get_meta_parameters", &Class::getPar,
           R"pbdoc(
            Returns the current meta parameter structure.
            )pbdoc")
       .def(
           "set_weights_uniform_random",
           [](Class &self, float min_value, float max_value) {
             std::lock_guard<std::mutex> lock(self.mutex_);
@@ -514,14 +558,19 @@
            )pbdoc")
 
       .def(
           "update",
           [](Class &self, const torch::Tensor &x_input_, const torch::Tensor &d_input_,
              bool bias = false, bool x_trans = false, bool d_trans = false,
              bool non_blocking = false) {
+            T lr = self.getLearningRate();
+            if (lr == (T)0.0) {
+              return;
+            }
+
             auto d_input = d_input_.contiguous();
             auto x_input = x_input_.contiguous();
 
             CHECK_TORCH_INPUT(d_input);
             CHECK_TORCH_INPUT(x_input);
 
             if ((x_input.dim() < 1) || (d_input.dim() < 1)) {
@@ -803,21 +852,51 @@
       .def(
           "get_hidden_update_index", &Class::getHiddenUpdateIdx,
           R"pbdoc(
            Get the current device index that is updated (in case multiple devices per cross-point).
 
            Args:
                idx: index of the (unit cell) devices, returns 0 in all other cases.
-           )pbdoc");
+           )pbdoc")
+      .def(
+          "get_pulse_counters",
+          [](Class &self) {
+            std::vector<uint64_t> v = self.getPulseCounters();
+            if (!v.size()) {
+              return torch::empty({0});
+            }
+            size_t size = self.getDSize() * self.getXSize();
+
+            auto options = torch::TensorOptions().dtype(torch::kInt64);
+            torch::Tensor pulse_counters =
+                torch::empty({(int)(v.size() / size), self.getDSize(), self.getXSize()}, options);
+
+            std::copy_n(
+                (int64_t *)v.data(), v.size(), pulse_counters.contiguous().data_ptr<int64_t>());
+
+            return pulse_counters;
+          },
+          R"pbdoc(
+           Get the pulse counters if available.
+
+           Returns:
+               3D tensor: Pulse counters: pos, neg (and for each sub-device)
+          )pbdoc")
+
+      ;
 
   py::class_<ClassPulsed, Class>(
       m, "AnalogTile",
       R"pbdoc(
     Analog tile.
 
     Args:
         x_size: ``X`` size of the tile.
         d_size: ``D`` size of the tile.
     )pbdoc")
       .def(py::init<int, int>(), py::arg("x_size"), py::arg("d_size"))
-      .def("get_parameters", &ClassPulsed::getMetaPar);
+      .def("__copy__", [](const ClassPulsed &self) { return ClassPulsed(self); })
+      .def(
+          "__deepcopy__", [](const ClassPulsed &self, py::dict) { return ClassPulsed(self); },
+          py::arg("memo"))
+      .def("get_meta_parameters", &ClassPulsed::getMetaPar);
 }
```

### Comparing `aihwkit-0.7.1/src/aihwkit/simulator/rpu_base_src/rpu_base_tiles_cuda.cpp` & `aihwkit-0.8.0/src/aihwkit/simulator/rpu_base_src/rpu_base_tiles_cuda.cpp`

 * *Files 3% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -42,17 +42,46 @@
     Floating point tile (CUDA).
 
     Args:
         tile: existing ``FloatingPointTile`` that will be copied.
     )pbdoc")
       .def(
           py::init([](RPU::RPUSimple<T> &rpu) {
-            return std::unique_ptr<Class>(new Class(at::cuda::getCurrentCUDAStream(), rpu));
+            auto ret = std::unique_ptr<Class>(new Class(at::cuda::getCurrentCUDAStream(), rpu));
+            return ret;
           }),
           py::arg("cpu_tile"))
+      .def("__copy__", [](const Class &self) { return Class(self); })
+      .def(
+          "__deepcopy__", [](const Class &self, py::dict) { return Class(self); }, py::arg("memo"))
+      .def(
+          "dump_extra",
+          [](Class &self) {
+            RPU::state_t state;
+            self.dumpExtra(state, "rpucuda");
+            return state;
+          },
+          R"pbdoc(
+           Return additional state variables for pickling. 
+ 
+           Returns:
+               state: dictionary of extra variables states
+           )pbdoc")
+      .def(
+          "load_extra",
+          [](Class &self, RPU::state_t state, bool strict) {
+            self.loadExtra(state, "rpucuda", strict);
+          },
+          py::arg("state"), py::arg("strict"),
+          R"pbdoc(
+           Load the state dictionary generated by dump_extra.
+
+           Args:
+               strict: Whether to throw a runtime error when a field is not found. 
+           )pbdoc")
       .def(
           "set_shared_weights",
           [](Class &self, torch::Tensor weights) {
             CHECK_TORCH_CUDA_INPUT(weights);
             // weight is d major for CUDA !
             if (weights.dim() != 2 || weights.size(1) != self.getDSize() ||
                 weights.size(0) != self.getXSize()) {
@@ -272,14 +301,19 @@
            )pbdoc")
 
       .def(
           "update",
           [](Class &self, const torch::Tensor &x_input_, const torch::Tensor &d_input_,
              bool bias = false, bool x_trans = false, bool d_trans = false,
              bool non_blocking = false) {
+            T lr = self.getLearningRate();
+            if (lr == (T)0.0) {
+              return;
+            }
+
             auto d_input = d_input_.contiguous();
             auto x_input = x_input_.contiguous();
 
             CHECK_TORCH_CUDA_INPUT(d_input);
             CHECK_TORCH_CUDA_INPUT(x_input);
 
             if ((x_input.dim() < 1) || (d_input.dim() < 1)) {
@@ -487,11 +521,15 @@
       .def(
           py::init([](RPU::RPUPulsed<T> &rpu) {
             // TODO: why does directly passing a stream is a problem?
             return std::unique_ptr<ClassPulsed>(
                 new ClassPulsed(at::cuda::getCurrentCUDAStream(), rpu));
           }),
           py::arg("tile"))
-      .def("get_parameters", &ClassPulsed::getMetaPar);
+      .def("__copy__", [](const ClassPulsed &self) { return ClassPulsed(self); })
+      .def(
+          "__deepcopy__", [](const ClassPulsed &self, py::dict) { return ClassPulsed(self); },
+          py::arg("memo"))
+      .def("get_meta_parameters", &ClassPulsed::getMetaPar);
 }
 
 #endif
```

### Comparing `aihwkit-0.7.1/src/aihwkit/simulator/tiles/__init__.py` & `aihwkit-0.8.0/src/aihwkit/simulator/tiles/__init__.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,22 +1,20 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
 # that they have been altered from the originals.
 
 """High level analog tiles."""
 
 # Convenience imports for easier access to the classes.
 
 from aihwkit.simulator.tiles.analog import AnalogTile
-from aihwkit.simulator.tiles.base import BaseTile
-from aihwkit.simulator.tiles.floating_point import (
-    FloatingPointTile
-)
+from aihwkit.simulator.tiles.floating_point import FloatingPointTile
 from aihwkit.simulator.tiles.inference import InferenceTile
+from aihwkit.simulator.tiles.inference_torch import TorchInferenceTile
```

### Comparing `aihwkit-0.7.1/src/aihwkit/simulator/tiles/analog.py` & `aihwkit-0.8.0/src/aihwkit/utils/fitting.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,255 +1,302 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
 # that they have been altered from the originals.
 
-"""High level analog tiles (analog)."""
+# pylint: disable=too-many-locals, too-many-branches
 
-from typing import Optional, Union, TYPE_CHECKING
+"""Fitting utilities.
 
-from torch import device as torch_device
-from torch.cuda import device as cuda_device
+This module includes fitting utilities for ``aihwkit``.
+Using this module has extra dependencies that can be installed via the
+extras mechanism::
 
-from aihwkit.exceptions import CudaError
-from aihwkit.simulator.rpu_base import cuda, tiles
-from aihwkit.simulator.tiles.base import BaseTile
+    pip install aihwkit[fitting]
+"""
 
-if TYPE_CHECKING:
-    from aihwkit.simulator.configs import (
-        InferenceRPUConfig, SingleRPUConfig, UnitCellRPUConfig
-    )
+from typing import Union, Dict, TypeVar, Tuple, Optional, List, Any
+from copy import deepcopy
+from dataclasses import fields
 
+from numpy import array, concatenate, newaxis, ndarray
+from torch import from_numpy, ones, stack, float32
+from lmfit import minimize, Parameters, report_fit
 
-class AnalogTile(BaseTile):
-    r"""Analog tile.
+from aihwkit.exceptions import ArgumentError, ConfigError
+from aihwkit.simulator.configs.devices import PulsedDevice
+from aihwkit.simulator.configs.configs import SingleRPUConfig
+from aihwkit.simulator.tiles.analog import AnalogTile
 
-    This analog tile implements an abstract analog tile where many
-    cycle-tp-cycle non-idealities and systematic parameter-spreads
-    that can be user-defined.
+RPUConfigGeneric = TypeVar("RPUConfigGeneric")
 
-    In general stochastic bit pulse trains are generate during update
-    and device materials (or unit cells) at each cross-point are only
-    updated if a coincidence of rows and columns pulses.
 
-    Here, a resistive device material is assumed that response with a
-    finite step change of its conductance value that is independent of
-    its own conductance value.
+def _apply_parameters_to_config(
+    device_config: Union[PulsedDevice, RPUConfigGeneric], params: Parameters
+) -> None:
+    """Apply the fit parameters to the device config.
 
-    In its basic parameter settings it implements the analog RPU tile
-    model described in `Gokmen & Vlasov (2016)`_, but with a number of
-    enhancements that are adjustable by parameter settings.
+    Args:
+         device_config: device config to be set (in place)
+         params: lmfit.Parameters structure
 
-    All tile parameters are given in
-    :class:`~aihwkit.simulator.parameters.AnalogTileParameters`.
+    Raises: ConfigError if parameter was not found"""
 
-    **Forward pass**:
+    parvals = params.valuesdict()
+    if isinstance(device_config, PulsedDevice):
+        device = device_config
+    else:
+        device = getattr(device_config, "device")  # type ignore
+
+    for par, value in parvals.items():
+        if not hasattr(device, par):
+            raise ConfigError(f" Cannot find parameter '{par}' in device config.")
+        setattr(device, par, value)
+
+
+def fit_measurements(
+    parameters: Union[Dict, Parameters],
+    pulse_data: Union[Tuple[ndarray], ndarray],
+    response_data: Union[Tuple[ndarray], ndarray],
+    device_config: Union[PulsedDevice, RPUConfigGeneric],
+    suppress_device_noise: bool = True,
+    max_pulses: Optional[int] = 1,
+    n_traces: int = 1,
+    fit_weights: Optional[Union[Tuple[int], int]] = None,
+    method: str = "powell",
+    verbose: bool = False,
+    **fit_kwargs: Any,
+) -> Tuple[Any, Union[PulsedDevice, RPUConfigGeneric], List[ndarray]]:
+    """Fit pulse response measurement to the given device model using lmfit.
+
+    For example:
+
+    .. code-block:: python
+
+        # responses are conductance data in response to pulses (-1, 1)
+
+        # choose device model and parameter to fit
+        device_config = SoftBoundsDevice(w_min=-1.0, w_max=1.0)
+        params = {'dw_min': (0.1, 0.001, 5.0),
+                  'up_down': (0.0, -0.99, 0.99),
+                  'w_max': (1.0, 0.1, 5.0)}
+
+        # fit the response
+        fit_res, fit_device_config, model_response = fit_measurements(
+            params, pulses, responses,
+            device_config=device_config,
+            suppress_device_noise=True,
+            method='powell',
+            fit_weights=fit_weights,
+        )
+        # fit parameter
+        print(fit_res.params.valuesdict())
+        # device of best fit
+        print(fit_device_config)
 
-    In general, the following analog forward pass is computed:
 
-    .. math::
+    Args:
 
-        \mathbf{y} = f_\text{ADC}((W + \sigma_\text{w}\Xi) \otimes
-        (f_\text{DAC}( x/\alpha ) +
-        \sigma_\text{inp}\,\boldsymbol{\xi}_1 ) +
-        \sigma_\text{out}\,\boldsymbol{\xi}_2)\,s_\alpha\,
-        s_\text{out}\,\alpha
+        parameters: Parameter to vary. Dictionary with parameter names
+            (attributes of the device config). Each value is either a
+            single value (thus only set, not varied) or a tuple
+            ``(x_init, x_min, x_max)``. ``lmfit.Parameters`` class can
+            also given directly.
 
-    where :math:`W` is the weight matrix, :math:`\mathbf{x}` the input
-    vector and the :math:`\Xi,\boldsymbol{\xi}_1,\boldsymbol{\xi}_2`
-    Gaussian noise variables (with corresponding matrix and vector
-    sizes). The :math:`\alpha` is a scale from the noise management
-    (see :data:`rpu_types.NoiseManagementTypeMap`). The symbol
-    :math:`\otimes` refers to the 'analog' matrix-vector
-    multiplication, that might have additional non-linearities.
+        pulse_data: Pulse data, ie array of number of pulses in up
+            (pos) or down (neg) direction. Can be a tuple of multiple
+            measurements
 
-    :math:`f_\text{Z}` (with `Z` either `ADC` or `DAC`) indicates the
-    discretization to a number of equidistant steps between a bound
-    value :math:`-b_\text{Z},\ldots,b_\text{Z}` potentially with
-    stochastic rounding (SR):
+        response_data: Corresponfing measured responses to the pulses
+            given by ``pulse_data`` as numpy array or list.
 
-    .. math::
+            Caution:
+                ``axes=1`` can be used for multiple device
+                fit. However, then all pulse data needs to have the
+                same axis=0 dimension
 
-        f_\text{Z}(x) = \text{round}(x\,
-        \frac{r_\text{Z}}{2\,b_\text{Z}} +
-        \zeta)\frac{2b_\text{Z}}{r_\text{Z}}
+        device_config: base device configuration
 
-    If SR is enabled :math:`\zeta` is an uniform random :math:`\in
-    [-0.5,0.5)`. Otherwise :math:`\zeta=0`.  Inputs are clipped below
-    :math:`-b_\text{Z}` and above :math:`b_\text{Z}`
+        suppress_device_noise: sets all dtod and std parameters of the device to 0
 
-    :math:`r_Z` is the resolution of the `ADC` or `DAC`. E.g. for 8
-    bit, it would be :math:`1/256`
+        n_traces: how many traces to simulate simulaenously
 
-    Note:
-        Typically the resolution is reduced by 2 level, eg. in case of
-        8 bits it is set to :math:`1/254` to account for a
-        discretization mirror symmetric around zero, including the zero
-        and discarding one value.
+        max_pulses: constrain the number of pulses given.
 
-    The scalar scale :math:`s_\text{out}` can be set by
-    ``out_scale``. The scalar scale :math:`s_\alpha` is an additional
-    scale that might be use to map weight better to conductance
-    ranges.
+        fit_weights: the weightening of the individual response traces
+            in the loss function
 
-    For parameters regarding the forward pass behavior, see
-    :class:`~aihwkit.simulator.parameters.AnalogTileInputOutputParameters`.
+        method: fitting method from ``lmfit`` (default "powell")
 
+        verbose: whether to print fitting results
 
-    **Backward pass**:
+        fit_kwargs: additional parameter passed to ``lmfit.minimize``
 
-    Identical to the forward direction except that the transposed
-    weight matrix is used.  Same parameters as during the forward pass
-    except that bound management is not supported.
+    Returns:
 
-    For parameters regarding the backward pass behavior, see
-    :class:`~aihwkit.simulator.parameters.AnalogTileInputOutputParameters`.
+         fit_results: Result of the fit in ``lmfit`` format
+         device_config: Device config with found parameter applied
+         model_response: Model response of parameter fit
 
+    Raises:
+         ArgumentError: in case wrong arguments are given
 
-    **General weight update**:
+    """
 
-    The weight update that theoretically needs to be computed is
+    if isinstance(pulse_data, tuple) != isinstance(response_data, tuple):
+        raise ArgumentError("Either all data inputs need to be tuples or None. ")
 
-    .. math:: w_{ij} = w_{ij} + \lambda d_i\,x_j
+    device_config = deepcopy(device_config)
+    if isinstance(device_config, PulsedDevice):
+        rpu_config = SingleRPUConfig(device=device_config)
+
+        # single pulse mode
+        if max_pulses is not None:
+            rpu_config.update.desired_bl = max_pulses
+            rpu_config.update.update_bl_management = False
+            rpu_config.update.update_management = False
+
+    else:
+        rpu_config = device_config  # type: ignore
+
+    if suppress_device_noise:
+        for field in fields(rpu_config.device):
+            if field.name.endswith("dtod") or field.name.endswith("std"):
+                setattr(rpu_config.device, field.name, 0.0)
+
+    params = Parameters()
+    if isinstance(parameters, Parameters):
+        params = parameters
+    elif isinstance(parameters, dict):
+        for par, values in parameters.items():
+            if isinstance(values, tuple):
+                x_init, x_min, x_max = values
+                params.add(par, value=x_init, min=x_min, max=x_max, vary=True)
+            else:
+                params.add(par, value=values, vary=False)
+    else:
+        raise ArgumentError("Expect dict or Parameters for parmeters.")
+
+    # fit parameters
+    args = (pulse_data, response_data, rpu_config, n_traces, fit_weights, verbose)
+    result = minimize(model_response, params, args=args, method=method, **fit_kwargs)
+    if verbose:
+        report_fit(result)
+
+    best_model_res = model_response(result.params, *args, only_response=True)
+
+    _apply_parameters_to_config(device_config, result.params)
+    return result, device_config, best_model_res  # type: ignore
+
+
+def model_response(
+    params: Parameters,
+    pulse_data: Union[Tuple[ndarray], ndarray],
+    response_data: Union[Tuple[ndarray], ndarray],
+    rpu_config: RPUConfigGeneric,
+    n_traces: int = 1,
+    fit_weights: Optional[Union[Tuple[int], int]] = None,
+    verbose: bool = True,
+    only_response: bool = False,
+) -> Union[ndarray, List[ndarray]]:
+    """Compute the model respunses given the pulses.
 
-    thus the outer product of error vector and input vector.
+    Args:
+        params: ``lmfit.Parameters`` of the current parameter setting
 
-    Although the update depends on the `ResistiveDevice` used, in
-    general, stochastic pulse trains of a given length are drawn,
-    where the probability of occurrence of an pulse is proportional to
-    :math:`\sqrt{\lambda}d_i` and :math:`\sqrt{\lambda}x_j`
-    respectively. Then for each cross-point, in case a coincidence of
-    column and row pulses occur, the weight is updated one `step`. For
-    details, see `Gokmen & Vlasov (2016)`_.
+        pulse_data: Pulse data, ie array of number of pulses in up
+            (pos) or down (neg) direction. Can be a tuple of multiple
+            measurements
 
-    The amount of how the weight changes per single step might be
-    different for the different resistive devices.
+        response_data: Corresponfing measured responses to the pulses
+            given by ``pulse_data`` as numpy array or list.
 
-    In pseudo code::
+            Caution:
+                ``axes=1`` can be used for multiple device
+                fit. However, then all pulse data needs to have the
+                same axis=0 dimension
 
-        # generate prob number
-        p_i  = quantize(A * d_i, res, sto_round)
-        q_j  = quantize(B * x_j, res, sto_round)
-        sign = sign(d_i)*sign(x_j)
+        rpu_config: base device configuration (will be modified)
 
-        # generate pulse trains of length BL
-        pulse_train_d = gen_pulse_train(p_i, BL) # e.g 101001001
-        pulse_train_x = gen_pulse_train(q_j, BL) # e.g 001010010
+        fit_weights: the weightening of the individual response traces
+            in the loss function
 
-        for t in range(BL):
-            if (pulse_train_x[t]==1) and (pulse_train_d[t]==1)
-                update_once(w_{ij}, direction = sign)
+        n_traces: how many traces to simulate simulaenously
 
-    The probabilities are generated using scaling factors ``A`` and ``B`` that
-    are determined by the learning rate and pulse train length ``BL`` (see
-    below). ``quantize`` is an optional discretization of the resulting
-    probability, to account for limited resolution number in the stochastic
-    pulse train generation process on the chip .
+        verbose: whether to print std of deviation
 
-    The ``update_once`` functionality is in general dependent on the
-    analog tile class.  For `ConstantStep` the step width is
-    independent of the actual weight, but has cycle-to-cycle
-    variation, device-to-device variation or systematic bias for up
-    versus down direction (see below).
+        only_response: whether to returns a list of model response
+           instead of the deviation
 
-    For parameters regarding the update behaviour, see
-    :class:`~aihwkit.simulator.parameters.AnalogTileUpdateParameters`.
+    Returns:
+        deviation vector or list of model responses (weight traces)
 
-    Args:
-        out_size: output vector size of the tile, ie. the dimension of
-            :math:`\mathbf{y}` in case of :math:`\mathbf{y} =
-            W\mathbf{x}` (or equivalently the dimension of the
-            :math:`\boldsymbol{\delta}` of the backward pass).
-        in_size: input vector size, ie. the dimension of the vector
-            :math:`\mathbf{x}` in case of :math:`\mathbf{y} =
-            W\mathbf{x}`).
-        rpu_config: resistive processing unit configuration.
-        bias: whether to add a bias column to the tile, ie. :math:`W`
-            has an extra column to code the biases. Internally, the
-            input :math:`\mathbf{x}` will be automatically expanded by
-            an extra dimension which will be set to 1 always.
-        in_trans: Whether to assume an transposed input (batch first).
-        out_trans: Whether to assume an transposed output (batch first).
+    Note:
+        overwrites the given rpu_config
 
-    .. _Gokmen & Vlasov (2016): https://www.frontiersin.org/articles/10.3389/fnins.2016.00333/full
     """
 
-    def __init__(
-            self,
-            out_size: int,
-            in_size: int,
-            rpu_config: Optional[Union['SingleRPUConfig', 'UnitCellRPUConfig',
-                                       'InferenceRPUConfig']] = None,
-            bias: bool = False,
-            in_trans: bool = False,
-            out_trans: bool = False,
-    ):
-        if not rpu_config:
-            # Import `SingleRPUConfig` dynamically to avoid import cycles.
-            # pylint: disable=import-outside-toplevel
-            from aihwkit.simulator.configs import SingleRPUConfig
-            rpu_config = SingleRPUConfig()
-
-        super().__init__(out_size, in_size, rpu_config, bias, in_trans, out_trans)
-
-    def cuda(
-            self,
-            device: Optional[Union[torch_device, str, int]] = None
-    ) -> 'BaseTile':
-        """Return a copy of this tile in CUDA memory.
-
-        Args:
-            device: CUDA device
-
-        Returns:
-            Self with the underlying C++ tile moved to CUDA memory.
-
-        Raises:
-            CudaError: if the library has not been compiled with CUDA.
-        """
-        if not cuda.is_compiled():
-            raise CudaError('aihwkit has not been compiled with CUDA support')
-
-        device = torch_device('cuda', cuda_device(device).idx)
-
-        if self.is_cuda and device != self.device:
-            raise CudaError('Cannot switch CUDA devices of existing Cuda tiles')
-
-        if isinstance(self.tile, tiles.AnalogTile):
-            with cuda_device(device):
-                self.tile = tiles.CudaAnalogTile(self.tile)
-                self.is_cuda = True
-                self.device = device
-                self.analog_ctx.cuda(device)
-                if self.mapping_scales is not None:
-                    self.mapping_scales = self.mapping_scales.cuda(device)
-                if self.out_scaling_alpha is not None:
-                    self.out_scaling_alpha.data = self.out_scaling_alpha.data.cuda(device)
-        return self
-
-    def _create_simulator_tile(
-            self,
-            x_size: int,
-            d_size: int,
-            rpu_config: Union['SingleRPUConfig', 'UnitCellRPUConfig', 'InferenceRPUConfig']
-    ) -> tiles.AnalogTile:
-        """Create a simulator tile.
-
-        Args:
-            x_size: input size
-            d_size: output size
-            rpu_config: resistive processing unit configuration
-
-        Returns:
-            a simulator tile based on the specified configuration.
-        """
-        meta_parameter = rpu_config.as_bindings()
-        device_parameter = rpu_config.device.as_bindings()
+    _apply_parameters_to_config(rpu_config, params)
 
-        return meta_parameter.create_array(x_size, d_size, device_parameter)
+    # likley somewhat inefficient since we need to always create a new
+    # tile, repeats are quick though
+    no_list = False
+    if not isinstance(pulse_data, tuple):
+        pulse_data = (pulse_data,)
+        response_data = (response_data,)  # type: ignore
+        if fit_weights is not None:
+            fit_weights = (fit_weights,)  # type: ignore
+        no_list = True
+
+    numpy_pulses = array(pulse_data[0])
+    n_devices = 1
+    if numpy_pulses.ndim > 1:
+        n_devices = numpy_pulses.shape[1]
+
+    analog_tile = AnalogTile(n_traces, n_devices, rpu_config)  # type: ignore
+    analog_tile.set_learning_rate(1)
+
+    deviation = array([], "float")
+    model_responses = []
+    for idx, (numpy_pulses, response) in enumerate(zip(pulse_data, response_data)):
+        if numpy_pulses.ndim == 1:
+            numpy_pulses = numpy_pulses.reshape(-1, 1)
+        if response.ndim == 1:
+            response = response.reshape(-1, 1)
+
+        w_init = response[0, :]
+        weights = from_numpy(array(w_init).flatten()[newaxis, :]).to(dtype=float32) * ones(
+            (n_traces, n_devices), dtype=float32
+        )
+        analog_tile.set_weights(weights)
+        pulses = from_numpy(numpy_pulses).to(dtype=float32)
+        w_trace = [weights]
+        for pulse in pulses[:-1]:
+            analog_tile.update(
+                pulse * ones(n_devices, dtype=float32), -ones((n_traces), dtype=float32)
+            )
+            w_trace.append(analog_tile.tile.get_weights())
+
+        stacked_w_trace = stack(w_trace).cpu().numpy()
+        # compute square error
+        num_samples = response.shape[0]
+        avg_w_trace = stacked_w_trace.mean(axis=1)[:num_samples, :]
+        model_responses.append(avg_w_trace)
+        dev = avg_w_trace - response
+        if fit_weights is not None:
+            dev = dev * array(fit_weights[idx])[: dev.shape[1]]  # type: ignore
+        deviation = concatenate([deviation, dev.flatten()])
+
+    if only_response:
+        if no_list:
+            return model_responses[0]
+        return model_responses
+    if verbose:
+        print(deviation.std())
+    return deviation
```

### Comparing `aihwkit-0.7.1/src/aihwkit/simulator/tiles/base.py` & `aihwkit-0.8.0/src/aihwkit/simulator/tiles/periphery.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,497 +1,242 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
 # that they have been altered from the originals.
 
-"""High level analog tiles (base)."""
-# pylint: disable=too-many-lines, wrong-import-position
+"""Base tile with added periphery and common utility methods."""
 
-from collections import OrderedDict
-from typing import (
-    Dict, Generic, List, Optional,
-    Tuple, TypeVar, Union, TYPE_CHECKING, Any
-)
-from copy import deepcopy
-from numpy.typing import ArrayLike
+# pylint: disable=too-many-lines
+
+from typing import Optional, Tuple, Union, Any, List
 from numpy import array
 
 from torch import (
-    Tensor, stack, zeros, as_tensor, cat,
-    unsqueeze, squeeze, ones,
-    float32, from_numpy, full, clamp,
-    zeros_like, eye, randn,
+    Tensor,
+    as_tensor,
+    cat,
+    ones,
+    where,
+    float32,
+    from_numpy,
+    full,
+    clamp,
+    zeros_like,
+    eye,
+    randn,
+    zeros,
+    logical_or,
 )
 
 from torch import device as torch_device
 from torch import max as torch_max
-from torch.nn import Parameter
+from torch.nn import Parameter, Module
 from torch.autograd import no_grad
 from torch.linalg import lstsq
 
-from aihwkit.simulator.rpu_base import tiles
 from aihwkit.exceptions import TileError, ConfigError
-from aihwkit.optim.context import AnalogContext
+from aihwkit.simulator.tiles.base import BaseTile, SimulatorTileWrapper, SimulatorTile
 
-RPUConfigGeneric = TypeVar('RPUConfigGeneric')
+from aihwkit.simulator.parameters.utils import MappingParameter, InputRangeParameter
+from aihwkit.simulator.parameters.base import PrePostProcessingRPU
 
-if TYPE_CHECKING:
-    from aihwkit.simulator.configs.utils import MappingParameter, InputRangeParameter
 
+class TileWithPeriphery(BaseTile, SimulatorTileWrapper):
+    """Partial class for tile modules with periphery.
 
-class AnalogTileStateNames:  # pylint: disable=too-few-public-methods
-    """ Class defining analog tile state name constants.
+    The function ``joint_forward`` should be called from the
+    TileModule level.
 
-    Caution:
-       Do *not* edit. Some names are attribute names of the tile.
-    """
+    The class also implements the digital bias and adds output scales as
+    well as mapping / reading / programming
+    functionality. Additionally input range and output scale learning
+    is implemented.
+
+    Note:
+
+        This is only a partial class implementation for the
+        periphery. All classes inherit from this need to also inherit
+        from :class:`~aihwkit.simulator.tiles.module.TileModule`.
+
+        All the module buffers and parameters will be
+        handled by the TileModule.
 
-    WEIGHTS = 'analog_tile_weights'
-    HIDDEN_PARAMETERS = 'analog_tile_hidden_parameters'
-    HIDDEN_PARAMETER_NAMES = 'analog_tile_hidden_parameter_names'
-    CLASS = 'analog_tile_class'
-    LR = 'analog_lr'
-    SHARED_WEIGHTS = 'shared_weights'
-    CONTEXT = 'analog_ctx'
-    OUT_SCALING = 'out_scaling_alpha'
-    MAPPING_SCALES = 'mapping_scales'
-    RPU_CONFIG = 'rpu_config'
-
-
-class BaseTile(Generic[RPUConfigGeneric]):
-    """Base class for tiles.
-
-    Args:
-        out_size: output size
-        in_size: input size
-        rpu_config: resistive processing unit configuration.
-        bias: whether to add a bias column to the tile.
-        in_trans: Whether to assume an transposed input (batch first)
-        out_trans: Whether to assume an transposed output (batch first)
     """
-    # pylint: disable=too-many-instance-attributes,too-many-public-methods
 
-    def __init__(
-            self,
-            out_size: int,
-            in_size: int,
-            rpu_config: RPUConfigGeneric,
-            bias: bool = True,
-            in_trans: bool = False,
-            out_trans: bool = False
-    ):
-        self.out_size = out_size
-        self.in_size = in_size
-        self.rpu_config = deepcopy(rpu_config)
-        self.bias = bias
-        self.in_trans = in_trans
-        self.out_trans = out_trans
-        self.shared_weights = None  # type: Parameter
+    # pylint: disable=no-member, too-many-public-methods, abstract-method
+
+    supports_indexed = True
+
+    def __init__(self) -> None:
+        # SimulatorTileWrapper.__init__ is called later. Only included here to
+        # make mypy happier
+        # pylint: disable=super-init-not-called
+
         self.out_scaling_alpha = None  # type: Parameter
         self.mapping_scales = None  # type: Tensor
         self.input_range = None  # type: Parameter
 
-        # Whether CUDA-calls should be blocking
-        self.non_blocking = False
-
-        # Only used for indexed.
         self.image_sizes = []  # type: List[int]
 
-        x_size = in_size + 1 if self.bias else in_size
-        d_size = out_size
-
-        self.tile = self._create_simulator_tile(x_size, d_size, rpu_config)
-        self.tile.set_learning_rate(0.01)
-        self.tile.set_weights_uniform_random(-0.01, 0.01)
-
-        self.device = torch_device('cpu')
-        self.is_cuda = False
+        if self.digital_bias:
+            # Note that the bias needs to be handled on the module level
+            self.bias = Parameter(zeros(self.out_size), requires_grad=True)
+        else:
+            self.bias = None
 
-        # create analog context
-        self.analog_ctx = AnalogContext(self)
+        # Whether CUDA-calls should be blocking
+        self.non_blocking = False
 
         # Helpers.
         self.reference_combined_weights = None  # type: Optional[Tensor]
 
         # init input / output processing
         self.init_learned_out_scales()
         self.init_mapping_scales()
         self.init_input_processing()
 
-    @no_grad()
-    def get_analog_ctx(self) -> AnalogContext:
-        """Return the analog context of the tile to be used in ``AnalogFunction``."""
-        return self.analog_ctx
-
-    @no_grad()
-    def ensure_shared_weights(self, shared_weights: Optional[Tensor] = None) -> None:
-        """Ensure that the shared_weights is set properly.
-
-        Caution:
-           This is only called from analog function.
-
-        No-op if shared weights is not used.
-        """
-        if shared_weights is not None:
-            self.shared_weights.data = shared_weights.data  # type: ignore
-
-        if self.shared_weights is not None:
-            self.tile.set_shared_weights(self.shared_weights.data)
-
-    @no_grad()
-    def set_delta_weights(self, delta_weights: Optional[Tensor] = None) -> None:
-        """Set the weight grad tensor and set the update to.
-
-        No-op if shared weights is not used.
-        """
-        if self.shared_weights is not None and delta_weights is not None:
-            self.tile.set_delta_weights(delta_weights)
-
-    @no_grad()
-    def reset_delta_weights(self) -> None:
-        """Reset the weight grad tensor to default update behavior (i.e. adding the
-        update directly to the weight).
-
-        No-op if shared weights is not used.
-        """
-        if self.shared_weights is not None:
-            self.tile.reset_delta_weights()
+        if isinstance(self, Module):
+            mapping_scales = self.__dict__.pop("mapping_scales")
+            self.register_buffer("mapping_scales", mapping_scales)
 
     @no_grad()
-    def get_brief_info(self) -> str:
-        """Return short info about the underlying C++ tile."""
-        return self.tile.get_brief_info().rstrip()
-
-    def __getstate__(self) -> Dict:
-        """Get the state for pickling.
-
-        This method removes the ``tile`` member, as the binding Tiles are not
-        serializable.
-        """
-        # Caution: all attributes of the tile will be saved.
-        current_dict = self.__dict__.copy()
-
-        SN = AnalogTileStateNames
-        current_dict[SN.WEIGHTS] = self.tile.get_weights()
-        current_dict[SN.HIDDEN_PARAMETERS] \
-            = self.tile.get_hidden_parameters().data
-        current_dict[SN.HIDDEN_PARAMETER_NAMES] \
-            = self.tile.get_hidden_parameter_names()
-        current_dict[SN.CLASS] = self.__class__.__name__
-        current_dict[SN.LR] = self.tile.get_learning_rate()
-        current_dict.pop('tile', None)
-
-        # don't save device. Will be determined by loading object
-        current_dict.pop('stream', None)
-        current_dict.pop('is_cuda', None)
-        current_dict.pop('device', None)
-
-        # this is should not be saved.
-        current_dict.pop('image_sizes', None)
-
-        return current_dict
-
-    def __setstate__(self, state: Dict) -> None:
-        """Set the state after unpickling.
-
-        This method recreates the ``tile`` member, creating a new one from
-        scratch, as the binding Tiles are not serializable.
-
-        Caution:
-            RPU configs are overwritten by loading the state.
-
-        Raises:
-            TileError: if tile class does not match or hidden parameters do not match
-        """
-        # pylint: disable=too-many-locals, too-many-statements, too-many-branches
-
-        # Note: self here is NOT initialized! So we need to recreate
-        # attributes that were not saved in getstate
-        SN = AnalogTileStateNames
-        current_dict = state.copy()
-        current_dict.pop('image_sizes', None)  # should not be saved
-        weights = current_dict.pop(SN.WEIGHTS)
-
-        hidden_parameters = current_dict.pop(SN.HIDDEN_PARAMETERS)
-        hidden_parameters_names = current_dict.pop(SN.HIDDEN_PARAMETER_NAMES, [])
-        alpha_scale = current_dict.pop('analog_alpha_scale', None)  # legacy
-        tile_class = current_dict.pop(SN.CLASS, self.__class__.__name__)
-        analog_lr = current_dict.pop(SN.LR, 0.01)
-        analog_ctx = current_dict.pop(SN.CONTEXT)
-        shared_weights = current_dict.pop(SN.SHARED_WEIGHTS)
-        shared_weights_if = shared_weights is not None
-
-        mapping_scales = current_dict.pop(SN.MAPPING_SCALES, None)
-        learned_out_scales = current_dict.pop(SN.OUT_SCALING, None)
-
-        current_dict.pop('noise_model', None)  # legacy
-        current_dict.pop('drift_compensation', None)  # legacy
-
-        # legacy
-        if 'non_blocking' not in current_dict:
-            current_dict['non_blocking'] = False
-
-        self.__dict__.update(current_dict)
-
-        self.device = torch_device('cpu')
-        self.is_cuda = False
-        # get the current map location from analog_ctx (which is restored)
-        to_device = analog_ctx.device
-
-        # recreate attributes not saved
-        # always first create on CPU
-        x_size = self.in_size + 1 if self.bias else self.in_size
-        d_size = self.out_size
-
-        # Recreate the tile.
-        # Check for tile mismatch
-        if tile_class != self.__class__.__name__:
-            raise TileError(
-                'Mismatch of tile class: {} versus {}. Can only load analog '
-                'state from the same tile class.'.format(self.__class__.__name__, tile_class))
-
-        self.tile = self._create_simulator_tile(x_size, d_size, self.rpu_config)
-        names = self.tile.get_hidden_parameter_names()
-        if len(hidden_parameters_names) > 0 and names != hidden_parameters_names:
-            # Check whether names match
-            raise TileError('Mismatch with loaded analog state: '
-                            'Hidden parameter structure is unexpected.')
-        if not isinstance(weights, Tensor):
-            weights = from_numpy(array(weights))
-        self.tile.set_weights(weights)
-
-        if not isinstance(hidden_parameters, Tensor):
-            hidden_parameters = from_numpy(array(hidden_parameters))
-        self.tile.set_hidden_parameters(hidden_parameters)
-
-        self.tile.set_learning_rate(analog_lr)
-
-        # re-generate shared weights (CPU)
-        if shared_weights_if:
-            if not hasattr(self, SN.SHARED_WEIGHTS):
-                # this is needed when pkl loading
-                self.shared_weights = shared_weights
-
-            with no_grad():
-                # always new will be populated with set weights.
-                self.shared_weights.data = zeros(d_size, x_size, requires_grad=True)
-            self.ensure_shared_weights()
-        else:
-            self.shared_weights = None
-
-        # Regenerate context but keep the object ID
-        if not hasattr(self, SN.CONTEXT):  # when loading
-            self.analog_ctx = AnalogContext(self, parameter=analog_ctx)
-        self.analog_ctx.reset(self)
-        self.analog_ctx.set_data(analog_ctx.data)
-
-        # set scales
-        self.out_scaling_alpha = None
-        self.mapping_scales = None
-        self.init_mapping_scales()
-        self.init_learned_out_scales()
-
-        if self.out_scaling_alpha is None and learned_out_scales is not None:
-            if mapping_scales is None:
-                mapping_scales = 1.0
-            x = learned_out_scales.view(learned_out_scales.numel()).clone()
-            mapping_scales = mapping_scales * x
-            learned_out_scales = None
-
-        self.set_mapping_scales(mapping_scales)
-        self.set_learned_out_scales(learned_out_scales)
-
-        if alpha_scale is not None:
-            # legacy. We apply the alpha scale instaed of the
-            # out_scaling_alpha when loading. The alpha_scale
-            # mechansim is now replaced with the out scaling factors
-            #
-            # Caution: will overwrite the loaded out_scaling_alphas
-            # if they would exist also (should not be for old checkpoints)
-
-            self.set_mapping_scales(alpha_scale)
-
-        if to_device.type.startswith('cuda'):
-            self.cuda(to_device)
-
-        if alpha_scale is not None:
-            # legacy. We apply the alpha scale instaed of the
-            # out_scaling_alpha when loading. The alpha_scale
-            # mechansim is now replaced with the out scaling factors
-            #
-            # Caution: will overwrite the loaded out_scaling_alphas
-            # if they would exist also (should not be for old checkpoints)
-
-            self.set_mapping_scales(alpha_scale)
-
-    def _create_simulator_tile(
-            self,
-            x_size: int,
-            d_size: int,
-            rpu_config: RPUConfigGeneric
-    ) -> Union[tiles.FloatingPointTile, tiles.AnalogTile]:
-        """Create a simulator tile.
-
-        Args:
-            x_size: input size
-            d_size: output size
-            rpu_config: resistive processing unit configuration
-
-        Returns:
-            a simulator tile based on the specified configuration.
-        """
-        raise NotImplementedError
-
-    def _combine_weights(self, weights: Union[Tensor, ArrayLike],
-                         biases: Optional[Union[Tensor, ArrayLike]] = None) -> Tensor:
-        """ Helper to combines weights and biases
-
-        In any case, a detached cpu weight and bias copy will be returned.
-
-        Args:
-            weights: weights without the bias
-            biases: The bias vector if available
-
-        Returns:
-            combined weights with biases
-
-        Raises:
-            ValueError: if the tile has bias but ``bias`` has not been
-                specified.
-        """
-        if not isinstance(weights, Tensor):
-            weights = from_numpy(array(weights))
-        weights = weights.clone().detach().cpu().to(float32)
-
-        if self.bias:
-            # Create a ``[out_size, in_size (+ 1)]`` matrix.
-            if biases is None:
-                raise ValueError('Analog tile has a bias, but no bias given')
-
-            if not isinstance(biases, Tensor):
-                biases = from_numpy(array(biases))
-
-            biases = unsqueeze(biases.clone().detach().cpu().to(float32), 1)
-            return cat((weights, biases), dim=1)
-        # Use only the ``[out_size, in_size]`` matrix.
-        return weights
-
-    def _separate_weights(self, combined_weights: Tensor) -> Tuple[Tensor, Optional[Tensor]]:
-        """ Helper to separate the combined weights and biases
-        """
-        # Split the internal weights (and potentially biases) matrix.
-        if self.bias:
-            # combined_weights is [out_size, in_size (+ 1)].
-            return Tensor(combined_weights[:, :-1]), Tensor(combined_weights[:, -1])
-
-        return combined_weights, None
-
     def set_weights(
-            self,
-            weights: Tensor,
-            biases: Optional[Tensor] = None,
-            apply_weight_scaling: bool = False,
-            weight_scaling_omega: Optional[float] = None
+        self,
+        weight: Tensor,
+        bias: Optional[Tensor] = None,
+        apply_weight_scaling: bool = True,
+        realistic: bool = False,
+        weight_scaling_omega: Optional[float] = None,
     ) -> None:
         """Set the tile weights (and biases).
 
-        Sets the internal tile weights to the specified values, and also the
-        internal tile biases if the tile was set to use bias (via
-        ``self.bias``).
+        Sets the internal tile weights (and biases) to the specified
+        values.
 
         Note:
-           This setting is **not** hardware realistic. Use the
-           :meth:`program_weights` for a realistic weight transfer.
+
+            By default this is **not** a hardware realistic weight
+            readout but an exact weight copy of the internal weights.
+
+        Caution:
+
+            By default the peripheral digital scales are applied to
+            the weights, so that the weight is scaled (in case
+            ``weight_scaling_omega`` is set accordingly).
 
         Args:
-            weights: ``[out_size, in_size]`` weight matrix.
-            biases: ``[out_size]`` bias vector. This parameter is required if
-                ``self.bias`` is ``True``, and ignored otherwise.
+            weight: ``[out_size, in_size]`` weight matrix.
+            bias: ``[out_size]`` bias vector. This parameter is required if
+                ``self.analog_bias`` is ``True``, and ignored otherwise.
             apply_weight_scaling: Whether to rescale the given weight matrix
                 and populate the digital output scaling factors as
                 specified in the configuration
-                :class:`~aihwkit.configs.utils.MappingParameter`. A
+                :class:`~aihwkit.simulator.parameters.utils.MappingParameter`. A
                 new ``weight_scaling_omega`` can be given. Note that
                 this will overwrite the existing digital out scaling
                 factors.
+            realistic: whether to enable realistic write for
+                getting the weights. Internally calls
+                `program_weights`.
             weight_scaling_omega: The weight scaling omega factor (see
-                :class:`~aihwkit.configs.utils.MappingParameter`). If
+                :class:`~aihwkit.simulator.parameters.utils.MappingParameter`). If
                 given explicitly here, it will overwrite the value in
                 the mapping field.
 
-        Returns:
-            None.
         """
         self.reference_combined_weights = None
-        combined_weights = self._combine_weights(weights, biases)
+
+        if bias is not None and self.digital_bias:
+            if not isinstance(bias, Tensor):
+                bias = from_numpy(array(bias))
+            self.bias.data[:] = bias[:].clone().detach().to(float32).to(self.bias.device)
+            bias = None
+
+        combined_weights = self._combine_weights(weight, bias)
 
         if apply_weight_scaling:
-            combined_weights = self.apply_weight_scaling(combined_weights,
-                                                         weight_scaling_omega)
-        return self.tile.set_weights(combined_weights)
+            combined_weights = self.apply_weight_scaling(combined_weights, weight_scaling_omega)
+        self.tile.set_weights(combined_weights)
 
-    def get_weights(self, apply_weight_scaling: bool = False
-                    ) -> Tuple[Tensor, Optional[Tensor]]:
+        if realistic:
+            self.program_weights()
+
+    @no_grad()
+    def get_weights(
+        self, apply_weight_scaling: bool = True, realistic: bool = False
+    ) -> Tuple[Tensor, Optional[Tensor]]:
         """Get the tile weights (and biases).
 
         Gets the tile weights and extracts the mathematical weight
-        matrix and biases (if present, by determined by the ``self.bias``
+        matrix and biases (if present, by determined by the ``self.analog_bias``
         parameter).
 
         Note:
              The returned weight is a copy of the internal weights (not a
              pointer) and is always on CPU and detached.
 
         Note:
-             This is **not** a hardware realistic weight readout. Use
+             By default tis is **not** a hardware realistic weight readout. Use
             :meth:`read_weights` for a realistic transfer.
 
         Args:
             apply_weight_scaling: Whether to return the weights with the
                 (digital) output scaling factors applied. Note the
                 "logical" weights of the layer which the DNN is
                 effectively using are those with the output scales
                 applied. If ``apply_weight_scaling`` is set to False, then
                 only the weight values that is programmed onto the
                 crossbar array are returned, without applying the
                 digital scales.
+            realistic: whether to enable realistic read/write for
+                getting the weights. Internally calls
+                `read_weights`.
 
         Returns:
             a tuple where the first item is the ``[out_size, in_size]`` weight
             matrix; and the second item is either the ``[out_size]`` bias vector
             or ``None`` if the tile is set not to use bias.
 
         """
+        if realistic:
+            return self.read_weights(apply_weight_scaling=apply_weight_scaling)
+
         # Retrieve the internal weights (and potentially biases) matrix.
         combined_weights = self.tile.get_weights()
-        weights, biases = self._separate_weights(combined_weights)
+        weight, bias = self._separate_weights(combined_weights)
+
+        if self.digital_bias:
+            bias = self.bias.detach().cpu()
 
         if not apply_weight_scaling:
-            return weights, biases
+            return weight, bias
 
         alpha = self.get_scales()
         if alpha is not None:
             alpha = alpha.detach().cpu()
-            return weights * alpha.view(-1, 1), biases * alpha if self.bias else None
-        return weights, biases
+            return (weight * alpha.view(-1, 1), bias * alpha if self.analog_bias else bias)
+        return weight, bias
 
     @no_grad()
-    def program_weights(self, from_reference: bool = True,
-                        x_values: Optional[Tensor] = None,
-                        learning_rate: float = 0.1,
-                        max_iter: int = 10000,
-                        tolerance: Optional[float] = 0.01,
-                        w_init: Union[float, Tensor] = 0.01) -> None:
+    def program_weights(
+        self,
+        from_reference: bool = True,
+        x_values: Optional[Tensor] = None,
+        learning_rate: float = 0.1,
+        max_iter: int = 10000,
+        tolerance: Optional[float] = 0.01,
+        w_init: Union[float, Tensor] = 0.01,
+    ) -> None:
         """Programm the target weights into the conductances using the
         pulse update defined.
 
         Programming is done using the defined tile-update (e.g. SGD)
         and matching inputs (`x_values` by default `eye`).
 
         Args:
@@ -522,40 +267,74 @@
         x_values = x_values.to(self.device)
         target_values = x_values @ target_weights.to(self.device).T
 
         target_max = target_values.abs().max().item()
         if isinstance(w_init, Tensor):
             self.tile.set_weights(w_init)
         else:
-            self.tile.set_weights_uniform_random(-w_init, w_init)
+            self.tile.set_weights_uniform_random(-w_init, w_init)  # type: ignore
 
-        lr_save = self.tile.get_learning_rate()
-        self.tile.set_learning_rate(learning_rate)
+        lr_save = self.tile.get_learning_rate()  # type: ignore
+        self.tile.set_learning_rate(learning_rate)  # type: ignore
 
         for _ in range(max_iter):
             y = self.tile.forward(x_values, False)
             error = y - target_values
-            if tolerance is not None and (error.abs().mean().item()
-                                          / target_max) < tolerance:
+            if tolerance is not None and (error.abs().mean().item() / target_max) < tolerance:
                 break
-            self.tile.update(x_values, error, False)
+            self.tile.update(x_values, error, False)  # type: ignore
 
-        self.tile.set_learning_rate(lr_save)
+        self.tile.set_learning_rate(lr_save)  # type: ignore
 
     @no_grad()
-    def read_weights(self,
-                     apply_weight_scaling: bool = False,
-                     x_values: Optional[Tensor] = None,
-                     over_sampling: int = 10
-                     ) -> Tuple[Tensor, Optional[Tensor]]:
+    def remap_weights(self, weight_scaling_omega: Optional[float] = 1.0) -> None:
+        """Gets and re-sets the weights in case of using the weight scaling.
+
+        This re-sets the weights with applied mapping scales, so that
+        the weight mapping scales are updated.
+
+        In case of hardware-aware training, this would update the
+        weight mapping scales so that the absolute max analog weights
+        are set to 1 (as specified in the ``weight_scaling``
+        configuration of
+        :class:`~aihwkit.parameters.utils.MappingParameter`).
+
+        Note:
+            By default the weight scaling omega factor is set to 1
+            here (overriding any setting in the ``rpu_config``). This
+            means that the max weight value is set to 1 internally for
+            the analog weights.
+
+        Caution:
+            This should typically *not* be called for analog. Use
+            ``program_weights`` to re-program.
+
+        Args:
+            weight_scaling_omega: The weight scaling omega factor (see
+                :class:`~aihwkit.parameters.utils.MappingParameter`). If
+                set to None here, it will take the value in the
+                mapping parameters. Default is however 1.0.
+        """
+        weight, bias = self.get_weights(apply_weight_scaling=True)
+        self.set_weights(
+            weight, bias, apply_weight_scaling=True, weight_scaling_omega=weight_scaling_omega
+        )
+
+    @no_grad()
+    def read_weights(
+        self,
+        apply_weight_scaling: bool = False,
+        x_values: Optional[Tensor] = None,
+        over_sampling: int = 10,
+    ) -> Tuple[Tensor, Optional[Tensor]]:
         """Reads the weights (and biases) in a realistic manner
         by using the forward pass for weights readout.
 
         Gets the tile weights and extracts the mathematical weight
-        matrix and biases (if present, by determined by the ``self.bias``
+        matrix and biases (if present, by determined by the ``self.analog_bias``
         parameter).
 
         The weight will not be directly read, but linearly estimated
         using random inputs using the analog forward pass.
 
         Note:
 
@@ -566,15 +345,15 @@
         Note:
             weights are estimated using the ``lstsq`` solver from torch.
 
         Args:
             apply_weight_scaling: Whether to rescale the given weight matrix
                 and populate the digital output scaling factors as
                 specified in the configuration
-                :class:`~aihwkit.configs.utils.MappingParameter`. A
+                :class:`~aihwkit.parameters.utils.MappingParameter`. A
                 new ``weight_scaling_omega`` can be given. Note that
                 this will overwrite the existing digital out scaling
                 factors.
 
             x_values: Values to use for estimating the matrix. If
                 not given, inputs are standard normal vectors.
 
@@ -583,46 +362,60 @@
                 for the estimation
 
         Returns:
             a tuple where the first item is the ``[out_size, in_size]`` weight
             matrix; and the second item is either the ``[out_size]`` bias vector
             or ``None`` if the tile is set not to use bias.
 
+        Raises:
+            TileError: in case wrong code usage of TileWithPeriphery
         """
 
         if x_values is None:
-            x_values = randn(self.in_size * over_sampling, self.in_size,
-                             device=self.device,
-                             dtype=float32)
+            x_values = randn(
+                self.in_size * over_sampling, self.in_size, device=self.device, dtype=float32
+            )
         else:
             x_values = x_values.to(self.device)
 
-        y_values = self.forward(x_values, is_test=True)
+        # joint forward does only apply the mapping scales
+        if not isinstance(self, Module):
+            raise TileError("TileWithPeriphery is expected to be part of a Module")
+
+        # forward pass in eval mode
+        was_training = self.training
+        self.eval()
+        y_values = self.forward(x_values)
+        if was_training:
+            self.train()
+
+        if self.bias is not None:
+            y_values -= self.bias
 
         # calculate pseudo inverse (with bias, if necessary)
-        if self.bias:
-            ones_column = ones(self.in_size * over_sampling, 1,
-                               device=self.device, dtype=float32)
+        if self.analog_bias:
+            ones_column = ones(self.in_size * over_sampling, 1, device=self.device, dtype=float32)
             x_values = cat([x_values, ones_column], axis=1)
 
-        est_weights = lstsq(x_values, y_values).solution.T.cpu()
-        weights, biases = self._separate_weights(est_weights)
+        est_weight = lstsq(x_values, y_values).solution.T.cpu()
+        weight, bias = self._separate_weights(est_weight)
+
+        if self.digital_bias:
+            bias = self.bias.detach().cpu()
 
         if not apply_weight_scaling:
-            # we de-apply (devide) because we want to use the full self.forward above
+            # we de-apply all scales
             alpha = self.get_scales()
             if alpha is not None:
                 alpha = alpha.detach().cpu()
-                return weights / alpha.view(-1, 1), biases / alpha if self.bias else None
-        return weights, biases
+                return (weight / alpha.view(-1, 1), bias / alpha if self.analog_bias else bias)
+        return weight, bias
 
     def apply_weight_scaling(
-            self,
-            combined_weights: Tensor,
-            weight_scaling_omega: Optional[float] = None
+        self, combined_weights: Tensor, weight_scaling_omega: Optional[float] = None
     ) -> Tensor:
         r"""Set the tile weights (and biases) in a scaled fashion.
 
         Scales the weights by a layerwise scale or columnwise scale (if
         ``weight_scaling_columnwise`` is set), that is then applied in digital
         at the output of forward and backward pass, and the learning rate for
         this tile is adjusted accordingly.
@@ -639,27 +432,28 @@
             combined_weights: ``[d_size, x_size]`` weight matrix.
             weight_scaling_omega: where the weight max should be mapped in terms of
                 the weight range. Note that for ``omega`` larger than
                 the maximal weight of the device, weights will get
                 clipped for most devices. If this parameter is not
                 given, it will default to the ``weight_scaling_omega``
                 value set in the
-                :class:`~aihwkit.configs.utils.MappingParameter` of the
+                :class:`~aihwkit.parameters.utils.MappingParameter` of the
                 ``rpu_config``
 
         Returns:
             scaled weights.
 
 
         .. _`Rasch, Gokmen & Haensch (2019)`: https://arxiv.org/abs/1906.02698
 
         """
         # Prepare the array expected by the pybind function, appending the
         # biases row if needed.
-        if not hasattr(self.rpu_config, 'mapping'):
+
+        if not hasattr(self.rpu_config, "mapping"):
             return combined_weights
 
         mapping = self.rpu_config.mapping  # type: MappingParameter
         omega = weight_scaling_omega
         if omega is None:
             omega = mapping.weight_scaling_omega
 
@@ -703,18 +497,15 @@
 
         if mapping_scales is None:
             self.mapping_scales = None
             return
 
         if isinstance(mapping_scales, float):
             if self.mapping_scales is None:
-                self.mapping_scales = ones((1, ),
-                                           dtype=float32,
-                                           device=self.device,
-                                           requires_grad=False)
+                self.mapping_scales = ones((1,), dtype=float32, device=self.device)
             self.mapping_scales[:] = mapping_scales
             return
 
         if isinstance(self.mapping_scales, Tensor) and len(mapping_scales) == 1:
             self.mapping_scales[:] = mapping_scales.to(self.device)
             return
 
@@ -724,56 +515,98 @@
     def init_mapping_scales(self) -> None:
         """Helper function to initialize the mapping scales used to scale the
         weights in digital and determine the conductance conversion.
 
         Note:
             This method is called from the constructor.
         """
-        if not hasattr(self.rpu_config, 'mapping'):
+        if not hasattr(self.rpu_config, "mapping"):
             self.set_mapping_scales(None)
             return
 
         mapping = self.rpu_config.mapping  # type: MappingParameter
         mapping_scales = None
         if mapping.weight_scaling_omega:
             if mapping.weight_scaling_columnwise:
-                mapping_scales = ones((self.out_size, ),
-                                      dtype=float32,
-                                      device=self.device,
-                                      requires_grad=False)
+                mapping_scales = ones(
+                    (self.out_size,), dtype=float32, device=self.device, requires_grad=False
+                )
             else:
-                mapping_scales = ones((1, ),
-                                      dtype=float32,
-                                      device=self.device,
-                                      requires_grad=False)
+                mapping_scales = ones((1,), dtype=float32, device=self.device, requires_grad=False)
         self.set_mapping_scales(mapping_scales)
 
     @no_grad()
-    def init_input_processing(self) -> None:
+    def init_input_processing(self) -> bool:
         """Helper function to initialize the input processing.
 
         Note:
             This method is called from the constructor.
 
-        Raises: ConfigError in case ``manage_output_clipping`` is
-            enabled but not supported.
+        Returns:
+            whether input processing is enabled
+
+        Raises:
+            ConfigError: in case ``manage_output_clipping`` is
+                enabled but not supported.
         """
         self.input_range = None
-        if not hasattr(self.rpu_config, 'pre_post'):
-            return
+
+        if not isinstance(self.rpu_config, PrePostProcessingRPU):
+            return False  # type: ignore
 
         ir_params = self.rpu_config.pre_post.input_range  # type: InputRangeParameter
         if ir_params.enable:
             self.input_range_update_idx = 0
-            self.input_range = full((1,), ir_params.init_value, dtype=float32,
-                                    device=self.device, requires_grad=True)
-
-            if (ir_params.manage_output_clipping
-                    and not ir_params.supports_manage_output_clipping(self.rpu_config)):
+            if ir_params.learn_input_range:
+                self.input_range = Parameter(
+                    full(
+                        (1,),
+                        ir_params.init_value,
+                        dtype=float32,
+                        device=self.device,
+                        requires_grad=True,
+                    )
+                )
+            else:
+                input_range = full(
+                    (1,),
+                    ir_params.init_value,
+                    dtype=float32,
+                    device=self.device,
+                    requires_grad=False,
+                )
+                self.register_buffer("input_range", input_range)  # type: ignore
+
+            if ir_params.manage_output_clipping and not ir_params.supports_manage_output_clipping(
+                self.rpu_config
+            ):
                 raise ConfigError("RPU Config does not support `manage_output_clipping`.")
+            return True
+        return False
+
+    @no_grad()
+    def set_input_range(self, value: Union[Tensor, float]) -> None:
+        """Sets the input range.
+
+        Args:
+           value: input range value
+
+        Raises:
+             ConfigError: in case input range is None
+        """
+        if self.input_range is None:
+            raise ConfigError("Input range is not enabled")
+        if isinstance(value, Tensor):
+            input_range = value[0].item()
+        else:
+            input_range = value
+        if isinstance(self.input_range, Parameter):
+            self.input_range.data[0] = abs(input_range)
+        else:
+            self.input_range[0] = abs(input_range)
 
     @no_grad()
     def set_scales(self, scales: Union[Tensor, float]) -> None:
         """Set all scales with a new scale.
 
         This will set the mapping scales to ``scales`` and set all other scales to 1.
 
@@ -782,20 +615,19 @@
         """
 
         self.set_mapping_scales(scales)
         self.set_learned_out_scales(1.0)
 
     @no_grad()
     def get_scales(self) -> Optional[Tensor]:
-        """ Set all scales with a new scale.
+        """Get all scales with a new scale.
 
         Returns:
             Scale tensor if any scale exist else None.
         """
-
         learned_out_scales = self.get_learned_out_scales()
         mapping_scales = self.get_mapping_scales()
         if mapping_scales is None and learned_out_scales is None:
             return None
         if mapping_scales is None:
             return learned_out_scales
         if learned_out_scales is None:
@@ -818,29 +650,27 @@
         """Helper function to initialize the learned out scaling used to scale the
         weights in digital.
 
         Note:
             This method is called from the constructor.
         """
 
-        if not hasattr(self.rpu_config, 'mapping'):
+        if not hasattr(self.rpu_config, "mapping"):
             return
 
-        mapping = self.rpu_config.mapping  # type: ignore
+        mapping = self.rpu_config.mapping
         if mapping.learn_out_scaling:
             if mapping.out_scaling_columnwise:
-                self.out_scaling_alpha = ones((self.out_size, ),
-                                              dtype=float32,
-                                              device=self.device,
-                                              requires_grad=True)
+                self.out_scaling_alpha = Parameter(
+                    ones((self.out_size,), dtype=float32, device=self.device, requires_grad=True)
+                )
             else:
-                self.out_scaling_alpha = ones((1, ),
-                                              dtype=float32,
-                                              device=self.device,
-                                              requires_grad=True)
+                self.out_scaling_alpha = Parameter(
+                    ones((1,), dtype=float32, device=self.device, requires_grad=True)
+                )
 
     @no_grad()
     def set_learned_out_scales(self, alpha: Union[Tensor, float]) -> None:
         """Helper function to set the out scaling alpha used to scale the
         weights in digital.
 
         Note:
@@ -854,220 +684,76 @@
             alpha: out scales as a parameter that is learned.
 
         """
         if self.out_scaling_alpha is None:
             return
 
         if isinstance(self.out_scaling_alpha, Parameter):
-            self.out_scaling_alpha.data[:] = squeeze(as_tensor(alpha)).to(self.device)
+            self.out_scaling_alpha.data = self.out_scaling_alpha.data.view(-1)
+            self.out_scaling_alpha.data[:] = as_tensor(alpha).to(self.device).view(-1)
         elif isinstance(self.out_scaling_alpha, Tensor):
-            self.out_scaling_alpha[:] = squeeze(as_tensor(alpha)).to(self.device)
+            self.out_scaling_alpha = self.out_scaling_alpha.view(-1)
+            self.out_scaling_alpha[:] = as_tensor(alpha).to(self.device).view(-1)
         else:
-            self.out_scaling_alpha = squeeze(as_tensor(alpha)).to(self.device)
+            self.out_scaling_alpha = as_tensor(alpha).to(self.device).view(-1)
 
-    def apply_out_scaling(self, values: Tensor,
-                          tensor_view: Optional[Tuple[int, ...]] = None) -> Tensor:
+    def apply_out_scaling(
+        self, values: Tensor, tensor_view: Optional[Tuple[int, ...]] = None
+    ) -> Tensor:
         """Apply the learned out scaling to the given tensor.
 
         Args:
             values: tensor to apply scaling to.
             tensor_view: view to cast the out scalings before multiplication
 
         Returns:
             output tensor with applied out scaling factors
         """
         if self.out_scaling_alpha is not None:
             if tensor_view is None:
-                tensor_view = self._get_tensor_view(values.dim(),
-                                                    0 if self.out_trans else values.dim() - 1)
+                tensor_view = self.get_tensor_view(values.dim())
             return values * self.out_scaling_alpha.view(*tensor_view)
         return values
 
-    @no_grad()
     def apply_input_range(self, values: Tensor, update_from_data: bool = False) -> Tensor:
-        """ Apply the input clipping.
+        """Apply the input clipping.
 
         Args:
             values: tensor to clip
             update_from_data: whether to update from data if applicable
 
         Returns:
             clipped output tensor
         """
+
         if self.input_range is None:
             return values
 
-        if update_from_data:
-            ir_params = self.rpu_config.pre_post.input_range  # type: ignore
+        if isinstance(self.rpu_config, PrePostProcessingRPU) and update_from_data:
+            ir_params = self.rpu_config.pre_post.input_range
             idx = self.input_range_update_idx
             if idx < ir_params.init_from_data:
-                self.input_range.data = (self.input_range.data * idx
-                                         + ir_params.init_std_alpha * values.std()
-                                         ) / (idx + 1)
-                self.input_range_update_idx += 1
-
-        self.input_range.data = self.input_range.data.abs()
-        return clamp(values,
-                     min=-self.input_range.item(),  # pylint: disable=invalid-unary-operand-type
-                     max=self.input_range.item())
-
-    def set_learning_rate(self, learning_rate: float) -> None:
-        """Set the tile learning rate.
-
-        Set the tile learning rate to ``-learning_rate``. Note that the
-        learning rate is always taken to be negative (because of the meaning in
-        gradient descent) and positive learning rates are not supported.
-
-        Args:
-            learning_rate: the desired learning rate.
-
-        Returns:
-            None.
-        """
-        return self.tile.set_learning_rate(learning_rate)
-
-    def get_learning_rate(self) -> float:
-        """Return the tile learning rate.
+                std = values.std()
+                if std > 0.0:
+                    self.input_range.data = (
+                        self.input_range.data * idx + ir_params.init_std_alpha * std
+                    ) / (idx + 1)
+                    self.input_range_update_idx += 1
+
+                self.input_range.data = self.input_range.data.abs()
+
+        return clamp(
+            values,
+            min=-abs(self.input_range.item()),  # pylint: disable=invalid-unary-operand-type
+            max=abs(self.input_range.item()),
+        )
 
-        Returns:
-            float: the tile learning rate.
-        """
-        return self.tile.get_learning_rate()
-
-    @no_grad()
-    def decay_weights(self, alpha: float = 1.0) -> None:
-        """Decays the weights once according to the decay parameters of the tile.
-
-        Args:
-            alpha: additional decay scale (such as LR). The base decay
-                rate is set during tile init.
-
-        Returns:
-            None.
-        """
-        return self.tile.decay_weights(alpha)
-
-    @no_grad()
-    def drift_weights(self, delta_t: float = 1.0) -> None:
-        """Drifts the weights once according to the drift parameters of the
-        tile.
-
-        See also :class:`~aihwkit.simulator.configs.utils.DriftParameter`.
-
-        Args:
-            delta_t: Time since last drift call.
-
-        Returns:
-            None.
-        """
-        return self.tile.drift_weights(delta_t)
-
-    @no_grad()
-    def diffuse_weights(self) -> None:
-        """Diffuses the weights once according to the diffusion parameters of
-        the tile.
-
-        The base diffusion rate is set during tile init.
-
-        Returns:
-            None
-        """
-        return self.tile.diffuse_weights()
-
-    @no_grad()
-    def reset_columns(
-            self,
-            start_column_idx: int = 0,
-            num_columns: int = 1,
-            reset_prob: float = 1.0
-    ) -> None:
-        r"""Reset (a number of) columns according to the reset parameters of the tile.
-
-        Resets the weights with device-to-device and cycle-to-cycle
-        variability (depending on device type), typically:
-
-        .. math::
-            W_{ij} = \xi*\sigma_\text{reset} + b^\text{reset}_{ij}
-
-        The reset parameters are set during tile init.
-
-        Args:
-            start_column_idx: a start index of columns (0..x_size-1)
-            num_columns: how many consecutive columns to reset (with circular warping)
-            reset_prob: individual probability of reset.
-
-        Returns:
-            None
-        """
-        return self.tile.reset_columns(start_column_idx, num_columns, reset_prob)
-
-    @no_grad()
-    def reset(
-            self,
-            reset_prob: float = 1.0
-    ) -> None:
-        r"""Reset the updated device tile according to the reset parameters of the tile.
-
-        Resets the weights with device-to-device and cycle-to-cycle
-        variability (depending on device type), typically:
-
-        .. math::
-            W_{ij} = \xi*\sigma_\text{reset} + b^\text{reset}_{ij}
-
-        The reset parameters are set during tile init.
-
-        Args:
-            reset_prob: individual probability of reset.
-
-        Returns:
-            None
-        """
-        return self.tile.reset_columns(0, -1, reset_prob)
-
-    def cpu(self) -> 'BaseTile':
-        """Return a copy of this tile in CPU memory.
-
-        Returns:
-            self in case of CPU
-
-        """
-        if not self.is_cuda:
-            return self
-
-        state_dict = self.__getstate__()
-        for value in state_dict.values():
-            if isinstance(value, AnalogContext):
-                value.data = value.data.cpu()
-        self.__setstate__(state_dict)
-        return self
-
-    def cuda(
-            self,
-            device: Optional[Union[torch_device, str, int]] = None
-    ) -> 'BaseTile':
-        """Return a copy of this tile in CUDA memory."""
-        raise NotImplementedError
-
-    def _get_tensor_view(self, ndim: int, dim: int) -> tuple:
-        """Return the tensor view for ndim vector at dim.
-
-        Args:
-            ndim: number of dimensions
-            dim: the dimension to set to -1
-
-        Returns:
-            List of ones with the `dim`` index sets to -1
-        """
-        tensor_view = [1] * ndim
-        tensor_view[dim] = -1
-        return tuple(tensor_view)
-
-    @no_grad()
-    def pre_forward(self, x_input: Tensor, dim: int,
-                    is_test: bool = False, ctx: Any = None) -> Tensor:
+    def pre_forward(
+        self, x_input: Tensor, dim: int, is_test: bool = False, ctx: Any = None
+    ) -> Tensor:
         """Operations before the actual forward step for pre processing.
 
         By default, this is an no-op. However, it could be overridden
         in derived tile classes.
 
         Args:
             x_input: input tensor for the analog MVM of the tile.
@@ -1079,60 +765,65 @@
             Output tensor of the same shape
         """
         # pylint: disable=unused-argument
         if self.input_range is not None:
             x_input = self.apply_input_range(x_input, not is_test) / self.input_range
         return x_input
 
-    @no_grad()
-    def post_forward(self, x_output: Tensor,
-                     dim: int,
-                     is_test: bool = False,
-                     ctx: Any = None) -> Tensor:
+    def post_forward(
+        self, x_output: Tensor, dim: int, is_test: bool = False, ctx: Any = None
+    ) -> Tensor:
         """Operations after the actual forward step for post processing.
 
         Args:
             x_output:  tensor that is the output from the forward pass of the tile
             dim: output channel dimension, ie the d_size dimension
             is_test: whether in eval mode
             ctx: torch auto-grad context [Optional]
 
         Returns:
             Output tensor of the same shape
         """
         # pylint: disable=unused-argument
+
+        bound = None
+        if self.handle_output_bound:
+            bound = self.get_forward_out_bound()  # pylint: disable=assignment-from-none
+        if bound is not None and ctx is not None:
+            # pylint: disable=invalid-unary-operand-type
+            grad_zero_msk = logical_or(x_output >= bound, x_output <= -bound)
+            ctx.saved_analog_tensors.append(grad_zero_msk)
+
         scale = None
         if self.input_range is not None:
             scale = self.input_range
 
-            # if output clip determines input clip learning
-            ir_params = self.rpu_config.pre_post.input_range  # type: ignore
-            if ctx is not None and ir_params.manage_output_clipping:
-                out_bound = self.rpu_config.forward.out_bound * 0.999  # type: ignore
-                output_percentage = (x_output.abs() < out_bound).float().mean()
-                ctx.output_percentage = output_percentage
-
         if self.mapping_scales is not None:
-            tensor_view = self._get_tensor_view(x_output.dim(), dim)
+            tensor_view = self.get_tensor_view(x_output.dim(), dim)
             if scale is not None:
                 scale = scale * self.get_mapping_scales().view(tensor_view)
             else:
                 scale = self.get_mapping_scales().view(tensor_view)
 
         if scale is not None:
             return x_output * scale
         return x_output
 
-    @no_grad()
-    def forward(self, x_input: Tensor, is_test: bool = False, ctx: Any = None) -> Tensor:
+    def joint_forward(self, x_input: Tensor, is_test: bool = False, ctx: Any = None) -> Tensor:
         """Perform the forward pass.
 
         Calls first the ``pre_forward``, then the tile forward, and
         finally the ``post_forward`` step.
 
+        Caution:
+
+            This will apply the (digital) mapping scales, but
+            *not* the learnable out-scales which are handled in the
+            forward pass of the module
+
         Note:
 
             The full forward pass is not using autograd, thus all pre
             and post functions need to be handled appropriately in the
             pre/post backward functions.
 
         Args:
@@ -1141,24 +832,22 @@
             ctx: torch auto-grad context [Optional]
 
         Returns:
             torch.Tensor: ``[N, out_size]`` tensor. If ``out_trans`` is set, transposed.
 
         """
         # We use no-grad as we do it explicitly in the optimizer.
-        x_input = self.pre_forward(x_input,
-                                   0 if self.in_trans else x_input.dim() - 1,
-                                   is_test, ctx)
-        x_output = self.tile.forward(x_input, self.bias, self.in_trans,
-                                     self.out_trans, is_test, self.non_blocking)
-        return self.post_forward(x_output,
-                                 0 if self.out_trans else x_output.dim() - 1,
-                                 is_test, ctx)
+        x_input = self.pre_forward(x_input, 0 if self.in_trans else x_input.dim() - 1, is_test, ctx)
+        x_output = self.tile.forward(
+            x_input, self.analog_bias, self.in_trans, self.out_trans, is_test, self.non_blocking
+        )
+        return self.post_forward(
+            x_output, 0 if self.out_trans else x_output.dim() - 1, is_test, ctx
+        )
 
-    @no_grad()
     def pre_backward(self, d_input: Tensor, dim: int, ctx: Any = None) -> Tensor:
         """Operations before the actual backward step for pre processing.
 
         By default, this is an no-op. However, it could be overridden
         in derived tile classes.
 
         Args:
@@ -1167,19 +856,26 @@
             ctx: torch auto-grad context [Optional]
 
         Returns:
             The preprocessed tensor of the same shape
         """
         # pylint: disable=unused-argument
         if self.mapping_scales is not None:
-            tensor_view = self._get_tensor_view(d_input.dim(), dim)
-            return d_input * self.get_mapping_scales().view(tensor_view)
+            tensor_view = self.get_tensor_view(d_input.dim(), dim)
+            d_input = d_input * self.get_mapping_scales().view(tensor_view)
+
+        if self.handle_output_bound and ctx is not None:
+            zero_grad_msk = (
+                None if len(ctx.saved_analog_tensors) < 2 else ctx.saved_analog_tensors[1]
+            )
+            if zero_grad_msk is not None:
+                d_input = where(zero_grad_msk, 0.0, d_input)
+
         return d_input
 
-    @no_grad()
     def post_backward(self, d_output: Tensor, dim: int, ctx: Any = None) -> Tensor:
         """Operations after the actual backward step for post processing.
 
         Here, the mapping scales are applied if exist.
 
         Args:
             d_output: The output tensor from the analog MVM of the tile.
@@ -1189,70 +885,77 @@
         Returns:
             The postprocessed tensor of the same shape
         """
         # pylint: disable=unused-argument
 
         if self.input_range is not None and ctx is not None:
             # compute gradient of the clip
-            x_input,  = ctx.saved_tensors
+            x_input = ctx.saved_analog_tensors[0]
             ir_params = self.rpu_config.pre_post.input_range  # type: ignore
 
             upper_thres = x_input >= self.input_range
             lower_thres = x_input <= -self.input_range  # pylint: disable=invalid-unary-operand-type
 
             grad = zeros_like(self.input_range)
 
             grad += clamp(upper_thres * d_output, min=None, max=0.0).sum()
             grad -= clamp(lower_thres * d_output, min=0.0, max=None).sum()
 
             if ir_params.gradient_relative:
                 grad *= self.input_range
             grad *= ir_params.gradient_scale
 
-            if ir_params.manage_output_clipping:
-                output_percentage = getattr(ctx, 'output_percentage', 1.0)
-                grad -= (1.0 - output_percentage) * self.input_range * (
-                    output_percentage < ir_params.output_min_percentage)
+            zero_grad_msk = (
+                None if len(ctx.saved_analog_tensors) < 2 else ctx.saved_analog_tensors[1]
+            )
+            if ir_params.manage_output_clipping and zero_grad_msk is not None:
+                output_percentage = 1.0 - zero_grad_msk.count_nonzero().item() / d_output.numel()
+                grad -= (
+                    (1.0 - output_percentage)
+                    * self.input_range
+                    * (output_percentage < ir_params.output_min_percentage)
+                )
 
             if ir_params.decay > 0:
                 percentage = (x_input.abs() < self.input_range).float().mean()
-                grad += ir_params.decay * self.input_range * (
-                    percentage > ir_params.input_min_percentage)
+                grad += (
+                    ir_params.decay
+                    * self.input_range
+                    * (percentage > ir_params.input_min_percentage)
+                )
 
             if self.input_range.grad is None:
                 self.input_range.grad = grad
             else:
                 self.input_range.grad += grad
 
         return d_output
 
-    @no_grad()
     def backward(self, d_input: Tensor, ctx: Any = None) -> Tensor:
         """Perform the backward pass.
 
         Args:
             d_input: ``[N, out_size]`` tensor. If ``out_trans`` is set, transposed.
             ctx: torch auto-grad context [Optional]
 
         Returns:
             torch.Tensor: ``[N, in_size]`` tensor. If ``in_trans`` is set, transposed.
         """
-        d_input = self.pre_backward(d_input, 0 if self.out_trans else d_input.dim() - 1,
-                                    ctx)
-        d_output = self.tile.backward(d_input, self.bias, self.out_trans, self.in_trans,
-                                      self.non_blocking)
-        return self.post_backward(d_output, 0 if self.in_trans else d_output.dim() - 1,
-                                  ctx)
-
-    @no_grad()
-    def pre_update(self, x_input: Tensor, x_dim: int,
-                   d_input: Tensor, d_dim: int) -> Tuple[Tensor, Tensor]:
+        d_input = self.pre_backward(d_input, 0 if self.out_trans else d_input.dim() - 1, ctx)
+        d_output = self.tile.backward(  # type: ignore
+            d_input, self.analog_bias, self.out_trans, self.in_trans, self.non_blocking
+        )
+        return self.post_backward(d_output, 0 if self.in_trans else d_output.dim() - 1, ctx)
+
+    def pre_update(
+        self, x_input: Tensor, x_dim: int, d_input: Tensor, d_dim: int
+    ) -> Tuple[Tensor, Tensor]:
         """Operations before the actual update step for pre processing.
 
-        Be default, if the mapping scales are used, the ``d_input``
+        By default, if the mapping scales are used, the ``d_input``
         will be divided by the mapping scales to compensate for the
         conductance mapping.
 
         Caution:
 
             The ``x_input`` and ``d_input`` here are the *original* inputs
             to the ``forward` and ``backward`` methods, thus the
@@ -1270,163 +973,111 @@
 
         """
         # pylint: disable=unused-argument
         if self.input_range is not None:
             x_input = self.apply_input_range(x_input, False) / self.input_range
 
         if self.mapping_scales is not None:
-            tensor_view = self._get_tensor_view(d_input.dim(), d_dim)
+            tensor_view = self.get_tensor_view(d_input.dim(), d_dim)
             return x_input, d_input / self.get_mapping_scales().view(tensor_view)
 
         return x_input, d_input
 
-    @no_grad()
     def update(self, x_input: Tensor, d_input: Tensor) -> None:
         """Perform the update pass.
 
         Calls the ``pre_update`` method to pre-process the inputs.
 
         Args:
             x_input: ``[..., in_size]`` tensor. If ``in_trans`` is set, ``[in_size, ...]``.
             d_input: ``[..., out_size]`` tensor. If ``out_trans`` is set, ``[out_size, ...]``.
 
         Returns:
             None
         """
-        x_input, d_input = self.pre_update(x_input,
-                                           0 if self.in_trans else x_input.dim() - 1,
-                                           d_input,
-                                           0 if self.out_trans else d_input.dim() - 1)
-        return self.tile.update(x_input, d_input, self.bias,
-                                self.in_trans, self.out_trans, self.non_blocking)
-
-    def get_hidden_parameters(self) -> OrderedDict:
-        """Get the hidden parameters of the tile.
-
-        Returns:
-            Ordered dictionary of hidden parameter tensors.
-        """
-        names = self.tile.get_hidden_parameter_names()
-        hidden_parameters = self.tile.get_hidden_parameters().detach_()
+        x_input, d_input = self.pre_update(
+            x_input,
+            0 if self.in_trans else x_input.dim() - 1,
+            d_input,
+            0 if self.out_trans else d_input.dim() - 1,
+        )
+        return self.tile.update(  # type: ignore
+            x_input, d_input, self.analog_bias, self.in_trans, self.out_trans, self.non_blocking
+        )
 
-        ordered_parameters = OrderedDict()
-        for idx, name in enumerate(names):
-            ordered_parameters[name] = hidden_parameters[idx].clone()
-
-        return ordered_parameters
-
-    def set_hidden_parameters(self, ordered_parameters: OrderedDict) -> None:
-        """Set the hidden parameters of the tile.
-
-        Caution:
-            Usually the hidden parameters are drawn according to the
-            parameter definitions (those given in the RPU config). If
-            the hidden parameters are arbitrary set by the user, then
-            this correspondence might be broken. This might cause problems
-            in the learning, in particular, the `weight granularity`
-            (usually ``dw_min``, depending on the device) is needed for
-            the dynamic adjustment of the bit length
-            (``update_bl_management``, see
-            :class:`~aihwkit.simulator.configs.utils.UpdateParameters`).
-
-            Currently, the new ``dw_min`` parameter is tried to be
-            estimated from the average of hidden parameters if the
-            discrepancy with the ``dw_min`` from the definition is too
-            large.
+    @no_grad()
+    def cuda(self, device: Optional[Union[torch_device, str, int]] = None) -> "BaseTile":
+        """Return a copy of this tile in CUDA memory.
 
         Args:
-            ordered_parameters: Ordered dictionary of hidden parameter tensors.
-
-        Raises:
-            TileError: In case the ordered dict keys do not conform
-                with the current rpu config tile structure of the hidden
-                parameters
-        """
-        if len(ordered_parameters) == 0:
-            return
-
-        hidden_parameters = stack(list(ordered_parameters.values()), dim=0)
-        names = self.tile.get_hidden_parameter_names()
-        if names != list(ordered_parameters.keys()):
-            raise TileError('Mismatch with loaded analog state:'
-                            'Hidden parameter structure is unexpected.')
-
-        self.tile.set_hidden_parameters(hidden_parameters)
-
-    def get_hidden_update_index(self) -> int:
-        """Get the current updated device index of the hidden devices.
-
-        Usually this is 0 as only one device is present per
-        cross-point for many tile RPU configs. However, some RPU
-        configs maintain internally multiple devices per cross-point
-        (e.g. :class:`~aihwkit.simulator.config.devices.VectorUnitCell`).
+            device: CUDA device
 
         Returns:
-            The next mini-batch updated device index.
-
-        Note:
-            Depending on the update and learning policy implemented
-            in the tile, updated devices might switch internally as
-            well.
+            Self with the underlying buffers to CUDA memory.
         """
-        return self.tile.get_hidden_update_index()
-
-    def set_hidden_update_index(self, index: int) -> None:
-        """Set the current updated hidden device index.
+        if self.mapping_scales is not None:
+            self.mapping_scales = self.mapping_scales.cuda(device)
+        return self
 
-        Usually this is ignored and fixed to 0 as only one device is
-        present per cross-point. Other devices, might not allow
-        explicit setting as it would interfere with the implemented
-        learning rule. However, some tiles have internally
-        multiple devices per cross-point (eg. unit cell) that can be
-        chosen depending on the update policy.
+    @no_grad()
+    def cpu(self) -> "BaseTile":
+        """Return a copy of this tile in CPU memory.
 
-        Args:
-            index: device index to be updated in the next mini-batch
+        Returns:
+            Self with the underlying buffers moved to CPU memory.
 
-        Note:
-            Depending on the update and learning policy implemented
-            in the tile, updated devices might switch internally as
-            well.
         """
-        self.tile.set_hidden_update_index(index)
+        if self.mapping_scales is not None:
+            self.mapping_scales = self.mapping_scales.cpu()
+        return self
 
     def is_indexed(self) -> bool:
         """Returns whether index matrix for convolutions has been set.
 
         Returns:
-           Whether index matrix has been set
+            Whether index matrix has been set
+
+        Raises:
+            TileError: if indexed not supported.
         """
+        if isinstance(self.tile, SimulatorTile):
+            raise TileError("Only RPUCuda simulator tiles support indexed interface.")
         return self.tile.has_matrix_indices()
 
     def set_indexed(self, indices: Tensor, image_sizes: List) -> None:
         """Set the index matrix for convolutions and switches to
         indexed forward/backward/update versions.
 
         Args:
             indices : torch.tensor with int indices
             image_sizes: [C_in, H_in, W_in, H_out, W_out] sizes
 
         Raises:
-            ValueError: if ``image_sizes`` does not have valid dimensions.
-            TileError: if the tile uses transposition.
+            ValueError: if ``image_sizes`` does not have valid dimensions
+            TileError: if the tile uses transposition or indexed not supported..
         """
+        if isinstance(self.tile, SimulatorTile):
+            raise TileError("Only RPUCuda simulator tiles support indexed interface")
+
         if len(image_sizes) not in (3, 5, 7):
-            raise ValueError('image_sizes expects 3, 5 or 7 sizes '
-                             '[C_in, (D_in), H_in, (W_in), (D_out), H_out, (W_out)]')
+            raise ValueError(
+                "image_sizes expects 3, 5 or 7 sizes "
+                "[C_in, (D_in), H_in, (W_in), (D_out), H_out, (W_out)]"
+            )
 
         if self.in_trans or self.out_trans:
-            raise TileError('Transposed indexed versions not supported (assumes NC(D)HW)')
-
+            raise TileError("Transposed indexed versions not supported (assumes NC(D)HW)")
+        self.analog_ctx.set_indexed()
         self.image_sizes = image_sizes
         self.tile.set_matrix_indices(indices)
 
     @no_grad()
-    def forward_indexed(self, x_input: Tensor, is_test: bool = False,
-                        ctx: Any = None) -> Tensor:
+    def joint_forward_indexed(
+        self, x_input: Tensor, is_test: bool = False, ctx: Any = None
+    ) -> Tensor:
         """Perform the forward pass for convolutions.
 
         Depending on the input tensor size it performs the forward pass for a
         2D image or a 3D one.
 
         Args:
             x_input: ``[N, in_size]`` tensor. If ``in_trans`` is set, transposed.
@@ -1437,37 +1088,37 @@
             torch.Tensor: ``[N, out_size]`` tensor. If ``out_trans`` is set, transposed.
 
         Raises:
             TileError: if the indexed tile has not been initialized, or if
                 ``self.images_sizes`` does not have a valid dimennion.
         """
         if not self.image_sizes:
-            raise TileError('self.image_sizes is not initialized. Please use '
-                            'set_indexed()')
+            raise TileError("self.image_sizes is not initialized. Please use set_indexed()")
 
         n_batch = x_input.size(0)
         channel_out = self.out_size
 
         if len(self.image_sizes) == 3:
             _, _, height_out = self.image_sizes
             d_tensor = x_input.new_empty((n_batch, channel_out, height_out))
         elif len(self.image_sizes) == 5:
             _, _, _, height_out, width_out = self.image_sizes
             d_tensor = x_input.new_empty((n_batch, channel_out, height_out, width_out))
         elif len(self.image_sizes) == 7:
             _, _, _, _, depth_out, height_out, width_out = self.image_sizes
             d_tensor = x_input.new_empty((n_batch, channel_out, depth_out, height_out, width_out))
         else:
-            raise TileError('self.image_sizes length is not 3, 5 or 7')
+            raise TileError("self.image_sizes length is not 3, 5 or 7")
 
         x_input = self.pre_forward(x_input, 1, is_test, ctx)
-        x_output = self.tile.forward_indexed(x_input, d_tensor, is_test, self.non_blocking)
+        x_output = self.tile.forward_indexed(  # type: ignore
+            x_input, d_tensor, is_test, self.non_blocking
+        )
         return self.post_forward(x_output, 1, is_test, ctx)
 
-    @no_grad()
     def backward_indexed(self, d_input: Tensor, ctx: Any = None) -> Tensor:
         """Perform the backward pass for convolutions.
 
         Depending on the input tensor size it performs the backward pass for a
         2D image or a 3D one.
 
         Args:
@@ -1478,34 +1129,32 @@
             torch.Tensor: ``[N, in_size]`` tensor. If ``in_trans`` is set, transposed.
 
         Raises:
             TileError: if the indexed tile has not been initialized, or if
                 ``self.images_sizes`` does not have a valid dimennion.
         """
         if not self.image_sizes:
-            raise TileError('self.image_sizes is not initialized. Please use '
-                            'set_indexed()')
+            raise TileError("self.image_sizes is not initialized. Please use set_indexed()")
 
         n_batch = d_input.size(0)
 
         if len(self.image_sizes) == 3:
             channel_in, height_in, _ = self.image_sizes
             x_tensor = d_input.new_empty((n_batch, channel_in, height_in))
         elif len(self.image_sizes) == 5:
             channel_in, height_in, width_in, _, _ = self.image_sizes
             x_tensor = d_input.new_empty((n_batch, channel_in, height_in, width_in))
         elif len(self.image_sizes) == 7:
-            channel_in, depth_in, height_in, width_in, _, _, _ \
-                = self.image_sizes
+            channel_in, depth_in, height_in, width_in, _, _, _ = self.image_sizes
             x_tensor = d_input.new_empty((n_batch, channel_in, depth_in, height_in, width_in))
         else:
-            raise TileError('self.image_sizes length is not 3, 5 or 7')
+            raise TileError("self.image_sizes length is not 3, 5 or 7")
 
         d_input = self.pre_backward(d_input, 1, ctx)
-        d_output = self.tile.backward_indexed(d_input, x_tensor, self.non_blocking)
+        d_output = self.tile.backward_indexed(d_input, x_tensor, self.non_blocking)  # type: ignore
         return self.post_backward(d_output, 1, ctx)
 
     @no_grad()
     def update_indexed(self, x_input: Tensor, d_input: Tensor) -> None:
         """Perform the update pass for convolutions.
 
         Calls the ``pre_update`` methods to pre-process the inputs.
@@ -1514,25 +1163,8 @@
             x_input: ``[N, in_size]`` tensor. If ``in_trans`` is set, transposed.
             d_input: ``[N, out_size]`` tensor. If ``out_trans`` is set, transposed.
 
         Returns:
             None
         """
         x_input, d_input = self.pre_update(x_input, 1, d_input, 1)
-        return self.tile.update_indexed(x_input, d_input, self.non_blocking)
-
-    @no_grad()
-    def post_update_step(self) -> None:
-        """Operators that need to be called once per mini-batch.
-
-        Note:
-            This function is called by the analog optimizer.
-
-        Caution:
-
-            If no analog optimizer is used, the post update steps will
-            not be performed.
-        """
-        if self.rpu_config.device.requires_diffusion():  # type: ignore
-            self.diffuse_weights()
-        if self.rpu_config.device.requires_decay():  # type: ignore
-            self.decay_weights()
+        return self.tile.update_indexed(x_input, d_input, self.non_blocking)  # type: ignore
```

### Comparing `aihwkit-0.7.1/src/aihwkit/simulator/tiles/floating_point.py` & `aihwkit-0.8.0/src/aihwkit/simulator/tiles/floating_point.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,35 +1,37 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
 # that they have been altered from the originals.
 
 """High level analog tiles (floating point)."""
 
-from typing import Optional, Union, TYPE_CHECKING
+from typing import Optional, Tuple, TYPE_CHECKING
 
-from torch import device as torch_device
-from torch.cuda import device as cuda_device
+from torch import Tensor
 
-from aihwkit.exceptions import CudaError
-from aihwkit.simulator.rpu_base import cuda, tiles
-from aihwkit.simulator.tiles.base import BaseTile
+from aihwkit.simulator.rpu_base import tiles
+from aihwkit.simulator.tiles.rpucuda import RPUCudaSimulatorTileWrapper
+from aihwkit.simulator.tiles.module import TileModule
+from aihwkit.simulator.tiles.periphery import TileWithPeriphery
+from aihwkit.simulator.tiles.functions import AnalogFunction
+from aihwkit.simulator.parameters.base import RPUConfigGeneric
 
 if TYPE_CHECKING:
     from aihwkit.simulator.configs import FloatingPointRPUConfig
 
 
-class FloatingPointTile(BaseTile):
+class FloatingPointTile(TileModule, TileWithPeriphery, RPUCudaSimulatorTileWrapper):
     r"""Floating point tile.
 
     Implements a floating point or ideal analog tile.
 
     A linear layer with this tile is perfectly linear, it just uses
     the RPUCuda library for execution.
 
@@ -97,73 +99,56 @@
             input :math:`\mathbf{x}` will be automatically expanded by
             an extra dimension which will be set to 1 always.
         in_trans: Whether to assume an transposed input (batch first).
         out_trans: Whether to assume an transposed output (batch first).
     """
 
     def __init__(
-            self,
-            out_size: int,
-            in_size: int,
-            rpu_config: Optional['FloatingPointRPUConfig'] = None,
-            bias: bool = False,
-            in_trans: bool = False,
-            out_trans: bool = False,
+        self,
+        out_size: int,
+        in_size: int,
+        rpu_config: "FloatingPointRPUConfig",
+        bias: bool = False,
+        in_trans: bool = False,
+        out_trans: bool = False,
     ):
-        if not rpu_config:
-            # Import `FloatingPointRPUConfig` dynamically to avoid import cycles.
-            # pylint: disable=import-outside-toplevel
-            from aihwkit.simulator.configs import FloatingPointRPUConfig
-            rpu_config = FloatingPointRPUConfig()
-        super().__init__(out_size, in_size, rpu_config, bias, in_trans, out_trans)
-
-    def cuda(
-            self,
-            device: Optional[Union[torch_device, str, int]] = None
-    ) -> 'BaseTile':
-        """Return a copy of this tile in CUDA memory.
-
-        Args:
-            device: CUDA device
-
-        Returns:
-            Self with the underlying C++ tile moved to CUDA memory.
-
-        Raises:
-            CudaError: if the library has not been compiled with CUDA.
-        """
-        if not cuda.is_compiled():
-            raise CudaError('aihwkit has not been compiled with CUDA support')
-
-        device = torch_device('cuda', cuda_device(device).idx)
-
-        if self.is_cuda and device != self.device:
-            raise CudaError('Cannot switch CUDA devices of existing Cuda tiles')
-
-        if isinstance(self.tile, tiles.FloatingPointTile):
-            with cuda_device(device):
-                self.tile = tiles.CudaFloatingPointTile(self.tile)
-                self.is_cuda = True
-                self.device = device
-                self.analog_ctx.cuda(device)
-
-        return self
+        TileModule.__init__(self)
+        RPUCudaSimulatorTileWrapper.__init__(
+            self, out_size, in_size, rpu_config, bias, in_trans, out_trans  # type: ignore
+        )
+        TileWithPeriphery.__init__(self)
 
     def _create_simulator_tile(
-            self,
-            x_size: int,
-            d_size: int,
-            rpu_config: 'FloatingPointRPUConfig'
+        self, x_size: int, d_size: int, rpu_config: RPUConfigGeneric
     ) -> tiles.FloatingPointTile:
         """Create a simulator tile.
 
+
         Args:
             x_size: input size
             d_size: output size
             rpu_config: resistive processing unit configuration
 
         Returns:
             a simulator tile based on the specified configuration.
         """
         meta_parameter = rpu_config.device.as_bindings()
 
         return meta_parameter.create_array(x_size, d_size)
+
+    def forward(
+        self, x_input: Tensor, tensor_view: Optional[Tuple] = None  # type: ignore
+    ) -> Tensor:
+        """Torch forward function that calls the analog forward"""
+        # pylint: disable=arguments-differ
+
+        out = AnalogFunction.apply(
+            self.get_analog_ctx(), self, x_input, self.shared_weights, not self.training
+        )
+
+        if tensor_view is None:
+            tensor_view = self.get_tensor_view(out.dim())
+        out = self.apply_out_scaling(out, tensor_view)
+
+        if self.digital_bias:
+            return out + self.bias.view(*tensor_view)
+        return out
```

### Comparing `aihwkit-0.7.1/src/aihwkit/simulator/tiles/inference.py` & `aihwkit-0.8.0/src/aihwkit/simulator/tiles/inference.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,268 +1,368 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
 # that they have been altered from the originals.
 
 """High level analog tiles (inference)."""
 
-from typing import List, Optional, Union, Any, TYPE_CHECKING
+# pylint: disable=too-many-ancestors
+
+from typing import Optional, Union, Any, Tuple, List, TYPE_CHECKING
 
 from torch import device as torch_device
-from torch import ones, zeros, Tensor, float32
+from torch import ones, Tensor, float32
+from torch.nn import Module
 from torch.autograd import no_grad
 
-from aihwkit.simulator.tiles.analog import AnalogTile
+from aihwkit.exceptions import ConfigError
+from aihwkit.simulator.tiles.functions import AnalogFunction
+from aihwkit.simulator.tiles.periphery import TileWithPeriphery
+from aihwkit.simulator.tiles.module import TileModule
+from aihwkit.simulator.tiles.rpucuda import RPUCudaSimulatorTileWrapper
+from aihwkit.simulator.tiles.base import BaseTile
+from aihwkit.simulator.rpu_base import tiles
+from aihwkit.simulator.parameters.base import RPUConfigGeneric
+from aihwkit.simulator.parameters.helpers import parameters_to_bindings
+from aihwkit.simulator.parameters.enums import WeightModifierType, WeightClipType, WeightRemapType
 
 if TYPE_CHECKING:
     from aihwkit.simulator.configs import InferenceRPUConfig
-    from aihwkit.simulator.tiles import BaseTile
 
-# pylint: disable=too-many-instance-attributes
 
+class InferenceTileWithPeriphery(TileWithPeriphery):
+    """Additional (peripheral) functionality for hardware-aware
+    training and inference.
 
-class InferenceTile(AnalogTile):
-    """Tile used for analog inference and hardware-aware training for inference.
+    Note:
 
-    Args:
-        out_size: output size
-        in_size: input size
-        rpu_config: resistive processing unit configuration.
-        bias: whether to add a bias column to the tile.
-        in_trans: Whether to assume an transposed input (batch first)
-        out_trans: Whether to assume an transposed output (batch first)
-        shared_weights: Whether to keep the weight in torch's memory space
-    """
+        Here it is assumed that the training is done in software and
+        only the inference pass is done on analog hardware.
 
-    def __init__(
-            self,
-            out_size: int,
-            in_size: int,
-            rpu_config: Optional['InferenceRPUConfig'] = None,
-            bias: bool = False,
-            in_trans: bool = False,
-            out_trans: bool = False,
-            shared_weights: bool = True,
-    ):
+    """
 
-        if not rpu_config:
-            # Import `InferenceRPUConfig` dynamically to avoid import cycles.
-            # pylint: disable=import-outside-toplevel
-            from aihwkit.simulator.configs import InferenceRPUConfig
-            rpu_config = InferenceRPUConfig()
+    # pylint: disable=abstract-method
 
-        # CAUTION: one cannot save parts of the RPUConfig as
-        # properties of the class! This is because those attributes
-        # would be restored even if the RPUConfig would be replaced
-        # during checkpoint loading
+    def __init__(self) -> None:
+        super().__init__()
 
         self.drift_baseline = None
         self.drift_readout_tensor = None  # type: Optional[Tensor]
-        self.alpha = ones((1,))
+        if isinstance(self, Module):
+            self.register_buffer("alpha", ones((1,), dtype=float32))
+        else:
+            self.alpha = ones((1,), dtype=float32)
 
         # Helpers.
         self.programmed_weights = None  # type: Optional[Tensor]
         self.nu_drift_list = None  # type: Optional[List[Tensor]]
 
-        super().__init__(out_size, in_size, rpu_config, bias, in_trans, out_trans)
+    def _create_simulator_tile(
+        self, x_size: int, d_size: int, rpu_config: RPUConfigGeneric
+    ) -> tiles.AnalogTile:
+        """Create a simulator tile.
 
-        if shared_weights:
-            self.shared_weights = zeros(out_size, in_size + int(bias),
-                                        requires_grad=True)  # type: Tensor
-            self.ensure_shared_weights()
+        Args:
+            x_size: input size
+            d_size: output size
+            rpu_config: resistive processing unit configuration
+
+        Returns:
+            a simulator tile based on the specified configuration.
+        """
+
+        meta_parameter = rpu_config.as_bindings()
+        device_parameter = rpu_config.device.as_bindings()
+
+        return meta_parameter.create_array(x_size, d_size, device_parameter)
 
     @no_grad()
     def init_mapping_scales(self) -> None:
         """Helper function to initialize the mapping scales used to scale the
         weights in digital and determine the conductance conversion.
 
         Note:
             This method is called from the constructor.
         """
-        # Import `aihwkit.simulator.configs` items dynamically to avoid import cycles.
-        # pylint: disable=import-outside-toplevel
-        from aihwkit.simulator.configs.utils import WeightRemapType
-
         super().init_mapping_scales()
-        remap = self.rpu_config.remap  # type: ignore
-        if remap.type != WeightRemapType.NONE:
+        if hasattr(self.rpu_config, "remap") and self.rpu_config.remap.type != WeightRemapType.NONE:
             # needs to be always out_size
-            mapping_scales = ones((self.out_size, ),
-                                  dtype=float32,
-                                  device=self.device,
-                                  requires_grad=False)
+            mapping_scales = ones(
+                (self.out_size,), dtype=float32, device=self.device, requires_grad=False
+            )
             self.set_mapping_scales(mapping_scales)
 
     @no_grad()
     def _forward_drift_readout_tensor(self, reset_if: bool = False) -> Optional[Tensor]:
         """Perform a forward pass using the drift read-out tensor.
 
         Args:
             reset_if: Will reset the readout tensor, otherwise use the stored one
 
         Returns:
             Readout tensor if drift compensation is on
         """
-
-        if self.rpu_config.drift_compensation is None:
+        if (
+            not hasattr(self.rpu_config, "drift_compensation")
+            or self.rpu_config.drift_compensation is None
+        ):
             return None
 
         if self.drift_readout_tensor is None or reset_if:
-            self.drift_readout_tensor = self.rpu_config.drift_compensation.get_readout_tensor(
-                self.tile.get_x_size()).detach().to(self.device)
+            self.drift_readout_tensor = (
+                self.rpu_config.drift_compensation.get_readout_tensor(self.tile.get_x_size())
+                .detach()
+                .to(self.device)
+            )
             if self.in_trans:
                 self.drift_readout_tensor = self.drift_readout_tensor.tranpose(0, 1).clone()
+        else:
+            self.drift_readout_tensor = self.drift_readout_tensor.to(self.device)
 
         # We need to take the bias as a common column here, also we do
         # not want to use indexed.
-        return self.tile.forward(self.drift_readout_tensor, False,
-                                 self.in_trans, self.out_trans, True)
+        return self.tile.forward(
+            self.drift_readout_tensor, False, self.in_trans, self.out_trans, True, self.non_blocking
+        )
 
     @no_grad()
     def program_weights(self, from_reference: bool = True) -> None:
         """Apply weights noise to the current tile weights and saves these for
         repeated drift experiments.
 
         This method also establishes the drift coefficients for each
         conductance slice.
 
         Will also reset the drift readout tensor and compuate a new
         drift compensation baseline
 
         Args:
             from_reference: Whether to use weights from reference
+
+        Raises:
+            ConfigError: in case of ``noise_model`` is not defined in
+                the RPUConfig
         """
         # pylint: disable=arguments-differ
 
+        if not hasattr(self.rpu_config, "noise_model"):
+            raise ConfigError("Seems that RPUConfig is not of type InferenceRPUConfig.")
+
         if not from_reference or self.reference_combined_weights is None:
             self.reference_combined_weights = Tensor(self.tile.get_weights())
 
-        self.programmed_weights, self.nu_drift_list = \
-            self.rpu_config.noise_model.apply_programming_noise(
-                self.reference_combined_weights)
+        (
+            self.programmed_weights,
+            self.nu_drift_list,
+        ) = self.rpu_config.noise_model.apply_programming_noise(self.reference_combined_weights)
 
         self.tile.set_weights(self.programmed_weights)
 
-        if self.rpu_config.drift_compensation is not None:
+        if (
+            hasattr(self.rpu_config, "drift_compensation")
+            and self.rpu_config.drift_compensation is not None
+        ):
             forward_output = self._forward_drift_readout_tensor(True)
             self.drift_baseline = self.rpu_config.drift_compensation.init_baseline(forward_output)
 
     @no_grad()
-    def drift_weights(
-            self,
-            t_inference: float = 0.0
-    ) -> None:
+    def drift_weights(self, t_inference: float = 0.0) -> None:
         """Programs and drifts the current reference weights.
 
         The current weight reference is either the current weights or
         the ones at the time when :meth:`initialize_drift_reference`
         was called, which then would overwrite the current weights
         with the drifted ones.
 
         Args:
             t_inference: Time (in sec) of assumed inference
                 time. Programming ends at t=0s.  The rest is waiting time,
                 where the devices might drift and accumulate noise. See
                 noise model used for details.
+
+        Raises:
+            ConfigError: in case of ``noise_model`` is not defined in
+                the RPUConfig
+
         """
         # pylint: disable=arguments-differ, arguments-renamed
 
+        if not hasattr(self.rpu_config, "noise_model"):
+            raise ConfigError("Seems that RPUConfig is not of type InferenceRPUConfig.")
+
         if self.programmed_weights is None:
             self.program_weights()
 
         drifted_weights = self.rpu_config.noise_model.apply_drift_noise(
-            self.programmed_weights, self.nu_drift_list, t_inference)
+            self.programmed_weights, self.nu_drift_list, t_inference
+        )
         self.tile.set_weights(drifted_weights)
 
-        if self.rpu_config.drift_compensation is not None:
+        if (
+            hasattr(self.rpu_config, "drift_compensation")
+            and self.rpu_config.drift_compensation is not None
+        ):
             forward_output = self._forward_drift_readout_tensor()
             self.alpha = self.rpu_config.drift_compensation.apply(
-                forward_output,
-                self.drift_baseline).to(self.device)
+                forward_output, self.drift_baseline
+            ).to(self.device)
 
-    @no_grad()
-    def post_forward(self, x_output: Tensor, dim: int, is_test: bool = False,
-                     ctx: Any = None) -> Tensor:
-        """Operations after the actual forward step for post processing """
+    def post_forward(
+        self, x_output: Tensor, dim: int, is_test: bool = False, ctx: Any = None
+    ) -> Tensor:
+        """Operations after the actual forward step for post processing"""
 
         x_output = super().post_forward(x_output, dim, is_test, ctx)
 
-        if not is_test or self.rpu_config.drift_compensation is None:
-            return x_output
-
-        # only do drift compensation in eval mode
-        return x_output * self.alpha
+        if (
+            is_test
+            and hasattr(self.rpu_config, "drift_compensation")
+            and self.rpu_config.drift_compensation is not None
+        ):
+            # only do drift compensation in eval mode
+            return x_output * self.alpha
+        return x_output
 
     @no_grad()
     def post_update_step(self) -> None:
         """Operators that need to be called once per mini-batch.
 
         In the :class:`~InferenceTile`, the following calls are made
         (if enabled in the ``rpu_config`` settings). First, the post
         update step of the parent is called, then the weight clipping
         is done, subsequently then remapping is done (if enforced),
         and finally the forward-backward weight modifier is
         called. The latter will modify the weights that are used
         during forward and backward (but not update) until the next
         time this function is called.
-
         """
-        # Import `aihwkit.simulator.configs` items dynamically to avoid import cycles.
-        # pylint: disable=import-outside-toplevel
-        from aihwkit.simulator.configs.helpers import parameters_to_bindings
-        from aihwkit.simulator.configs.utils import (
-            WeightClipType, WeightModifierType, WeightRemapType
-        )
-
         super().post_update_step()
 
         # TODO: make this a little nicer. Now each time bindings are
         # generated, which however has the advantage that parameters
         # could be changed-on-the-fly
 
-        if self.rpu_config.clip.type != WeightClipType.NONE:
+        if hasattr(self.rpu_config, "clip") and self.rpu_config.clip.type != WeightClipType.NONE:
             weight_clip_params = parameters_to_bindings(self.rpu_config.clip)
             self.tile.clip_weights(weight_clip_params)
 
-        if self.rpu_config.remap.type != WeightRemapType.NONE:
+        if hasattr(self.rpu_config, "remap") and self.rpu_config.remap.type != WeightRemapType.NONE:
             weight_remap_params = parameters_to_bindings(self.rpu_config.remap)
             scales = self.get_scales()
             scales = self.tile.remap_weights(weight_remap_params, scales)
             self.set_scales(scales)
 
         # update the forward / backward modified weights here
-        if (self.rpu_config.modifier.type != WeightModifierType.COPY or
-                self.rpu_config.modifier.pdrop > 0.0):
-            weight_modify_params = parameters_to_bindings(self.rpu_config.modifier)
-            self.tile.modify_weights(weight_modify_params)
+        if not hasattr(self.rpu_config, "modifier"):
+            return
+        if self.rpu_config.modifier.type == WeightModifierType.NONE:
+            return
+        if (
+            self.rpu_config.modifier.type == WeightModifierType.COPY
+            and self.rpu_config.modifier.pdrop <= 0.0
+        ):
+            return
+        weight_modify_params = parameters_to_bindings(self.rpu_config.modifier)
+        self.tile.modify_weights(weight_modify_params)  # type: ignore
+
+    def cuda(self, device: Optional[Union[torch_device, str, int]] = None) -> "BaseTile":
+        self.alpha = self.alpha.cuda(device)
+        ret = super().cuda(device)
+        return ret
+
+    def cpu(self) -> "BaseTile":
+        self.alpha = self.alpha.cpu()
+        ret = super().cpu()
+        return ret
+
+
+class InferenceTile(TileModule, InferenceTileWithPeriphery, RPUCudaSimulatorTileWrapper):
+    """Tile used for analog inference and hardware-aware training for inference.
+
+    Note:
+
+        This tile uses RPUCuda library with backward and update set to
+        perfect.
+
+    Args:
+        out_size: output size
+        in_size: input size
+        rpu_config: resistive processing unit configuration.
+        bias: whether to add a bias column to the tile.
+        in_trans: Whether to assume an transposed input (batch first)
+        out_trans: Whether to assume an transposed output (batch first)
+        shared_weights: Whether to keep the weight in torch's memory space
 
-    def cuda(
+    """
+
+    def __init__(
+        self,
+        out_size: int,
+        in_size: int,
+        rpu_config: Optional["InferenceRPUConfig"] = None,
+        bias: bool = False,
+        in_trans: bool = False,
+        out_trans: bool = False,
+        shared_weights: bool = True,
+    ):
+        if not rpu_config:
+            # Import `InferenceRPUConfig` dynamically to avoid import cycles.
+            # pylint: disable=import-outside-toplevel
+            from aihwkit.simulator.configs import InferenceRPUConfig
+
+            rpu_config = InferenceRPUConfig()
+
+        TileModule.__init__(self)
+        RPUCudaSimulatorTileWrapper.__init__(
             self,
-            device: Optional[Union[torch_device, str, int]] = None
-    ) -> 'BaseTile':
-        """Return a copy of this tile in CUDA memory.
+            out_size,
+            in_size,
+            rpu_config,
+            bias,
+            in_trans,
+            out_trans,
+            shared_weights=shared_weights,
+        )
+        InferenceTileWithPeriphery.__init__(self)
+
+    def _create_simulator_tile(  # type: ignore
+        self, x_size: int, d_size: int, rpu_config: "InferenceRPUConfig"
+    ) -> tiles.AnalogTile:
+        """Create a simulator tile.
 
         Args:
-            device: CUDA device
+            x_size: input size
+            d_size: output size
+            rpu_config: resistive processing unit configuration
 
         Returns:
-            Self with the underlying C++ tile moved to CUDA memory.
-
-        Raises:
-            CudaError: if the library has not been compiled with CUDA.
+            a simulator tile based on the specified configuration.
         """
-        super().cuda(device)
+        meta_parameter = rpu_config.as_bindings()
+        device_parameter = rpu_config.device.as_bindings()
 
-        self.alpha = self.alpha.cuda(device)
-        self.shared_weights.data = zeros(self.tile.get_x_size(),
-                                         self.tile.get_d_size(),
-                                         requires_grad=True).cuda(device)
-        self.ensure_shared_weights()
+        return meta_parameter.create_array(x_size, d_size, device_parameter)
+
+    def forward(
+        self, x_input: Tensor, tensor_view: Optional[Tuple] = None  # type: ignore
+    ) -> Tensor:
+        """Torch forward function that calls the analog forward"""
+        # pylint: disable=arguments-differ
+
+        out = AnalogFunction.apply(
+            self.get_analog_ctx(), self, x_input, self.shared_weights, not self.training
+        )
 
-        return self
+        if tensor_view is None:
+            tensor_view = self.get_tensor_view(out.dim())
+        out = self.apply_out_scaling(out, tensor_view)
+
+        if self.digital_bias:
+            return out + self.bias.view(*tensor_view)
+        return out
```

### Comparing `aihwkit-0.7.1/src/aihwkit/utils/analog_info.py` & `aihwkit-0.8.0/src/aihwkit/utils/analog_info.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -21,35 +21,37 @@
 import operator
 from typing import Optional, Any, List
 
 # Imports from PyTorch.
 from torch import zeros
 from torch.nn import Module
 
-from aihwkit.nn.modules.base import AnalogModuleBase, RPUConfigAlias
+from aihwkit.nn.modules.base import AnalogLayerBase
 from aihwkit.nn.modules.conv_mapped import _AnalogConvNdMapped
 from aihwkit.nn.modules.linear_mapped import AnalogLinearMapped
 from aihwkit.nn.modules.conv import _AnalogConvNd
 from aihwkit.nn import AnalogLinear
-from aihwkit.simulator.tiles import BaseTile
+from aihwkit.simulator.tiles.module import TileModule
+from aihwkit.simulator.parameters.base import RPUConfigBase
+
 
 COLUMN_DEFINITIONS = ["Layer Information", "Tile Information"]
 
 COLUMN_NAMES = {
     "name": (0, "Layer Name"),
     "isanalog": (0, "Is Analog"),
     "input_size": (0, "In Shape"),
     "output_size": (0, "Out Shape"),
     "kernel_size": (0, "Kernel Shape"),
     "num_tiles": (0, "# of Tiles"),
     # "macs": "Multi_Adds",
     "log_shape": (1, "Log. tile shape"),
     "phy_shape": (1, "Phys. tile shape"),
     "utilization": (1, "utilization (%)"),
-    "reuse_factor": (0, "Reuse Factor")
+    "reuse_factor": (0, "Reuse Factor"),
 }
 
 FORMATTING_WIDTH = 200
 COLUMN_WIDTH = 10
 FLOAT_FORMAT = "{0:.2f}"
 
 
@@ -58,60 +60,64 @@
 
     log_in_size: Any
     log_out_size: Any
     phy_in_size: Any
     phy_out_size: Any
     utilization: float
 
-    def __init__(self, tile: BaseTile, is_mapped: bool):
+    def __init__(self, tile: TileModule, is_mapped: bool):
         self.log_in_size = tile.in_size
         self.log_out_size = tile.out_size
         self.phy_in_size = tile.rpu_config.mapping.max_input_size
         self.phy_out_size = tile.rpu_config.mapping.max_output_size
         self.is_mapped = is_mapped
-        max_space = (self.phy_in_size*self.phy_out_size)
-        log_space = (self.log_in_size * self.log_out_size)
+        max_space = self.phy_in_size * self.phy_out_size
+        log_space = self.log_in_size * self.log_out_size
         self.utilization = log_space * 100 / max_space if is_mapped else 100
 
     def tile_summary_dict(self) -> dict:
         """Return a dictionary with the tile info."""
-        phys_shape = 'N/A' if not self.is_mapped else (self.phy_out_size, self.phy_in_size)
-        return {"log_shape": str((self.log_out_size, self.log_in_size)),
-                "phys_shape": str(phys_shape),
-                "utilization": self.utilization
-                }
+        phys_shape = "N/A" if not self.is_mapped else (self.phy_out_size, self.phy_in_size)
+        return {
+            "log_shape": str((self.log_out_size, self.log_in_size)),
+            "phys_shape": str(phys_shape),
+            "utilization": self.utilization,
+        }
 
     def __repr__(self) -> str:
         """Print Tile's information."""
         tile_info = self.tile_summary_dict().values()
         return "{:<20}{:<20}{:<20}\n".format(*(tile_info))
 
 
 class LayerInfo:
     """Class for storing layer statistics and information."""
+
     # pylint: disable=too-many-instance-attributes
     module: Module
     name: str
     isanalog: bool
     num_tiles: int
     tiles_info: List[TileInfo]
     input_size: Any
     output_size: Any
     kernel_size: Any
     reuse_factor: int
 
-    def __init__(self,
-                 module: Module,
-                 rpu_config: Optional[RPUConfigAlias] = None,
-                 input_size: Any = None,
-                 output_size: Any = None):
+    def __init__(
+        self,
+        module: Module,
+        rpu_config: Optional[RPUConfigBase] = None,
+        input_size: Any = None,
+        output_size: Any = None,
+    ):
         self.module = module
         self.name = self.module.__class__.__name__
-        self.isanalog = isinstance(self.module, AnalogModuleBase)
-        self.num_tiles = 0 if not self.isanalog else self.module._analog_tile_counter
+        self.isanalog = isinstance(self.module, AnalogLayerBase)
+        self.num_tiles = 0 if not self.isanalog else len(list(self.module.analog_tiles()))
         self.tiles_info = self.set_tiles_info()
         self.kernel_size = None
         self.reuse_factor = 0
         self.input_size, self.output_size = input_size, output_size
         self.rpu_config = rpu_config
         self.set_kernel_size()
         self.calculate_reuse_factor()
@@ -121,22 +127,22 @@
         self.reuse_factor = reuse_factor
 
     def set_tiles_info(self) -> List[TileInfo]:
         """Create TileInfo objects for each tile of the layer."""
         tiles_info = []
         is_mapped = isinstance(self.module, AnalogLinearMapped)
         is_mapped = is_mapped or isinstance(self.module, _AnalogConvNdMapped)
-        if isinstance(self.module, AnalogModuleBase):
+        if isinstance(self.module, AnalogLayerBase):
             for tile in self.module.analog_tiles():
                 tiles_info.append(TileInfo(tile, is_mapped))
         return tiles_info
 
     def set_kernel_size(self) -> None:
         """Set kernel size attribute."""
-        if hasattr(self.module, 'kernel_size'):
+        if hasattr(self.module, "kernel_size"):
             self.kernel_size = self.module.kernel_size
 
     def calculate_reuse_factor(self) -> None:
         """Compute the reuse factor.
 
         The reuse factor is the number of vector matrix multiplication
         a layer computes.
@@ -156,67 +162,78 @@
             tiles_summary.update(tile.tile_summary_dict())
         return tiles_summary
 
     def layer_summary_dict(self) -> dict:
         """Return a dictionary with all layer's information."""
 
         analog = "analog" if self.isanalog else "digital"
-        return {"name": self.name,
-                "isanalog": analog,
-                "input_size": str(self.input_size) if self.input_size is not None else "-",
-                "output_size": str(self.output_size) if self.output_size is not None else "-",
-                "kernel_size": str(self.kernel_size) if self.kernel_size is not None else "-",
-                "num_tiles": self.num_tiles,
-                "reuse_factor": str(self.reuse_factor) if self.reuse_factor is not None else "-",
-                "log_shape": "-",
-                "phy_shape": "-",
-                "utilization": "-"}
+        return {
+            "name": self.name,
+            "isanalog": analog,
+            "input_size": str(self.input_size) if self.input_size is not None else "-",
+            "output_size": str(self.output_size) if self.output_size is not None else "-",
+            "kernel_size": str(self.kernel_size) if self.kernel_size is not None else "-",
+            "num_tiles": self.num_tiles,
+            "reuse_factor": str(self.reuse_factor) if self.reuse_factor is not None else "-",
+            "log_shape": "-",
+            "phy_shape": "-",
+            "utilization": "-",
+        }
 
     def __repr__(self) -> str:
         """Print layer's information in the summary table."""
         stats = self.layer_summary_dict().values()
-        result = (("{:<20}"*len(stats)).format(*stats))
+        result = ("{:<20}" * len(stats)).format(*stats)
         result += "\n"
         for tile in self.tiles_info:
             tile_info = tile.tile_summary_dict()
             tile_info["utilization"] = FLOAT_FORMAT.format(tile_info["utilization"])
-            result += (" "*20*(len(stats)-3))
-            result += ("{:<20}{:<20}{:<20}\n".format(*(tile_info.values())))
+            result += " " * 20 * (len(stats) - 3)
+            result += "{:<20}{:<20}{:<20}\n".format(*(tile_info.values()))
         return result
 
 
 class AnalogInfo:
     """Class for computing and storing results of the analog summary."""
 
-    def __init__(self,
-                 model: Module,
-                 input_size: Any = None,
-                 rpu_config: Optional[RPUConfigAlias] = None):
-
+    def __init__(
+        self, model: Module, input_size: Any = None, rpu_config: Optional[RPUConfigBase] = None
+    ):
         self.model = model
         self.input_size = input_size
         self.rpu_config = rpu_config
         self.layer_summary = self.create_layer_summary()
         self.total_tile_number = self.calculate_num_tiles()
         self.total_nb_analog = self.calculate_num_analog()
 
     def register_hooks_recursively(self, module: Module, hook: Any) -> None:
-        """Hooks the function into all layers with no children."""
-        if not list(module.children()):
+        """Hooks the function into all layers with no children (or
+        only analog tiles as childrens).
+        """
+
+        if len(list(module.children())) == 0:
             module.register_forward_hook(hook)
+        elif (
+            isinstance(module, AnalogLayerBase)
+            and not module.IS_CONTAINER
+            and len(
+                [ch for ch in module.children() if isinstance(ch, AnalogLayerBase)]  # type: ignore
+            )
+            == 0
+        ):  # type: ignore
+            module.register_forward_hook(hook)  # type: ignore
         else:
-            for _, layer in module.named_children():
+            for layer in module.children():
                 self.register_hooks_recursively(layer, hook)
 
     def create_layer_summary(self) -> List[LayerInfo]:
         """Create the layer summary list.
 
         This list contains LayerInfo elements that corresponds to each
         layer of the model.
-
         """
         layer_summary = []
 
         def get_size_hook(_: Module, _input: Any, _output: Any) -> None:
             nonlocal layer_summary
             input_size = list(_input[0].size())
             output_size = list(_output.size())
@@ -249,38 +266,38 @@
         result = divider + name + divider
         result += "Per-layer Information\n" + divider
 
         # Add header
         header = [*COLUMN_NAMES.values()]
         for i, category in enumerate(COLUMN_DEFINITIONS):
             header_i = [v for x, v in header if x == i]
-            trim_length = (COLUMN_WIDTH * len(header_i) - len(category))
+            trim_length = COLUMN_WIDTH * len(header_i) - len(category)
             result += category + " " * trim_length
-            if i == len(COLUMN_DEFINITIONS)-1:
+            if i == len(COLUMN_DEFINITIONS) - 1:
                 break
-            result += '| '
-        result += "\n"+divider
+            result += "| "
+        result += "\n" + divider
         for i, category in enumerate(COLUMN_DEFINITIONS):
             header_i = [v for x, v in header if x == i]
-            result += (("{:<20}"*len(header_i)).format(*header_i))
+            result += ("{:<20}" * len(header_i)).format(*header_i)
         result += "\n"
 
         for x in self.layer_summary:
             result += repr(x)
 
         result += divider
         result += "General Information\n" + divider
         result += "Total number of tiles: " + str(self.total_tile_number) + "\n"
         result += "Total number of analog layers: " + str(self.total_nb_analog) + "\n"
         return result
 
 
-def analog_summary(model: Module,
-                   input_size: Optional[Any] = None,
-                   rpu_config: Optional[RPUConfigAlias] = None) -> AnalogInfo:
+def analog_summary(
+    model: Module, input_size: Optional[Any] = None, rpu_config: Optional[RPUConfigBase] = None
+) -> AnalogInfo:
     """Summarize the given PyTorch model.
 
     Summarized information includes:
 
         1) Layer names,
         2) input/output shapes,
         3) kernel shape,
@@ -296,9 +313,8 @@
 
         rpu_config: resistive processing unit configuration.
 
     Returns:
         AnalogInfo Object.
     """
     results = AnalogInfo(model, input_size, rpu_config)
-    print(results)
     return results
```

### Comparing `aihwkit-0.7.1/src/aihwkit/utils/visualization.py` & `aihwkit-0.8.0/src/aihwkit/utils/visualization.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -26,39 +26,44 @@
 from copy import deepcopy
 from typing import Any, Tuple, Union, Optional
 
 import matplotlib.pyplot as plt
 import numpy as np
 
 from matplotlib import ticker
-from matplotlib.figure import Figure
+from matplotlib.figure import Figure, Axes
 from numpy import ndarray
 from torch import device as torch_device
 from torch import eye, from_numpy, ones, stack
+from torch.autograd import no_grad
 
 from aihwkit.exceptions import ConfigError
-from aihwkit.simulator.configs import (
-    SingleRPUConfig, UnitCellRPUConfig, InferenceRPUConfig
-)
+from aihwkit.simulator.parameters.base import RPUConfigBase
+from aihwkit.simulator.configs import SingleRPUConfig, UnitCellRPUConfig, InferenceRPUConfig
 from aihwkit.simulator.configs.devices import PulsedDevice
 from aihwkit.simulator.configs.compounds import UnitCell
-from aihwkit.simulator.configs.utils import (
-    BoundManagementType, IOParameters, NoiseManagementType, PulseType,
-    UpdateParameters, WeightNoiseType
+from aihwkit.simulator.parameters.utils import (
+    BoundManagementType,
+    IOParameters,
+    NoiseManagementType,
+    PulseType,
+    UpdateParameters,
+    WeightNoiseType,
 )
-from aihwkit.simulator.tiles import AnalogTile, BaseTile, InferenceTile
+from aihwkit.simulator.presets import PresetIOParameters
+from aihwkit.simulator.tiles import AnalogTile, InferenceTile
+from aihwkit.simulator.tiles.module import TileModule
 from aihwkit.simulator.rpu_base import cuda
 from aihwkit.inference.noise.base import BaseNoiseModel
 from aihwkit.inference.noise.pcm import PCMLikeNoiseModel
 
 
+@no_grad()
 def compute_pulse_response(
-        analog_tile: BaseTile,
-        direction: ndarray,
-        use_forward: bool = False
+    analog_tile: TileModule, direction: ndarray, use_forward: bool = False
 ) -> ndarray:
     """Compute the pulse response of a given device configuration.
 
     Args:
         analog_tile: Base tile used for computing the weight traces
         direction: numpy vector of directions to sequentially apply (-1 or 1)
         use_forward: Whether to use the (noisy) forward pass to read out the weights
@@ -67,43 +72,41 @@
     Returns:
         An numpy array ``w_trace`` of dimensions ``len(direction) x out_size x in_size``
     """
     out_size = analog_tile.out_size
     in_size = analog_tile.in_size
 
     if analog_tile.is_cuda:
-        device = torch_device('cuda')
+        device = torch_device("cuda")
     else:
-        device = torch_device('cpu')
+        device = torch_device("cpu")
 
     total_iters = len(direction)
     w_trace = np.zeros((total_iters, out_size, in_size))
     in_vector = -ones(1, in_size, device=device)
     out_vector = ones(1, out_size, device=device)
     in_eye = eye(in_size, device=device)
     dir_tensor = from_numpy(direction).float().to(device)
 
     for i in range(total_iters):
         # Update the pulses.
         analog_tile.update(in_vector, out_vector * dir_tensor[i])
 
         if use_forward:
             # Save weights by using the forward pass (to get the short-term read noise).
-            w_trace[i, :, :] = analog_tile.forward(in_eye).detach().cpu().numpy().T
+            w_trace[i, :, :] = analog_tile(in_eye).detach().cpu().numpy().T
         else:
             # Noise free.
             w_trace[i, :, :] = analog_tile.get_weights()[0].detach().cpu().numpy()
 
     return w_trace
 
 
 def plot_pulse_response(
-        analog_tile: BaseTile,
-        direction: ndarray,
-        use_forward: bool = False
+    analog_tile: TileModule, direction: ndarray, use_forward: bool = False
 ) -> ndarray:
     """Plot the pulse response of a direction vector for each weight of
     the analog tile.
 
     Args:
         analog_tile: Base tile used for computing the weight traces
         direction: vector of directions to sequentially apply (-1 or 1)
@@ -115,28 +118,29 @@
     """
     w_trace = compute_pulse_response(analog_tile, direction, use_forward)
 
     plt.plot(w_trace.reshape(w_trace.shape[0], -1))
     if use_forward:
         plt.title(analog_tile.rpu_config.device.__class__.__name__)
     else:
-        plt.title('{} (without cycle/read noise)'
-                  .format(analog_tile.rpu_config.device.__class__.__name__))
-    plt.ylabel('Weight [conductance]')
-    plt.xlabel('Pulse number \\#')
+        plt.title(
+            "{} (without cycle/read noise)".format(analog_tile.rpu_config.device.__class__.__name__)
+        )
+    plt.ylabel("Weight [conductance]")
+    plt.xlabel("Pulse number \\#")
 
     return w_trace
 
 
 def compute_pulse_statistics(
-        w_nodes: ndarray,
-        w_trace: ndarray,
-        direction: ndarray,
-        up_direction: bool,
-        smoothness: float = 0.5
+    w_nodes: ndarray,
+    w_trace: ndarray,
+    direction: ndarray,
+    up_direction: bool,
+    smoothness: float = 0.5,
 ) -> Tuple[ndarray, ndarray]:
     """Compute the statistics of the step trace from :func:`compute_pulse_response`.
 
     Args:
         w_nodes: weight range vector to estimate the step histogram
         w_trace: weight trace from :func:`compute_pulse_response`
         direction: direction vector used to generate the weight traces
@@ -146,32 +150,30 @@
 
     Returns:
         Tuple of ``(dw_mean, dw_std)``.
     """
     # pylint: disable=too-many-locals
 
     def calc_mean_and_std(
-            node: ndarray,
-            w_values: ndarray,
-            delta_w: ndarray,
-            lam: float
+        node: ndarray, w_values: ndarray, delta_w: ndarray, lam: float
     ) -> Tuple[ndarray, ndarray]:
         """Calculate the mean and std of a w location (node).
 
         Note:
             In case there are multiple trials then it also includes
             device-to-device variation.
         """
-        alpha = np.exp(-0.5*(node - w_values)**2/lam**2)
+        alpha = np.exp(-0.5 * (node - w_values) ** 2 / lam**2)
         beta = alpha.sum(axis=0)
         alpha[:, beta < 0.1] = np.nan
         alpha /= np.expand_dims(beta, axis=0)  # type: ignore
-        mean = np.sum(alpha*delta_w, axis=0)
-        std = np.sqrt(np.sum(alpha*(delta_w - np.expand_dims(mean, axis=0))**2,  # type: ignore
-                             axis=0))
+        mean = np.sum(alpha * delta_w, axis=0)
+        std = np.sqrt(
+            np.sum(alpha * (delta_w - np.expand_dims(mean, axis=0)) ** 2, axis=0)  # type: ignore
+        )
         return (mean, std)
 
     # dw statistics.
     delta_w = np.diff(w_trace, axis=0)  # type: ignore
     w_trace_s = w_trace[:-1, :, :]
     if up_direction:
         msk = np.logical_and(direction[:-1] > 0, np.diff(direction) == 0)  # type: ignore
@@ -181,27 +183,27 @@
         msk = np.logical_and(direction[:-1] < 0, np.diff(direction) == 0)  # type: ignore
         w_values = w_trace_s[msk, :, :]
         delta_w_values = delta_w[msk, :, :]
 
     dw_mean = np.zeros((len(w_nodes), w_trace.shape[1], w_trace.shape[2]))
     dw_std = np.zeros((len(w_nodes), w_trace.shape[1], w_trace.shape[2]))
 
-    lam = (w_nodes[1]-w_nodes[0])/2*smoothness
+    lam = (w_nodes[1] - w_nodes[0]) / 2 * smoothness
     for i, node in enumerate(w_nodes):
         dw_mean[i, :, :], dw_std[i, :, :] = calc_mean_and_std(node, w_values, delta_w_values, lam)
 
     return dw_mean, dw_std
 
 
 def plot_pulse_statistics(
-        w_trace: ndarray,
-        direction: ndarray,
-        up_direction: bool,
-        num_nodes: int = 100,
-        smoothness: float = 0.5
+    w_trace: ndarray,
+    direction: ndarray,
+    up_direction: bool,
+    num_nodes: int = 100,
+    smoothness: float = 0.5,
 ) -> Tuple[ndarray, ndarray, ndarray]:
     """Plot the dG-G curve from a given weight trace and direction vector.
 
     Args:
         w_trace: weight trace from :func:`compute_pulse_response`
         direction: direction vector used to generate ``w_trace``
         up_direction: whether and plot to compute the statistics for up or down direction
@@ -212,100 +214,104 @@
     Returns:
         A tuple (w_nodes, dw_mean, dw_std) from :func:`compute_pulse_statistics`
     """
 
     def errorbar_patch(x: ndarray, mean: ndarray, std: ndarray) -> None:
         """Plot a patchy error bar."""
         axis = plt.plot(x, mean)[0]
-        plt.fill_between(x, mean - std, mean + std, edgecolor=None,
-                         facecolor=axis.get_color(), alpha=0.5)
+        plt.fill_between(
+            x, mean - std, mean + std, edgecolor=None, facecolor=axis.get_color(), alpha=0.5
+        )
 
     # Compute statistics.
     w_nodes = np.linspace(w_trace.min(), w_trace.max(), num_nodes)
-    dw_mean, dw_std = compute_pulse_statistics(w_nodes, w_trace, direction, up_direction,
-                                               smoothness)
+    dw_mean, dw_std = compute_pulse_statistics(
+        w_nodes, w_trace, direction, up_direction, smoothness
+    )
 
     n_traces = dw_mean.shape[1] * dw_mean.shape[2]
 
     for i in range(n_traces):
-        errorbar_patch(w_nodes,
-                       dw_mean.reshape(-1, n_traces)[:, i],
-                       dw_std.reshape(-1, n_traces)[:, i])
+        errorbar_patch(
+            w_nodes, dw_mean.reshape(-1, n_traces)[:, i], dw_std.reshape(-1, n_traces)[:, i]
+        )
 
-    plt.xlabel('Weight $w$')
-    plt.ylabel('Avg. step $\\Delta w$')
+    plt.xlabel("Weight $w$")
+    plt.ylabel("Avg. step $\\Delta w$")
     if up_direction:
-        plt.title('up-direction')
+        plt.title("up-direction")
     else:
-        plt.title('down-direction')
+        plt.title("down-direction")
 
     return w_nodes, dw_mean, dw_std
 
 
 def get_tile_for_plotting(
-        rpu_config: Union[SingleRPUConfig, UnitCellRPUConfig],
-        n_traces: int,
-        use_cuda: bool = False,
-        noise_free: bool = False
-) -> BaseTile:
+    rpu_config: Union[SingleRPUConfig, UnitCellRPUConfig],
+    n_traces: int,
+    use_cuda: bool = False,
+    noise_free: bool = False,
+    w_init: Optional[float] = None,
+) -> TileModule:
     """Return an analog tile for plotting the response curve.
 
     Args:
         rpu_config: RPU Configuration to use for plotting
         n_traces: Number of traces to plot
         use_cuda: Whether to use the CUDA implementation (if available)
         noise_free: Whether to turn-off cycle-to-cycle noises (if possible)
+        w_init: init value if given otherwise  ``w_min`` is taken
 
     Returns:
         Instantiated tile.
     """
 
     def set_noise_free(dev: Any) -> Any:
-        if hasattr(dev, 'dw_min_std'):
+        if hasattr(dev, "dw_min_std"):
             dev.dw_min_std = 0.0  # Noise free.
 
-        if hasattr(dev, 'refresh_forward'):
-            setattr(dev, 'refresh_forward', IOParameters(is_perfect=True))
+        if hasattr(dev, "refresh_forward"):
+            setattr(dev, "refresh_forward", IOParameters(is_perfect=True))
 
-        if hasattr(dev, 'refresh_update'):
-            setattr(dev, 'refresh_update', UpdateParameters(pulse_type=PulseType.NONE))
+        if hasattr(dev, "refresh_update"):
+            setattr(dev, "refresh_update", UpdateParameters(pulse_type=PulseType.NONE))
 
-        if hasattr(dev, 'transfer_forward'):
-            setattr(dev, 'refresh_forward', IOParameters(is_perfect=True))
+        if hasattr(dev, "transfer_forward"):
+            setattr(dev, "refresh_forward", IOParameters(is_perfect=True))
 
-        if hasattr(dev, 'transfer_update'):
-            setattr(dev, 'transfer_update', UpdateParameters(pulse_type=PulseType.NONE))
+        if hasattr(dev, "transfer_update"):
+            setattr(dev, "transfer_update", UpdateParameters(pulse_type=PulseType.NONE))
 
-        if (hasattr(dev, 'write_noise_std') and
-           getattr(dev, 'write_noise_std') > 0.0):
+        if hasattr(dev, "write_noise_std") and getattr(dev, "write_noise_std") > 0.0:
             # Just make very small to avoid hidden parameter mismatch.
-            setattr(dev, 'write_noise_std', 1e-6)
+            setattr(dev, "write_noise_std", 1e-6)
 
     config = deepcopy(rpu_config)
 
     # Make sure we use single pulses for the overview.
     config.update.update_bl_management = False
     config.update.update_management = False
     config.update.desired_bl = 1
 
     if noise_free:
         config.forward.is_perfect = True
 
         set_noise_free(config.device)
-        if hasattr(config.device, 'unit_cell_devices'):
-            for dev in getattr(config.device, 'unit_cell_devices'):
+        if hasattr(config.device, "unit_cell_devices"):
+            for dev in getattr(config.device, "unit_cell_devices"):
                 set_noise_free(dev)
-        if hasattr(config.device, 'device'):
-            set_noise_free(getattr(config.device, 'device'))
+        if hasattr(config.device, "device"):
+            set_noise_free(getattr(config.device, "device"))
 
-    analog_tile = AnalogTile(n_traces, 1, config)  # type: BaseTile
+    analog_tile = AnalogTile(n_traces, 1, config)  # type: TileModule
     analog_tile.set_learning_rate(1)
-    w_min = getattr(config.device.as_bindings(), 'w_min', -1.0)
+    if w_init is None:
+        w_init = getattr(config.device.as_bindings(), "w_min", -1.0)
 
-    weights = w_min * ones((n_traces, 1))
+    weights = w_init * ones((n_traces, 1))
     analog_tile.set_weights(weights)
 
     if use_cuda and cuda.is_compiled():
         return analog_tile.cuda()
     return analog_tile
 
 
@@ -330,33 +336,34 @@
             :class:`~aihwkit.simulator.configs.devices.PulseDevice`)
     """
     if not isinstance(rpu_config, SingleRPUConfig):
         return 1000
 
     device_binding = rpu_config.device.as_bindings()
 
-    if not hasattr(device_binding, 'w_min'):
-        raise ConfigError('n_step estimation only for PulsedDevice. ' +
-                          'Provide n_step explicitly.')
+    if not hasattr(device_binding, "w_min"):
+        raise ConfigError(
+            "n_step estimation only for PulsedDevice. " + "Provide n_step explicitly."
+        )
 
     weight_granularity = device_binding.calc_weight_granularity()
     w_min = device_binding.w_min
     w_max = device_binding.w_max
 
     n_steps = int(np.round((w_max - w_min) / weight_granularity))  # type: ignore
     return n_steps
 
 
 def plot_response_overview(
-        rpu_config: Union[SingleRPUConfig, UnitCellRPUConfig],
-        n_loops: int = 5,
-        n_steps: Optional[int] = None,
-        n_traces: int = 5,
-        use_cuda: bool = False,
-        smoothness: float = 0.1
+    rpu_config: Union[SingleRPUConfig, UnitCellRPUConfig],
+    n_loops: int = 5,
+    n_steps: Optional[int] = None,
+    n_traces: int = 5,
+    use_cuda: bool = False,
+    smoothness: float = 0.1,
 ) -> None:
     """Plot the step response and statistics of a given device configuration.
 
     Args:
         rpu_config: RPU Configuration to use for plotting
         n_loops: How many hyper-cycles (up/down pulse sequences) to plot
         n_steps: Number of up/down steps per cycle. If not given, will
@@ -366,16 +373,16 @@
         use_cuda: Whether to use the CUDA implementation (if available)
         smoothness: value for smoothing the estimation of the
             statistical step response curves
     """
     if n_steps is None:
         n_steps = estimate_n_steps(rpu_config)
 
-    total_iters = min(max(n_loops*2*n_steps, 1000), max(50000, 2*n_steps))
-    direction = np.sign(np.sin(np.pi*(np.arange(total_iters)+1)/n_steps))
+    total_iters = min(max(n_loops * 2 * n_steps, 1000), max(50000, 2 * n_steps))
+    direction = np.sign(np.sin(np.pi * (np.arange(total_iters) + 1) / n_steps))
 
     plt.clf()
 
     # 1. Noisy tile.
     analog_tile = get_tile_for_plotting(rpu_config, n_traces, use_cuda, noise_free=False)
     plt.subplot(3, 1, 1)
     plot_pulse_response(analog_tile, direction, use_forward=True)
@@ -409,157 +416,190 @@
     Args:
         device: PulsedDevice parameters
         w_noise: Weight noise standard deviation during read
         kwargs: for other parameters, see :func:`plot_response_overview`
     """
     plt.figure(figsize=[7, 7])
     # To simulate some weight read noise.
-    io_pars = IOParameters(out_noise=0.0,    # no out noise
-                           w_noise=w_noise,  # quite low
-                           inp_res=-1.,      # turn off DAC
-                           out_bound=100.,   # not limiting
-                           out_res=-1.,      # turn off ADC
-                           bound_management=BoundManagementType.NONE,
-                           noise_management=NoiseManagementType.NONE,
-                           w_noise_type=WeightNoiseType.ADDITIVE_CONSTANT)
+    io_pars = IOParameters(
+        out_noise=0.0,  # no out noise
+        w_noise=w_noise,  # quite low
+        inp_res=-1.0,  # turn off DAC
+        out_bound=100.0,  # not limiting
+        out_res=-1.0,  # turn off ADC
+        bound_management=BoundManagementType.NONE,
+        noise_management=NoiseManagementType.NONE,
+        w_noise_type=WeightNoiseType.ADDITIVE_CONSTANT,
+    )
 
     if isinstance(device, PulsedDevice):
         plot_response_overview(SingleRPUConfig(device=device, forward=io_pars), **kwargs)
     else:
         plot_response_overview(UnitCellRPUConfig(device=device, forward=io_pars), **kwargs)
 
 
 def plot_device_compact(
-        device: Union[PulsedDevice, UnitCell],
-        w_noise: float = 0.0,
-        n_steps: Optional[int] = None,
-        n_traces: int = 3,
-        use_cuda: bool = False,
-) -> Figure:
+    device: Union[PulsedDevice, UnitCell],
+    w_noise: float = 0.0,
+    n_steps: Optional[int] = None,
+    n_traces: int = 3,
+    n_loops: int = 2,
+    w_init: Optional[float] = None,
+    use_cuda: bool = False,
+    axes: Optional[Axes] = None,
+    **plot_kwargs: Any,
+) -> Union[Figure, Axes]:
     """Plot a compact step response figure for a given device (preset).
 
     Note:
         It will use an amount of read weight noise ``w_noise`` for
         reading the weights.
 
     Args:
         device: ``PulsedDevice`` or ``UnitCell`` parameters
         w_noise: Weight noise standard deviation during read
         n_steps: Number of up/down steps per cycle. If not given, will
             be tried to be estimated (only for ``PulsedDevice``
             possible otherwise defaults to 1000 if ``n_steps=None``).
         n_traces: Number of traces to plot (for device-to-device variation)
+        n_loops: Number of cycles
+        w_init: init weight values if given, otherwise ``w_min`` is taken
         use_cuda: Whether to use CUDA for the computation
+        axes: axis to plot on. If given the statistics are not
+            computed and only one axes is returned
+        plot_kwargs: additional
+            plotting keywords for the main plot
 
     Returns:
-        the compact step response figure.
+        the compact step response figure or the axes if given as argument.
+
     """
+
     # pylint: disable=too-many-locals,too-many-statements
-    def get_rpu_config(device: Union[PulsedDevice, UnitCell], io_pars: IOParameters) \
-            -> Union[SingleRPUConfig, UnitCellRPUConfig]:
+    def get_rpu_config(
+        device: Union[PulsedDevice, UnitCell], io_pars: IOParameters
+    ) -> Union[SingleRPUConfig, UnitCellRPUConfig]:
         if isinstance(device, PulsedDevice):
             return SingleRPUConfig(device=device, forward=io_pars)
         return UnitCellRPUConfig(device=device, forward=io_pars)
 
-    figure = plt.figure(figsize=[12, 4])
+    if axes is None:
+        figure = plt.figure(figsize=[12, 4])
 
     # To simulate some weight read noise.
-    io_pars = IOParameters(out_noise=0.0,    # no out noise
-                           w_noise=w_noise,  # quite low
-                           inp_res=-1.,      # turn off DAC
-                           out_bound=100.,   # not limiting
-                           out_res=-1.,      # turn off ADC
-                           bound_management=BoundManagementType.NONE,
-                           noise_management=NoiseManagementType.NONE,
-                           w_noise_type=WeightNoiseType.ADDITIVE_CONSTANT)
+    io_pars = IOParameters(
+        out_noise=0.0,  # no out noise
+        w_noise=w_noise,  # quite low
+        inp_res=-1.0,  # turn off DAC
+        out_bound=100.0,  # not limiting
+        out_res=-1.0,  # turn off ADC
+        bound_management=BoundManagementType.NONE,
+        noise_management=NoiseManagementType.NONE,
+        w_noise_type=WeightNoiseType.ADDITIVE_CONSTANT,
+    )
 
     rpu_config = get_rpu_config(device, io_pars)
 
     if n_steps is None:
         n_steps = estimate_n_steps(rpu_config)
 
     # Noisy tile response curves.
-    n_loops = 2
-    total_iters = n_loops*2*n_steps
-    direction = np.sign(np.sin(np.pi*(np.arange(total_iters)+1)/n_steps))
+    total_iters = n_loops * 2 * n_steps
+    direction = np.sign(np.sin(np.pi * (np.arange(total_iters) + 1) / n_steps))
 
-    analog_tile = get_tile_for_plotting(rpu_config, n_traces, use_cuda, noise_free=False)
-    w_trace = compute_pulse_response(analog_tile, direction, use_forward=True)\
-        .reshape(-1, n_traces)
-    axis = figure.add_subplot(1, 1, 1)
-    axis.plot(w_trace, linewidth=1)
-    axis.set_title(analog_tile.rpu_config.device.__class__.__name__)
-    axis.set_xlabel('Pulse number \\#')
-    limit = np.abs(w_trace).max()*1.2
-    axis.set_ylim(-limit, limit)
-    axis.set_xlim(0, total_iters-1)
-    axis.xaxis.set_major_formatter(ticker.ScalarFormatter(useMathText=True))
+    analog_tile = get_tile_for_plotting(
+        rpu_config, n_traces, use_cuda, noise_free=False, w_init=w_init
+    )
+    w_trace = compute_pulse_response(analog_tile, direction, use_forward=True).reshape(-1, n_traces)
+
+    do_statistics = False
+    if axes is None:
+        axes = figure.add_subplot(1, 1, 1)
+        do_statistics = True
+    if "linewidth" not in plot_kwargs:
+        plot_kwargs["linewidth"] = 1
+    axes.plot(w_trace, **plot_kwargs)
+    axes.set_title(analog_tile.rpu_config.device.__class__.__name__)
+    axes.set_xlabel("Pulse number \\#")
+    limit = np.abs(w_trace).max() * 1.2
+    axes.set_ylim(-limit, limit)
+    axes.set_xlim(0, total_iters - 1)
+    axes.xaxis.set_major_formatter(ticker.ScalarFormatter(useMathText=True))
 
-    # Noise-free tile for statistics.
-    n_loops = 1
-    total_iters = min(max(n_loops*2*n_steps, 1000), max(50000, 2*n_steps))
-    direction = np.sign(np.sin(np.pi*(np.arange(total_iters)+1)/n_steps))
+    if not do_statistics:
+        return axes
 
-    analog_tile_noise_free = get_tile_for_plotting(rpu_config, n_traces, use_cuda, noise_free=True)
+    # Noise-free tile for statistics.
+    n_loops_noise_free = 1
+    total_iters = min(max(n_loops_noise_free * 2 * n_steps, 1000), max(50000, 2 * n_steps))
+    direction = np.sign(np.sin(np.pi * (np.arange(total_iters) + 1) / n_steps))
+
+    analog_tile_noise_free = get_tile_for_plotting(
+        rpu_config, n_traces, use_cuda, noise_free=True, w_init=w_init
+    )
     analog_tile_noise_free.set_hidden_parameters(analog_tile.get_hidden_parameters())
 
     w_trace = compute_pulse_response(analog_tile_noise_free, direction, False)
 
     # Compute statistics.
     num_nodes = min(n_steps, 100)
     w_nodes = np.linspace(w_trace.min(), w_trace.max(), num_nodes)
 
-    dw_mean_up = compute_pulse_statistics(w_nodes, w_trace, direction, True)[0]\
-        .reshape(-1, n_traces)
-    dw_mean_down = compute_pulse_statistics(w_nodes, w_trace, direction, False)[0]\
-        .reshape(-1, n_traces)
+    dw_mean_up = compute_pulse_statistics(w_nodes, w_trace, direction, True)[0].reshape(
+        -1, n_traces
+    )
+    dw_mean_down = compute_pulse_statistics(w_nodes, w_trace, direction, False)[0].reshape(
+        -1, n_traces
+    )
 
     # Plot mean up statistics.
-    pos = axis.get_position().bounds
+    pos = axes.get_position().bounds
     space = 0.1
     gap = 0.01
-    axis.set_position([pos[0] + gap + space, pos[1], pos[2] - 2*gap - 2*space, pos[3]])
-    axis.set_yticks([])
+    axes.set_position([pos[0] + gap + space, pos[1], pos[2] - 2 * gap - 2 * space, pos[3]])
+    axes.set_yticks([])
 
     axis_left = figure.add_axes([pos[0], pos[1], space, pos[3]])
     dw_mean_up = dw_mean_up.reshape(-1, n_traces)
     for i in range(n_traces):
         axis_left.plot(dw_mean_up[:, i], w_nodes)
 
     axis_left.set_position([pos[0], pos[1], space, pos[3]])
-    axis_left.set_xlabel('Up pulse size')
-    axis_left.set_ylabel('Weight \n [conductance]')
+    axis_left.set_xlabel("Up pulse size")
+    axis_left.set_ylabel("Weight \n [conductance]")
     axis_left.set_ylim(-limit, limit)
 
     # Plot mean down statistics.
     axis_right = figure.add_axes([pos[0] + pos[2] - space, pos[1], space, pos[3]])
     dw_mean_down = dw_mean_down.reshape(-1, n_traces)
     for i in range(n_traces):
         axis_right.plot(np.abs(dw_mean_down[:, i]), w_nodes)
 
     axis_right.set_yticks([])
-    axis_right.set_xlabel('Down pulse size')
+    axis_right.set_xlabel("Down pulse size")
     axis_right.set_ylim(-limit, limit)
 
     # Set xlim's.
-    limit = np.maximum(np.nanmax(np.abs(dw_mean_down)),  # type: ignore
-                       np.nanmax(np.abs(dw_mean_up))) * 1.2  # type: ignore
+    limit = (
+        np.maximum(np.nanmax(np.abs(dw_mean_down)), np.nanmax(np.abs(dw_mean_up)))  # type: ignore
+        * 1.2
+    )  # type: ignore
     axis_left.set_xlim(0.0, limit)
     axis_right.set_xlim(0.0, limit)
 
     return figure
 
 
 def plot_device_symmetry(
-        device: PulsedDevice,
-        w_noise: float = 0.0,
-        n_pulses: int = 10000,
-        n_traces: int = 3,
-        use_cuda: bool = False,
-        w_init: float = 1.0,
+    device: PulsedDevice,
+    w_noise: float = 0.0,
+    n_pulses: int = 10000,
+    n_traces: int = 3,
+    use_cuda: bool = False,
+    w_init: float = 1.0,
 ) -> None:
     """Plot the response figure for a given device (preset).
 
     It will show the response to alternating up down pulses.
 
     Note:
         It will use an amount of read weight noise ``w_noise`` for
@@ -571,41 +611,45 @@
         w_noise: Weight noise standard deviation during read
         n_traces: Number of device traces
         use_cuda: Whether to use CUDA,
         w_init: Initial value of the weights
     """
     plt.figure(figsize=[10, 5])
 
-    io_pars = IOParameters(out_noise=0.0,    # no out noise
-                           w_noise=w_noise,  # quite low
-                           inp_res=-1.,      # turn off DAC
-                           out_bound=100.,   # not limiting
-                           out_res=-1.,      # turn off ADC
-                           bound_management=BoundManagementType.NONE,
-                           noise_management=NoiseManagementType.NONE,
-                           w_noise_type=WeightNoiseType.ADDITIVE_CONSTANT)
+    io_pars = IOParameters(
+        out_noise=0.0,  # no out noise
+        w_noise=w_noise,  # quite low
+        inp_res=-1.0,  # turn off DAC
+        out_bound=100.0,  # not limiting
+        out_res=-1.0,  # turn off ADC
+        bound_management=BoundManagementType.NONE,
+        noise_management=NoiseManagementType.NONE,
+        w_noise_type=WeightNoiseType.ADDITIVE_CONSTANT,
+    )
 
     rpu_config = SingleRPUConfig(device=device, forward=io_pars)
 
-    direction = np.sign(np.cos(np.pi*np.arange(n_pulses)))
+    direction = np.sign(np.cos(np.pi * np.arange(n_pulses)))
     plt.clf()
 
-    analog_tile = get_tile_for_plotting(rpu_config, n_traces, use_cuda, noise_free=False)
-    weights = w_init*ones((n_traces, 1))
-    analog_tile.set_weights(weights)
+    analog_tile = get_tile_for_plotting(
+        rpu_config, n_traces, use_cuda, noise_free=False, w_init=w_init
+    )
 
     plot_pulse_response(analog_tile, direction, use_forward=False)
     plt.ylim([-1, 1])
     plt.grid(True)
 
 
-def plot_weight_drift(noise_model: Optional[BaseNoiseModel] = None,
-                      t_inference_list: Optional[ndarray] = None,
-                      w_inits: Optional[ndarray] = None,
-                      n_repeats: int = 25) -> None:
+def plot_weight_drift(
+    noise_model: Optional[BaseNoiseModel] = None,
+    t_inference_list: Optional[ndarray] = None,
+    w_inits: Optional[ndarray] = None,
+    n_repeats: int = 25,
+) -> None:
     """Plots the weight drift behavior of a given noise model over time.
 
     Args:
         noise_model: Noise model of derived from
             :class:`~aihwkit.simulator.noise_models.BaseNoiseModel`
         t_inference_list: Numpy array of times of inference after
             programming at time 0 (in seconds)
@@ -614,17 +658,17 @@
     """
     # pylint: disable=too-many-locals
 
     plt.figure(figsize=[10, 5])
     if noise_model is None:
         noise_model = PCMLikeNoiseModel()
     if t_inference_list is None:
-        t_inference_list = np.logspace(0., 7.0, 15)
+        t_inference_list = np.logspace(0.0, 7.0, 15)
     if w_inits is None:
-        w_inits = np.linspace(-1., 1., 9)
+        w_inits = np.linspace(-1.0, 1.0, 9)
 
     rpu_config = InferenceRPUConfig(noise_model=noise_model, drift_compensation=None)
 
     weights = w_inits.flatten()
     weights.sort()
     weights = np.tile(weights, [n_repeats, 1])  # type: ignore
 
@@ -645,16 +689,134 @@
 
     m_array = stack(m_list, axis=1)
     s_array = stack(s_list, axis=1)
 
     for i in range(w_inits.size):
         curve = plt.plot(t_inference_list, m_array[i])
 
-        plt.fill_between(t_inference_list,
-                         m_array[i] - s_array[i],
-                         m_array[i] + s_array[i],
-                         edgecolor=None, linewidth=0, alpha=0.3, antialiased=True,
-                         facecolor=curve[0].get_color())
-
-    plt.gca().set_xscale('log')
-    plt.xlabel('Time after programming [sec]')
-    plt.ylabel('Weight value [norm. units]')
+        plt.fill_between(
+            t_inference_list,
+            m_array[i] - s_array[i],
+            m_array[i] + s_array[i],
+            edgecolor=None,
+            linewidth=0,
+            alpha=0.3,
+            antialiased=True,
+            facecolor=curve[0].get_color(),
+        )
+
+    plt.gca().set_xscale("log")
+    plt.xlabel("Time after programming [sec]")
+    plt.ylabel("Weight value [norm. units]")
+
+
+def plot_programming_error(
+    config: Union[PulsedDevice, UnitCell, RPUConfigBase],
+    w_range: Tuple[float, float] = (-1.0, 1.0),
+    w_init: Optional[Union[ndarray]] = None,
+    n_rows: int = 100,
+    realistic_read: bool = False,
+    n_bins: int = 101,
+    use_cuda: bool = False,
+    label: Optional[str] = None,
+    axes: Optional[Axes] = None,
+    t_inference: Optional[int] = None,
+    **kwargs: Any,
+) -> Axes:
+    """Plot weight programming error with weight value.
+
+    Programming of weights will be done by calling the
+    ``program_weights`` method and read out can also optionally be
+    done with ``read_weights``. This means that for analog training
+    device configurations, programming will be done using SGD on
+    random vectors and stochastic pulsing or as defined in the
+    ``RPUConfig`` if given explicitely. Read is done perfectly unless
+    ``realistic_read`` is set.
+
+    It plots mean with plus/minus standard error of the mean as
+    confidence interval.
+
+
+    Note:
+
+        If RPUConfig is not directly given, it will use the
+        ``PresetIOParameters`` configuration by default.
+
+    Args:
+        config: ``PulsedDevice``, ``UnitCell`` parameters or directly a ``RPUConfig``
+        w_range: Range of weights.
+        w_init: Weight matrix to program (default n_rows x n_rows uniform in ``w_range``)
+        n_rows: Number of rows (and columns) of the decault weight matrix
+        realistic_read: Whether to use ``read_weights`` estimation instead of a perfect read.
+        n_bins: Number of bins for plotting
+        use_cuda: Whether to use CUDA for the computation
+        label: Plotting curve label
+        axes: Plot axes (default current axis)
+        t_inference: Setting of the inference time (only in case of ``InferenceRPUConfig``)
+        kwargs: Additional arguments for ``program_weights`` call.
+
+    Returns:
+        the axis used for the plot.
+
+    """
+
+    # pylint: disable=too-many-locals,too-many-statements
+    def get_rpu_config(config: Union[PulsedDevice, UnitCell, RPUConfigBase]) -> RPUConfigBase:
+        if isinstance(config, RPUConfigBase):
+            return config
+        if isinstance(config, PulsedDevice):
+            return SingleRPUConfig(device=config, forward=PresetIOParameters())
+        return UnitCellRPUConfig(device=config, forward=PresetIOParameters())
+
+    if axes is None:
+        axes = plt.gca()
+
+    rpu_config = get_rpu_config(config)
+
+    if w_init is None:
+        w_init = np.random.uniform(*w_range, size=(n_rows, n_rows)).astype("float32")
+
+    analog_tile = rpu_config.tile_class(w_init.shape[1], w_init.shape[0], rpu_config, bias=False)
+    if use_cuda:
+        analog_tile.cuda()
+
+    # this is an estimation using the forward pass.
+    if not isinstance(rpu_config, InferenceRPUConfig) or t_inference is None:
+        analog_tile.set_weights(w_init, apply_weight_scaling=False, realistic=False)
+        analog_tile.program_weights(**kwargs)
+        programmed_weights, _ = analog_tile.get_weights(
+            apply_weight_scaling=False, realistic=realistic_read
+        )
+    else:
+        analog_tile.set_weights(w_init, apply_weight_scaling=False, realistic=False)
+        analog_tile.drift_weights(t_inference)
+        programmed_weights, _ = analog_tile.get_weights(
+            apply_weight_scaling=False, realistic=realistic_read
+        )
+
+    x = np.abs(programmed_weights.numpy().flatten() - w_init.flatten())
+    bins = np.linspace(w_range[0], w_range[1], n_bins)
+    indices = np.digitize(w_init.flatten(), bins)
+    num_samples = np.bincount(indices)
+    msk = num_samples != 0.0
+
+    mean_error = np.bincount(indices, weights=x)
+    mean_error[msk] /= num_samples[msk]
+
+    std_error = np.bincount(indices, weights=x**2)
+    std_error[msk] /= num_samples[msk]
+    std_error = np.sqrt(std_error - mean_error**2)
+    std_error[msk] /= np.sqrt(num_samples[msk])
+
+    w_max = (w_range[1] - w_range[0]) / 2
+    scale = 100 / w_max
+    axes.errorbar(
+        bins[:-1] + (bins[1] - bins[0]) / 2,
+        mean_error[1:] * scale,
+        std_error[1:] * scale,
+        label=label,
+    )
+
+    axes.set_xlabel("Weight value")
+    axes.set_ylabel("Programming error [% of gmax]")
+
+    return axes
```

### Comparing `aihwkit-0.7.1/src/aihwkit/utils/visualization_web.py` & `aihwkit-0.8.0/src/aihwkit/utils/visualization_web.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -19,49 +19,62 @@
 from cycler import cycler
 import matplotlib.pyplot as plt
 
 from matplotlib.axes import Axes
 from matplotlib.figure import Figure
 
 from aihwkit.simulator.presets.devices import (
-    ReRamSBPresetDevice, ReRamESPresetDevice, CapacitorPresetDevice,
-    EcRamPresetDevice, IdealizedPresetDevice
+    ReRamSBPresetDevice,
+    ReRamESPresetDevice,
+    CapacitorPresetDevice,
+    EcRamPresetDevice,
+    IdealizedPresetDevice,
 )
 from aihwkit.simulator.presets.compounds import PCMPresetUnitCell
 from aihwkit.simulator.configs.devices import PulsedDevice
 from aihwkit.utils.visualization import plot_device_compact
 
 # Colors used by the frontend.
-WEB_COLORS = ['#8A3FFC', '#33B1FF', '#007D79', '#FF7EB6', '#FA4D56', '#FFF1F1',
-              '#6FDC8C', '#4589FF', '#D12771', '#D2A106', '#08BDBA', '#BAE6FF',
-              '#BA4E00', '#D4BBFF']
+WEB_COLORS = [
+    "#8A3FFC",
+    "#33B1FF",
+    "#007D79",
+    "#FF7EB6",
+    "#FA4D56",
+    "#FFF1F1",
+    "#6FDC8C",
+    "#4589FF",
+    "#D12771",
+    "#D2A106",
+    "#08BDBA",
+    "#BAE6FF",
+    "#BA4E00",
+    "#D4BBFF",
+]
 # Devices for which plots should be generated.
 DEVICES = {
     ReRamESPresetDevice: 1000,
     ReRamSBPresetDevice: 1000,
     CapacitorPresetDevice: 400,
     EcRamPresetDevice: 1000,
     IdealizedPresetDevice: 10000,
-    PCMPresetUnitCell: 80
+    PCMPresetUnitCell: 80,
 }
 
 
 def set_dark_style(axes: Axes) -> None:
     """Sets a nice color cycle for a given axes."""
 
     axes.set_prop_cycle(cycler(color=WEB_COLORS))
 
-    axes.set_facecolor('#262626')
+    axes.set_facecolor("#262626")
 
 
 def plot_device_compact_web(
-        device: PulsedDevice,
-        w_noise: float = 0.0,
-        n_steps: Optional[int] = None,
-        n_traces: int = 3
+    device: PulsedDevice, w_noise: float = 0.0, n_steps: Optional[int] = None, n_traces: int = 3
 ) -> Figure:
     """Plots a compact step response figure for a given device (preset).
 
     Note:
         It will use an amount of read weight noise ``w_noise`` for
         reading the weights.
 
@@ -71,78 +84,74 @@
         n_steps: Number of steps for up/down cycle
         n_traces: Number of traces to plot (for device-to-device variation)
         show: if `True`, displays the figure.
 
     Returns:
         the compact step response figure.
     """
-    plt.style.use('dark_background')
+    plt.style.use("dark_background")
 
     figure = plot_device_compact(device, w_noise, n_steps, n_traces)
 
     # Tune for web.
     for axes in figure.axes:
         for i, line in enumerate(axes.lines):
             line.set_color(WEB_COLORS[i])
         # set_dark_style(axes)
 
     return figure
 
 
-def save_plots_for_web(
-        path: Path = Path('/tmp'),
-        file_format: str = 'svg'
-) -> None:
+def save_plots_for_web(path: Path = Path("/tmp"), file_format: str = "svg") -> None:
     """Create the plots for the web.
 
     Args:
         path: the path where the images will be stored.
         file_format: the image format.
     """
+
     def camel_to_snake(source: str) -> str:
         """Convert a CamelCase string into snake-case."""
-        return ''.join(['_' + char.lower() if char.isupper() else char
-                        for char in source]).lstrip('_')
+        return "".join(["_" + char.lower() if char.isupper() else char for char in source]).lstrip(
+            "_"
+        )
 
     for device, n_steps in DEVICES.items():
         # Images for the detailed modal.
-        file_name = '{}.{}'.format(camel_to_snake(device.__name__), file_format)
+        file_name = "{}.{}".format(camel_to_snake(device.__name__), file_format)
         file_path = path.absolute() / file_name
 
         figure = plot_device_compact_web(device(), n_steps=n_steps)
-        figure.savefig(file_path, format=file_format, transparent=True,
-                       bbox_inches='tight')
+        figure.savefig(file_path, format=file_format, transparent=True, bbox_inches="tight")
 
         # Images for the mini leftbar.
-        file_name = '{}-mini.{}'.format(camel_to_snake(device.__name__), file_format)
+        file_name = "{}-mini.{}".format(camel_to_snake(device.__name__), file_format)
         file_path = path.absolute() / file_name
 
         figure = plot_device_compact_web(device(), n_traces=1, n_steps=n_steps)
         for axes in figure.axes:
             # Disable texts.
-            axes.set_title('')
-            axes.set_xlabel('')
-            axes.set_ylabel('')
+            axes.set_title("")
+            axes.set_xlabel("")
+            axes.set_ylabel("")
             # Disable tick labels.
             axes.xaxis.set_ticklabels([])
             axes.yaxis.set_ticklabels([])
             # Disable axis entirely.
             axes.get_xaxis().set_visible(False)
             axes.get_yaxis().set_visible(False)
             # Increase axis width.
-            for axis in ['top', 'bottom', 'left', 'right']:
+            for axis in ["top", "bottom", "left", "right"]:
                 axes.spines[axis].set_linewidth(3)
             for line in axes.lines:
                 line.set_linewidth(4)
-        figure.savefig(file_path, format=file_format, transparent=True,
-                       bbox_inches='tight')
+        figure.savefig(file_path, format=file_format, transparent=True, bbox_inches="tight")
 
 
-if __name__ == '__main__':
-    parser = argparse.ArgumentParser(description='Generate plots for frontend.')
-    parser.add_argument('destination', type=str,
-                        help='folder where the plots will be stored')
+if __name__ == "__main__":
+    parser = argparse.ArgumentParser(description="Generate plots for frontend.")
+    parser.add_argument("destination", type=str, help="folder where the plots will be stored")
     args = parser.parse_args()
 
     destination_path = Path(args.destination)
     destination_path.mkdir(exist_ok=True)
     save_plots_for_web(destination_path)
```

### Comparing `aihwkit-0.7.1/src/aihwkit/version.py` & `aihwkit-0.8.0/src/aihwkit/version.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,19 +1,19 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
 # that they have been altered from the originals.
 
 """Package version string."""
 
 import os
 
-VERSION_FILE = os.path.join(os.path.dirname(__file__), 'VERSION.txt')
-with open(VERSION_FILE, encoding='utf-8') as version_file:
+VERSION_FILE = os.path.join(os.path.dirname(__file__), "VERSION.txt")
+with open(VERSION_FILE, encoding="utf-8") as version_file:
     __version__ = version_file.read().strip()
```

### Comparing `aihwkit-0.7.1/src/aihwkit.egg-info/PKG-INFO` & `aihwkit-0.8.0/src/aihwkit.egg-info/PKG-INFO`

 * *Files 6% similar despite different names*

```diff
@@ -1,16 +1,17 @@
 Metadata-Version: 2.1
 Name: aihwkit
-Version: 0.7.1
+Version: 0.8.0
 Summary: IBM Analog Hardware Acceleration Kit
 Home-page: https://github.com/IBM/aihwkit
 Author: IBM Research
 Author-email: aihwkit@us.ibm.com
 License: Apache 2.0
-Keywords: ai,analog,rpu,torch
+Keywords: ai,analog,rpu,torch,memristor,pcm,reram,crossbar,in-memory,nvm,non-von-neumann,non-volatile memory,phase-change material
+Platform: UNKNOWN
 Classifier: Development Status :: 4 - Beta
 Classifier: Environment :: Console
 Classifier: Environment :: GPU :: NVIDIA CUDA
 Classifier: Intended Audience :: Science/Research
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Operating System :: MacOS
 Classifier: Operating System :: Microsoft :: Windows
@@ -84,20 +85,20 @@
 
 ### Other features
 
 Along with the two main components, the toolkit includes other
 functionalities such as:
 
 * A library of device presets that are calibrated to real hardware data and
-  based on models in the literature, along with configuration that specifies a particular device and optimizer choice.
+  based on models in the literature, along with a configuration that specifies a particular device and optimizer choice.
 * A module for executing high-level use cases ("experiments"), such as neural
   network training with minimal code overhead.
 * A utility to automatically convert a downloaded model (e.g., pre-trained) to its equivalent Analog
   model by replacing all linear/conv layers to Analog layers (e.g., for convenient hardware-aware training).
-* Integration with the [AIHW Composer] platform, a no-code web experience, that allows executing
+* Integration with the [AIHW Composer] platform, a no-code web experience that allows executing
   experiments in the cloud.
 
 ## Example
 
 ### Training example
 
 ```python
@@ -128,28 +129,29 @@
     print('Loss error: {:.16f}'.format(loss))
 ```
 
 You can find more examples in the [`examples/`] folder of the project, and
 more information about the library in the [documentation]. Please note that
 the examples have some additional dependencies - you can install them via
 `pip install -r requirements-examples.txt`.
+You can find interactive notebooks and tutorials in the [`notebooks/`] directory. 
 
 
 ## What is Analog AI?
 
 In traditional hardware architecture, computation and memory are siloed in
 different locations. Information is moved back and forth between computation
 and memory units every time an operation is performed, creating a limitation
 called the [von Neumann bottleneck].
 
 Analog AI delivers radical performance improvements by combining compute and
 memory in a single device, eliminating the von Neumann bottleneck. By leveraging
 the physical properties of memory devices, computation happens at the same place
 where the data is stored. Such in-memory computing hardware increases the speed
-and energy-efficiency needed for next generation AI workloads.
+and energy efficiency needed for next-generation AI workloads.
 
 ## What is an in-memory computing chip?
 
 An in-memory computing chip typically consists of multiple arrays of memory
 devices that communicate with each other. Many types of memory devices such as
 [phase-change memory] (PCM), [resistive random-access memory] (RRAM), and
 [Flash memory] can be used for in-memory computing.
@@ -176,28 +178,50 @@
 > Malte J. Rasch, Diego Moreda, Tayfun Gokmen, Manuel Le Gallo, Fabio Carta,
 > Cindy Goldberg, Kaoutar El Maghraoui, Abu Sebastian, Vijay Narayanan.
 > "A flexible and fast PyTorch toolkit for simulating training and inference on
 > analog crossbar arrays" (2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems)
 >
 > https://ieeexplore.ieee.org/abstract/document/9458494
 
+## Awards and Media Mentions 
+We are proud to share the AIHWKIT and the companion cloud composer received the IEEE OPEN SOURCE SCIENCE [award] in 2023. 
+![image](https://github.com/IBM/aihwkit/assets/7916630/1eb2ee6a-31c6-42c1-aa30-da5d396b24d7)
+
 ## Installation
 
 ### Installing from PyPI
 
-The preferred way to install this package is by using the
+The preferred way to install this package is by using the 
 [Python package index]:
 
 ```bash
 $ pip install aihwkit
 ```
+### Conda-based Installation
+We are working on publishing the package in the conda-forge channel. Until then, you need to manually download the package for installation.
+
+Download the aihwkit conda package tar file::
+
+    $ wget https://aihwkit-gpu-demo.s3.us-east.cloud-object-storage.appdomain.cloud/aihwkit-condapkg.tar
+
+Untar the file to a directory such as $HOME/aihwkit-condapkg
+Create a conda environment::
+
+    $ conda create -n aihwkit
+    $ conda activate aihwkit
 
-> :warning: Note that currently we provide CPU-only pre-built packages for
-> specific combinations of architectures and versions, and in some cases a
-> pre-built package might still not be available.
+Install one of the conda packages.  For example:
+
+  - CPU::
+
+    $ conda install python=3.9 aihwkit -c conda-forge -c file:/$HOME/aihwkit-condapkg
+
+  - GPU::
+
+    $ conda install python=3.9 aihwkit-gpu -c conda-forge -c file:/$HOME/aihwkit-condapkg
 
 If you encounter any issues during download or want to compile the package
 for your environment, please refer to the [advanced installation] guide.
 That section describes the additional libraries and tools required for
 compiling the sources, using a build system based on `cmake`.
 
 ## Authors
@@ -216,18 +240,21 @@
 [Apache License 2.0]: LICENSE.txt
 [`CUDA Toolkit`]: https://developer.nvidia.com/accelerated-computing-toolkit
 [`OpenBLAS`]: https://www.openblas.net/
 [Python package index]: https://pypi.org/project/aihwkit
 [`PyTorch`]: https://pytorch.org/
 
 [`examples/`]: examples/
+[`notebooks/`]: notebooks/
 [documentation]: https://aihwkit.readthedocs.io/
 [contributors]: https://github.com/IBM/aihwkit/graphs/contributors
 [advanced installation]: https://aihwkit.readthedocs.io/en/latest/advanced_install.html
 
 [von Neumann bottleneck]: https://en.wikipedia.org/wiki/Von_Neumann_architecture#Von_Neumann_bottleneck
 [phase-change memory]: https://en.wikipedia.org/wiki/Phase-change_memory
 [resistive random-access memory]: https://en.wikipedia.org/wiki/Resistive_random-access_memory
 [Flash memory]: https://en.wikipedia.org/wiki/Flash_memory
 [Kirchhoff’s circuits laws]: https://en.wikipedia.org/wiki/Kirchhoff%27s_circuit_laws
 [online demo]: https://analog-ai-demo.mybluemix.net/
 [AIHW Composer]: https://aihw-composer.draco.res.ibm.com
+[award]: https://conferences.computer.org/services/2023/awards/
+
```

### Comparing `aihwkit-0.7.1/src/aihwkit.egg-info/SOURCES.txt` & `aihwkit-0.8.0/src/aihwkit.egg-info/SOURCES.txt`

 * *Files 2% similar despite different names*

```diff
@@ -63,27 +63,29 @@
 src/aihwkit/experiments/runners/i_cloud.py
 src/aihwkit/experiments/runners/i_local.py
 src/aihwkit/experiments/runners/i_metrics.py
 src/aihwkit/experiments/runners/local.py
 src/aihwkit/experiments/runners/metrics.py
 src/aihwkit/inference/__init__.py
 src/aihwkit/inference/utils.py
+src/aihwkit/inference/calibration/__init__.py
+src/aihwkit/inference/calibration/calibration.py
 src/aihwkit/inference/compensation/__init__.py
 src/aihwkit/inference/compensation/base.py
 src/aihwkit/inference/compensation/drift.py
 src/aihwkit/inference/converter/__init__.py
 src/aihwkit/inference/converter/base.py
 src/aihwkit/inference/converter/conductance.py
 src/aihwkit/inference/noise/__init__.py
 src/aihwkit/inference/noise/base.py
 src/aihwkit/inference/noise/custom.py
 src/aihwkit/inference/noise/pcm.py
+src/aihwkit/inference/noise/reram.py
 src/aihwkit/nn/__init__.py
 src/aihwkit/nn/conversion.py
-src/aihwkit/nn/functions.py
 src/aihwkit/nn/modules/__init__.py
 src/aihwkit/nn/modules/base.py
 src/aihwkit/nn/modules/container.py
 src/aihwkit/nn/modules/conv.py
 src/aihwkit/nn/modules/conv_mapped.py
 src/aihwkit/nn/modules/linear.py
 src/aihwkit/nn/modules/linear_mapped.py
@@ -97,61 +99,84 @@
 src/aihwkit/simulator/CMakeLists.txt
 src/aihwkit/simulator/__init__.py
 src/aihwkit/simulator/noise_models.py
 src/aihwkit/simulator/configs/__init__.py
 src/aihwkit/simulator/configs/compounds.py
 src/aihwkit/simulator/configs/configs.py
 src/aihwkit/simulator/configs/devices.py
-src/aihwkit/simulator/configs/enums.py
 src/aihwkit/simulator/configs/helpers.py
 src/aihwkit/simulator/configs/utils.py
+src/aihwkit/simulator/parameters/__init__.py
+src/aihwkit/simulator/parameters/base.py
+src/aihwkit/simulator/parameters/enums.py
+src/aihwkit/simulator/parameters/helpers.py
+src/aihwkit/simulator/parameters/utils.py
 src/aihwkit/simulator/presets/__init__.py
 src/aihwkit/simulator/presets/compounds.py
 src/aihwkit/simulator/presets/configs.py
 src/aihwkit/simulator/presets/devices.py
+src/aihwkit/simulator/presets/inference.py
 src/aihwkit/simulator/presets/utils.py
 src/aihwkit/simulator/presets/web.py
 src/aihwkit/simulator/rpu_base_src/rpu_base.cpp
 src/aihwkit/simulator/rpu_base_src/rpu_base.h
 src/aihwkit/simulator/rpu_base_src/rpu_base_devices.cpp
 src/aihwkit/simulator/rpu_base_src/rpu_base_tiles.cpp
 src/aihwkit/simulator/rpu_base_src/rpu_base_tiles_cuda.cpp
 src/aihwkit/simulator/tiles/__init__.py
 src/aihwkit/simulator/tiles/analog.py
+src/aihwkit/simulator/tiles/array.py
 src/aihwkit/simulator/tiles/base.py
+src/aihwkit/simulator/tiles/custom.py
 src/aihwkit/simulator/tiles/floating_point.py
+src/aihwkit/simulator/tiles/functions.py
 src/aihwkit/simulator/tiles/inference.py
+src/aihwkit/simulator/tiles/inference_torch.py
+src/aihwkit/simulator/tiles/module.py
+src/aihwkit/simulator/tiles/periphery.py
+src/aihwkit/simulator/tiles/rpucuda.py
+src/aihwkit/simulator/tiles/torch_tile.py
+src/aihwkit/simulator/tiles/utils.py
 src/aihwkit/utils/__init__.py
 src/aihwkit/utils/analog_info.py
 src/aihwkit/utils/fitting.py
+src/aihwkit/utils/legacy.py
 src/aihwkit/utils/visualization.py
 src/aihwkit/utils/visualization_web.py
 src/rpucuda/CMakeLists.txt
 src/rpucuda/dense_bit_line_maker.cpp
 src/rpucuda/dense_bit_line_maker.h
 src/rpucuda/math_util.cpp
 src/rpucuda/math_util.h
 src/rpucuda/rng.cpp
 src/rpucuda/rng.h
 src/rpucuda/rpu.cpp
 src/rpucuda/rpu.h
 src/rpucuda/rpu_buffered_transfer_device.cpp
 src/rpucuda/rpu_buffered_transfer_device.h
+src/rpucuda/rpu_chopped_transfer_device.cpp
+src/rpucuda/rpu_chopped_transfer_device.h
 src/rpucuda/rpu_constantstep_device.cpp
 src/rpucuda/rpu_constantstep_device.h
+src/rpucuda/rpu_dynamic_transfer_device.cpp
+src/rpucuda/rpu_dynamic_transfer_device.h
 src/rpucuda/rpu_expstep_device.cpp
 src/rpucuda/rpu_expstep_device.h
 src/rpucuda/rpu_forward_backward_pass.cpp
 src/rpucuda/rpu_forward_backward_pass.h
+src/rpucuda/rpu_hidden_device.cpp
+src/rpucuda/rpu_hidden_device.h
 src/rpucuda/rpu_linearstep_device.cpp
 src/rpucuda/rpu_linearstep_device.h
 src/rpucuda/rpu_mixedprec_device.cpp
 src/rpucuda/rpu_mixedprec_device.h
 src/rpucuda/rpu_mixedprec_device_base.cpp
 src/rpucuda/rpu_mixedprec_device_base.h
+src/rpucuda/rpu_mixedprec_int_device.cpp
+src/rpucuda/rpu_mixedprec_int_device.h
 src/rpucuda/rpu_onesided_device.cpp
 src/rpucuda/rpu_onesided_device.h
 src/rpucuda/rpu_onesided_device_test.cpp
 src/rpucuda/rpu_piecewisestep_device.cpp
 src/rpucuda/rpu_piecewisestep_device.h
 src/rpucuda/rpu_powstep_device.cpp
 src/rpucuda/rpu_powstep_device.h
@@ -173,14 +198,15 @@
 src/rpucuda/rpu_transfer_device_test.cpp
 src/rpucuda/rpu_vector_device.cpp
 src/rpucuda/rpu_vector_device.h
 src/rpucuda/rpu_weight_updater.cpp
 src/rpucuda/rpu_weight_updater.h
 src/rpucuda/sparse_bit_line_maker.cpp
 src/rpucuda/sparse_bit_line_maker.h
+src/rpucuda/utility_functions.cpp
 src/rpucuda/utility_functions.h
 src/rpucuda/weight_clipper.cpp
 src/rpucuda/weight_clipper.h
 src/rpucuda/weight_drifter.cpp
 src/rpucuda/weight_drifter.h
 src/rpucuda/weight_modifier.cpp
 src/rpucuda/weight_modifier.h
@@ -188,14 +214,16 @@
 src/rpucuda/weight_remapper.h
 src/rpucuda/cuda/CMakeLists.txt
 src/rpucuda/cuda/bit_line_maker.cu
 src/rpucuda/cuda/bit_line_maker.h
 src/rpucuda/cuda/bit_line_maker_test.cpp
 src/rpucuda/cuda/chopped_weight_output.cu
 src/rpucuda/cuda/chopped_weight_output.h
+src/rpucuda/cuda/cuda_buffer.cu
+src/rpucuda/cuda/cuda_buffer.h
 src/rpucuda/cuda/cuda_math_util.cu
 src/rpucuda/cuda/cuda_math_util.h
 src/rpucuda/cuda/cuda_util.cu
 src/rpucuda/cuda/cuda_util.h
 src/rpucuda/cuda/forward_backward_pass.cu
 src/rpucuda/cuda/forward_backward_pass.h
 src/rpucuda/cuda/forward_backward_pass_test.cpp
@@ -217,27 +245,38 @@
 src/rpucuda/cuda/pwu_kernel_parameter_base.h
 src/rpucuda/cuda/rpu_cub.h
 src/rpucuda/cuda/rpucuda.cu
 src/rpucuda/cuda/rpucuda.h
 src/rpucuda/cuda/rpucuda_buffered_transfer_device.cu
 src/rpucuda/cuda/rpucuda_buffered_transfer_device.h
 src/rpucuda/cuda/rpucuda_buffered_transfer_device_test.cpp
+src/rpucuda/cuda/rpucuda_chopped_transfer_device.cu
+src/rpucuda/cuda/rpucuda_chopped_transfer_device.h
+src/rpucuda/cuda/rpucuda_chopped_transfer_device_test.cpp
 src/rpucuda/cuda/rpucuda_constantstep_device.cu
 src/rpucuda/cuda/rpucuda_constantstep_device.h
+src/rpucuda/cuda/rpucuda_dynamic_transfer_device.cu
+src/rpucuda/cuda/rpucuda_dynamic_transfer_device.h
 src/rpucuda/cuda/rpucuda_expstep_device.cu
 src/rpucuda/cuda/rpucuda_expstep_device.h
 src/rpucuda/cuda/rpucuda_expstep_test.cpp
+src/rpucuda/cuda/rpucuda_hidden_device.cu
+src/rpucuda/cuda/rpucuda_hidden_device.h
+src/rpucuda/cuda/rpucuda_hidden_test.cpp
 src/rpucuda/cuda/rpucuda_linearstep_device.cu
 src/rpucuda/cuda/rpucuda_linearstep_device.h
 src/rpucuda/cuda/rpucuda_linearstep_test.cpp
 src/rpucuda/cuda/rpucuda_mixedprec_device.cu
 src/rpucuda/cuda/rpucuda_mixedprec_device.h
 src/rpucuda/cuda/rpucuda_mixedprec_device_base.cu
 src/rpucuda/cuda/rpucuda_mixedprec_device_base.h
 src/rpucuda/cuda/rpucuda_mixedprec_device_test.cpp
+src/rpucuda/cuda/rpucuda_mixedprec_int_device.cu
+src/rpucuda/cuda/rpucuda_mixedprec_int_device.h
+src/rpucuda/cuda/rpucuda_mixedprec_int_device_test.cpp
 src/rpucuda/cuda/rpucuda_onesided_device.cu
 src/rpucuda/cuda/rpucuda_onesided_device.h
 src/rpucuda/cuda/rpucuda_onesided_device_test.cpp
 src/rpucuda/cuda/rpucuda_piecewisestep_device.cu
 src/rpucuda/cuda/rpucuda_piecewisestep_device.h
 src/rpucuda/cuda/rpucuda_powstep_device.cu
 src/rpucuda/cuda/rpucuda_powstep_device.h
@@ -275,25 +314,29 @@
 src/rpucuda/cuda/weight_drifter_cuda_test.cpp
 src/rpucuda/cuda/weight_modifier_cuda.cu
 src/rpucuda/cuda/weight_modifier_cuda.h
 src/rpucuda/cuda/weight_remapper_cuda.cu
 src/rpucuda/cuda/weight_remapper_cuda.h
 tests/__init__.py
 tests/test_bindings_tiles.py
+tests/test_calibration.py
 tests/test_client.py
 tests/test_cloud_runner.py
 tests/test_conversions.py
 tests/test_experiment_runners.py
 tests/test_experiments.py
 tests/test_inference.py
 tests/test_inference_tiles.py
+tests/test_layer_base.py
 tests/test_layers.py
 tests/test_layers_convolution.py
 tests/test_layers_linear.py
 tests/test_layers_mapped.py
 tests/test_layers_rnn.py
+tests/test_localrunner_infer.py
 tests/test_optimizers.py
 tests/test_presets.py
 tests/test_rpu_configurations.py
 tests/test_simulator_tiles.py
 tests/test_specific_tiles.py
+tests/test_torch_tiles.py
 tests/test_utils.py
```

### Comparing `aihwkit-0.7.1/src/rpucuda/CMakeLists.txt` & `aihwkit-0.8.0/src/rpucuda/CMakeLists.txt`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/CMakeLists.txt` & `aihwkit-0.8.0/src/rpucuda/cuda/CMakeLists.txt`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/bit_line_maker.cu` & `aihwkit-0.8.0/src/rpucuda/cuda/bit_line_maker.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -1249,18 +1249,18 @@
 #define RPU_BLM_ITEMS_PER_THREAD 4
 #define RPU_BLM_BL_TO_SELECT_SIMPLE_LOOP 0
 
 template <typename T>
 BitLineMaker<T>::BitLineMaker(CudaContextPtr c, int x_size, int d_size)
     : context_{c}, x_size_{x_size}, d_size_{d_size}, umh_{nullptr}, buffer_m_batch_{0} {
 
-  max_block_count_ =
-      context_->getSMCount() * (context_->maxThreadsPerSM() / RPU_THREADS_PER_BLOCK_UPDATE);
+  int nthreads_default = MIN(context_->maxThreadsPerBlock(), RPU_THREADS_PER_BLOCK_UPDATE);
+  max_block_count_ = context_->getSMCount() * (context_->maxThreadsPerSM() / nthreads_default);
 
-  nthreads_ = RPU_THREADS_PER_BLOCK_UPDATE;
+  nthreads_ = nthreads_default;
 }
 
 template <typename T>
 BLMOutputFormat BitLineMaker<T>::getFormat(int use_bo64, bool implicit_pulses) {
 
   if (implicit_pulses && use_bo64 == 0) {
     return BLMOutputFormat::FP;
@@ -2003,14 +2003,23 @@
   } break;
 
   default:
     RPU_FATAL("PulseType not supported by BitLineMaker");
   }
 };
 
+template <typename T>
+void BitLineMaker<T>::dumpExtra(RPU::state_t &extra, const std::string prefix) {
+  // does not handle the counts.. assumimg that BLM will be called for each sample / batch
+  // does not handle umh
+}
+
+template <typename T>
+void BitLineMaker<T>::loadExtra(const RPU::state_t &extra, const std::string prefix, bool strict) {}
+
 template class BitLineMaker<float>;
 #ifdef RPU_USE_DOUBLE
 template class BitLineMaker<double>;
 #endif
 
 #define RPU_BLM_ITER_TEMPLATE(NUM_T, XITERT, DITERT)                                               \
   template void BitLineMaker<NUM_T>::makeCounts(                                                   \
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/bit_line_maker.h` & `aihwkit-0.8.0/src/rpucuda/cuda/bit_line_maker.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -99,14 +99,16 @@
       bool flexible_in_size,
       bool verbose);
 
   UpdateManagementHelper<T> *getUmh() const { return &*umh_; };
   void getAverageAbsMax(T &m_x, T &m_d) const;
   void getAverageLogAbsMax(T &m_x, T &m_d) const;
   void getAbsMax(T &m_x, T &m_d) const;
+  void dumpExtra(RPU::state_t &extra, const std::string prefix);
+  void loadExtra(const RPU::state_t &extra, const std::string prefix, bool strict);
 
 private:
   CudaContextPtr context_ = nullptr;
   int x_size_ = 0;
   int d_size_ = 0;
   int nthreads_ = 0;
   int max_block_count_ = 0;
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/bit_line_maker_test.cpp` & `aihwkit-0.8.0/src/rpucuda/cuda/bit_line_maker_test.cpp`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/chopped_weight_output.cu` & `aihwkit-0.8.0/src/rpucuda/cuda/chopped_weight_output.cu`

 * *Files 17% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -180,14 +180,61 @@
   ss << "\t val start:\t" << getValStart() << std::endl;
   ss << "\t num wo:\t" << getNumWeightOutputs() << std::endl;
   ss << "\t flexible:\t" << getFlexibleInSize() << std::endl;
   par_.printToStream(ss);
   ss << std::endl;
 };
 
+template <typename T>
+void ChoppedWeightOutput<T>::dumpExtra(RPU::state_t &extra, const std::string prefix) {
+  RPU::state_t state;
+
+  // don't handle maximizers (no states)
+  RPU::insert(state, "current_m_batch", current_m_batch_);
+  RPU::insert(state, "cwo_counter", cwo_counter_);
+  RPU::insert(state, "nwo_counter", nwo_counter_);
+  RPU::insert(state, "flexible_in_size", flexible_in_size_);
+  RPU::insert(state, "swapped_choppers", swapped_choppers_);
+  RPU::insert(state, "dev_switching_probs", dev_switching_probs_);
+  RPU::insert(state, "dev_weight_output_out_chopper", dev_weight_output_out_chopper_);
+  RPU::insert(state, "dev_weight_output_in_chopper", dev_weight_output_in_chopper_);
+  RPU::insert(state, "dev_weight_output_signals", dev_weight_output_signals_);
+  RPU::insert(state, "dev_x_chopper_buffer_1", dev_x_chopper_buffer_1_);
+  RPU::insert(state, "dev_x_chopper_buffer_2", dev_x_chopper_buffer_2_);
+  RPU::insert(state, "dev_d_chopper_buffer_1", dev_d_chopper_buffer_1_);
+  RPU::insert(state, "dev_d_chopper_buffer_2", dev_d_chopper_buffer_2_);
+
+  RPU::insertWithPrefix(extra, state, prefix);
+}
+
+template <typename T>
+void ChoppedWeightOutput<T>::loadExtra(
+    const RPU::state_t &extra, const std::string prefix, bool strict) {
+
+  using V = std::vector<T>;
+  auto state = RPU::selectWithPrefix(extra, prefix);
+
+  RPU::load(state, "current_m_batch", current_m_batch_, strict);
+  RPU::load(state, "cwo_counter", cwo_counter_, strict);
+  RPU::load(state, "nwo_counter", nwo_counter_, strict);
+  RPU::load(state, "flexible_in_size", flexible_in_size_, strict);
+  RPU::load(state, "swapped_choppers", swapped_choppers_, strict);
+  RPU::load(this->context_, state, "dev_switching_probs", dev_switching_probs_, strict);
+  RPU::load(
+      this->context_, state, "dev_weight_output_out_chopper", dev_weight_output_out_chopper_,
+      strict);
+  RPU::load(
+      this->context_, state, "dev_weight_output_in_chopper", dev_weight_output_in_chopper_, strict);
+  RPU::load(this->context_, state, "dev_weight_output_signals", dev_weight_output_signals_, strict);
+  RPU::load(this->context_, state, "dev_x_chopper_buffer_1", dev_x_chopper_buffer_1_, strict);
+  RPU::load(this->context_, state, "dev_x_chopper_buffer_2", dev_x_chopper_buffer_2_, strict);
+  RPU::load(this->context_, state, "dev_d_chopper_buffer_1", dev_d_chopper_buffer_1_, strict);
+  RPU::load(this->context_, state, "dev_d_chopper_buffer_2", dev_d_chopper_buffer_2_, strict);
+}
+
 /******************************************************************************************************************/
 
 template <typename T> int ChoppedWeightOutput<T>::getValStart() const {
 
   if (par_.every <= 0) {
     return 0;
   }
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/chopped_weight_output.h` & `aihwkit-0.8.0/src/rpucuda/cuda/chopped_weight_output.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -94,14 +94,18 @@
     std::stringstream ss;
     printToStream(ss);
     std::cout << ss.str();
   };
   void printToStream(std::stringstream &ss) const;
   void makeWeightOutputChoppers(const BitLineMaker<T> *blm);
   void releaseBuffers();
+
+  void dumpExtra(RPU::state_t &extra, const std::string prefix);
+  void loadExtra(const RPU::state_t &extra, const std::string prefix, bool strict);
+
   inline void setPar(const ChoppedWeightOutputParameter<T> &par) { par_ = par; };
   inline void setInChopRandom(bool in_chop_random) { par_.in_chop_random = in_chop_random; };
   inline void setInChopProb(T in_chop_prob) { par_.in_chop_prob = in_chop_prob; };
   inline const ChoppedWeightOutputParameter<T> &getPar() const { return par_; };
 
   inline int getCounter() const { return cwo_counter_; };
   inline int getCurrentMBatch() const { return current_m_batch_; };
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/cuda_math_util.cu` & `aihwkit-0.8.0/src/rpucuda/cuda/cuda_math_util.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -679,14 +679,76 @@
     const double *,
     const float *,
     const float *,
     const double,
     const float *);
 #endif
 
+// MSK = P<thres
+// W(MSK) = A(MSK) + B(MSK)
+template <typename T>
+__global__ void kernelElemReset(
+    T *dev_W,
+    const int size,
+    const T *dev_A,
+    const float *dev_B,
+    const float *dev_P,
+    const T thres) {
+
+  bool with_A = dev_A != nullptr;
+  bool with_B = dev_B != nullptr;
+  bool with_P = dev_P != nullptr;
+
+  RPU_CUDA_1D_KERNEL_LOOP(idx, size) {
+    T th = thres;
+    T a = (with_A) ? dev_A[idx] : (float)0.0;
+    float p = (with_P) ? dev_P[idx] : (float)0.0;
+    float b = (with_B) ? dev_B[idx] : (float)0.0;
+    T w = dev_W[idx];
+
+    if (p < th) {
+      w = a + b;
+      dev_W[idx] = w;
+    }
+  }
+}
+template <typename T>
+void elemreset(
+    const CudaContextPtr context,
+    T *dev_W,
+    const int size,
+    const T *dev_A,
+    const float *dev_B,
+    const float *dev_P,
+    T thres) {
+
+  int nthreads = context->getNThreads();
+  int nblocks = context->getNBlocks(size, nthreads);
+  kernelElemReset<T>
+      <<<nblocks, nthreads, 0, context->getStream()>>>(dev_W, size, dev_A, dev_B, dev_P, thres);
+}
+template void elemreset<float>(
+    const CudaContextPtr,
+    float *,
+    const int,
+    const float *,
+    const float *,
+    const float *,
+    const float);
+#ifdef RPU_USE_DOUBLE
+template void elemreset<double>(
+    const CudaContextPtr,
+    double *,
+    const int,
+    const double *,
+    const float *,
+    const float *,
+    const double);
+#endif
+
 // MSK != 0
 // W(MSK) = sat(reset_bias(MSK) + std*randn())
 #define RESET_TOLERANCE 1e-6
 template <typename T>
 __global__ void kernelElemResetSatMsk(
     T *weights,
     const int size,
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/cuda_math_util.h` & `aihwkit-0.8.0/src/rpucuda/cuda/cuda_math_util.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -172,14 +172,26 @@
     const int size,
     const T *A,
     const float *B, // float for random
     const float *P, // float for random
     T thres,
     const float *dev_4params);
 
+// MSK = P<thres
+// W(MSK) = A(MSK) + B(MSK)
+template <typename T>
+void elemreset(
+    const CudaContextPtr context,
+    T *W,
+    const int size,
+    const T *A,
+    const float *B, // float for random
+    const float *P, // float for random
+    T thres);
+
 // MSK != 0
 // W(MSK) = sat(reset_bias(MSK) + std*randn())
 template <typename T>
 void elemresetsatmsk(
     CudaContextPtr context, // non const because might use random states
     T *W,
     const int size,
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/cuda_util.cu` & `aihwkit-0.8.0/src/rpucuda/cuda/cuda_util.cu`

 * *Files 12% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -39,14 +39,125 @@
   }
 
 int64_t rpu_global_mem_counter = 0;
 std::mutex rpu_global_mem_counter_mutex;
 
 namespace RPU {
 
+/*****************************************************/
+/*  states */
+
+#define RPU_LOAD_ARRAY(T)                                                                          \
+  template <>                                                                                      \
+  void load(                                                                                       \
+      CudaContextPtr context, RPU::state_t &state, std::string key, CudaArray<T> &value,           \
+      bool strict) {                                                                               \
+    std::vector<double> tmp;                                                                       \
+    try {                                                                                          \
+      tmp = state.at(key);                                                                         \
+    } catch (const std::out_of_range &oor) {                                                       \
+      if (strict) {                                                                                \
+        RPU_FATAL("Cannot find the cuda unique vector key `" << key << "` in state.");             \
+      }                                                                                            \
+      return;                                                                                      \
+    }                                                                                              \
+    std::vector<T> out(tmp.begin(), tmp.end());                                                    \
+    if (!out.size()) {                                                                             \
+      value = CudaArray<T>(context, 0);                                                            \
+    } else {                                                                                       \
+      value = CudaArray<T>(context, out);                                                          \
+    }                                                                                              \
+  }
+
+RPU_LOAD_ARRAY(float);
+#ifdef RPU_USE_DOUBLE
+RPU_LOAD_ARRAY(double);
+#endif
+RPU_LOAD_ARRAY(int);
+RPU_LOAD_ARRAY(chop_t);
+RPU_LOAD_ARRAY(uint64_t);
+RPU_LOAD_ARRAY(char);
+RPU_LOAD_ARRAY(kagg_t);
+#undef RPU_LOAD_ARRAY
+
+#define RPU_LOAD_UNIQUE(T)                                                                         \
+  template <>                                                                                      \
+  void load(                                                                                       \
+      CudaContextPtr context, RPU::state_t &state, std::string key,                                \
+      std::unique_ptr<CudaArray<T>> &value, bool strict) {                                         \
+    std::vector<double> tmp;                                                                       \
+    try {                                                                                          \
+      tmp = state.at(key);                                                                         \
+    } catch (const std::out_of_range &oor) {                                                       \
+      if (strict) {                                                                                \
+        RPU_FATAL("Cannot find the cuda vector key `" << key << "` in state.");                    \
+      }                                                                                            \
+      return;                                                                                      \
+    }                                                                                              \
+    std::vector<T> out(tmp.begin(), tmp.end());                                                    \
+    if (!out.size()) {                                                                             \
+      value = nullptr;                                                                             \
+    } else {                                                                                       \
+      value = RPU::make_unique<CudaArray<T>>(context, out);                                        \
+    }                                                                                              \
+  }
+
+RPU_LOAD_UNIQUE(float);
+#ifdef RPU_USE_DOUBLE
+RPU_LOAD_UNIQUE(double);
+#endif
+RPU_LOAD_UNIQUE(int);
+RPU_LOAD_UNIQUE(chop_t);
+RPU_LOAD_UNIQUE(uint64_t);
+RPU_LOAD_UNIQUE(char);
+RPU_LOAD_UNIQUE(kagg_t);
+#undef RPU_LOAD_UNIQUE
+
+#define RPU_INSERT_CUDA(TYPE)                                                                      \
+  template <> void insert(RPU::state_t &state, std::string key, const CudaArray<TYPE> &value) {    \
+    std::vector<TYPE> tmp = value.cpu();                                                           \
+    std::vector<double> out(tmp.begin(), tmp.end());                                               \
+    state[key] = out;                                                                              \
+  }
+
+RPU_INSERT_CUDA(float);
+#ifdef RPU_USE_DOUBLE
+RPU_INSERT_CUDA(double);
+#endif
+RPU_INSERT_CUDA(int);
+RPU_INSERT_CUDA(chop_t);
+RPU_INSERT_CUDA(uint64_t);
+RPU_INSERT_CUDA(char);
+RPU_INSERT_CUDA(kagg_t);
+#undef RPU_INSERT_CUDA
+
+#define RPU_INSERT_UNIQUE_CUDA(TYPE)                                                               \
+  template <>                                                                                      \
+  void insert(                                                                                     \
+      RPU::state_t &state, std::string key, const std::unique_ptr<CudaArray<TYPE>> &value) {       \
+    if (value != nullptr) {                                                                        \
+      std::vector<TYPE> tmp = value->cpu();                                                        \
+      std::vector<double> out(tmp.begin(), tmp.end());                                             \
+      state[key] = out;                                                                            \
+    } else {                                                                                       \
+      state[key] = std::vector<double>{};                                                          \
+    }                                                                                              \
+  }
+
+RPU_INSERT_UNIQUE_CUDA(float);
+#ifdef RPU_USE_DOUBLE
+RPU_INSERT_UNIQUE_CUDA(double);
+#endif
+RPU_INSERT_UNIQUE_CUDA(int);
+RPU_INSERT_UNIQUE_CUDA(chop_t);
+RPU_INSERT_UNIQUE_CUDA(uint64_t);
+RPU_INSERT_UNIQUE_CUDA(char);
+RPU_INSERT_UNIQUE_CUDA(kagg_t);
+#undef RPU_INSERT_UNIQUE_CUDA
+
 __global__ void kernelCurandSetup(unsigned long long rseed, curandState_t *state, int n) {
   int id = threadIdx.x + blockIdx.x * blockDim.x;
   /* Each thread gets same seed, a different sequence
      number, no offset */
   if (id < n) {
     curand_init(rseed, id, 0, &state[id]);
   }
@@ -295,62 +406,14 @@
     return 1;
   }
 }
 
 #endif
 
 //**********************************************************************//
-
-template <typename T> T *CudaBuffer<T>::get(CudaContextPtr c, int size) {
-  mutex_.lock(); // need to be explicitely released to avoid multi-threading issues
-  if (buffer_ == nullptr || buffer_->getSize() < size || &*(buffer_->getContext()) != &*c) {
-    if (buffer_ != nullptr) {
-      buffer_->synchronize();
-    }
-    buffer_ = RPU::make_unique<CudaArray<T>>(c, size);
-    c->synchronize();
-  }
-  return buffer_->getData();
-}
-
-template <typename T> void CudaBuffer<T>::release() { mutex_.unlock(); }
-
-// copy constructor
-template <typename T> CudaBuffer<T>::CudaBuffer(const CudaBuffer<T> &other) {
-  if (other.buffer_ != nullptr) {
-    buffer_ = RPU::make_unique<CudaArray<T>>(*other.buffer_);
-    buffer_->synchronize();
-  }
-}
-
-// copy assignment
-template <typename T> CudaBuffer<T> &CudaBuffer<T>::operator=(const CudaBuffer &other) {
-  CudaBuffer tmp(other);
-  swap(*this, tmp);
-  if (tmp.buffer_ != nullptr) {
-    tmp.buffer_->context_.synchronize();
-  }
-  return *this;
-}
-
-// move constructor
-template <typename T> CudaBuffer<T>::CudaBuffer(CudaBuffer<T> &&other) {
-  { const std::lock_guard<std::recursive_mutex> lock(other.mutex_); }
-  *this = std::move(other);
-}
-
-// move assignment
-template <typename T> CudaBuffer<T> &CudaBuffer<T>::operator=(CudaBuffer<T> &&other) {
-
-  const std::lock_guard<std::recursive_mutex> lock(other.mutex_);
-  buffer_ = std::move(other.buffer_);
-  return *this;
-}
-
-//**********************************************************************//
 void CudaContext::init() {
   DEBUG_OUT("Init context...");
 
   if (gpu_id_ >= 0) {
     CUDA_CALL(cudaSetDevice(gpu_id_));
   } else {
     CUDA_CALL(cudaGetDevice(&gpu_id_));
@@ -389,19 +452,23 @@
 }
 
 CudaContext::~CudaContext() {
   DEBUG_OUT("Destroy CudaContext...");
 
   shared_random_states_.clear();
   shared_float_buffer_.clear();
+#ifdef RPU_USE_DOUBLE
   shared_double_buffer_.clear();
+#endif
 
   random_states_.clear();
   float_buffer_.clear();
+#ifdef RPU_USE_DOUBLE
   double_buffer_.clear();
+#endif
 
   int i_start = shared_ ? 1 : 0;
   for (int i = i_start; i < streams_.size(); i++) {
     cudaStreamSynchronize(streams_[i]);
     cudaStreamDestroy(streams_[i]);
   }
 
@@ -495,18 +562,19 @@
   event_ = other.event_;
   other.event_ = nullptr;
 
   random_states_ = std::move(other.random_states_);
   shared_random_states_ = std::move(other.shared_random_states_);
 
   shared_float_buffer_ = std::move(other.shared_float_buffer_);
-  shared_double_buffer_ = std::move(other.shared_double_buffer_);
-
   float_buffer_ = std::move(other.float_buffer_);
+#ifdef RPU_USE_DOUBLE
+  shared_double_buffer_ = std::move(other.shared_double_buffer_);
   double_buffer_ = std::move(other.double_buffer_);
+#endif
 
   return *this;
 }
 
 void CudaContext::synchronizeContext() const {
   enforceDeviceId();
   for (int i = 0; i < streams_.size(); i++) {
@@ -561,19 +629,21 @@
   DEBUG_OUT("Synchronize stream id " << stream_id_);
   enforceDeviceId();
   CUDA_CALL(cudaStreamSynchronize(streams_[stream_id_]));
 }
 
 int CudaContext::getNBlocks(int size, int nthreads) const {
   DEBUG_OUT("get NBlocks for  size " << size);
+  nthreads = MIN(maxThreadsPerBlock(), nthreads);
   return (size + nthreads - 1) / nthreads;
 }
 
 int CudaContext::getNStrideBlocks(int size, int nthreads) const {
   DEBUG_OUT("get N Stride Blocks for  size " << size);
+  nthreads = MIN(maxThreadsPerBlock(), nthreads);
   int max_blocks = getSMCount() * maxThreadsPerBlock() / nthreads;
   return MIN(getNBlocks(size, nthreads), max_blocks);
 }
 
 cudaStream_t CudaContext::getStream(int idx) {
 
   enforceDeviceId();
@@ -754,22 +824,19 @@
 
   auto *buffer = &float_buffer_;
   auto stream_id = stream_id_;
   if (shared_ && stream_id_ == 0) {
     buffer = &shared_float_buffer_;
     stream_id = shared_stream_id_;
   }
-  const auto *array = (*buffer)[stream_id][id].getCudaArray();
-  if (!array) {
-    return;
-  }
   RPU_INFO("Float buffer " << id);
-  array->printValues(size);
+  (*buffer)[stream_id][id].print(size);
 }
 
+#ifdef RPU_USE_DOUBLE
 template <> double *CudaContext::getSharedBuffer<double>(int id, int size) {
   // somehow this needs to be a MAX_BUFFER vector to avoid dynamical
   // resizing. Not sure why, but dynamical allocation of the
   // CudaBuffer vector elements does not work without uniptr (which
   // then has sync problems)
 
   auto *buffer = &double_buffer_;
@@ -802,22 +869,20 @@
   auto *buffer = &double_buffer_;
   auto stream_id = stream_id_;
   if (shared_ && stream_id_ == 0) {
     buffer = &shared_double_buffer_;
     stream_id = shared_stream_id_;
   }
 
-  const auto *array = (*buffer)[stream_id][id].getCudaArray();
-  if (!array) {
-    return;
-  }
   RPU_INFO("Double buffer " << id);
-  array->printValues(size);
+  (*buffer)[stream_id][id].print(size);
 }
 
+#endif
+
 void CudaContext::recordWaitEvent(CudaContextPtr wait_on_context) {
   this->recordWaitEvent(wait_on_context->getStream(), wait_on_context->getEvent());
 }
 void CudaContext::recordEvent() { CUDA_CALL(cudaEventRecord(event_, streams_[stream_id_])); }
 void CudaContext::waitEvent(cudaEvent_t wait_on_event) {
   CUDA_CALL(cudaStreamWaitEvent(streams_[stream_id_], wait_on_event, 0));
 }
@@ -970,17 +1035,19 @@
   }
 }
 
 template <> void CudaArray<curandStateXORWOW>::setConst(curandStateXORWOW set_value) {
   RPU_FATAL("Cannot set curandstates to some values.");
 }
 
+#ifdef RPU_USE_DOUBLE
 template <> void CudaArray<double *>::setConst(double *set_value) {
   RPU_FATAL("Cannot set pointer types to some values.");
 }
+#endif
 
 template <> void CudaArray<float *>::setConst(float *set_value) {
   RPU_FATAL("Cannot set pointer types to some values.");
 }
 
 template <typename T> void CudaArray<T>::printValues(int nmax) const {
   T *values = new T[size_];
@@ -1155,24 +1222,32 @@
 }
 
 template <typename T> void CudaArray<T>::copyTo(std::vector<T> &host_vector) const {
   host_vector.resize(size_);
   copyTo(host_vector.data());
 }
 
+template <typename T> std::vector<T> CudaArray<T>::cpu() const {
+  if (!size_) {
+    return std::vector<T>{};
+  }
+  std::vector<T> host_vector(size_);
+  copyTo(host_vector.data());
+  return host_vector;
+}
+
 template <typename T> T *CudaArray<T>::getDataSafe(CudaContextPtr c) {
   context_->synchronizeWith(c);
   return values_;
 }
 
 #ifdef RPU_USE_DOUBLE
-template class CudaArray<double>;
 template class CudaArray<double *>;
+template class CudaArray<double>;
 #endif
-
 template class CudaArray<float>;
 template class CudaArray<float *>;
 
 template class CudaArray<int>;
 template class CudaArray<char>;
 template class CudaArray<int8_t>;
 template class CudaArray<uint32_t>;
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/cuda_util.h` & `aihwkit-0.8.0/src/rpucuda/cuda/cuda_util.h`

 * *Files 6% similar despite different names*

```diff
@@ -1,21 +1,22 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
  * that they have been altered from the originals.
  */
 
 #pragma once
 
+#include "cuda_buffer.h"
 #include "utility_functions.h"
 
 #include <iostream>
 #include <memory>
 #include <mutex>
 #include <queue>
 #include <string.h>
@@ -180,17 +181,17 @@
     CONTEXT->synchronize();                                                                        \
   }
 
 #define RPU_CUDA_1D_KERNEL_LOOP(i, n)                                                              \
   for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < (n); i += blockDim.x * gridDim.x)
 
 #define RPU_THREADS_PER_BLOCK 512
-#define RPU_THREADS_PER_BLOCK_UPDATE 512
+#define RPU_THREADS_PER_BLOCK_UPDATE 256
 #define RPU_MAX_RAND_STATE_SIZE 10000
-#define RPU_UPDATE_BLOCKS_PER_SM 1.1
+#define RPU_UPDATE_BLOCKS_PER_SM 8.1
 
 #define RPU_GEN_IITER_TEMPLATES(NUM_T, OUT_T, FUNC, ARGS)                                          \
   template OUT_T FUNC(const NUM_T *ARGS);                                                          \
   template OUT_T FUNC(NUM_T *ARGS);                                                                \
   template OUT_T FUNC(IndexReaderInputIterator<NUM_T> ARGS);                                       \
   template OUT_T FUNC(IndexReaderTransInputIterator<NUM_T> ARGS);                                  \
   template OUT_T FUNC(PermuterTransInputIterator<NUM_T> ARGS);                                     \
@@ -247,46 +248,18 @@
   cublasHandle_t *device_handle_ = nullptr;
   bool device_handle_created_ = false;
   void createDeviceHandle();
 #endif
 };
 
 template <typename T> class CudaArray;
+
 class CudaContext;
 typedef CudaContext *CudaContextPtr;
 
-template <typename T> class CudaBuffer {
-public:
-  CudaBuffer(){};
-  CudaBuffer(const CudaBuffer<T> &);
-  CudaBuffer &operator=(const CudaBuffer<T> &);
-  CudaBuffer(CudaBuffer<T> &&);
-  CudaBuffer<T> &operator=(CudaBuffer<T> &&);
-  ~CudaBuffer() = default;
-
-  friend void swap(CudaBuffer<T> &a, CudaBuffer<T> &b) noexcept {
-    using std::swap;
-    const std::lock_guard<std::recursive_mutex> locka(a.mutex_);
-    const std::lock_guard<std::recursive_mutex> lockb(b.mutex_);
-    swap(a.buffer_, b.buffer_);
-  }
-
-  T *get(CudaContextPtr context, int size);
-  void release();
-
-  const CudaArray<T> *getCudaArray() {
-    const std::lock_guard<std::recursive_mutex> lock(mutex_);
-    return &*buffer_;
-  };
-
-private:
-  std::unique_ptr<CudaArray<T>> buffer_ = nullptr;
-  std::recursive_mutex mutex_;
-};
-
 class CudaContext : public std::enable_shared_from_this<CudaContext>, public Context {
 
 public:
   explicit CudaContext() : CudaContext(-1){};
   // NOTE: not tested on gpu_id (does a streams implicitely specifies a GPU id?)
   explicit CudaContext(int gpu_id, bool non_blocking = true);
   explicit CudaContext(cudaStream_t shared_stream, int gpu_id = -1);
@@ -316,19 +289,20 @@
 
     swap(a.non_blocking_, b.non_blocking_);
     swap(a.event_, b.event_);
     swap(a.prop_, b.prop_);
 
     swap(a.shared_random_states_, b.shared_random_states_);
     swap(a.shared_float_buffer_, b.shared_float_buffer_);
+#ifdef RPU_USE_DOUBLE
     swap(a.shared_double_buffer_, b.shared_double_buffer_);
-
+    swap(a.double_buffer_, b.double_buffer_);
+#endif
     swap(a.random_states_, b.random_states_);
     swap(a.float_buffer_, b.float_buffer_);
-    swap(a.double_buffer_, b.double_buffer_);
   }
 
   void synchronizeDevice() const;
   void synchronizeContext()
       const; // synchronizes whole context (if multiple streams are used explicitly)
   void synchronizeStream(int idx) const; // individual stream
   void synchronizeStream() const;        // current stream
@@ -398,18 +372,19 @@
   bool non_blocking_ = true;
   cudaEvent_t event_ = nullptr;
   cudaDeviceProp *prop_ = nullptr;
 
   std::vector<std::unique_ptr<CudaArray<curandState_t>>> shared_random_states_ = {};
   std::vector<std::unique_ptr<CudaArray<curandState_t>>> random_states_ = {};
   std::vector<std::vector<CudaBuffer<float>>> shared_float_buffer_ = {};
-  std::vector<std::vector<CudaBuffer<double>>> shared_double_buffer_ = {};
   std::vector<std::vector<CudaBuffer<float>>> float_buffer_ = {};
+#ifdef RPU_USE_DOUBLE
+  std::vector<std::vector<CudaBuffer<double>>> shared_double_buffer_ = {};
   std::vector<std::vector<CudaBuffer<double>>> double_buffer_ = {};
-
+#endif
   void init();
 };
 
 template <typename T> class CudaArray {
 
 public:
   CudaArray(){};
@@ -444,14 +419,15 @@
   inline void assign(const std::shared_ptr<CudaArray<T>> source) { this->assign(*source); };
 
   void assignFromDevice(const T *device_array);
   void setShared(T *device_array);
 
   void copyTo(T *host_array) const;
   void copyTo(std::vector<T> &host_vector) const;
+  std::vector<T> cpu() const;
 
   void setConst(T set_value);
 
   T *getDataSafe(CudaContextPtr c);
   inline CudaContextPtr getContext() const { return context_; };
   inline void synchronize() { context_->synchronize(); };
 
@@ -475,19 +451,23 @@
   size_t pitch_ = 0;
   int width_ = 0;
   int height_ = 0;
 
   CudaContextPtr context_ = nullptr;
 };
 
+/****************************************************/
 void resetCuda(int gpu_id = -1);
 
-// helper for random init
 void curandSetup(CudaArray<curandState_t> &, unsigned long long rseed = 0, bool same_seed = false);
 void curandSetup(
     CudaContextPtr c,
     std::unique_ptr<CudaArray<curandState_t>> &dev_states,
     int n,
     unsigned long long rseed = 0,
     bool same_seed = false);
 
+/* state helper functions*/
+template <typename T>
+void load(CudaContextPtr context, RPU::state_t &state, std::string key, T &value, bool strict);
+
 } // namespace RPU
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/forward_backward_pass.cu` & `aihwkit-0.8.0/src/rpucuda/cuda/forward_backward_pass.cu`

 * *Files 11% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -200,14 +200,18 @@
   return *this;
 }
 
 template <typename T>
 void ForwardBackwardPassIOManagedCuda<T>::populateFrom(const FBParameter<T> &fb_pars_host) {
 
   auto populate = [this](MVParameterCuda<T> &mv_pars, const MVParameter<T> &mv_pars_host) -> void {
+    if (mv_pars_host.out_noise_values.size() > (size_t)0) {
+      mv_pars.out_noise_values = CudaArray<T>(this->context_, mv_pars_host.out_noise_values);
+    }
+
     if (mv_pars_host.v_offset.size() > (size_t)0) {
       mv_pars.v_offset = CudaArray<T>(this->context_, mv_pars_host.v_offset);
     }
     if (mv_pars_host.out_nonlinearity.size() > (size_t)0) {
       mv_pars.out_nonlinearity = CudaArray<T>(this->context_, mv_pars_host.out_nonlinearity);
     }
     mv_pars.out_nonlinearity_factor = mv_pars_host.out_nonlinearity_factor;
@@ -220,14 +224,56 @@
     this->context_->synchronize();
   };
   populate(fb_pars_.fwd, fb_pars_host.fwd);
   populate(fb_pars_.bwd, fb_pars_host.bwd);
 }
 
 template <typename T>
+void ForwardBackwardPassIOManagedCuda<T>::dumpExtra(RPU::state_t &extra, const std::string prefix) {
+  RPU::state_t state;
+
+  RPU::insert(state, "fwd.v_offset", fb_pars_.fwd.v_offset);
+  RPU::insert(state, "fwd.w_asymmetry", fb_pars_.fwd.w_asymmetry);
+  RPU::insert(state, "fwd.out_nonlinearity", fb_pars_.fwd.out_nonlinearity);
+  RPU::insert(state, "fwd.out_nonlinearity_factor", fb_pars_.fwd.out_nonlinearity_factor);
+  RPU::insert(state, "fwd.out_noise_values", fb_pars_.fwd.out_noise_values);
+
+  RPU::insert(state, "bwd.v_offset", fb_pars_.bwd.v_offset);
+  RPU::insert(state, "bwd.w_asymmetry", fb_pars_.bwd.w_asymmetry);
+  RPU::insert(state, "bwd.out_nonlinearity", fb_pars_.bwd.out_nonlinearity);
+  RPU::insert(state, "bwd.out_nonlinearity_factor", fb_pars_.bwd.out_nonlinearity_factor);
+  RPU::insert(state, "bwd.out_noise_values", fb_pars_.bwd.out_noise_values);
+
+  RPU::insertWithPrefix(extra, state, prefix);
+}
+
+template <typename T>
+void ForwardBackwardPassIOManagedCuda<T>::loadExtra(
+    const RPU::state_t &extra, const std::string prefix, bool strict) {
+
+  using V = std::vector<T>;
+  auto state = RPU::selectWithPrefix(extra, prefix);
+  V tmp;
+
+  // forward
+  RPU::load(this->context_, state, "fwd.v_offset", fb_pars_.fwd.v_offset, strict);
+  RPU::load(this->context_, state, "fwd.w_asymmetry", fb_pars_.fwd.w_asymmetry, strict);
+  RPU::load(this->context_, state, "fwd.out_nonlinearity", fb_pars_.fwd.out_nonlinearity, strict);
+  RPU::load(state, "fwd.out_nonlinearity_factor", fb_pars_.fwd.out_nonlinearity_factor, strict);
+  RPU::load(this->context_, state, "fwd.out_noise_values", fb_pars_.fwd.out_noise_values, strict);
+
+  // backward
+  RPU::load(this->context_, state, "bwd.v_offset", fb_pars_.bwd.v_offset, strict);
+  RPU::load(this->context_, state, "bwd.w_asymmetry", fb_pars_.bwd.w_asymmetry, strict);
+  RPU::load(this->context_, state, "bwd.out_nonlinearity", fb_pars_.bwd.out_nonlinearity, strict);
+  RPU::load(state, "bwd.out_nonlinearity_factor", fb_pars_.bwd.out_nonlinearity_factor, strict);
+  RPU::load(this->context_, state, "bwd.out_noise_values", fb_pars_.bwd.out_noise_values, strict);
+}
+
+template <typename T>
 void ForwardBackwardPassIOManagedCuda<T>::applyOutputWeightNoise(
     InputOutputManager<T> &iom, const bool out_trans, const bool tranposed) {
 
   auto io = iom.getIO();
   if (io.w_noise <= 0) {
     return;
   }
@@ -348,14 +394,67 @@
       context_->getRandomStates(MIN(nblocks_om_batch * nthreads, out_size * m_batch)));
 
   context->template releaseSharedBuffer<T>(RPU_BUFFER_TEMP_2);
   context->template releaseSharedBuffer<T>(RPU_BUFFER_TEMP_3);
   context->template releaseSharedBuffer<T>(RPU_BUFFER_TEMP_1);
 }
 
+/* output dtod noise*/
+template <typename T>
+__global__ void kernelOutputNoiseOtoOBatch(
+    T *output,
+    const int out_size,
+    const bool trans, // true if m_batch first dimensions
+    const int m_batch,
+    const T *out_noise_values,
+    curandState *random_states) {
+  const int tid = blockDim.x * blockIdx.x + threadIdx.x;
+  T stoch_value;
+  curandState local_state;
+  const int total_threads = blockDim.x * gridDim.x;
+  const int total_size = m_batch * out_size;
+  int total_states = MIN(total_size, total_threads);
+  if (tid < total_states) {
+    local_state = random_states[tid];
+  }
+  for (int idx = tid; idx < total_size; idx += total_threads) {
+    T value = output[idx];
+    const int out_idx = trans ? (idx / m_batch) : (idx % out_size);
+    const T noise_std = out_noise_values[out_idx];
+    stoch_value = curand_normal(&local_state);
+    value += stoch_value * noise_std;
+    output[idx] = value;
+  }
+  if (tid < total_states) {
+    random_states[tid] = local_state;
+  }
+}
+
+template <typename T>
+void ForwardBackwardPassIOManagedCuda<T>::applyOutputNoiseOtoO(
+    InputOutputManager<T> &iom,
+    const MVParameterCuda<T> &mv_pars,
+    const bool out_trans,
+    const bool tranposed) {
+
+  auto io = iom.getIO();
+  auto m_batch = iom.getMBatch();
+  auto context = iom.getContext();
+  auto out_size = iom.getOutSize();
+  auto nblocks_om_batch = iom.getOutBlocksBatch(m_batch);
+  auto nthreads = iom.getNThreads();
+
+  // out buffer is out_size_ x m_batch or m_batch x out_size_ (if out_trans)
+  T *out_temp = iom.getOutBuffer();
+
+  kernelOutputNoiseOtoOBatch<<<nblocks_om_batch, nthreads, 0, context->getStream()>>>(
+      out_temp, out_size, out_trans, m_batch, mv_pars.out_noise_values.getDataConst(),
+      context_->getRandomStates(MIN(nblocks_om_batch * nthreads, out_size * m_batch)));
+}
+
 /* output non-linearity*/
 template <typename T>
 __global__ void kernelOutputNonLinearityBatch(
     T *output,
     const int size,
     const bool trans, // true if m_batch first dimensions
     const int m_batch,
@@ -385,15 +484,14 @@
   if (!io.hasNLCalibration()) {
     return;
   }
 
   auto m_batch = iom.getMBatch();
   auto context = iom.getContext();
   auto out_size = iom.getOutSize();
-  auto nblocks_om = iom.getOutBlocks();
   auto nblocks_om_batch = iom.getOutBlocksBatch(m_batch);
   auto nthreads = iom.getNThreads();
 
   // out buffer is out_size_ x m_batch or m_batch x out_size_ (if out_trans)
   T *out_temp = iom.getOutBuffer();
 
   kernelOutputNonLinearityBatch<<<nblocks_om_batch, nthreads, 0, context->getStream()>>>(
@@ -410,15 +508,14 @@
   }
 
   auto m_batch = iom.getMBatch();
   auto in_trans = iom.getInTrans();
   auto context = iom.getContext();
   auto in_size = iom.getInSize();
   auto out_size = iom.getOutSize();
-  auto nblocks_om = iom.getOutBlocks();
   auto nblocks_om_batch = iom.getOutBlocksBatch(m_batch);
   auto nthreads = iom.getNThreads();
 
   // computes for each output (row):
   //
   // a_i = sum_j(|w_ij|*|x_j|)*n/Gw*gmax
   // c_i = a_i*(a_i*(0.05*a_i - 0.2) + 0.5);
@@ -516,15 +613,14 @@
   }
 
   auto m_batch = iom.getMBatch();
   auto in_trans = iom.getInTrans();
   auto context = iom.getContext();
   auto in_size = iom.getInSize();
   auto out_size = iom.getOutSize();
-  auto nblocks_om = iom.getOutBlocks();
   auto nblocks_om_batch = iom.getOutBlocksBatch(m_batch);
   auto nthreads = iom.getNThreads();
 
   // out buffer is out_size_ x m_batch or m_batch x out_size_ (if out_trans)
   const T *in_temp = iom.getInBuffer();
   T *out_temp = iom.getOutBuffer();
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/forward_backward_pass.h` & `aihwkit-0.8.0/src/rpucuda/cuda/forward_backward_pass.h`

 * *Files 3% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -95,21 +95,23 @@
 };
 
 } // namespace detail
 
 template <typename T> class MVParameterCuda {
 public:
   MVParameterCuda(){};
+  CudaArray<T> out_noise_values;
   CudaArray<T> v_offset;
   CudaArray<T> w_asymmetry;
   CudaArray<T> out_nonlinearity;
   T out_nonlinearity_factor = 0.0;
 
   friend void swap(MVParameterCuda<T> &a, MVParameterCuda<T> &b) noexcept {
     using std::swap;
+    swap(a.out_noise_values, b.out_noise_values);
     swap(a.v_offset, b.v_offset);
     swap(a.w_asymmetry, b.w_asymmetry);
     swap(a.out_nonlinearity_factor, b.out_nonlinearity_factor);
     swap(a.out_nonlinearity, b.out_nonlinearity);
   }
 };
 
@@ -143,14 +145,16 @@
   swap(ForwardBackwardPassIOManagedCuda<T> &a, ForwardBackwardPassIOManagedCuda<T> &b) noexcept {
     using std::swap;
     swap(a.x_size_, b.x_size_);
     swap(a.d_size_, b.d_size_);
     swap(a.context_, b.context_);
     swap(a.fb_pars_, b.fb_pars_);
   }
+  void dumpExtra(RPU::state_t &extra, const std::string prefix);
+  void loadExtra(const RPU::state_t &extra, const std::string prefix, bool strict);
 
   void populateFrom(const FBParameter<T> &fb_pars_host);
   bool checkFlexibleInSize(const IOMetaParameter<T> &io) {
     // TODO: need to check whether that is all correct...
     if (io.w_noise > 0 || io.ir_drop > 0 || io.w_read_asymmetry_dtod > 0) {
       return false;
     } else {
@@ -291,16 +295,24 @@
   int x_size_ = 0;
   int d_size_ = 0;
   CudaContextPtr context_ = nullptr;
 
 private:
   inline void
   applyOutputWeightNoise(InputOutputManager<T> &iom, const bool out_trans, const bool tranposed);
+
+  inline void applyOutputNoiseOtoO(
+      InputOutputManager<T> &iom,
+      const MVParameterCuda<T> &mv_pars,
+      const bool out_trans,
+      const bool tranposed);
+
   inline void applyOutputPCMReadNoise(
       const T *dev_weights, InputOutputManager<T> &iom, const bool out_trans, const bool tranposed);
+
   inline void applyIrDrop(
       const T *dev_weights,
       InputOutputManager<T> &iom,
       const bool out_trans,
       const bool transposed);
 
   template <typename OutputIteratorT>
@@ -310,14 +322,18 @@
       const MVParameterCuda<T> &mv_pars,
       const bool out_trans,
       const bool transposed) {
     auto io = iom.getIO();
     if (io.hasNLCalibration()) {
       applyOutputNonLinearity(iom, mv_pars, out_trans, transposed);
     }
+    if (io.out_noise_std > (T)0.0) {
+      applyOutputNoiseOtoO(iom, mv_pars, out_trans, transposed);
+      return iom.applyToOutput(out_values, out_trans, false);
+    }
     return iom.applyToOutput(out_values, out_trans);
   }
 
   void applyOutputNonLinearity(
       InputOutputManager<T> &iom,
       const MVParameterCuda<T> &mv_pars,
       const bool out_trans,
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/forward_backward_pass_test.cpp` & `aihwkit-0.8.0/src/rpucuda/cuda/forward_backward_pass_test.cpp`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/io_iterator.h` & `aihwkit-0.8.0/src/rpucuda/cuda/io_iterator.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/io_iterator_test.cpp` & `aihwkit-0.8.0/src/rpucuda/cuda/io_iterator_test.cpp`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/io_manager.cu` & `aihwkit-0.8.0/src/rpucuda/cuda/io_manager.cu`

 * *Files 6% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -451,18 +451,18 @@
     T value = input[idx];
 
     if (noise_management) {
       int bidx = trans ? (idx % m_batch) : (idx / size);
       local_nm_scale = nm_scale_values[bidx];
     }
 
-    APPLY_ASYMMETRY;
-
     ADD_NOISE;
 
+    APPLY_ASYMMETRY;
+
     DISCRETIZE_VALUE_STOCH;
 
     BOUND_CHECK;
 
     if (noise_management) {
       APPLY_OUTPUT_NOISE_MANAGMENT(local_nm_scale);
     }
@@ -501,18 +501,18 @@
   const bool test_neg = bm_test_negative_bound;
 
   STOCH_DEFINITIONS(stoch_if, size);
 
   STRIDE_LOOP(
       size, value * osc,
 
-      APPLY_ASYMMETRY;
-
       ADD_NOISE;
 
+      APPLY_ASYMMETRY;
+
       DISCRETIZE_VALUE_STOCH;
 
       if (value > bu) {
         value = bu;
         exceeded++;
       }
 
@@ -571,18 +571,18 @@
 
   STRIDE_LOOP(
       total_size, value * osc,
 
       int sidx = trans ? (idx % m_batch) : (idx / size);
       T local_scale = scale_values[sidx];
 
-      APPLY_ASYMMETRY;
-
       ADD_NOISE;
 
+      APPLY_ASYMMETRY;
+
       DISCRETIZE_VALUE_STOCH;
 
       if (value > bu) {
         value = bu;
         exceeded_values_out[sidx] = 1;
         exceeded++;
       } if (value < bl) {
@@ -836,64 +836,65 @@
     }
   }
 }
 
 template <typename T>
 template <typename OutputIteratorT>
 bool InputOutputManager<T>::applyToOutputWithBoundManagement(
-    OutputIteratorT dev_output, const bool out_trans) {
+    OutputIteratorT dev_output, const bool out_trans, const bool with_out_noise) {
 
   cudaStream_t s = context_->getStream();
   int m_batch = temp_m_batch_;
   int nblocks_om_batch = getOutBlocksBatch(m_batch);
 
   // actual bound management
   if (m_batch == RPU_IO_USE_SINGLE_BATCH_VERSION) {
     int nblocks_om = getOutBlocks();
     kernelOutputBoundManagement<<<nblocks_om, nthreads_, 0, s>>>(
-        dev_output, temp_output_applied_, out_size_, io_->out_noise, dev_scale_values_->getData(),
-        -io_->out_bound, io_->out_bound, io_->out_res, io_->out_sto_round, temp_out_scale_,
-        io_->bm_test_negative_bound, io_->out_asymmetry,
+        dev_output, temp_output_applied_, out_size_, with_out_noise ? io_->out_noise : (T)0.0,
+        dev_scale_values_->getData(), -io_->out_bound, io_->out_bound, io_->out_res,
+        io_->out_sto_round, temp_out_scale_, io_->bm_test_negative_bound, io_->out_asymmetry,
         context_->getRandomStates(MIN(nblocks_om * nthreads_, out_size_)),
         dev_any_exceeded_->getData());
   } else {
 
     kernelOutputBoundManagementBatch<<<nblocks_om_batch, nthreads_, 0, s>>>(
         dev_output, temp_output_applied_, out_size_, m_batch, out_trans,
         dev_scale_values_->getData(),
         dev_bound_exceeded_->getData(), // out
         dev_any_exceeded_->getData(),   // out
-        io_->out_noise, -io_->out_bound, io_->out_bound, io_->out_res, io_->out_sto_round,
-        temp_out_scale_, io_->bm_test_negative_bound, io_->out_asymmetry,
+        with_out_noise ? io_->out_noise : (T)0.0, -io_->out_bound, io_->out_bound, io_->out_res,
+        io_->out_sto_round, temp_out_scale_, io_->bm_test_negative_bound, io_->out_asymmetry,
         context_->getRandomStates(MIN(nblocks_om_batch * nthreads_, out_size_ * m_batch)));
   }
 
   dev_any_exceeded_->copyTo(h_exceeded_);
   return (
       ((*h_exceeded_) == 0) || (reduction_due_to_bound_management_ > io_->max_bm_factor) ||
       (io_->inp_res > 0 && reduction_due_to_bound_management_ > io_->max_bm_res / io_->inp_res));
 }
 
 template <typename T>
 template <typename OutputIteratorT>
-bool InputOutputManager<T>::applyToOutput(OutputIteratorT dev_output, const bool out_trans) {
+bool InputOutputManager<T>::applyToOutput(
+    OutputIteratorT dev_output, const bool out_trans, const bool with_out_noise) {
 
   if (io_->isPerfect()) {
     // short-cut (still need to copy though to apply the iterator)
     int m_batch = temp_m_batch_;
     const T *tmp = getOutBuffer();
     RPU::math::copyWithIterator(context_, dev_output, tmp, m_batch * out_size_);
     return true;
   }
 
   // do the bound/ noise management (and ADC/DAC + out noise)
   if (io_->bound_management != BoundManagementType::None) {
 
     // bound management
-    return applyToOutputWithBoundManagement(dev_output, out_trans);
+    return applyToOutputWithBoundManagement(dev_output, out_trans, with_out_noise);
 
   } else {
 
     cudaStream_t s = context_->getStream();
     int m_batch = temp_m_batch_;
     int nblocks_om_batch =
         MIN(nblocks_batch_max_, this->context_->getNBlocks(out_size_ * m_batch, nthreads_));
@@ -901,34 +902,58 @@
     float *nm_scale_values = noise_manager_->getScaleValues();
 
     // no bound management
     if (m_batch == RPU_IO_USE_SINGLE_BATCH_VERSION) {
       int nblocks_om = MIN(context_->getNBlocks(out_size_, nthreads_), nblocks_batch_max_);
       LAUNCH_NM_KERNEL(
           kernelOutputManagement, OutputIteratorT, nblocks_om,
-          (dev_output, temp_output_applied_, out_size_, io_->out_noise, nm_scale_values,
-           -io_->out_bound, io_->out_bound, io_->out_res, io_->out_sto_round, temp_out_scale_,
-           io_->out_asymmetry, context_->getRandomStates(MIN(nblocks_om * nthreads_, out_size_))));
+          (dev_output, temp_output_applied_, out_size_, with_out_noise ? io_->out_noise : (T)0.0,
+           nm_scale_values, -io_->out_bound, io_->out_bound, io_->out_res, io_->out_sto_round,
+           temp_out_scale_, io_->out_asymmetry,
+           context_->getRandomStates(MIN(nblocks_om * nthreads_, out_size_))));
 
     } else {
       // batched
       LAUNCH_NM_KERNEL(
           kernelOutputManagementBatch, OutputIteratorT, nblocks_om_batch,
           (dev_output, temp_output_applied_, out_size_, m_batch, out_trans, nm_scale_values,
-           io_->out_noise, -io_->out_bound, io_->out_bound, io_->out_res, io_->out_sto_round,
-           temp_out_scale_, io_->out_asymmetry,
+           with_out_noise ? io_->out_noise : (T)0.0, -io_->out_bound, io_->out_bound, io_->out_res,
+           io_->out_sto_round, temp_out_scale_, io_->out_asymmetry,
            context_->getRandomStates(MIN(nblocks_om_batch * nthreads_, out_size_ * m_batch))));
     }
 
     return true; // no bound managment
   }
 }
 
+template <typename T>
+void InputOutputManager<T>::dumpExtra(RPU::state_t &extra, const std::string prefix) {
+  context_->synchronize();
+  RPU::state_t state;
+
+  noise_manager_->dumpExtra(state, "noise_manager");
+  // output_maximizer_->dumpExtra(state, "output_maximizer");
+
+  // will not handle buffers in to store data between applyToInput and applyToOutput
+  RPU::insertWithPrefix(extra, state, prefix);
+}
+
+template <typename T>
+void InputOutputManager<T>::loadExtra(
+    const RPU::state_t &extra, const std::string prefix, bool strict) {
+  context_->synchronize();
+
+  auto state = RPU::selectWithPrefix(extra, prefix);
+
+  noise_manager_->loadExtra(state, "noise_manager", strict);
+  // output_maximizer_->loadExtra(state, "output_maximizer", strict);
+}
+
 // init necessary templates..
-#define OARG(NUM_T) , const bool
+#define OARG(NUM_T) , const bool, const bool
 #define IARG(NUM_T)                                                                                \
   , const IOMetaParameter<NUM_T> &, const int, const int, const bool, const NUM_T, const bool
 
 template class InputOutputManager<float>;
 RPU_GEN_IITER_TEMPLATES(float, void, InputOutputManager<float>::applyToInput, );
 RPU_GEN_IITER_TEMPLATES(float, void, InputOutputManager<float>::initWithInput, IARG(float));
 RPU_GEN_OITER_TEMPLATES(float, bool, InputOutputManager<float>::applyToOutput, OARG(float));
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/io_manager.h` & `aihwkit-0.8.0/src/rpucuda/cuda/io_manager.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -40,15 +40,16 @@
       const bool trans = false,
       const T add_out_scale = 1.0,
       const bool is_test = false); // additional output scaling (takes add_out_scale*io->out_scale)
 
   template <typename InputIteratorT> void applyToInput(InputIteratorT dev_input);
 
   template <typename OutputIteratorT>
-  bool applyToOutput(OutputIteratorT dev_output, const bool trans = false);
+  bool applyToOutput(
+      OutputIteratorT dev_output, const bool trans = false, const bool with_out_noise = true);
 
   // only valid after init
   inline T *getInBuffer() const { return temp_input_applied_; };
   inline T *getOutBuffer() const { return temp_output_applied_; };
 
   // careful, no checks, mem not owned or deleted!
   inline void setInBuffer(T *in_buffer) { temp_input_applied_ = in_buffer; };
@@ -62,18 +63,21 @@
     CudaArray<T> tmp(context_, getMBatch() * getInSize());
     tmp.assignFromDevice(getInBuffer());
     tmp.copyTo(host_array);
     context_->synchronize();
   }
   void copyExceededArrayToHost(int *host_array) { dev_bound_exceeded_->copyTo(host_array); }
 
-  bool applyToOutputInPlace(T *dev_output, const bool trans = false) {
+  bool
+  applyToOutputInPlace(T *dev_output, const bool trans = false, const bool with_out_noise = true) {
     RPU::math::copy<T>(context_, getMBatch() * getOutSize(), dev_output, 1, getOutBuffer(), 1);
-    return applyToOutput(dev_output, trans);
+    return applyToOutput(dev_output, trans, with_out_noise);
   }
+  void dumpExtra(RPU::state_t &extra, const std::string prefix);
+  void loadExtra(const RPU::state_t &extra, const std::string prefix, bool strict);
 
   NoiseManager<T> *getNM() { return &*noise_manager_; };
 
   inline int getInSize() const { return temp_in_size_; };
   inline int getOutSize() const { return out_size_; };
   inline bool getInTrans() const { return temp_trans_; };
   inline int getMBatch() const { return temp_m_batch_; };
@@ -102,15 +106,16 @@
   void initializeBatchBuffer(int m_batch);
   void setInSize(int in_size);
   void setOutSize(int out_size);
 
   template <typename InputIteratorT> void applyToInputWithBoundManagement(InputIteratorT dev_input);
 
   template <typename OutputIteratorT>
-  bool applyToOutputWithBoundManagement(OutputIteratorT dev_output, const bool trans);
+  bool applyToOutputWithBoundManagement(
+      OutputIteratorT dev_output, const bool trans, const bool with_out_noise = true);
 
   CudaContextPtr context_ = nullptr;
   int in_size_ = 0;
   int out_size_ = 0;
   std::unique_ptr<NoiseManager<T>> noise_manager_ = nullptr;
   std::unique_ptr<Maximizer<T>> output_maximizer_ = nullptr;
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/io_manager_test.cpp` & `aihwkit-0.8.0/src/rpucuda/cuda/io_manager_test.cpp`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/maximizer.cu` & `aihwkit-0.8.0/src/rpucuda/cuda/maximizer.cu`

 * *Files 5% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -176,31 +176,33 @@
   for (int i = 0; i < size * m_batch; i++) {
     tmp[i] = i + 1;
   }
   CudaArray<int> dev_in_index(c, size * m_batch, tmp);
   CUDA_CALL(cudaDeviceSynchronize());
 
   IndexReader<T> idx_reader(dev_in.getData());
-  RPU::cub::TransformInputIterator<T, IndexReader<T>, int *> in_itr(
+  RPU_CUB_NS_QUALIFIER TransformInputIterator<T, IndexReader<T>, int *> in_itr(
       dev_in_index.getData(), idx_reader);
 
-  RPU::cub::CountingInputIterator<int> index(0);
+  RPU_CUB_NS_QUALIFIER CountingInputIterator<int> index(0);
   BatchTransposer<T> batch_transposer(dev_in.getData(), size, m_batch);
-  RPU::cub::TransformInputIterator<T, BatchTransposer<T>, RPU::cub::CountingInputIterator<int>>
+  RPU_CUB_NS_QUALIFIER
+  TransformInputIterator<T, BatchTransposer<T>, RPU_CUB_NS_QUALIFIER CountingInputIterator<int>>
       in_trans_itr(index, batch_transposer);
 
   IndexReader<int> idx_reader_host(tmp);
-  RPU::cub::TransformInputIterator<int, IndexReader<int>, int *> test_host(tmp, idx_reader_host);
+  RPU_CUB_NS_QUALIFIER TransformInputIterator<int, IndexReader<int>, int *> test_host(
+      tmp, idx_reader_host);
   std::cout << test_host[0] << std::endl;
 
   CustomMaxAbs max_abs;
   // Determine temporary device storage requirements
   void *d_temp_storage = NULL;
   size_t temp_storage_bytes = 0;
-  RPU::cub::DeviceSegmentedReduce::Reduce(
+  RPU_CUB_NS_QUALIFIER DeviceSegmentedReduce::Reduce(
       d_temp_storage, temp_storage_bytes, in_itr, dev_max_values.getData(), m_batch,
       dev_offsets.getData(), dev_offsets.getData() + 1, max_abs, 0, c->getStream());
   // Allocate temporary storage
   cudaMalloc(&d_temp_storage, temp_storage_bytes);
   CUDA_CALL(cudaDeviceSynchronize());
 
   int nthreads = c->getNThreads();
@@ -209,15 +211,15 @@
 
   CUDA_TIMING_INIT;
   CUDA_TIMING_START(c);
 
   if (trans) {
 
     // this works, too, but has some performance hit, because of non-aligned memory reads
-    // RPU::cub::DeviceSegmentedReduce::Reduce(d_temp_storage, temp_storage_bytes,
+    // RPU_CUB_NS_QUALIFIER DeviceSegmentedReduce::Reduce(d_temp_storage, temp_storage_bytes,
     // 				   in_trans_itr, dev_max_values.getData(),
     // 				   m_batch, dev_offsets.getData(),
     // 				   dev_offsets.getData()+1, max_abs,0,c.getStream());
 
     if (m_batch > nthreads) {
       kernelMaximizeBatchTrans_LargeBatch<<<nblocks, nthreads, 0, s>>>(
           in_itr, size * m_batch, m_batch, dev_max_values.getData(), dev_max_values0.getData());
@@ -226,15 +228,15 @@
       kernelMaximizeBatchTrans<<<nblocks, nthreads, nthreads * sizeof(int), s>>>(
           in_itr, size * m_batch, m_batch, dev_max_values.getData(), dev_max_values0.getData());
     }
 
   } else {
     // only trans==false
     // Fast Segmented reduction (much faster than loop from outside)
-    RPU::cub::DeviceSegmentedReduce::Reduce(
+    RPU_CUB_NS_QUALIFIER DeviceSegmentedReduce::Reduce(
         d_temp_storage, temp_storage_bytes, in_itr, dev_max_values.getData(), m_batch,
         dev_offsets.getData(), dev_offsets.getData() + 1, max_abs, 0, c->getStream());
   }
 
   CUDA_TIMING_STOP(c, "Max Batch");
 
   CUDA_CALL(cudaPeekAtLastError());
@@ -264,19 +266,19 @@
 template <typename T>
 Maximizer<T>::Maximizer(CudaContextPtr c, int size, bool abs_if)
     : size_{size}, context_{c}, buffer_m_batch_{0}, abs_if_{abs_if} {
   // initialize for m_batch=1
   dev_max_values_ = RPU::make_unique<CudaArray<float>>(context_, 1);
   size_t temp_storage_bytes = 0;
   if (abs_if_) {
-    RPU::cub::DeviceReduce::Reduce(
+    RPU_CUB_NS_QUALIFIER DeviceReduce::Reduce(
         nullptr, temp_storage_bytes, dev_max_values_->getData(), dev_max_values_->getData(), size_,
-        max_abs_op_, 0, context_->getStream());
+        max_abs_op_, (T)0, context_->getStream());
   } else {
-    RPU::cub::DeviceReduce::Max(
+    RPU_CUB_NS_QUALIFIER DeviceReduce::Max(
         nullptr, temp_storage_bytes, dev_max_values_->getData(), dev_max_values_->getData(), size_,
         context_->getStream());
   }
 
   dev_v_temp_storage_ = RPU::make_unique<CudaArray<char>>(context_, temp_storage_bytes);
 }
 
@@ -296,28 +298,27 @@
       offsets[i] = i * size_;
     }
 
     dev_offsets_ = RPU::make_unique<CudaArray<int>>(context_, m_batch + 1, offsets);
 
     size_t temp_storage_bytes = 0;
     if (abs_if_) {
-      RPU::cub::DeviceSegmentedReduce::Reduce(
+      RPU_CUB_NS_QUALIFIER DeviceSegmentedReduce::Reduce(
           nullptr, temp_storage_bytes, dev_max_values_->getData(), dev_max_values_->getData(),
           m_batch, dev_offsets_->getData(), dev_offsets_->getData() + 1, max_abs_op_, 0,
           context_->getStream());
     } else {
-      RPU::cub::DeviceSegmentedReduce::Max(
+      RPU_CUB_NS_QUALIFIER DeviceSegmentedReduce::Max(
           nullptr, temp_storage_bytes, dev_max_values_->getData(), dev_max_values_->getData(),
           m_batch, dev_offsets_->getData(), dev_offsets_->getData() + 1, context_->getStream());
     }
     dev_m_temp_storage_ = RPU::make_unique<CudaArray<char>>(context_, temp_storage_bytes);
 
     context_->synchronize();
     delete[] offsets;
-    // dev_offsets_->printValues();
   }
 }
 
 template <typename T> void Maximizer<T>::setZeroBelow(T thres) {
   RPU::math::elemsetbelowzero(
       context_, dev_max_values_->getData(), dev_max_values_->getSize(), (float)thres);
 }
@@ -333,19 +334,19 @@
 
   // does not check for positive m_batch!
   cudaStream_t s = context_->getStream();
 
   if (m_batch == 1) {
     size_t ssz = dev_v_temp_storage_->getSize();
     if (abs_if_) {
-      RPU::cub::DeviceReduce::Reduce(
+      RPU_CUB_NS_QUALIFIER DeviceReduce::Reduce(
           (void *)dev_v_temp_storage_->getData(), ssz, dev_input, dev_max_values_->getData(), size_,
           max_abs_op_, (T)0, s);
     } else {
-      RPU::cub::DeviceReduce::Max(
+      RPU_CUB_NS_QUALIFIER DeviceReduce::Max(
           (void *)dev_v_temp_storage_->getData(), ssz, dev_input, dev_max_values_->getData(), size_,
           s);
     }
 
   } else {
 
     if (buffer_m_batch_ < m_batch) {
@@ -373,19 +374,19 @@
       }
 
     } else {
 
       // Fast Segmented reduction (much faster than loop from outside)
       size_t ssz = dev_m_temp_storage_->getSize();
       if (abs_if_) {
-        RPU::cub::DeviceSegmentedReduce::Reduce(
+        RPU_CUB_NS_QUALIFIER DeviceSegmentedReduce::Reduce(
             (void *)dev_m_temp_storage_->getData(), ssz, dev_input, dev_max_values_->getData(),
             m_batch, dev_offsets_->getData(), dev_offsets_->getData() + 1, max_abs_op_, (T)0.0, s);
       } else {
-        RPU::cub::DeviceSegmentedReduce::Max(
+        RPU_CUB_NS_QUALIFIER DeviceSegmentedReduce::Max(
             (void *)dev_m_temp_storage_->getData(), ssz, dev_input, dev_max_values_->getData(),
             m_batch, dev_offsets_->getData(), dev_offsets_->getData() + 1, s);
       }
     }
   }
 }
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/maximizer.h` & `aihwkit-0.8.0/src/rpucuda/cuda/maximizer.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/maximizer_test.cpp` & `aihwkit-0.8.0/src/rpucuda/cuda/maximizer_test.cpp`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/noise_manager.cu` & `aihwkit-0.8.0/src/rpucuda/cuda/noise_manager.cu`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -207,17 +207,17 @@
   dev_ravg_scale_value_->setConst(1.0);
   dev_nzeros_value_ = RPU::make_unique<CudaArray<float>>(context_, 1);
 
   amaximizer_ = RPU::make_unique<Maximizer<T>>(context_, size, true);
   maximizer_ = RPU::make_unique<Maximizer<T>>(context_, size, false);
 
   size_t temp_storage_bytes = 0;
-  RPU::cub::DeviceReduce::Reduce(
+  RPU_CUB_NS_QUALIFIER DeviceReduce::Reduce(
       nullptr, temp_storage_bytes, dev_psum_values_->getData(), dev_psum_values_->getData(), size_,
-      nsum_op_, 0, context_->getStream());
+      nsum_op_, (T)0.0, context_->getStream());
 
   dev_v_temp_storage_ = RPU::make_unique<CudaArray<char>>(context_, temp_storage_bytes);
   context_->synchronize();
 }
 
 template <typename T> void NoiseManager<T>::initializeBatchBuffer(int m_batch) {
   // this inits all the buffers needed for PMSum only !!
@@ -239,37 +239,76 @@
     for (int i = 0; i <= m_batch; i++) {
       offsets[i] = i * size_;
     }
 
     dev_offsets_ = RPU::make_unique<CudaArray<int>>(context_, m_batch + 1, offsets);
 
     size_t temp_storage_bytes = 0;
-    RPU::cub::DeviceSegmentedReduce::Reduce(
+    RPU_CUB_NS_QUALIFIER DeviceSegmentedReduce::Reduce(
         nullptr, temp_storage_bytes, dev_psum_values_->getData(), dev_psum_values_->getData(),
         m_batch, dev_offsets_->getData(), dev_offsets_->getData() + 1, psum_op_, 0,
         context_->getStream());
     dev_m_temp_storage_ = RPU::make_unique<CudaArray<char>>(context_, temp_storage_bytes);
 
     const_set_if_ = false;
     context_->synchronize();
     delete[] offsets;
   }
 }
 
 template <typename T>
+void NoiseManager<T>::dumpExtra(RPU::state_t &extra, const std::string prefix) {
+
+  using V = std::vector<T>;
+  RPU::state_t state;
+  context_->synchronize();
+
+  RPU::insert(state, "ravg_initialized", ravg_initialized_);
+  RPU::insert(state, "const_set_if", const_set_if_);
+
+  RPU::insert(state, "dev_ravg_scale_value", dev_ravg_scale_value_);
+  RPU::insert(state, "dev_scale_values", dev_scale_values_);
+  RPU::insert(state, "dev_nzeros_value", dev_nzeros_value_);
+
+  // amaximizer_->dumpExtra(state, "amaximizer");
+  // maximizer_->dumpExtra(state, "maximizer");
+
+  // will not handle buffers in to store data between applyToInput and applyToOutput
+
+  RPU::insertWithPrefix(extra, state, prefix);
+}
+
+template <typename T>
+void NoiseManager<T>::loadExtra(const RPU::state_t &extra, const std::string prefix, bool strict) {
+
+  context_->synchronize();
+  auto state = RPU::selectWithPrefix(extra, prefix);
+
+  // amaximizer_->loadExtra(state, "amaximizer", strict);
+  // maximizer_->loadExtra(state, "maximizer", strict);
+
+  RPU::load(state, "ravg_initialized", ravg_initialized_, strict);
+  RPU::load(state, "const_set_if", const_set_if_, strict);
+
+  RPU::load(context_, state, "dev_ravg_scale_value", dev_ravg_scale_value_, strict);
+  RPU::load(context_, state, "dev_scale_values", dev_scale_values_, strict);
+  RPU::load(context_, state, "dev_nzeros_value", dev_nzeros_value_, strict);
+}
+
+template <typename T>
 template <typename InputIteratorT>
 void NoiseManager<T>::computeNPSum(InputIteratorT dev_input, int m_batch, bool trans) {
   cudaStream_t s = context_->getStream();
 
   if (m_batch == 1) {
     size_t ssz = dev_v_temp_storage_->getSize();
-    RPU::cub::DeviceReduce::Reduce(
+    RPU_CUB_NS_QUALIFIER DeviceReduce::Reduce(
         (void *)dev_v_temp_storage_->getData(), ssz, dev_input, dev_psum_values_->getData(), size_,
         psum_op_, (T)0, s);
-    RPU::cub::DeviceReduce::Reduce(
+    RPU_CUB_NS_QUALIFIER DeviceReduce::Reduce(
         (void *)dev_v_temp_storage_->getData(), ssz, dev_input, dev_nsum_values_->getData(), size_,
         nsum_op_, (T)0, s);
 
   } else {
 
     if (buffer_m_batch_ < m_batch) {
       this->initializeBatchBuffer(m_batch);
@@ -299,18 +338,18 @@
              dev_psum_values0_->getData(), dev_nsum_values0_->getData()));
       }
 
     } else {
 
       // Fast Segmented reduction
       size_t ssz = dev_m_temp_storage_->getSize();
-      RPU::cub::DeviceSegmentedReduce::Reduce(
+      RPU_CUB_NS_QUALIFIER DeviceSegmentedReduce::Reduce(
           (void *)dev_m_temp_storage_->getData(), ssz, dev_input, dev_psum_values_->getData(),
           m_batch, dev_offsets_->getData(), dev_offsets_->getData() + 1, psum_op_, (T)0.0, s);
-      RPU::cub::DeviceSegmentedReduce::Reduce(
+      RPU_CUB_NS_QUALIFIER DeviceSegmentedReduce::Reduce(
           (void *)dev_m_temp_storage_->getData(), ssz, dev_input, dev_nsum_values_->getData(),
           m_batch, dev_offsets_->getData(), dev_offsets_->getData() + 1, nsum_op_, (T)0.0, s);
     }
   }
 }
 
 template <typename T> void NoiseManager<T>::setAverageAbsMax(float value) {
@@ -431,24 +470,24 @@
 
       if (m_batch > 1) {
         // first compute the average of the max over batch
         if (!dev_a_temp_storage_ || m_batch > last_m_batch_) {
           dev_avgmax_value_ = RPU::make_unique<CudaArray<float>>(context_, 1);
 
           size_t temp_storage_bytes = 0;
-          RPU::cub::DeviceReduce::Sum(
+          RPU_CUB_NS_QUALIFIER DeviceReduce::Sum(
               nullptr, temp_storage_bytes, amaximizer_->getMaxValues(),
               dev_avgmax_value_->getData(), m_batch, s);
           dev_a_temp_storage_ = RPU::make_unique<CudaArray<char>>(context_, temp_storage_bytes);
           last_m_batch_ = m_batch;
           context_->synchronize();
         }
 
         size_t ssz = dev_a_temp_storage_->getSize();
-        RPU::cub::DeviceReduce::Sum(
+        RPU_CUB_NS_QUALIFIER DeviceReduce::Sum(
             (void *)dev_a_temp_storage_->getData(), ssz, amaximizer_->getMaxValues(),
             dev_avgmax_value_->getData(), m_batch, s);
       }
 
       if (nm_type_ == NoiseManagementType::AverageAbsMax) {
         // now update the running scale and set the current scales constant for all m_batch
         kernelAverageAbsMaxSetScales<T>
@@ -458,19 +497,19 @@
                 dev_scale_values_->getSize(), ravg_initialized_ ? MIN(io.nm_decay, 1.) : 1.0);
 
       } else {
         // count non-zero
 
         if (m_batch > 1) {
           NonZeroFunctor<T> nonzero_functor;
-          RPU::cub::TransformInputIterator<T, NonZeroFunctor<T>, T *> nz_input(
+          RPU_CUB_NS_QUALIFIER TransformInputIterator<T, NonZeroFunctor<T>, T *> nz_input(
               amaximizer_->getMaxValues(), nonzero_functor);
           // temp storage already requested above
           size_t ssz = dev_a_temp_storage_->getSize();
-          RPU::cub::DeviceReduce::Sum(
+          RPU_CUB_NS_QUALIFIER DeviceReduce::Sum(
               (void *)dev_a_temp_storage_->getData(), ssz, nz_input, dev_nzeros_value_->getData(),
               m_batch, s);
         }
         // just update the running avg value as only single output requested
         kernelAverageAbsMaxSingleMomentum<T><<<1, 1, 0, s>>>(
             dev_ravg_scale_value_->getData(),
             m_batch > 1 ? dev_avgmax_value_->getData() : amaximizer_->getMaxValues(), m_batch,
@@ -479,16 +518,14 @@
                               : 1.0); // Note that meaning of decay is per batch here
       }
       ravg_initialized_ = true;
     }
     return;
   }
   case NoiseManagementType::AbsMaxSingleValue: {
-    // CAUTION: the running average will not be saved for checkpointing... so there might be a
-    // glitch when continueing training from checkpoint...
 
     // this is overall max over m_batch
     if (!is_test) {
 
       this->amaximizer_->compute(dev_input, m_batch, trans);
       context_->synchronize();
       cudaStream_t s = context_->getStream();
@@ -497,24 +534,24 @@
 
       if (m_batch > 1) {
         // another max pass
         if (!dev_a_temp_storage_ || m_batch > last_m_batch_) {
           dev_avgmax_value_ = RPU::make_unique<CudaArray<float>>(context_, 1);
 
           size_t temp_storage_bytes = 0;
-          RPU::cub::DeviceReduce::Sum(
+          RPU_CUB_NS_QUALIFIER DeviceReduce::Sum(
               nullptr, temp_storage_bytes, amaximizer_->getMaxValues(),
               dev_avgmax_value_->getData(), m_batch, s);
           dev_a_temp_storage_ = RPU::make_unique<CudaArray<char>>(context_, temp_storage_bytes);
           last_m_batch_ = m_batch;
           context_->synchronize();
         }
 
         size_t ssz = dev_a_temp_storage_->getSize();
-        RPU::cub::DeviceReduce::Max(
+        RPU_CUB_NS_QUALIFIER DeviceReduce::Max(
             (void *)dev_a_temp_storage_->getData(), ssz, amaximizer_->getMaxValues(),
             dev_avgmax_value_->getData(), m_batch, s);
       }
 
       // just update the running avg value as only single output requested
       kernelAbsMaxSingleMomentum<T><<<1, 1, 0, s>>>(
           dev_ravg_scale_value_->getData(),
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/noise_manager.h` & `aihwkit-0.8.0/src/rpucuda/cuda/noise_manager.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -57,14 +57,16 @@
 
   /* sets the computed max values to zero below thres. Caution: This
      is in-place. does not check whether compute was called. */
   float *getScaleValues() const;
 
   float getAverageAbsMax() const;
   void setAverageAbsMax(float value);
+  void dumpExtra(RPU::state_t &extra, const std::string prefix);
+  void loadExtra(const RPU::state_t &extra, const std::string prefix, bool strict);
 
 private:
   template <typename InputIteratorT>
   void computeNPSum(InputIteratorT dev_input, int m_batch = 1, bool trans = false);
 
   void initializeBatchBuffer(int m_batch);
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/pulsed_weight_updater.cu` & `aihwkit-0.8.0/src/rpucuda/cuda/pulsed_weight_updater.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -38,14 +38,43 @@
   blm_ = RPU::make_unique<BitLineMaker<T>>(c, x_size, d_size);
 
   up_context_ = nullptr;
   is_async_update_ = false;
 };
 
 template <typename T>
+void PulsedWeightUpdater<T>::dumpExtra(RPU::state_t &extra, const std::string prefix) {
+
+  RPU::state_t state;
+  context_->synchronize();
+
+  RPU::insert(state, "is_async_update", is_async_update_);
+  RPU::insert(state, "update_count", update_count_);
+  RPU::insert(state, "verbose", verbose_);
+
+  blm_->dumpExtra(state, "blm");
+
+  RPU::insertWithPrefix(extra, state, prefix);
+}
+
+template <typename T>
+void PulsedWeightUpdater<T>::loadExtra(
+    const RPU::state_t &extra, const std::string prefix, bool strict) {
+
+  context_->synchronize();
+  auto state = RPU::selectWithPrefix(extra, prefix);
+
+  RPU::load(state, "is_async_update", is_async_update_, strict);
+  RPU::load(state, "update_count", update_count_, strict);
+  RPU::load(state, "verbose", verbose_, strict);
+
+  blm_->loadExtra(state, "blm", strict);
+}
+
+template <typename T>
 pwukpvec_t<T> PulsedWeightUpdater<T>::getValidUpdateKernels(
     PulsedRPUDeviceCudaBase<T> *rpucuda_device,
     int m_batch,
     const PulsedUpdateMetaParameter<T> &up) {
   pwukpvec_t<T> v;
   for (int use_bo64 : {1, 0}) { // omit 2 (ie bo64 translation)
     for (int out_trans : {true, false}) {
@@ -230,15 +259,14 @@
         context_, d_trans, !x_trans,
         d_size_, // M
         x_size_, // N
         m_batch, // K
         -lr, d_out, d_trans ? m_batch : d_size_, x_out, x_trans ? m_batch : x_size_, beta,
         dev_weights, d_size_);
   }
-
   context_->template releaseSharedBuffer<T>(RPU_BUFFER_IN);
   context_->template releaseSharedBuffer<T>(RPU_BUFFER_OUT);
 }
 
 template <typename T>
 template <typename XInputIteratorT, typename DInputIteratorT>
 void PulsedWeightUpdater<T>::doDirectUpdate(
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/pulsed_weight_updater.h` & `aihwkit-0.8.0/src/rpucuda/cuda/pulsed_weight_updater.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -65,14 +65,16 @@
       const T lr,
       const PulsedUpdateMetaParameter<T> &up,
       const int m_batch,
       const bool x_trans,
       const bool d_trans,
       const T beta = (T)1.0);
   void setVerbosityLevel(int level) { verbose_ = level; };
+  void dumpExtra(RPU::state_t &extra, const std::string prefix);
+  void loadExtra(const RPU::state_t &extra, const std::string prefix, bool strict);
 
 private:
   // void setUpdateType(PulsedUpdateType update_type);
   pwukpvec_t<T> getValidUpdateKernels(
       PulsedRPUDeviceCudaBase<T> *rpucuda_device,
       int m_batch,
       const PulsedUpdateMetaParameter<T> &up);
@@ -110,15 +112,14 @@
   CudaContextPtr context_ = nullptr;
   int x_size_ = 0;
   int d_size_ = 0;
   int update_count_ = 0;
   bool is_async_update_ = false;
   int verbose_ = 0;
   DeviceUpdateType update_type_ = DeviceUpdateType::Undefined;
-  int n_states = 0;
   pwukp_t<T> kernel_pars_;
   pwukpvec_t<T> valid_kernels_;
   std::unique_ptr<BitLineMaker<T>> blm_ = nullptr;
   std::unique_ptr<CudaContext> up_context_ = nullptr;
 };
 
 } // namespace RPU
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/pulsed_weight_updater_test.cpp` & `aihwkit-0.8.0/src/rpucuda/cuda/pulsed_weight_updater_test.cpp`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/pwu_kernel.h` & `aihwkit-0.8.0/src/rpucuda/cuda/pwu_kernel.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -1500,14 +1500,122 @@
     if (x_index == 0 && d_chopper_out) {
       d_chopper_out[d_index] = d_chop;
     }
   }
   RPU_UWBS_CLOSE_STRIDE_LOOP;
 }
 
+/* Pulse counter*/
+template <typename T, int one_sided, typename count_t, bool x_trans, bool d_trans>
+__global__ void kernelPulseCounter(
+    uint64_t *pos_pulses,
+    uint64_t *neg_pulses,
+    count_t *x_counts,
+    int x_size,
+    count_t *d_counts,
+    int d_size,
+    int nK32_in,
+    int m_batch_in,
+    kagg_t *Kn = nullptr) {
+  int m_batch = m_batch_in;
+  if (Kn != nullptr) {
+    m_batch = ((*Kn) + 31) / 32; // overwrite m_batch in case of BO64 UBLM
+  }
+  volatile unsigned int tid = blockDim.x * blockIdx.x + threadIdx.x;
+  const int total_threads = blockDim.x * gridDim.x;
+
+  const int xsz = x_size;
+  const int dsz = d_size;
+  const int sz = xsz * dsz;
+  const int nK32 = nK32_in;
+  count_t *xptr, *dptr;
+
+  uint64_t n_pos = 0;
+  uint64_t n_neg = 0;
+
+  const int x_count_offset = xsz * nK32;
+  const int d_count_offset = dsz * nK32;
+
+  for (int i_stride = 0; i_stride < sz; i_stride += total_threads) {
+
+    int idx = tid + i_stride;
+
+    if (idx < sz) { // stride over all elements of W
+
+      int dIdx = idx % dsz; // first d
+      int xIdx = idx / dsz;
+
+      n_pos = pos_pulses[idx];
+      n_neg = neg_pulses[idx];
+
+      uint32_t sum_n = 0;
+      uint32_t last_negative = 0;
+
+      for (int i_batch = 0; i_batch < m_batch; i_batch++) {
+
+        xptr = &x_counts[getIdxToLoad<x_trans>(i_batch, xIdx, xsz, m_batch, x_count_offset)];
+        dptr = &d_counts[getIdxToLoad<d_trans>(i_batch, dIdx, dsz, m_batch, d_count_offset)];
+
+        bool mixed;
+        uint32_t negative;
+        uint32_t n;
+
+        getNfromCount<one_sided, count_t>(n, negative, mixed, xptr, dptr, nK32, xsz, dsz);
+
+        if (mixed) {
+          for (int i_bit = 0; i_bit < 32; i_bit++) {
+            uint32_t bit_n = testBit(n, i_bit) ? 1 : 0;
+            if (bit_n != 0) {
+              uint32_t bit_neg = testBit(negative, i_bit) ? 1 : 0;
+              if (bit_neg == last_negative) {
+                sum_n += 1;
+              } else {
+                if (sum_n > 0) {
+                  if (last_negative) {
+                    n_neg += sum_n;
+                  } else {
+                    n_pos += sum_n;
+                  }
+                }
+                sum_n = 1;
+                last_negative = bit_neg;
+              }
+            }
+          }
+        } else {
+          if ((n == 0) || (last_negative == negative)) {
+            sum_n += n;
+          } else {
+            if (sum_n > 0) {
+              if (last_negative) {
+                n_neg += sum_n;
+              } else {
+                n_pos += sum_n;
+              }
+            }
+            sum_n = n;
+            last_negative = negative;
+          }
+        }
+
+      } // batch
+      // last update
+      if (sum_n > 0) {
+        if (last_negative) {
+          n_neg += sum_n;
+        } else {
+          n_pos += sum_n;
+        }
+      }
+      pos_pulses[idx] = n_pos;
+      neg_pulses[idx] = n_neg;
+    }
+  }
+}
+
 } // namespace RPU
 #undef RPU_SAVE_WEIGHT_OUTPUT
 #undef RPU_FUNCTOR_LOAD_PARAMS
 #undef RPU_UPDATE_WITH_SUM_N_INNER
 #undef RPU_UPDATE_WITH_SUM_N_INNER_BOUND_CHECK
 #undef RPU_UWBS_DEF_AND_STRIDE_LOOP
 #undef RPU_UWBS_READ_INTO_SHARED
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/pwu_kernel_parameter.h` & `aihwkit-0.8.0/src/rpucuda/cuda/pwu_kernel_parameter.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -217,14 +217,26 @@
     this->nthreads = MIN(RPU_THREADS_PER_BLOCK_UPDATE, this->size);
     this->nthreads = (this->nthreads + 31) / 32 * 32;
     this->nblocks =
         MIN(this->max_block_count, construction_context->getNBlocks(this->size, this->nthreads));
     this->nstates = this->nthreads * this->nblocks;);
 
 /********************************************************************************
+ * PWUKernelParameterBatchBaseInf // no limit on size
+ *********************************************************************************/
+DEFINE_PWU_KERNEL_BASE(BatchBaseInf,
+                       /*ctor*/
+                       this->nthreads = MIN(RPU_THREADS_PER_BLOCK_UPDATE, this->size);
+                       this->nthreads = (this->nthreads + 31) / 32 * 32;
+                       this->nblocks =
+                           MIN(this->max_block_count,
+                               construction_context->getNBlocks(this->size, this->nthreads));
+                       this->nstates = this->nthreads * this->nblocks;);
+
+/********************************************************************************
  * PWUKernelParameterBatchFunctor
  *********************************************************************************/
 
 template <typename T, typename FunctorT, int gp_count>
 DEFINE_PWU_KERNEL_PARAMETER(
     BatchFunctor,
     BatchBase,
@@ -442,15 +454,15 @@
 
 #define RPU_PWU_START_BATCH_SHARED_INIT_WO                                                         \
   int nK32_in = blm->getNK32Current();                                                             \
   if (nK32_in != this->nK32) {                                                                     \
     RPU_FATAL("nK32 changed. This is not supported");                                              \
   };                                                                                               \
                                                                                                    \
-  int batch_load_stride = MIN(this->max_batch_load_stride, m_batch);                               \
+  int batch_load_stride = MIN(this->max_batch_load_stride, (m_batch + 1) / 2 * 2);                 \
   int shared_mem = this->shared_mem_per_batch * batch_load_stride;
 
 /********************************************************************************
  * PWUKernelParameterBatchSharedWeightOutputFunctor
  *********************************************************************************/
 
 template <typename T, typename FunctorT, int gp_count>
@@ -550,14 +562,51 @@
              rpucuda_device->get1ParamsData(), rpucuda_device->getGlobalParamsData(),
              blm->getCurrentBL(), m_batch, batch_load_stride, dw_min_std, nullptr, nullptr, nullptr,
              0, 0, 0, false, 0, 0, false, (T)0, (T)0, nullptr, nullptr, nullptr, nullptr, nullptr,
              nullptr, dev_states));
       }
     });
 
+/********************************************************************************
+ * PWUKernelCounter
+ *********************************************************************************/
+
+#define RPU_PWU_COUNTER_KERNEL                                                                     \
+  if (this->implicit_pulses) {                                                                     \
+    RPU_SWITCH_TRANS_TEMPLATE(                                                                     \
+        T, one_sided, float, this->out_trans, this->out_trans, s, this->nblocks, this->nthreads,   \
+        this->shared_mem, kernelPulseCounter,                                                      \
+        (rpucuda_device->getPosPulseCountData(), rpucuda_device->getNegPulseCountData(),           \
+         blm->getXData(), this->x_size, blm->getDData(), this->d_size, 1, m_batch));               \
+  } else if (this->use_bo64) {                                                                     \
+    RPU_SWITCH_TRANS_TEMPLATE(                                                                     \
+        T, one_sided, uint64_t, this->out_trans, this->out_trans, s, this->nblocks,                \
+        this->nthreads, this->shared_mem, kernelPulseCounter,                                      \
+        (rpucuda_device->getPosPulseCountData(), rpucuda_device->getNegPulseCountData(),           \
+         blm->getXCountsBo64Data(), this->x_size, blm->getDCountsBo64Data(), this->d_size,         \
+         this->nK32, blm->getBo64Batch(m_batch),                                                   \
+         blm->getKnData(up.update_bl_management, m_batch)));                                       \
+  } else {                                                                                         \
+    RPU_SWITCH_TRANS_TEMPLATE(                                                                     \
+        T, one_sided, uint32_t, this->out_trans, this->out_trans, s, this->nblocks,                \
+        this->nthreads, this->shared_mem, kernelPulseCounter,                                      \
+        (rpucuda_device->getPosPulseCountData(), rpucuda_device->getNegPulseCountData(),           \
+         x_counts_chunk ? x_counts_chunk : blm->getXCountsData(), this->x_size,                    \
+         d_counts_chunk ? d_counts_chunk : blm->getDCountsData(), this->d_size, this->nK32,        \
+         m_batch));                                                                                \
+  }
+
+template <typename T>
+DEFINE_PWU_KERNEL_PARAMETER(PulseCounter,
+                            BatchBaseInf,
+                            /*run*/
+                            RPU_PWU_COUNTER_KERNEL;);
+
+#undef RPU_PWU_COUNTER_KERNEL
+
 #undef RPU_PWU_START_BATCH_SHARED_KERNEL
 #undef RPU_PWU_START_BATCH_SHARED_INIT
 
 #undef RPU_SWITCH_TRANS_TEMPLATE
 #undef RPU_SWITCH_TRANS_TEMPLATE_OS
 #undef RPU_SWITCH_TRANS_TEMPLATE_FUNCTOR
 #undef RPU_SWITCH_TRANS_TEMPLATE_FUNCTOR_OS
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/pwu_kernel_parameter_base.h` & `aihwkit-0.8.0/src/rpucuda/cuda/pwu_kernel_parameter_base.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -116,14 +116,15 @@
 
   inline int getNStates() const { return this->nstates; };
   inline void setNStates(int n) { this->nstates = n; };
   inline bool isValid() const { return this->valid; };
   inline std::string getName() const { return this->name; };
   inline bool getOutTrans() const { return this->out_trans; };
   inline int getUseBo64() const { return this->use_bo64; };
+  inline int getnK32() const { return this->nK32; };
   inline int getImplicitPulses() const { return this->implicit_pulses; };
 
   inline void forceBo64Translate() {
     if (this->use_bo64 == 1) {
       this->use_bo64 = 2;
     }
   };                                                        // debug hack
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpu_cub.h` & `aihwkit-0.8.0/src/rpucuda/cuda/rpu_cub.h`

 * *Files 14% similar despite different names*

```diff
@@ -1,23 +1,26 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
  * that they have been altered from the originals.
  */
 
 #pragma once
 
+#ifndef RPU_CUB_NS_QUALIFIER
 #ifndef CUB_NS_QUALIFIER
 #undef CUB_NS_PREFIX
 #undef CUB_NS_POSTFIX
 #define CUB_NS_PREFIX namespace RPU {
 #define CUB_NS_POSTFIX }
 #define CUB_NS_QUALIFIER ::RPU::cub
+#define RPU_CUB_NS_QUALIFIER RPU::cub::
+#endif
 #endif
 
 #include <cub/cub.cuh>
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda.cu` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda.cu`

 * *Files 5% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -190,14 +190,94 @@
   wdrifter_cuda_ = std::move(other.wdrifter_cuda_);
   wremapper_cuda_ = std::move(other.wremapper_cuda_);
 
   shared_weights_if_ = other.shared_weights_if_;
   return *this;
 }
 
+template <typename T>
+void RPUCudaSimple<T>::dumpExtra(RPU::state_t &extra, const std::string prefix) {
+
+  context_->synchronize();
+
+  RPUSimple<T>::dumpExtra(extra, prefix);
+  RPU::state_t state;
+
+  if (this->use_delayed_update_) {
+    RPU::insert(state, "dev_weights_buffer", dev_weights_buffer_);
+  }
+  RPU::insert(state, "dev_fb_weights", dev_fb_weights_);
+
+  if (dev_fb_weights_) {
+    fb_wmodifier_cuda_->dumpExtra(state, "fb_wmodifier_cuda");
+  }
+  if (wdrifter_cuda_) {
+    wdrifter_cuda_->dumpExtra(state, "wdrifter_cuda");
+  }
+  if (wremapper_cuda_) {
+    wremapper_cuda_->dumpExtra(state, "wremapper_cuda");
+  }
+  if (wclipper_cuda_) {
+    wclipper_cuda_->dumpExtra(state, "wclipper_cuda");
+  }
+  RPU::insertWithPrefix(extra, state, prefix);
+
+  // extern not handled
+  // tmp buffers not needed
+}
+
+template <typename T>
+void RPUCudaSimple<T>::loadExtra(const RPU::state_t &extra, const std::string prefix, bool strict) {
+  using V = std::vector<T>;
+  context_->synchronize();
+
+  RPUSimple<T>::loadExtra(extra, prefix, strict);
+  auto state = RPU::selectWithPrefix(extra, prefix);
+
+  if (this->use_delayed_update_) {
+    RPU::load(context_, state, "dev_weights_buffer", dev_weights_buffer_, strict);
+  }
+  if (state.count("dev_fb_weights")) {
+    RPU::load(context_, state, "dev_fb_weights", dev_fb_weights_, strict);
+  }
+
+  if (dev_fb_weights_) {
+    if (!fb_wmodifier_cuda_) {
+      fb_wmodifier_cuda_ =
+          RPU::make_unique<WeightModifierCuda<T>>(context_, this->x_size_, this->d_size_);
+    }
+    fb_wmodifier_cuda_->loadExtra(state, "fb_wmodifier_cuda", strict);
+  }
+
+  if (state.count("wdrifter_cuda")) {
+    if (!wdrifter_cuda_) {
+      auto wd = WeightDrifter<T>(this->x_size_ * this->d_size_, this->getPar().drift);
+      wdrifter_cuda_ =
+          RPU::make_unique<WeightDrifterCuda<T>>(this->context_, wd, this->x_size_, this->d_size_);
+    }
+    wdrifter_cuda_->loadExtra(state, "wdrifter_cuda", strict);
+  }
+
+  if (state.count("wremapper_cuda")) {
+    if (!wremapper_cuda_) {
+      wremapper_cuda_ =
+          RPU::make_unique<WeightRemapperCuda<T>>(context_, this->x_size_, this->d_size_);
+    }
+    wremapper_cuda_->loadExtra(state, "wremapper_cuda", strict);
+  }
+
+  if (state.count("wclipper_cuda")) {
+    if (!wclipper_cuda_) {
+      wclipper_cuda_ =
+          RPU::make_unique<WeightClipperCuda<T>>(this->context_, this->x_size_, this->d_size_);
+    }
+    wclipper_cuda_->loadExtra(state, "wclipper_cuda", strict);
+  }
+}
+
 /***************************************************************************************/
 template <typename T> void RPUCudaSimple<T>::copyWeightsToHost(T *weightsptr) const {
   // copies the weights to the host and returns weight pointer, without changing the simple weights
   DEBUG_OUT("RPUCuda: Get weights.");
   T **transposed_weights = Array_2D_Get<T>(this->x_size_, this->d_size_);
 
   context_->synchronizeDevice();
@@ -333,26 +413,26 @@
 void RPUCudaSimple<T>::copyFromVectorBiasBuffer(T *x_output_without_bias, int x_inc) {
   RPU::math::copy<T>(
       context_, this->x_size_ - 1, dev_x_vector_bias_->getData(), 1, x_output_without_bias, x_inc);
 };
 
 /*********************************************************************************/
 template <typename T>
-void RPUCudaSimple<T>::getTensorBuffer(T **x_tensor, T **d_tensor, int m_batch, int dim3) {
+void RPUCudaSimple<T>::getTensorBuffer(T **x_tensor_ptr, T **d_tensor_ptr, int m_batch, int dim3) {
   int x_size = this->getXSize();
   int d_size = this->getDSize();
 
   int n = (x_size + d_size) * dim3 * m_batch;
   if ((dev_temp_tensor_ == nullptr) || (dev_temp_tensor_->getSize() < n)) {
     this->context_->synchronize();
     dev_temp_tensor_ = RPU::make_unique<CudaArray<T>>(context_, n);
     this->context_->synchronize();
   }
-  *x_tensor = dev_temp_tensor_->getData();
-  *d_tensor = *x_tensor + (x_size)*dim3 * m_batch;
+  *x_tensor_ptr = dev_temp_tensor_->getData();
+  *d_tensor_ptr = *x_tensor_ptr + (x_size)*dim3 * m_batch;
 }
 
 template <typename T>
 void RPUCudaSimple<T>::permute132(
     T *out_tensor, const T *in_tensor, int dim1, int dim2, int dim3, bool bias2) {
   RPU::math::permute132<T>(context_, out_tensor, in_tensor, dim1, dim2, dim3, bias2);
 }
@@ -658,16 +738,14 @@
       diffusion);
 
   rnd_diffusion_context_->recordWaitEvent(context_->getStream());
   rnd_diffusion_context_->randNormal(
       dev_diffusion_nrnd_->getData(), dev_diffusion_nrnd_->getSize());
 }
 
-/***********************************************************************/
-
 template <typename T> void RPUCudaSimple<T>::setDeltaWeights(T *dw_extern) {
 
   ENFORCE_NO_DELAYED_UPDATE;
   dev_delta_weights_extern_ = dw_extern;
 }
 
 template <typename T> T *RPUCudaSimple<T>::getFBWeightsCuda(bool is_test) const {
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda.h` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -85,15 +85,15 @@
 
   void copyWeightsToHost(T *weightsptr) const;
   T **copyWeightsToHost();
 
   // when overriding copy methods below, _Vector_Bias can be used in derived
   T *copyToVectorBiasBuffer(const T *x_input_without_bias, int x_inc) override;
   void copyFromVectorBiasBuffer(T *x_output_without_bias, int x_inc) override;
-  T *getVectorBiasBuffer() const override { return dev_x_vector_bias_->getData(); };
+  T *getVectorBiasBuffer() override { return dev_x_vector_bias_->getData(); };
 
   // new matrix and bias stuff (for Caffe2 interface)
   void copyWeightsFromBuffer() override;
   void copyWeightsToBuffer() override;
   T *getWeightsBufferCuda();
 
   T *copyToMatrixBiasBuffer(const T *X_input_without_bias, int m_batch, bool x_trans) override;
@@ -120,15 +120,15 @@
       const T *X_input,
       const T *D_input,
       int m_batch,
       bool x_trans = false,
       bool d_trans = false) override;
 
   // for tensor interface
-  void getTensorBuffer(T **x_tensor, T **d_tensor, int m_batch, int dim3) override;
+  void getTensorBuffer(T **x_tensor_ptr, T **d_tensor_ptr, int m_batch, int dim3) override;
   void
   permute132(T *out_tensor, const T *in_tensor, int dim1, int dim2, int dim3, bool bias2) override;
 
   // for indexed interface
   void copyIndexedInput(
       T *out_tensor,
       const T *src_tensor,
@@ -180,15 +180,14 @@
 
   void decayWeights(bool bias_no_decay) override;
   void decayWeights(T alpha, bool bias_no_decay) override;
 
   void driftWeights(T time_since_last_call) override;
   void diffuseWeights() override;
   void remapWeights(const WeightRemapParameter &wrmpar, T *scales, T *biases = nullptr) override;
-
   void clipWeights(T clip) override;
   void clipWeights(const WeightClipParameter &wclpar) override;
 
   T **getWeights() override; // host weights. implicit copy from CUDA
 
   void getWeights(T *weightsptr) const override;
   void setWeights(const T *weightsptr) override;
@@ -204,14 +203,16 @@
   cudaStream_t getStream() { return context_->getStream(); };
   int getGPUId() { return context_->getGPUId(); };
 
   void modifyFBWeights(const WeightModifierParameter<T> &wmpar) override;
   void finishAllCalculations() override;
 
   ContextPtr getContext() const override { return context_; };
+  void dumpExtra(RPU::state_t &extra, const std::string prefix) override;
+  void loadExtra(const RPU::state_t &extra, const std::string prefix, bool strict) override;
 
 protected:
   CudaContextPtr context_ = nullptr;
   std::unique_ptr<CudaContext> internal_context_ = nullptr;
   std::unique_ptr<CudaContext> rnd_diffusion_context_ = nullptr;
   std::unique_ptr<CudaArray<float>> dev_diffusion_nrnd_ = nullptr;
   std::unique_ptr<CudaArray<T>> dev_weights_ = nullptr;
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_buffered_transfer_device.cu` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_buffered_transfer_device.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_buffered_transfer_device.h` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_buffered_transfer_device.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_buffered_transfer_device_test.cpp` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_buffered_transfer_device_test.cpp`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_constantstep_device.cu` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_constantstep_device.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_constantstep_device.h` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_constantstep_device.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_expstep_device.cu` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_expstep_device.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_expstep_device.h` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_expstep_device.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_expstep_test.cpp` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_expstep_test.cpp`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_linearstep_device.cu` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_linearstep_device.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_linearstep_device.h` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_linearstep_device.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_linearstep_test.cpp` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_linearstep_test.cpp`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_mixedprec_device.cu` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_mixedprec_device.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_mixedprec_device.h` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_mixedprec_device.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_mixedprec_device_base.cu` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_mixedprec_device_base.cu`

 * *Files 12% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -149,14 +149,60 @@
 
   const auto &par = this->getPar();
   granularity_ = rpu_device.getGranularity();
   this->context_->synchronize();
 }
 
 template <typename T>
+void MixedPrecRPUDeviceBaseCuda<T>::dumpExtra(RPU::state_t &extra, const std::string prefix) {
+  SimpleRPUDeviceCuda<T>::dumpExtra(extra, prefix);
+
+  RPU::state_t state;
+
+  rpucuda_device_->dumpExtra(state, "rpucuda_device");
+  transfer_pwu_->dumpExtra(state, "transfer_pwu");
+  noise_manager_x_->dumpExtra(state, "noise_manager_x");
+  noise_manager_d_->dumpExtra(state, "noise_manager_d");
+
+  RPU::insert(state, "current_update_index", current_update_index_);
+  RPU::insert(state, "current_row_index", current_row_index_);
+  RPU::insert(state, "granularity", granularity_);
+
+  RPU::insert(state, "dev_avg_sparsity", dev_avg_sparsity_);
+  RPU::insert(state, "dev_sparsity_d", dev_sparsity_d_);
+  RPU::insert(state, "dev_sparsity_x", dev_sparsity_x_);
+
+  // dev_transfer_d_vecs not handled (generated on the fly)
+
+  RPU::insertWithPrefix(extra, state, prefix);
+}
+
+template <typename T>
+void MixedPrecRPUDeviceBaseCuda<T>::loadExtra(
+    const RPU::state_t &extra, const std::string prefix, bool strict) {
+  SimpleRPUDeviceCuda<T>::loadExtra(extra, prefix, strict);
+
+  auto state = RPU::selectWithPrefix(extra, prefix);
+  using V = std::vector<T>;
+
+  rpucuda_device_->loadExtra(state, "rpucuda_device", strict);
+  transfer_pwu_->loadExtra(state, "transfer_pwu", strict);
+  noise_manager_x_->loadExtra(state, "noise_manager_x", strict);
+  noise_manager_d_->loadExtra(state, "noise_manager_d", strict);
+
+  RPU::load(state, "granularity", granularity_, strict);
+  RPU::load(state, "current_row_index", current_row_index_, strict);
+  RPU::load(state, "current_update_index", current_update_index_, strict);
+
+  RPU::load(this->context_, state, "dev_avg_sparsity", dev_avg_sparsity_, strict);
+  RPU::load(this->context_, state, "dev_sparsity_d", dev_sparsity_d_, strict);
+  RPU::load(this->context_, state, "dev_sparsity_x", dev_sparsity_x_, strict);
+}
+
+template <typename T>
 __global__ void kernelAddSparsity(
     T *sparsity,
     const T *x_sparsity,
     const T *d_sparsity,
     int64_t current_update_index,
     const int m_batch) {
   volatile unsigned int tid = blockDim.x * blockIdx.x + threadIdx.x;
@@ -172,22 +218,22 @@
     T *sparsity, const T *values, const int size) {
 
   IndicatorInputIterator<T> input_values(values, (T)0.0, (T)1.0 / ((T)size));
   if (size > current_zero_size_) {
     // init
     current_zero_size_ = size;
     size_t temp_storage_bytes = 0;
-    RPU::cub::DeviceReduce::Sum(
+    RPU_CUB_NS_QUALIFIER DeviceReduce::Sum(
         nullptr, temp_storage_bytes, input_values, sparsity, size, this->context_->getStream());
     dev_zc_temp_storage_ = RPU::make_unique<CudaArray<char>>(this->context_, temp_storage_bytes);
   }
 
   // Run sum-reduction (use T as output)
   size_t temp_storage_bytes = dev_zc_temp_storage_->getSize();
-  RPU::cub::DeviceReduce::Sum(
+  RPU_CUB_NS_QUALIFIER DeviceReduce::Sum(
       dev_zc_temp_storage_->getData(), temp_storage_bytes, input_values, sparsity, size,
       this->context_->getStream());
 }
 
 template <typename T>
 void MixedPrecRPUDeviceBaseCuda<T>::computeSparsity(
     const T *x_values, const T *d_values, const int m_batch) {
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_mixedprec_device_base.h` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_mixedprec_device_base.h`

 * *Files 6% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -59,14 +59,16 @@
     swap(a.up_ptr_, b.up_ptr_);
     swap(a.up_, b.up_);
     swap(a.nblocks_batch_max_, b.nblocks_batch_max_);
     swap(a.granularity_, b.granularity_);
   };
   bool hasDirectUpdate() const override { return true; };
   std::vector<T> getHiddenWeights() const override;
+  void dumpExtra(RPU::state_t &extra, const std::string prefix) override;
+  void loadExtra(const RPU::state_t &extra, const std::string prefix, bool strict) override;
 
   void decayWeights(T *dev_weights, bool bias_no_decay) override;
   void decayWeights(T *dev_weights, T alpha, bool bias_no_decay) override;
   void driftWeights(T *dev_weights, T time_since_epoch) override;
   void diffuseWeights(T *dev_weights) override;
   void clipWeights(T *dev_weights, T clip) override;
   void resetCols(T *dev_weights, int start_col, int n_cols, T reset_prob) override;
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_mixedprec_device_test.cpp` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_mixedprec_device_test.cpp`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_onesided_device.cu` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_onesided_device.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_onesided_device.h` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_onesided_device.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_onesided_device_test.cpp` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_onesided_device_test.cpp`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_piecewisestep_device.cu` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_piecewisestep_device.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_piecewisestep_device.h` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_piecewisestep_device.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_powstep_device.cu` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_powstep_device.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_powstep_device.h` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_powstep_device.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_powstep_reference_device.cu` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_powstep_reference_device.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_powstep_reference_device.h` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_powstep_reference_device.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_powstep_test.cpp` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_powstep_test.cpp`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_pulsed.cu` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_pulsed.cu`

 * *Files 3% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -460,14 +460,53 @@
 
 template <typename T> void RPUCudaPulsed<T>::setHiddenUpdateIdx(int idx) {
   CHECK_RPU_DEVICE_INIT;
   rpucuda_device_->setHiddenUpdateIdx(idx);
   rpu_device_->setHiddenUpdateIdx(idx);
 };
 
+/*********************************************************************************/
+/* dump / load state */
+
+template <typename T>
+void RPUCudaPulsed<T>::dumpExtra(RPU::state_t &extra, const std::string prefix) {
+  RPUCudaSimple<T>::dumpExtra(extra, prefix);
+
+  RPU::state_t state;
+
+  rpu_device_->dumpExtra(state, "rpu_device");
+  rpucuda_device_->dumpExtra(state, "rpucuda_device");
+
+  f_iom_->dumpExtra(state, "f_iom");
+  b_iom_->dumpExtra(state, "b_iom");
+
+  up_pwu_->dumpExtra(state, "up_pwu");
+  fb_pass_->dumpExtra(state, "fb_pass");
+
+  // tmp vectors are ignored
+  RPU::insertWithPrefix(extra, state, prefix);
+}
+
+template <typename T>
+void RPUCudaPulsed<T>::loadExtra(const RPU::state_t &extra, const std::string prefix, bool strict) {
+  RPUCudaSimple<T>::loadExtra(extra, prefix, strict);
+
+  auto state = RPU::selectWithPrefix(extra, prefix);
+
+  rpu_device_->loadExtra(state, "rpu_device", strict);
+  rpucuda_device_->loadExtra(state, "rpucuda_device", strict);
+
+  f_iom_->loadExtra(state, "f_iom", strict);
+  b_iom_->loadExtra(state, "b_iom", strict);
+
+  up_pwu_->loadExtra(state, "up_pwu", strict);
+  fb_pass_->loadExtra(state, "fb_pass", strict);
+}
+
+/*********************************************************************************/
 template <typename T> void RPUCudaPulsed<T>::setWeights(const T *host_source) {
 
   CHECK_RPU_DEVICE_INIT;
   RPUSimple<T>::setWeights(host_source); // sets host
 
   if (rpu_device_) {
     if (rpu_device_->onSetWeights(this->getWeightsPtr())) {
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_pulsed.h` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_pulsed.h`

 * *Files 3% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -166,35 +166,47 @@
   void setWeights(const T *weightsptr) override;
 
   void applyWeightUpdate(T *dw_and_current_weights_out) override;
 
   void getDeviceParameterNames(std::vector<std::string> &names) const override;
   void getDeviceParameter(std::vector<T *> &data_ptrs) override;
   void setDeviceParameter(const std::vector<T *> &data_ptrs) override;
+  void dumpExtra(RPU::state_t &extra, const std::string prefix) override;
+  void loadExtra(const RPU::state_t &extra, const std::string prefix, bool strict) override;
 
   int getHiddenUpdateIdx() const override;
   void setHiddenUpdateIdx(int idx) override;
 
   void populateParameter(PulsedMetaParameter<T> *p, PulsedRPUDeviceMetaParameter<T> *dp);
 
   void printRPUParameter(int x_count, int d_count) const;
 
   // CAUTION: make sure to call Finish before weight usage !! Not done in the code
   void finishUpdateCalculations() override;
   void makeUpdateAsync() override;
 
+  std::vector<uint64_t> getPulseCounters() const override {
+    return rpucuda_device_ != nullptr ? rpucuda_device_->getPulseCounters()
+                                      : std::vector<uint64_t>();
+  }
   // for debugging
   void getCountsDebug(uint32_t *x_counts, uint32_t *d_counts) {
     up_pwu_->getCountsDebug(x_counts, d_counts);
   };
 
   virtual const PulsedMetaParameter<T> &getMetaPar() const { return par_; };
 
   const AbstractRPUDeviceCuda<T> &getRPUDeviceCuda() { return *rpucuda_device_; };
 
+  void setVerbosityLevel(int verbose) {
+    if (up_pwu_) {
+      up_pwu_->setVerbosityLevel(verbose);
+    }
+  };
+
 protected:
   std::unique_ptr<AbstractRPUDevice<T>> rpu_device_ = nullptr;
   std::unique_ptr<AbstractRPUDeviceCuda<T>> rpucuda_device_ = nullptr;
 
   std::unique_ptr<InputOutputManager<T>> f_iom_ = nullptr;
   std::unique_ptr<InputOutputManager<T>> b_iom_ = nullptr;
   std::unique_ptr<PulsedWeightUpdater<T>> up_pwu_ = nullptr;
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_pulsed_device.cu` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_pulsed_device.cu`

 * *Files 16% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -12,14 +12,15 @@
 
 #include "pwu_kernel_parameter.h"
 #include "rpu_pulsed_meta_parameter.h"
 #include "rpucuda_pulsed_device.h"
 #include <memory>
 
 namespace RPU {
+
 /******************************************************************************************/
 /* PulsedRPUDeviceCuda
 
    Base class which maintains the basic hard bounds and dw_min
    up/down and decays etc for the pulsed updates.
 
    Note that it is still Abstract. Need to implement the getUpdateKernels in derived.
@@ -32,14 +33,16 @@
 template <typename T> void PulsedRPUDeviceCuda<T>::initialize() {
 
   dev_4params_ = RPU::make_unique<CudaArray<float>>(this->context_, 4 * this->size_);
   dev_decay_scale_ = RPU::make_unique<CudaArray<T>>(this->context_, this->size_);
   dev_diffusion_rate_ = nullptr; // on the fly
   dev_reset_bias_ = nullptr;
   dev_persistent_weights_ = nullptr;
+  dev_neg_pulse_counter_ = nullptr;
+  dev_pos_pulse_counter_ = nullptr;
 
   this->context_->synchronize();
 };
 
 template <typename T>
 PulsedRPUDeviceCuda<T>::PulsedRPUDeviceCuda(const PulsedRPUDeviceCuda<T> &other)
     : PulsedRPUDeviceCudaBase<T>(other) {
@@ -56,44 +59,51 @@
     dev_reset_bias_ = RPU::make_unique<CudaArray<T>>(this->context_, this->size_);
     dev_reset_bias_->assign(*other.dev_reset_bias_);
   }
   if (other.dev_persistent_weights_ != nullptr) {
     dev_persistent_weights_ = RPU::make_unique<CudaArray<T>>(this->context_, this->size_);
     dev_persistent_weights_->assign(*other.dev_persistent_weights_);
   }
+  if (other.dev_neg_pulse_counter_ != nullptr) {
+    dev_neg_pulse_counter_ = RPU::make_unique<CudaArray<uint64_t>>(this->context_, this->size_);
+    dev_neg_pulse_counter_->assign(*other.dev_neg_pulse_counter_);
+  }
+  if (other.dev_pos_pulse_counter_ != nullptr) {
+    dev_pos_pulse_counter_ = RPU::make_unique<CudaArray<uint64_t>>(this->context_, this->size_);
+    dev_pos_pulse_counter_->assign(*other.dev_pos_pulse_counter_);
+  }
 
   this->context_->synchronize();
 };
 
-// template <typename T>
-// PulsedRPUDeviceCuda<T>& PulsedRPUDeviceCuda<T>::operator=(const PulsedRPUDeviceCuda<T>& other){
-//   PulsedRPUDeviceCuda<T> tmp(other);
-//   swap(*this,tmp);
-//   context_->synchronize(); // need sync because of tmp
-//   return *this;
-// };
-
-// template <typename T>
-// PulsedRPUDeviceCuda<T>::PulsedRPUDeviceCuda(PulsedRPUDeviceCuda<T>&& other) {
-//   *this = std::move(other);
-// };
-
-// template <typename T>
-// PulsedRPUDeviceCuda<T>& PulsedRPUDeviceCuda<T>::operator=(PulsedRPUDeviceCuda<T>&& other){
-
-//   PulsedRPUDeviceCudaBase<T>::operator=(std::move(other));
-
-//   dev_4params_ = std::move(other.dev_4params_);
-//   dev_diffusion_rate_ = std::move(other.dev_diffusion_rate_);
-//   dev_reset_bias_ = std::move(other.dev_reset_bias_);
-//   dev_decay_scale_ = std::move(other.dev_decay_scale_);
-//   dev_persistent_weights_ = std::move(other.dev_persistent_weights_);
+template <typename T>
+PulsedRPUDeviceCuda<T> &PulsedRPUDeviceCuda<T>::operator=(const PulsedRPUDeviceCuda<T> &other) {
+  PulsedRPUDeviceCudaBase<T> tmp(other);
+  swap(*this, tmp);
+  return *this;
+};
 
-//   return *this;
-// };
+template <typename T> PulsedRPUDeviceCuda<T>::PulsedRPUDeviceCuda(PulsedRPUDeviceCuda<T> &&other) {
+  *this = std::move(other);
+};
+
+template <typename T>
+PulsedRPUDeviceCuda<T> &PulsedRPUDeviceCuda<T>::operator=(PulsedRPUDeviceCuda<T> &&other) {
+
+  PulsedRPUDeviceCudaBase<T>::operator=(std::move(other));
+
+  dev_4params_ = std::move(other.dev_4params_);
+  dev_diffusion_rate_ = std::move(other.dev_diffusion_rate_);
+  dev_reset_bias_ = std::move(other.dev_reset_bias_);
+  dev_decay_scale_ = std::move(other.dev_decay_scale_);
+  dev_persistent_weights_ = std::move(other.dev_persistent_weights_);
+  dev_pos_pulse_counter_ = std::move(other.dev_pos_pulse_counter_);
+  dev_neg_pulse_counter_ = std::move(other.dev_neg_pulse_counter_);
+  return *this;
+};
 
 template <typename T>
 void PulsedRPUDeviceCuda<T>::populateFrom(const AbstractRPUDevice<T> &rpu_device_in) {
 
   const auto &rpu_device = dynamic_cast<const PulsedRPUDevice<T> &>(rpu_device_in);
   if (&rpu_device == nullptr) {
     RPU_FATAL("populateFrom expects PulsedRPUDevice.");
@@ -177,14 +187,37 @@
   delete[] tmp_df;
   delete[] tmp_rb;
   delete[] tmp_pw;
   delete[] tmp;
 }
 
 template <typename T>
+void PulsedRPUDeviceCuda<T>::dumpExtra(RPU::state_t &extra, const std::string prefix) {
+  PulsedRPUDeviceCudaBase<T>::dumpExtra(extra, prefix);
+
+  RPU::state_t state;
+
+  RPU::insert(state, "dev_pos_pulse_counter", dev_pos_pulse_counter_);
+  RPU::insert(state, "dev_neg_pulse_counter", dev_neg_pulse_counter_);
+
+  RPU::insertWithPrefix(extra, state, prefix);
+};
+
+template <typename T>
+void PulsedRPUDeviceCuda<T>::loadExtra(
+    const RPU::state_t &extra, const std::string prefix, bool strict) {
+  PulsedRPUDeviceCudaBase<T>::loadExtra(extra, prefix, strict);
+
+  auto state = RPU::selectWithPrefix(extra, prefix);
+
+  RPU::load(this->context_, state, "dev_pos_pulse_counter", dev_pos_pulse_counter_, strict);
+  RPU::load(this->context_, state, "dev_neg_pulse_counter", dev_neg_pulse_counter_, strict);
+};
+
+template <typename T>
 void PulsedRPUDeviceCuda<T>::applyWeightUpdate(T *weights, T *dw_and_current_weight_out) {
 
   if (getPar().usesPersistentWeight()) {
     RPU_FATAL("ApplyWeightUpdate is not supported with write_noise_std>0!");
   }
   RPU::math::elemaddcopysat<T>(
       this->context_, weights, dw_and_current_weight_out, this->size_,
@@ -263,27 +296,14 @@
   RPU::math::elemsat<T>(this->context_, w, this->size_, dev_4params_->getData());
   if (clip >= 0) {
     RPU::math::aclip<T>(this->context_, w, this->size_, clip);
   }
   applyUpdateWriteNoise(weights);
 }
 
-template <typename T> void PulsedRPUDeviceCuda<T>::initResetRnd() {
-
-  if (this->rnd_context_ == nullptr) {
-    this->initRndContext();
-  }
-  dev_reset_nrnd_ =
-      RPU::make_unique<CudaArray<float>>(&*this->rnd_context_, (this->size_ + 31) / 32 * 32);
-  dev_reset_flag_ =
-      RPU::make_unique<CudaArray<float>>(&*this->rnd_context_, (this->size_ + 31) / 32 * 32);
-  dev_reset_flag_->setConst(0);
-  this->rnd_context_->synchronize();
-}
-
 template <typename T> std::vector<T> PulsedRPUDeviceCuda<T>::getHiddenWeights() const {
   std::vector<T> data;
   if (!getPar().usesPersistentWeight()) {
     return data;
   }
   int offset = 0;
 
@@ -295,14 +315,42 @@
       // transpose d_size maj -> x_size maj
       data[offset + i] = w_vec[TRANSPOSE_X2D(i, this->x_size_, this->d_size_)];
     }
   }
   return data;
 }
 
+template <typename T> std::vector<uint64_t> PulsedRPUDeviceCuda<T>::getPulseCounters() const {
+  std::vector<uint64_t> data;
+  if (!dev_neg_pulse_counter_) {
+    return data;
+  }
+  int offset = 0;
+
+  std::vector<uint64_t> counter_vec(this->size_);
+  data.resize(2 * this->size_);
+
+  if (dev_pos_pulse_counter_ != nullptr) {
+    dev_pos_pulse_counter_->copyTo(counter_vec.data());
+    for (int i = 0; i < this->size_; i++) {
+      // transpose d_size maj -> x_size maj
+      data[offset + i] = counter_vec[TRANSPOSE_X2D(i, this->x_size_, this->d_size_)];
+    }
+  }
+  offset = this->size_;
+  if (dev_neg_pulse_counter_ != nullptr) {
+    dev_neg_pulse_counter_->copyTo(counter_vec.data());
+    for (int i = 0; i < this->size_; i++) {
+      // transpose d_size maj -> x_size maj
+      data[offset + i] = counter_vec[TRANSPOSE_X2D(i, this->x_size_, this->d_size_)];
+    }
+  }
+  return data;
+}
+
 template <typename T> void PulsedRPUDeviceCuda<T>::applyUpdateWriteNoise(T *dev_weights) {
 
   const auto &par = getPar();
 
   if (!par.usesPersistentWeight() || !par.implementsWriteNoise()) {
     return;
   }
@@ -350,67 +398,84 @@
   int n = n_cols * this->d_size_;
   int offset = start_col * this->d_size_;
   bool with_bias = dev_reset_bias_ != nullptr;
   bool with_flag = false;
   bool with_nrnd = false;
 
   if (getPar().reset_std > 0) {
-    if (dev_reset_nrnd_ == nullptr) {
-      initResetRnd();
+    if (this->dev_reset_nrnd_ == nullptr) {
+      this->initResetRnd();
     }
     this->rnd_context_->randNormal(
-        dev_reset_nrnd_->getData(), n_cols * this->d_size_, 0.0, getPar().reset_std);
+        this->dev_reset_nrnd_->getData(), n_cols * this->d_size_, 0.0, getPar().reset_std);
     with_nrnd = true;
   }
   if (reset_prob < 1) {
-    if (dev_reset_flag_ == nullptr) {
-      initResetRnd();
+    if (this->dev_reset_flag_ == nullptr) {
+      this->initResetRnd();
     }
-    this->rnd_context_->randUniform(dev_reset_flag_->getData(), n_cols * this->d_size_);
+    this->rnd_context_->randUniform(this->dev_reset_flag_->getData(), n_cols * this->d_size_);
     with_flag = true;
   }
   if (with_flag || with_nrnd) {
     this->context_->recordWaitEvent(this->rnd_context_->getStream());
   }
 
   if (n >= this->size_) {
     // reset whole matrix
     RPU::math::elemresetsat<T>(
         this->context_, w, this->size_, with_bias ? dev_reset_bias_->getDataConst() : nullptr,
-        with_nrnd ? dev_reset_nrnd_->getDataConst() : nullptr,
-        with_flag ? dev_reset_flag_->getDataConst() : nullptr, reset_prob, dev_4params_->getData());
+        with_nrnd ? this->dev_reset_nrnd_->getDataConst() : nullptr,
+        with_flag ? this->dev_reset_flag_->getDataConst() : nullptr, reset_prob,
+        dev_4params_->getData());
 
   } else if (offset + n <= this->size_) {
     // one pass enough
     RPU::math::elemresetsat<T>(
         this->context_, w + offset, n,
         with_bias ? dev_reset_bias_->getDataConst() + offset : nullptr,
-        with_nrnd ? dev_reset_nrnd_->getDataConst() : nullptr,
-        with_flag ? dev_reset_flag_->getDataConst() : nullptr, reset_prob,
+        with_nrnd ? this->dev_reset_nrnd_->getDataConst() : nullptr,
+        with_flag ? this->dev_reset_flag_->getDataConst() : nullptr, reset_prob,
         dev_4params_->getData() + 4 * offset);
   } else {
     // two passes
     int m = this->size_ - offset;
 
     RPU::math::elemresetsat<T>(
         this->context_, w + offset, m,
         with_bias ? dev_reset_bias_->getDataConst() + offset : nullptr,
-        with_nrnd ? dev_reset_nrnd_->getDataConst() : nullptr,
-        with_flag ? dev_reset_flag_->getDataConst() : nullptr, reset_prob,
+        with_nrnd ? this->dev_reset_nrnd_->getDataConst() : nullptr,
+        with_flag ? this->dev_reset_flag_->getDataConst() : nullptr, reset_prob,
         dev_4params_->getData() + 4 * offset);
 
     RPU::math::elemresetsat<T>(
         this->context_, w, n - m, with_bias ? dev_reset_bias_->getDataConst() : nullptr,
-        with_nrnd ? dev_reset_nrnd_->getDataConst() + m : nullptr,
-        with_flag ? dev_reset_flag_->getDataConst() + m : nullptr, reset_prob,
+        with_nrnd ? this->dev_reset_nrnd_->getDataConst() + m : nullptr,
+        with_flag ? this->dev_reset_flag_->getDataConst() + m : nullptr, reset_prob,
         dev_4params_->getData());
   }
   applyUpdateWriteNoise(weights);
 }
 
+template <typename T> uint64_t *PulsedRPUDeviceCuda<T>::getNegPulseCountData() {
+  if (dev_neg_pulse_counter_ == nullptr) {
+    dev_neg_pulse_counter_ = RPU::make_unique<CudaArray<uint64_t>>(this->context_, this->size_);
+    dev_neg_pulse_counter_->setConst(0);
+  }
+  return dev_neg_pulse_counter_->getData();
+}
+
+template <typename T> uint64_t *PulsedRPUDeviceCuda<T>::getPosPulseCountData() {
+  if (dev_pos_pulse_counter_ == nullptr) {
+    dev_pos_pulse_counter_ = RPU::make_unique<CudaArray<uint64_t>>(this->context_, this->size_);
+    dev_pos_pulse_counter_->setConst(0);
+  }
+  return dev_pos_pulse_counter_->getData();
+}
+
 template <typename T>
 void PulsedRPUDeviceCuda<T>::runUpdateKernel(
     pwukp_t<T> kpars,
     CudaContextPtr c,
     T *dev_weights,
     int m_batch,
     const BitLineMaker<T> *blm,
@@ -418,14 +483,23 @@
     const T lr,
     curandState_t *dev_states,
     int one_sided,
     uint32_t *x_counts_chunk,
     uint32_t *d_counts_chunk,
     const ChoppedWeightOutput<T> *cwo) {
 
+  if (this->getPar().count_pulses) {
+    PWUKernelParameterPulseCounter<T> pulse_counter(
+        c, this->x_size_, this->d_size_, m_batch, kpars->getnK32(), kpars->getUseBo64(),
+        kpars->getOutTrans(), up, "PulseCounter");
+    pulse_counter.run(
+        c->getStream(), nullptr, m_batch, blm, this, up, nullptr, one_sided, x_counts_chunk,
+        d_counts_chunk);
+  }
+
   kpars->run(
       c->getStream(), dev_weights, m_batch, blm, this, up, dev_states, one_sided, x_counts_chunk,
       d_counts_chunk, cwo);
 }
 
 template class PulsedRPUDeviceCuda<float>;
 #ifdef RPU_USE_DOUBLE
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_pulsed_device.h` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_pulsed_device.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -21,42 +21,61 @@
 namespace RPU {
 
 template <typename T> struct PulsedUpdateMetaParameter;
 
 /* Base class for all devices that support pulsed update*/
 template <typename T> class PulsedRPUDeviceCudaBase : public SimpleRPUDeviceCuda<T> {
 public:
-  PulsedRPUDeviceCudaBase() = default;
-  ~PulsedRPUDeviceCudaBase() = default;
+  explicit PulsedRPUDeviceCudaBase() = default;
   explicit PulsedRPUDeviceCudaBase(CudaContextPtr c, int x_size, int d_size)
       : SimpleRPUDeviceCuda<T>(c, x_size, d_size){};
 
-  PulsedRPUDeviceCudaBase(const PulsedRPUDeviceCudaBase<T> &other)
-      : SimpleRPUDeviceCuda<T>(other){};
+  ~PulsedRPUDeviceCudaBase() = default;
+  PulsedRPUDeviceCudaBase(const PulsedRPUDeviceCudaBase<T> &other) = default;
   PulsedRPUDeviceCudaBase<T> &operator=(const PulsedRPUDeviceCudaBase<T> &other) = default;
   PulsedRPUDeviceCudaBase(PulsedRPUDeviceCudaBase<T> &&other) = default;
   PulsedRPUDeviceCudaBase<T> &operator=(PulsedRPUDeviceCudaBase<T> &&other) = default;
 
   friend void swap(PulsedRPUDeviceCudaBase<T> &a, PulsedRPUDeviceCudaBase<T> &b) noexcept {
     using std::swap;
     swap(static_cast<SimpleRPUDeviceCuda<T> &>(a), static_cast<SimpleRPUDeviceCuda<T> &>(b));
     swap(a.weight_granularity_, b.weight_granularity_);
     swap(a.num_states_, b.num_states_);
   }
+
   void populateFrom(const AbstractRPUDevice<T> &rpu_device_in) override {
     SimpleRPUDeviceCuda<T>::populateFrom(rpu_device_in);
 
     const auto &rpu_device = dynamic_cast<const PulsedRPUDeviceBase<T> &>(rpu_device_in);
     if (&rpu_device == nullptr) {
       RPU_FATAL("populateFrom expects PulsedRPUDeviceBase.");
     }
     setWeightGranularity(rpu_device.getWeightGranularity());
     setNumStates(rpu_device.getNumStates());
   };
 
+  void dumpExtra(RPU::state_t &extra, const std::string prefix) override {
+    SimpleRPUDeviceCuda<T>::dumpExtra(extra, prefix);
+
+    RPU::state_t state;
+    RPU::insert(state, "num_states", num_states_);
+    RPU::insert(state, "weight_granularity", weight_granularity_);
+
+    RPU::insertWithPrefix(extra, state, prefix);
+  };
+
+  void loadExtra(const RPU::state_t &extra, const std::string prefix, bool strict) override {
+    SimpleRPUDeviceCuda<T>::loadExtra(extra, prefix, strict);
+
+    auto state = RPU::selectWithPrefix(extra, prefix);
+
+    RPU::load(state, "num_states", num_states_, strict);
+    RPU::load(state, "weight_granularity", weight_granularity_, strict);
+  };
+
   bool isPulsedDevice() const override { return true; };
   bool hasDirectUpdate() const override { return false; };
   void doDirectUpdate(
       const T *x_input,
       const T *d_input,
       T *dev_weights,
       const T lr,
@@ -83,22 +102,22 @@
       const BitLineMaker<T> *blm,
       const PulsedUpdateMetaParameter<T> &up,
       const T lr,
       curandState_t *dev_states,
       int one_sided = 0,
       uint32_t *x_counts_chunk = nullptr,
       uint32_t *d_counts_chunk = nullptr,
-      const ChoppedWeightOutput<T> *cwo = nullptr) = 0;
+      const ChoppedWeightOutput<T> *cwo = nullptr) {
+    RPU_FATAL("Needs implementation");
+  };
 
   virtual pwukpvec_t<T> getUpdateKernels(
-      int m_batch,
-      int nK32,
-      int use_bo64,
-      bool out_trans,
-      const PulsedUpdateMetaParameter<T> &up) = 0;
+      int m_batch, int nK32, int use_bo64, bool out_trans, const PulsedUpdateMetaParameter<T> &up) {
+    RPU_FATAL("Needs implementation");
+  };
 
   PulsedRPUDeviceCudaBase<T> *clone() const override { RPU_FATAL("Needs implementation"); };
 
   inline T getWeightGranularity() const { return weight_granularity_; };
   inline T getNumStates() const { return num_states_; };
   virtual T getPulseCountLearningRate(
       T learning_rate, int current_m_batch, const PulsedUpdateMetaParameter<T> &up) {
@@ -124,29 +143,29 @@
 public:
   explicit PulsedRPUDeviceCuda(){};
   explicit PulsedRPUDeviceCuda(CudaContextPtr c, int x_size, int d_size);
   // explicit PulsedRPUDeviceCuda(CudaContextPtr  c, const PulsedRPUDevice<T> * other);
 
   ~PulsedRPUDeviceCuda(){};
   PulsedRPUDeviceCuda(const PulsedRPUDeviceCuda<T> &other);
-  PulsedRPUDeviceCuda<T> &operator=(const PulsedRPUDeviceCuda<T> &other) = default;
-  PulsedRPUDeviceCuda(PulsedRPUDeviceCuda<T> &&other) = default;
-  PulsedRPUDeviceCuda<T> &operator=(PulsedRPUDeviceCuda<T> &&other) = default;
+  PulsedRPUDeviceCuda<T> &operator=(const PulsedRPUDeviceCuda<T> &other);
+  PulsedRPUDeviceCuda(PulsedRPUDeviceCuda<T> &&other);
+  PulsedRPUDeviceCuda<T> &operator=(PulsedRPUDeviceCuda<T> &&other);
 
   friend void swap(PulsedRPUDeviceCuda<T> &a, PulsedRPUDeviceCuda<T> &b) noexcept {
     using std::swap;
     swap(
         static_cast<PulsedRPUDeviceCudaBase<T> &>(a), static_cast<PulsedRPUDeviceCudaBase<T> &>(b));
     swap(a.dev_diffusion_rate_, b.dev_diffusion_rate_);
     swap(a.dev_reset_bias_, b.dev_reset_bias_);
     swap(a.dev_decay_scale_, b.dev_decay_scale_);
     swap(a.dev_4params_, b.dev_4params_);
-    swap(a.dev_reset_nrnd_, b.dev_reset_nrnd_);
-    swap(a.dev_reset_flag_, b.dev_reset_flag_);
     swap(a.dev_persistent_weights_, b.dev_persistent_weights_);
+    swap(a.dev_neg_pulse_counter_, b.dev_neg_pulse_counter_);
+    swap(a.dev_pos_pulse_counter_, b.dev_pos_pulse_counter_);
   };
 
   // implement abstract functions
   void decayWeights(T *dev_weights, bool bias_no_decay) override;
   void decayWeights(T *dev_weights, T alpha, bool bias_no_decay) override;
   void driftWeights(T *dev_weights, T time_since_epoch) override;
   void diffuseWeights(T *dev_weights) override;
@@ -179,22 +198,26 @@
 
   // for interfacing with pwu_kernel
   virtual T *getGlobalParamsData() { return nullptr; };
   virtual T *get1ParamsData() { return nullptr; };
   virtual float *get2ParamsData() { return nullptr; };
   virtual float *get4ParamsData() { return dev_4params_->getData(); }
   virtual T getWeightGranularityNoise() const { return getPar().dw_min_std; };
+  virtual uint64_t *getPosPulseCountData();
+  virtual uint64_t *getNegPulseCountData();
+  std::vector<uint64_t> getPulseCounters() const override;
+
+  void dumpExtra(RPU::state_t &extra, const std::string prefix) override;
+  void loadExtra(const RPU::state_t &extra, const std::string prefix, bool strict) override;
 
 protected:
   virtual void applyUpdateWriteNoise(T *dev_weights);
-
-  void initResetRnd();
-  std::unique_ptr<CudaArray<float>> dev_reset_nrnd_ = nullptr;
-  std::unique_ptr<CudaArray<float>> dev_reset_flag_ = nullptr;
   std::unique_ptr<CudaArray<float>> dev_persistent_weights_ = nullptr;
+  std::unique_ptr<CudaArray<uint64_t>> dev_pos_pulse_counter_ = nullptr;
+  std::unique_ptr<CudaArray<uint64_t>> dev_neg_pulse_counter_ = nullptr;
 
 private:
   void initialize();
 
   std::unique_ptr<CudaArray<float>> dev_4params_ = nullptr;
   std::unique_ptr<CudaArray<T>> dev_diffusion_rate_ = nullptr;
   std::unique_ptr<CudaArray<T>> dev_decay_scale_ = nullptr;
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_pulsed_device_test.cpp` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_pulsed_device_test.cpp`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -13,14 +13,15 @@
 #include "cuda.h"
 #include "cuda_util.h"
 #include "io_manager.h"
 #include "rng.h"
 #include "rpu_pulsed.h"
 #include "rpucuda_constantstep_device.h"
 #include "rpucuda_expstep_device.h"
+#include "rpucuda_hidden_device.h"
 #include "rpucuda_linearstep_device.h"
 #include "rpucuda_piecewisestep_device.h"
 #include "rpucuda_powstep_device.h"
 #include "rpucuda_pulsed.h"
 #include "rpucuda_pulsed_device.h"
 #include "rpucuda_softbounds_reference_device.h"
 #include "utility_functions.h"
@@ -244,14 +245,15 @@
   int K;
 
   num_t **refweights;
 };
 
 // types
 typedef ::testing::Types<
+    HiddenStepRPUDeviceMetaParameter<num_t>,
     LinearStepRPUDeviceMetaParameter<num_t>,
     ExpStepRPUDeviceMetaParameter<num_t>,
     PowStepRPUDeviceMetaParameter<num_t>,
     ConstantStepRPUDeviceMetaParameter<num_t>,
     PiecewiseStepRPUDeviceMetaParameter<num_t>,
     SoftBoundsReferenceRPUDeviceMetaParameter<num_t>>
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_pulsed_test.cpp` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_pulsed_test.cpp`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_simple_device.cu` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_simple_device.cu`

 * *Files 27% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -12,18 +12,22 @@
 
 #include "pwu_kernel_parameter.h"
 #include "rpu_pulsed_meta_parameter.h"
 #include "rpucuda_pulsed_device.h"
 #include <memory>
 
 #include "rpucuda_buffered_transfer_device.h"
+#include "rpucuda_chopped_transfer_device.h"
 #include "rpucuda_constantstep_device.h"
+#include "rpucuda_dynamic_transfer_device.h"
 #include "rpucuda_expstep_device.h"
+#include "rpucuda_hidden_device.h"
 #include "rpucuda_linearstep_device.h"
 #include "rpucuda_mixedprec_device.h"
+#include "rpucuda_mixedprec_int_device.h"
 #include "rpucuda_onesided_device.h"
 #include "rpucuda_piecewisestep_device.h"
 #include "rpucuda_powstep_device.h"
 #include "rpucuda_powstep_reference_device.h"
 #include "rpucuda_softbounds_reference_device.h"
 #include "rpucuda_transfer_device.h"
 #include "rpucuda_vector_device.h"
@@ -37,14 +41,17 @@
 template <typename T>
 AbstractRPUDeviceCuda<T> *
 AbstractRPUDeviceCuda<T>::createFrom(CudaContextPtr c, const AbstractRPUDevice<T> &rpu_device) {
   switch (rpu_device.getPar().implements()) {
   case DeviceUpdateType::ConstantStep:
     return new ConstantStepRPUDeviceCuda<T>(
         c, static_cast<const ConstantStepRPUDevice<T> &>(rpu_device));
+  case DeviceUpdateType::HiddenStep:
+    return new HiddenStepRPUDeviceCuda<T>(
+        c, static_cast<const HiddenStepRPUDevice<T> &>(rpu_device));
   case DeviceUpdateType::LinearStep:
   case DeviceUpdateType::SoftBounds:
     return new LinearStepRPUDeviceCuda<T>(
         c, static_cast<const LinearStepRPUDevice<T> &>(rpu_device));
   case DeviceUpdateType::ExpStep:
     return new ExpStepRPUDeviceCuda<T>(c, static_cast<const ExpStepRPUDevice<T> &>(rpu_device));
   case DeviceUpdateType::Vector:
@@ -56,22 +63,31 @@
   case DeviceUpdateType::BufferedTransfer:
     return new BufferedTransferRPUDeviceCuda<T>(
         c, static_cast<const BufferedTransferRPUDevice<T> &>(rpu_device));
   case DeviceUpdateType::FloatingPoint:
     return new SimpleRPUDeviceCuda<T>(c, static_cast<const SimpleRPUDevice<T> &>(rpu_device));
   case DeviceUpdateType::MixedPrec:
     return new MixedPrecRPUDeviceCuda<T>(c, static_cast<const MixedPrecRPUDevice<T> &>(rpu_device));
+  case DeviceUpdateType::MixedPrecInt:
+    return new MixedPrecIntRPUDeviceCuda<T>(
+        c, static_cast<const MixedPrecIntRPUDevice<T> &>(rpu_device));
   case DeviceUpdateType::PowStep:
     return new PowStepRPUDeviceCuda<T>(c, static_cast<const PowStepRPUDevice<T> &>(rpu_device));
   case DeviceUpdateType::PowStepReference:
     return new PowStepReferenceRPUDeviceCuda<T>(
         c, static_cast<const PowStepReferenceRPUDevice<T> &>(rpu_device));
   case DeviceUpdateType::PiecewiseStep:
     return new PiecewiseStepRPUDeviceCuda<T>(
         c, static_cast<const PiecewiseStepRPUDevice<T> &>(rpu_device));
+  case DeviceUpdateType::ChoppedTransfer:
+    return new ChoppedTransferRPUDeviceCuda<T>(
+        c, static_cast<const ChoppedTransferRPUDevice<T> &>(rpu_device));
+  case DeviceUpdateType::DynamicTransfer:
+    return new DynamicTransferRPUDeviceCuda<T>(
+        c, static_cast<const DynamicTransferRPUDevice<T> &>(rpu_device));
   case DeviceUpdateType::SoftBoundsReference:
     return new SoftBoundsReferenceRPUDeviceCuda<T>(
         c, static_cast<const SoftBoundsReferenceRPUDevice<T> &>(rpu_device));
   default:
     RPU_FATAL("Pulsed device type not implemented in CUDA. Maybe not added to createFrom in "
               "rpucuda_simple_device.cu?");
   }
@@ -122,14 +138,16 @@
   initialize(other.context_, other.x_size_, other.d_size_);
   if (other.par_storage_ != nullptr) {
     par_storage_ = other.par_storage_->cloneUnique();
   }
   if (other.wdrifter_cuda_) {
     wdrifter_cuda_ = RPU::make_unique<WeightDrifterCuda<T>>(*other.wdrifter_cuda_);
   }
+
+  // rnd buffers are not copied.
 };
 
 template <typename T>
 SimpleRPUDeviceCuda<T> &SimpleRPUDeviceCuda<T>::operator=(const SimpleRPUDeviceCuda<T> &other) {
   SimpleRPUDeviceCuda<T> tmp(other);
   swap(*this, tmp);
   this->context_->synchronize();
@@ -142,18 +160,44 @@
 
 template <typename T>
 SimpleRPUDeviceCuda<T> &SimpleRPUDeviceCuda<T>::operator=(SimpleRPUDeviceCuda<T> &&other) {
 
   initialize(other.context_, other.x_size_, other.d_size_);
   par_storage_ = std::move(other.par_storage_);
   wdrifter_cuda_ = std::move(other.wdrifter_cuda_);
+
+  dev_reset_nrnd_ = std::move(dev_reset_nrnd_);
+  dev_reset_flag_ = std::move(dev_reset_flag_);
+  rnd_context_ = std::move(rnd_context_);
+  dev_diffusion_nrnd_ = std::move(dev_diffusion_nrnd_);
+
   return *this;
 };
 
 template <typename T>
+void SimpleRPUDeviceCuda<T>::dumpExtra(RPU::state_t &extra, const std::string prefix) {
+  RPU::state_t state;
+  context_->synchronize();
+  if (wdrifter_cuda_) {
+    wdrifter_cuda_->dumpExtra(state, "wdrifter_cuda");
+    RPU::insertWithPrefix(extra, state, prefix);
+  }
+}
+
+template <typename T>
+void SimpleRPUDeviceCuda<T>::loadExtra(
+    const RPU::state_t &extra, const std::string prefix, bool strict) {
+  context_->synchronize();
+  if (wdrifter_cuda_) {
+    auto state = RPU::selectWithPrefix(extra, prefix);
+    wdrifter_cuda_->loadExtra(state, "wdrifter", strict);
+  }
+};
+
+template <typename T>
 void SimpleRPUDeviceCuda<T>::populateFrom(const AbstractRPUDevice<T> &rpu_device_in) {
 
   const auto &rpu_device = dynamic_cast<const SimpleRPUDevice<T> &>(rpu_device_in);
   if (&rpu_device == nullptr) {
     RPU_FATAL("populateFrom expects SimpleRPUDevice.");
   }
 
@@ -260,14 +304,87 @@
 
 template <typename T> void SimpleRPUDeviceCuda<T>::clipWeights(T *weights, T clip) {
 
   if (clip >= 0) {
     RPU::math::aclip<T>(context_, weights, size_, clip);
   }
 }
+template <typename T> void SimpleRPUDeviceCuda<T>::initResetRnd() {
+
+  if (this->rnd_context_ == nullptr) {
+    this->initRndContext();
+  }
+  dev_reset_nrnd_ =
+      RPU::make_unique<CudaArray<float>>(&*this->rnd_context_, (this->size_ + 31) / 32 * 32);
+  dev_reset_flag_ =
+      RPU::make_unique<CudaArray<float>>(&*this->rnd_context_, (this->size_ + 31) / 32 * 32);
+  dev_reset_flag_->setConst(0);
+  this->rnd_context_->synchronize();
+}
+
+template <typename T>
+void SimpleRPUDeviceCuda<T>::resetCols(T *weights, int start_col, int n_cols_in, T reset_prob) {
+  // col-major in CUDA.
+
+  T *w = weights;
+
+  int n_cols = (n_cols_in >= 0) ? n_cols_in : this->x_size_;
+
+  int n = n_cols * this->d_size_;
+  int offset = start_col * this->d_size_;
+  bool with_flag = false;
+  bool with_nrnd = false;
+
+  if (getPar().reset_std > 0) {
+    if (dev_reset_nrnd_ == nullptr) {
+      initResetRnd();
+    }
+    this->rnd_context_->randNormal(
+        dev_reset_nrnd_->getData(), n_cols * this->d_size_, 0.0, getPar().reset_std);
+    with_nrnd = true;
+  }
+  if (reset_prob < 1) {
+    if (dev_reset_flag_ == nullptr) {
+      initResetRnd();
+    }
+    this->rnd_context_->randUniform(dev_reset_flag_->getData(), n_cols * this->d_size_);
+    with_flag = true;
+  }
+  if (with_flag || with_nrnd) {
+    this->context_->recordWaitEvent(this->rnd_context_->getStream());
+  }
+
+  if (n >= this->size_) {
+    // reset whole matrix
+    RPU::math::elemreset<T>(
+        this->context_, w, this->size_, nullptr,
+        with_nrnd ? dev_reset_nrnd_->getDataConst() : nullptr,
+        with_flag ? dev_reset_flag_->getDataConst() : nullptr, reset_prob);
+
+  } else if (offset + n <= this->size_) {
+    // one pass enough
+    RPU::math::elemreset<T>(
+        this->context_, w + offset, n, nullptr,
+        with_nrnd ? dev_reset_nrnd_->getDataConst() : nullptr,
+        with_flag ? dev_reset_flag_->getDataConst() : nullptr, reset_prob);
+  } else {
+    // two passes
+    int m = this->size_ - offset;
+
+    RPU::math::elemreset<T>(
+        this->context_, w + offset, m, nullptr,
+        with_nrnd ? dev_reset_nrnd_->getDataConst() : nullptr,
+        with_flag ? dev_reset_flag_->getDataConst() : nullptr, reset_prob);
+
+    RPU::math::elemreset<T>(
+        this->context_, w, n - m, nullptr,
+        with_nrnd ? dev_reset_nrnd_->getDataConst() + m : nullptr,
+        with_flag ? dev_reset_flag_->getDataConst() + m : nullptr, reset_prob);
+  }
+}
 
 template class SimpleRPUDeviceCuda<float>;
 #ifdef RPU_USE_DOUBLE
 template class SimpleRPUDeviceCuda<double>;
 #endif
 
 } // namespace RPU
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_simple_device.h` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_simple_device.h`

 * *Files 12% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -46,18 +46,21 @@
       const T beta,
       const PulsedUpdateMetaParameter<T> &up,
       T *x_buffer,
       T *d_buffer) = 0;
   virtual bool hasDirectUpdate() const = 0;
   virtual int getHiddenUpdateIdx() const { return 0; };
   virtual void setHiddenUpdateIdx(int idx){};
+  virtual void dumpExtra(RPU::state_t &extra, const std::string prefix) = 0;
+  virtual void loadExtra(const RPU::state_t &extra, const std::string prefix, bool strict) = 0;
   virtual void populateFrom(const AbstractRPUDevice<T> &rpu_device) = 0;
   virtual DeviceUpdateType implements() const = 0;
   virtual bool isPulsedDevice() const { return false; };
   virtual AbstractRPUDeviceCuda<T> *clone() const = 0;
+  virtual std::vector<uint64_t> getPulseCounters() const { return std::vector<uint64_t>(); };
   std::unique_ptr<AbstractRPUDeviceCuda<T>> cloneUnique() const {
     return std::unique_ptr<AbstractRPUDeviceCuda<T>>(clone());
   };
   static AbstractRPUDeviceCuda<T> *
   createFrom(CudaContextPtr c, const AbstractRPUDevice<T> &rpu_device);
   static std::unique_ptr<AbstractRPUDeviceCuda<T>>
   createFromUnique(CudaContextPtr c, const AbstractRPUDevice<T> &rpu_device);
@@ -79,31 +82,34 @@
   SimpleRPUDeviceCuda<T> &operator=(SimpleRPUDeviceCuda<T> &&other);
 
   friend void swap(SimpleRPUDeviceCuda<T> &a, SimpleRPUDeviceCuda<T> &b) noexcept {
     using std::swap;
     swap(a.context_, b.context_);
     swap(a.x_size_, b.x_size_);
     swap(a.d_size_, b.d_size_);
+    swap(a.size_, b.size_);
     swap(a.par_storage_, b.par_storage_);
     swap(a.wdrifter_cuda_, b.wdrifter_cuda_);
+    swap(a.dev_reset_nrnd_, b.dev_reset_nrnd_);
+    swap(a.dev_reset_flag_, b.dev_reset_flag_);
+    swap(a.rnd_context_, b.rnd_context_);
+    swap(a.dev_diffusion_nrnd_, b.dev_diffusion_nrnd_);
   };
 
   // implement abstract functions
   std::vector<T> getHiddenWeights() const override {
     std::vector<T> tmp;
     return tmp;
   };
   void decayWeights(T *dev_weights, bool bias_no_decay) override;
   void decayWeights(T *dev_weights, T alpha, bool bias_no_decay) override;
   void driftWeights(T *dev_weights, T time_since_epoch) override;
   void diffuseWeights(T *dev_weights) override;
   void clipWeights(T *dev_weights, T clip) override;
-  void resetCols(T *dev_weights, int start_col, int n_cols, T reset_prob) override {
-    RPU_FATAL("Not supported by simple device.");
-  };
+  void resetCols(T *dev_weights, int start_col, int n_cols, T reset_prob) override;
   void applyWeightUpdate(T *dev_weights, T *dw_and_current_weight_out) override;
   void
   populateFrom(const AbstractRPUDevice<T> &rpu_device) override; // need to be called by derived
   SimpleRPUDeviceMetaParameter<T> &getPar() const override {
     return static_cast<SimpleRPUDeviceMetaParameter<T> &>(*par_storage_);
   };
   DeviceUpdateType implements() const override { return this->getPar().implements(); };
@@ -118,27 +124,32 @@
       const int m_batch,
       const bool x_trans,
       const bool d_trans,
       const T beta,
       const PulsedUpdateMetaParameter<T> &up,
       T *x_buffer = nullptr,
       T *d_buffer = nullptr) override;
+  void dumpExtra(RPU::state_t &extra, const std::string prefix) override;
+  void loadExtra(const RPU::state_t &extra, const std::string prefix, bool strict) override;
 
 protected:
+  void initDiffusionRnd();
+  void initRndContext();
+  void initResetRnd();
+
   int x_size_ = 0;
   int d_size_ = 0;
   int size_ = 0;
   CudaContextPtr context_;
 
   std::unique_ptr<WeightDrifterCuda<T>> wdrifter_cuda_ = nullptr;
-  // these are helpers and not copied
-  void initDiffusionRnd();
-  void initRndContext();
   std::unique_ptr<CudaContext> rnd_context_;
   std::unique_ptr<CudaArray<float>> dev_diffusion_nrnd_ = nullptr;
+  std::unique_ptr<CudaArray<float>> dev_reset_nrnd_ = nullptr;
+  std::unique_ptr<CudaArray<float>> dev_reset_flag_ = nullptr;
 
 private:
   std::unique_ptr<AbstractRPUDeviceMetaParameter<T>> par_storage_;
   void initialize(CudaContextPtr context, int x_size, int d_size);
 };
 
 } // namespace RPU
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_simple_device_test.cpp` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_simple_device_test.cpp`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_softbounds_reference_device.cu` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_softbounds_reference_device.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_softbounds_reference_device.h` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_softbounds_reference_device.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_test.cpp` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_test.cpp`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_transfer_device.cu` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_transfer_device.cu`

 * *Files 3% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -141,14 +141,48 @@
   std::fill(current_slice_indices_.begin(), current_slice_indices_.end(), (int)0);
 
   this->current_update_idx_ = 0;
 
   fully_hidden_ = par.fullyHidden();
 }
 
+template <typename T>
+void TransferRPUDeviceCuda<T>::dumpExtra(RPU::state_t &extra, const std::string prefix) {
+
+  VectorRPUDeviceCuda<T>::dumpExtra(extra, prefix);
+
+  RPU::state_t state;
+
+  RPU::insert(state, "current_slice_indices", current_slice_indices_);
+  // RPU::insert(state, "transfer_vecs", transfer_vecs_);
+
+  transfer_fb_pass_->dumpExtra(state, "transfer_fb_pass");
+  transfer_pwu_->dumpExtra(state, "transfer_pwu");
+  transfer_iom_->dumpExtra(state, "transfer_iom");
+
+  RPU::insertWithPrefix(extra, state, prefix);
+}
+
+template <typename T>
+void TransferRPUDeviceCuda<T>::loadExtra(
+    const RPU::state_t &extra, const std::string prefix, bool strict) {
+
+  VectorRPUDeviceCuda<T>::loadExtra(extra, prefix, strict);
+
+  auto state = RPU::selectWithPrefix(extra, prefix);
+
+  RPU::load(state, "current_slice_indices", current_slice_indices_, strict);
+  if (state.count("transfer_vecs")) {
+    RPU::load(this->context_, state, "transfer_vecs", transfer_vecs_, strict);
+  }
+  transfer_fb_pass_->loadExtra(state, "transfer_fb_pass", strict);
+  transfer_pwu_->loadExtra(state, "transfer_pwu", strict);
+  transfer_iom_->loadExtra(state, "transfer_iom", strict);
+}
+
 /*********************************************************************************/
 /* getPulseCountLearningRate */
 /* Here we compute the LR for the A matrix (the SGD update). Because
    of the device properties it is beneficial to use a constant LR
    here, but scale the buffer with the scheduled SGD learning rate
    later*/
 template <typename T>
@@ -487,15 +521,14 @@
 
 template <typename T>
 void TransferRPUDeviceCuda<T>::decayWeights(T *dev_weights, T alpha, bool bias_no_decay) {
 
   if (fully_hidden_) {
     this->dev_weights_ptrs_[this->n_devices_ - 1] = dev_weights;
   }
-
   VectorRPUDeviceCuda<T>::decayWeights(dev_weights, alpha, bias_no_decay);
 }
 
 template <typename T>
 void TransferRPUDeviceCuda<T>::decayWeights(T *dev_weights, bool bias_no_decay) {
 
   if (fully_hidden_) {
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_transfer_device.h` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_transfer_device.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -49,14 +49,16 @@
   void populateFrom(const AbstractRPUDevice<T> &rpu_device) override;
   TransferRPUDeviceMetaParameter<T> &getPar() const {
     return static_cast<TransferRPUDeviceMetaParameter<T> &>(SimpleRPUDeviceCuda<T>::getPar());
   };
   TransferRPUDeviceCuda<T> *clone() const override { return new TransferRPUDeviceCuda<T>(*this); };
 
   void setHiddenUpdateIdx(int idx) override{};
+  void dumpExtra(RPU::state_t &extra, const std::string prefix) override;
+  void loadExtra(const RPU::state_t &extra, const std::string prefix, bool strict) override;
 
   void runUpdateKernel(
       pwukp_t<T> kpars,
       CudaContextPtr up_context,
       T *dev_weights,
       int m_batch,
       const BitLineMaker<T> *blm,
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_transfer_device_test.cpp` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_transfer_device_test.cpp`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_vector_device.cu` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_vector_device.cu`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -176,14 +176,45 @@
     this->context_->synchronize(); // to be safe
   }
 
   this->context_->synchronizeDevice();
 }
 
 template <typename T>
+void VectorRPUDeviceCuda<T>::dumpExtra(RPU::state_t &extra, const std::string prefix) {
+  PulsedRPUDeviceCudaBase<T>::dumpExtra(extra, prefix);
+
+  RPU::state_t state;
+
+  for (size_t k = 0; k < rpucuda_device_vec_.size(); k++) {
+    rpucuda_device_vec_[k]->dumpExtra(state, std::to_string(k));
+  }
+  RPU::insert(state, "dev_reduce_weightening", dev_reduce_weightening_);
+  RPU::insert(state, "current_device_idx", current_device_idx_);
+  RPU::insert(state, "current_update_idx", current_update_idx_);
+
+  RPU::insertWithPrefix(extra, state, prefix);
+}
+
+template <typename T>
+void VectorRPUDeviceCuda<T>::loadExtra(
+    const RPU::state_t &extra, const std::string prefix, bool strict) {
+  PulsedRPUDeviceCudaBase<T>::loadExtra(extra, prefix, strict);
+
+  auto state = RPU::selectWithPrefix(extra, prefix);
+
+  for (size_t k = 0; k < rpucuda_device_vec_.size(); k++) {
+    rpucuda_device_vec_[k]->loadExtra(state, std::to_string(k), strict);
+  }
+  RPU::load(this->context_, state, "dev_reduce_weightening", dev_reduce_weightening_, strict);
+  RPU::load(state, "current_device_idx", current_device_idx_, strict);
+  RPU::load(state, "current_update_idx", current_update_idx_, strict);
+}
+
+template <typename T>
 void VectorRPUDeviceCuda<T>::reduceToWeights(CudaContextPtr c, T *dev_weights) {
 
   RPU::math::gemv(
       c, false, this->size_, n_devices_, (T)1.0, dev_weights_vec_->getData(), this->size_,
       dev_reduce_weightening_->getData(), 1, (T)0.0, dev_weights, 1);
 }
 
@@ -397,14 +428,24 @@
 template <typename T> std::vector<T> VectorRPUDeviceCuda<T>::getReduceWeightening() const {
   std::vector<T> vec;
   vec.resize(n_devices_);
   dev_reduce_weightening_->copyTo(&vec[0]);
   return vec;
 }
 
+template <typename T> std::vector<uint64_t> VectorRPUDeviceCuda<T>::getPulseCounters() const {
+  std::vector<uint64_t> data;
+
+  for (int k = 0; k < n_devices_; k++) {
+    std::vector<uint64_t> tmp_data = rpucuda_device_vec_[k]->getPulseCounters();
+    data.insert(data.end(), tmp_data.begin(), tmp_data.end());
+  }
+  return data;
+}
+
 #undef LOOP_DEVICES_WITH_CONTEXTS_K
 
 template class VectorRPUDeviceCuda<float>;
 #ifdef RPU_USE_DOUBLE
 template class VectorRPUDeviceCuda<double>;
 #endif
 } // namespace RPU
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_vector_device.h` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_vector_device.h`

 * *Files 10% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -58,14 +58,16 @@
   void resetCols(T *dev_weights, int start_col, int n_cols, T reset_prob) override;
   void applyWeightUpdate(T *dev_weights, T *dw_and_current_weight_out) override {
     // for parallel: would need to sync all weights separately. too costly anyway
     RPU_FATAL("Not supported for vector devices.");
   };
 
   void populateFrom(const AbstractRPUDevice<T> &rpu_device) override;
+  void dumpExtra(RPU::state_t &extra, const std::string prefix) override;
+  void loadExtra(const RPU::state_t &extra, const std::string prefix, bool strict) override;
 
   VectorRPUDeviceMetaParameter<T> &getPar() const override {
     return static_cast<VectorRPUDeviceMetaParameter<T> &>(SimpleRPUDeviceCuda<T>::getPar());
   };
   VectorRPUDeviceCuda<T> *clone() const override { return new VectorRPUDeviceCuda<T>(*this); };
 
   void runUpdateKernel(
@@ -86,14 +88,15 @@
       int m_batch,
       int nK32,
       int use_bo64,
       bool out_trans,
       const PulsedUpdateMetaParameter<T> &up) override;
 
   std::vector<T> getReduceWeightening() const;
+  std::vector<uint64_t> getPulseCounters() const override;
 
 protected:
   virtual void reduceToWeights(CudaContextPtr c, T *dev_weights);
 
   int n_devices_ = 0;
   RealWorldRNG<T> rw_rng_{0};
   std::vector<T *> dev_weights_ptrs_;
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/rpucuda_vector_device_test.cpp` & `aihwkit-0.8.0/src/rpucuda/cuda/rpucuda_vector_device_test.cpp`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/test_helper.cu` & `aihwkit-0.8.0/src/rpucuda/cuda/test_helper.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/test_helper.h` & `aihwkit-0.8.0/src/rpucuda/cuda/test_helper.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/update_management_helper.cu` & `aihwkit-0.8.0/src/rpucuda/cuda/update_management_helper.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -459,23 +459,25 @@
 
 template <int thread_block_size>
 __global__ void kernelGetKBlockAggregate(
     int *K_values, int m_batch_in, kagg_t *Kc_block, kagg_t *Kc_block_aggregate) {
 
   const int m_batch = m_batch_in;
   const int tid = blockIdx.x * blockDim.x + threadIdx.x;
-  __shared__ typename RPU::cub::BlockScan<kagg_t, thread_block_size>::TempStorage temp_storage;
+  __shared__
+      typename RPU_CUB_NS_QUALIFIER BlockScan<kagg_t, thread_block_size>::TempStorage temp_storage;
 
   int K = 0;
   if (tid < m_batch) {
     K = K_values[tid];
   }
   kagg_t Kc = 0;
   kagg_t block_aggregate = 0;
-  RPU::cub::BlockScan<kagg_t, thread_block_size>(temp_storage).ExclusiveSum(K, Kc, block_aggregate);
+  RPU_CUB_NS_QUALIFIER BlockScan<kagg_t, thread_block_size>(temp_storage)
+      .ExclusiveSum(K, Kc, block_aggregate);
 
   if (tid < m_batch) {
     Kc_block[tid] = Kc;
   }
 
   if (threadIdx.x == 0) {
     Kc_block_aggregate[blockIdx.x] = block_aggregate;
@@ -512,22 +514,22 @@
   dev_Kc_block_aggregate_ = RPU::make_unique<CudaArray<kagg_t>>(context_, nblocks);
 
   // Determine temporary device storage requirements
   void *temp_storage = NULL;
   size_t temp_storage_bytes = 0;
   auto s = context_->getStream();
 
-  CUDA_CALL(RPU::cub::DeviceScan::InclusiveSum(
+  CUDA_CALL(RPU_CUB_NS_QUALIFIER DeviceScan::InclusiveSum(
       temp_storage, temp_storage_bytes, dev_K_values_->getData(), dev_Kc_values_->getData() + 1,
       m_batch, s));
   context_->synchronize();
   dev_Kc_temp_storage_ = RPU::make_unique<CudaArray<char>>(context_, (int)temp_storage_bytes);
 
   // average max sum
-  CUDA_CALL(RPU::cub::DeviceReduce::Sum(
+  CUDA_CALL(RPU_CUB_NS_QUALIFIER DeviceReduce::Sum(
       nullptr, temp_storage_bytes, x_maximizer_->getMaxValues(), dev_sumabsmax_value_->getData(),
       m_batch, s));
   dev_sumabsmax_temp_storage_ = RPU::make_unique<CudaArray<char>>(context_, temp_storage_bytes);
   context_->synchronize();
 }
 
 template <typename T> void UpdateManagementHelper<T>::computeKcBlock(int m_batch) {
@@ -541,15 +543,15 @@
       dev_Kc_block_aggregate_->getData());
 }
 
 template <typename T> void UpdateManagementHelper<T>::computeKc(int m_batch) {
 
   // CAUTION: needs K_values to be already computed !!
   size_t temp_storage_bytes = dev_Kc_temp_storage_->getSize();
-  CUDA_CALL(RPU::cub::DeviceScan::InclusiveSum(
+  CUDA_CALL(RPU_CUB_NS_QUALIFIER DeviceScan::InclusiveSum(
       (void *)dev_Kc_temp_storage_->getData(), temp_storage_bytes, dev_K_values_->getData(),
       dev_Kc_values_->getData() + 1, m_batch, context_->getStream()));
 }
 
 template <typename T>
 kagg_t UpdateManagementHelper<T>::getKnValue(bool ublm, int m_batch, int K) const {
   if (!ublm) {
@@ -573,18 +575,18 @@
     x_maximizer_->copyMaxValuesToHost(&m_x);
     d_maximizer_->copyMaxValuesToHost(&m_d);
     return;
   }
 
   // first compute the average of the max over batch
   size_t ssz = dev_sumabsmax_temp_storage_->getSize();
-  CUDA_CALL(RPU::cub::DeviceReduce::Sum(
+  CUDA_CALL(RPU_CUB_NS_QUALIFIER DeviceReduce::Sum(
       (void *)dev_sumabsmax_temp_storage_->getData(), ssz, x_maximizer_->getMaxValues(),
       dev_sumabsmax_value_->getData(), m_batch, context_->getStream()));
-  CUDA_CALL(RPU::cub::DeviceReduce::Sum(
+  CUDA_CALL(RPU_CUB_NS_QUALIFIER DeviceReduce::Sum(
       (void *)dev_sumabsmax_temp_storage_->getData(), ssz, d_maximizer_->getMaxValues(),
       dev_sumabsmax_value_->getData() + 1, m_batch, context_->getStream()));
   T result[2];
   dev_sumabsmax_value_->copyTo(result);
   m_x = result[0] / m_batch;
   m_d = result[1] / m_batch;
 }
@@ -600,18 +602,18 @@
   }
 
   // first compute the average of the max over batch
   size_t ssz = dev_sumabsmax_temp_storage_->getSize();
   LogInputIterator<T> x_input_iter(x_maximizer_->getMaxValues());
   LogInputIterator<T> d_input_iter(d_maximizer_->getMaxValues());
 
-  CUDA_CALL(RPU::cub::DeviceReduce::Sum(
+  CUDA_CALL(RPU_CUB_NS_QUALIFIER DeviceReduce::Sum(
       (void *)dev_sumabsmax_temp_storage_->getData(), ssz, x_input_iter,
       dev_sumabsmax_value_->getData(), m_batch, context_->getStream()));
-  CUDA_CALL(RPU::cub::DeviceReduce::Sum(
+  CUDA_CALL(RPU_CUB_NS_QUALIFIER DeviceReduce::Sum(
       (void *)dev_sumabsmax_temp_storage_->getData(), ssz, d_input_iter,
       dev_sumabsmax_value_->getData() + 1, m_batch, context_->getStream()));
   T result[2];
   dev_sumabsmax_value_->copyTo(result);
   m_x = expf(result[0] / m_batch);
   m_d = expf(result[1] / m_batch);
 }
@@ -623,18 +625,18 @@
     x_maximizer_->copyMaxValuesToHost(&m_x);
     d_maximizer_->copyMaxValuesToHost(&m_d);
     return;
   }
 
   // first compute the average of the max over batch
   size_t ssz = dev_sumabsmax_temp_storage_->getSize();
-  CUDA_CALL(RPU::cub::DeviceReduce::Max(
+  CUDA_CALL(RPU_CUB_NS_QUALIFIER DeviceReduce::Max(
       (void *)dev_sumabsmax_temp_storage_->getData(), ssz, x_maximizer_->getMaxValues(),
       dev_sumabsmax_value_->getData(), m_batch, context_->getStream()));
-  CUDA_CALL(RPU::cub::DeviceReduce::Max(
+  CUDA_CALL(RPU_CUB_NS_QUALIFIER DeviceReduce::Max(
       (void *)dev_sumabsmax_temp_storage_->getData(), ssz, d_maximizer_->getMaxValues(),
       dev_sumabsmax_value_->getData() + 1, m_batch, context_->getStream()));
   T result[2];
   dev_sumabsmax_value_->copyTo(&result[0]);
   m_x = result[0];
   m_d = result[1];
 }
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/update_management_helper.h` & `aihwkit-0.8.0/src/rpucuda/cuda/update_management_helper.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/update_management_helper_test.cpp` & `aihwkit-0.8.0/src/rpucuda/cuda/update_management_helper_test.cpp`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/weight_clipper_cuda.cu` & `aihwkit-0.8.0/src/rpucuda/cuda/weight_clipper_cuda.cu`

 * *Files 9% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -52,17 +52,17 @@
 // ctor
 template <typename T>
 WeightClipperCuda<T>::WeightClipperCuda(CudaContextPtr context, int x_size, int d_size)
     : context_(context), x_size_(x_size), d_size_(d_size), size_(x_size * d_size) {
 
   T *tmp = nullptr;
   StdFunctor<T> std_functor((T)x_size_, tmp);
-  RPU::cub::TransformInputIterator<T, StdFunctor<T>, T *> std_input(tmp, std_functor);
+  RPU_CUB_NS_QUALIFIER TransformInputIterator<T, StdFunctor<T>, T *> std_input(tmp, std_functor);
 
-  RPU::cub::DeviceReduce::Sum(
+  RPU_CUB_NS_QUALIFIER DeviceReduce::Sum(
       nullptr, temp_storage_bytes_, std_input, tmp, size_, context_->getStream());
   dev_temp_storage_ = RPU::make_unique<CudaArray<char>>(context, temp_storage_bytes_);
 }
 
 template <typename T>
 void WeightClipperCuda<T>::apply(T *weights, const WeightClipParameter &wclpar) {
 
@@ -78,15 +78,15 @@
 
     if (!row_amaximizer_) {
       row_amaximizer_ = RPU::make_unique<Maximizer<T>>(context_, x_size_, true);
       dev_sum_value_ = RPU::make_unique<CudaArray<T>>(context_, 1);
     }
     row_amaximizer_->compute(weights, d_size_, true);
 
-    RPU::cub::DeviceReduce::Sum(
+    RPU_CUB_NS_QUALIFIER DeviceReduce::Sum(
         dev_temp_storage_->getData(), temp_storage_bytes_, row_amaximizer_->getMaxValues(),
         dev_sum_value_->getData(), d_size_, s);
 
     kernelAClipC<T><<<nblocks, nthreads, 0, s>>>(
         weights, size_, dev_sum_value_->getData(), (T)d_size_, wclpar.fixed_value);
     break;
   }
@@ -97,23 +97,24 @@
       dev_sum_value_ = RPU::make_unique<CudaArray<T>>(context_, 1);
     }
     if (!dev_std_value_) {
       dev_std_value_ = RPU::make_unique<CudaArray<T>>(context_, 1);
     }
 
     StdFunctor<T> std_functor((T)size_, dev_sum_value_->getData());
-    RPU::cub::TransformInputIterator<T, StdFunctor<T>, T *> std_input(weights, std_functor);
+    RPU_CUB_NS_QUALIFIER TransformInputIterator<T, StdFunctor<T>, T *> std_input(
+        weights, std_functor);
 
     // mean (sum)
-    RPU::cub::DeviceReduce::Sum(
+    RPU_CUB_NS_QUALIFIER DeviceReduce::Sum(
         dev_temp_storage_->getData(), temp_storage_bytes_, weights, dev_sum_value_->getData(),
         size_, s);
 
     // std
-    RPU::cub::DeviceReduce::Sum(
+    RPU_CUB_NS_QUALIFIER DeviceReduce::Sum(
         dev_temp_storage_->getData(), temp_storage_bytes_, std_input, dev_std_value_->getData(),
         size_, s);
 
     kernelAClipSqrt<T><<<nblocks, nthreads, 0, s>>>(
         weights, size_, dev_std_value_->getData(), wclpar.sigma, wclpar.fixed_value);
 
     break;
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/weight_clipper_cuda.h` & `aihwkit-0.8.0/src/rpucuda/cuda/weight_clipper_cuda.h`

 * *Files 12% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -22,14 +22,17 @@
 
 public:
   explicit WeightClipperCuda(CudaContextPtr context, int x_size, int d_size);
   WeightClipperCuda(){};
 
   void apply(T *weights, const WeightClipParameter &wclpar);
 
+  void dumpExtra(RPU::state_t &extra, const std::string prefix){};
+  void loadExtra(const RPU::state_t &extra, const std::string prefix, bool strict){};
+
 private:
   CudaContextPtr context_ = nullptr;
   int x_size_ = 0;
   int d_size_ = 0;
   int size_ = 0;
   size_t temp_storage_bytes_ = 0;
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/weight_clipper_cuda_test.cpp` & `aihwkit-0.8.0/src/rpucuda/cuda/weight_clipper_cuda_test.cpp`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/weight_drifter_cuda.cu` & `aihwkit-0.8.0/src/rpucuda/cuda/weight_drifter_cuda.cu`

 * *Files 12% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -127,14 +127,42 @@
     dev_nu_ = RPU::make_unique<CudaArray<T>>(*other.dev_nu_);
   }
 
   context_->synchronize();
 }
 
 template <typename T>
+void WeightDrifterCuda<T>::dumpExtra(RPU::state_t &extra, const std::string prefix) {
+  RPU::state_t state;
+
+  RPU::insert(state, "active", active_);
+  RPU::insert(state, "current_t", current_t_);
+  RPU::insert(state, "dev_previous_weights", dev_previous_weights_);
+  RPU::insert(state, "dev_w0", dev_w0_);
+  RPU::insert(state, "dev_t", dev_t_);
+  RPU::insert(state, "dev_nu", dev_nu_);
+
+  RPU::insertWithPrefix(extra, state, prefix);
+}
+
+template <typename T>
+void WeightDrifterCuda<T>::loadExtra(
+    const RPU::state_t &extra, const std::string prefix, bool strict) {
+
+  auto state = RPU::selectWithPrefix(extra, prefix);
+
+  RPU::load(context_, state, "dev_previous_weights", dev_previous_weights_, strict);
+  RPU::load(context_, state, "dev_w0", dev_w0_, strict);
+  RPU::load(context_, state, "dev_t", dev_t_, strict);
+  RPU::load(context_, state, "dev_nu", dev_nu_, strict);
+  RPU::load(state, "current_t", current_t_, strict);
+  RPU::load(state, "active", active_, strict);
+}
+
+template <typename T>
 void WeightDrifterCuda<T>::populateFrom(const WeightDrifter<T> &wd, int x_size, int d_size) {
   // only copies the parameter from nu. Other parameters are set when set to active.
 
   if (x_size * d_size != size_ || wd.getSize() != size_) {
     RPU_FATAL("Size mismatch!");
   }
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/weight_drifter_cuda.h` & `aihwkit-0.8.0/src/rpucuda/cuda/weight_drifter_cuda.h`

 * *Files 14% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -33,14 +33,17 @@
   void apply(T *weights, T time_since_last_call);
 
   inline bool isActive() { return active_; };
 
   void saturate(T *weights, float *dev_4params);
   const T *getNu() const { return dev_nu_ == nullptr ? nullptr : dev_nu_->getDataConst(); };
 
+  void dumpExtra(RPU::state_t &extra, const std::string prefix);
+  void loadExtra(const RPU::state_t &extra, const std::string prefix, bool strict);
+
 protected:
   CudaContextPtr context_ = nullptr;
   int size_ = 0;
   int max_size_ = 0;
   bool active_ = false;
   T current_t_ = 0.0;
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/weight_drifter_cuda_test.cpp` & `aihwkit-0.8.0/src/rpucuda/cuda/weight_drifter_cuda_test.cpp`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/weight_modifier_cuda.cu` & `aihwkit-0.8.0/src/rpucuda/cuda/weight_modifier_cuda.cu`

 * *Files 24% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -180,15 +180,15 @@
 
                      T w = weights[i];
                      T stoch_value = curand_normal(&local_state);
 
                      new_weights[i] = w * (1 + stddev * stoch_value););
 }
 
-template <typename T>
+template <typename T, bool preserve_sign>
 __global__ void kernelModifyWeightsProgNoiseN(
     int size_in,
     int d_size,
     const bool copy_last_column,
     T *new_weights,
     const T *weights,
     const T stddev_in, // additional scale:
@@ -202,28 +202,32 @@
 
   const T stddev = stddev_in;
 
   RPU_WM_KERNEL_LOOP(
       true,
 
       T w = weights[i];
-      T stoch_value = curand_normal(&local_state);
+      T stoch_value = curand_normal(&local_state); T aw = fabs(w) / amax;
 
-      T aw = fabs(w) / amax; T paw = 1; T sig = coeffs[0];
+      T paw = 1; T sig = coeffs[0];
 
       for (int j = 1; j < n_coeffs; j++) {
         paw *= aw;
         sig += coeffs[j] * paw;
       }
 
       sig *= stddev;
-      new_weights[i] = w + amax * sig * stoch_value;);
+      T out_w = w + amax * sig * stoch_value;
+
+      if (preserve_sign) { new_weights[i] = (w < (T)0.0) ? -fabs(out_w) : fabs(out_w); } else {
+        new_weights[i] = out_w;
+      });
 }
 
-template <typename T>
+template <typename T, bool preserve_sign>
 __global__ void kernelModifyWeightsProgNoise(
     int size_in,
     int d_size,
     const bool copy_last_column,
     T *new_weights,
     const T *weights,
     const T stddev_in, // additional scale:
@@ -238,21 +242,27 @@
   amax = amax > 0.0 ? amax : (T)1.0;
 
   const T stddev = stddev_in;
   const T p0 = p0_in;
   const T p1 = p1_in;
   const T p2 = p2_in;
 
-  RPU_WM_KERNEL_LOOP(true,
+  RPU_WM_KERNEL_LOOP(
+      true,
 
-                     T w = weights[i];
-                     T stoch_value = curand_normal(&local_state);
+      T w = weights[i];
+      T stoch_value = curand_normal(&local_state);
 
-                     T aw = fabs(w) / amax; T sig = (p0 + aw * p1 + aw * aw * p2) * stddev;
-                     new_weights[i] = w + amax * sig * stoch_value;);
+      T aw = fabs(w) / amax;
+
+      T sig = (p0 + aw * p1 + aw * aw * p2) * stddev; T out_w = w + amax * sig * stoch_value;
+
+      if (preserve_sign) { new_weights[i] = (w < (T)0.0) ? -fabs(out_w) : fabs(out_w); } else {
+        new_weights[i] = out_w;
+      });
 }
 
 template <typename T>
 __global__ void kernelModifyWeightsDropConnections(
     int size_in,
     int d_size,
     const bool copy_last_column,
@@ -293,14 +303,20 @@
       amaximizer_ = RPU::make_unique<Maximizer<T>>(
           context_, wmpar.copy_last_column ? (size_ - d_size_) : size_, true);
     }
     amaximizer_->compute(weights, 1, false);
     amax = amaximizer_->getMaxValues();
   }
 
+  if (wmpar.type != WeightModifierType::Copy) {
+    if (wmpar.per_batch_sample) {
+      RPU_FATAL("Per batch sample is not implemented in RPUCuda");
+    }
+  }
+
   // note: all methods need to work in
   switch (wmpar.type) {
   case WeightModifierType::Copy: {
 
     if (new_weights == weights) {
       RPU_FATAL("cannot use WeightModifierType::Copy with in-place weights.");
     }
@@ -355,16 +371,15 @@
   }
 
   case WeightModifierType::Poly: {
     int n_coeffs = wmpar.coeffs.size();
     if (wmpar.std_dev > 0 && n_coeffs > 0) {
 
       if (n_coeffs <= 3) {
-
-        kernelModifyWeightsProgNoise<T><<<nblocks, nthreads, 0, s>>>(
+        kernelModifyWeightsProgNoise<T, false><<<nblocks, nthreads, 0, s>>>(
             size_, d_size_, wmpar.copy_last_column, new_weights, weights, wmpar.std_dev,
             wmpar.coeffs.at(0), (n_coeffs > 1) ? wmpar.coeffs.at(1) : (T)0.0,
             (n_coeffs > 2) ? wmpar.coeffs.at(2) : (T)0.0, wmpar.assumed_wmax, amax,
             context_->getRandomStates(nblocks * nthreads));
 
       } else {
         // n-poly
@@ -374,23 +389,59 @@
           coeffs_ = wmpar.coeffs;
           context_->synchronize();
         } else if (coeffs_ != wmpar.coeffs) {
           dev_coeffs_->assign(wmpar.coeffs.data());
           coeffs_ = wmpar.coeffs;
         }
 
-        kernelModifyWeightsProgNoiseN<T><<<nblocks, nthreads, 0, s>>>(
+        kernelModifyWeightsProgNoiseN<T, false><<<nblocks, nthreads, 0, s>>>(
             size_, d_size_, wmpar.copy_last_column, new_weights, weights, wmpar.std_dev, n_coeffs,
             dev_coeffs_->getData(), wmpar.assumed_wmax, amax,
             context_->getRandomStates(nblocks * nthreads));
       }
       done = true;
     }
     break;
   }
+
+  case WeightModifierType::ProgNoise: {
+    int n_coeffs = wmpar.coeffs.size();
+    if (wmpar.std_dev > 0 && n_coeffs > 0) {
+
+      T std = wmpar.std_dev / wmpar.g_max;
+
+      if (n_coeffs <= 3) {
+        kernelModifyWeightsProgNoise<T, true><<<nblocks, nthreads, 0, s>>>(
+            size_, d_size_, wmpar.copy_last_column, new_weights, weights, std, wmpar.coeffs.at(0),
+            (n_coeffs > 1) ? wmpar.coeffs.at(1) : (T)0.0,
+            (n_coeffs > 2) ? wmpar.coeffs.at(2) : (T)0.0, wmpar.assumed_wmax, amax,
+            context_->getRandomStates(nblocks * nthreads));
+
+      } else {
+        // n-poly
+
+        if (wmpar.coeffs.size() != coeffs_.size() || dev_coeffs_ == nullptr) {
+          dev_coeffs_ = RPU::make_unique<CudaArray<T>>(context_, n_coeffs, wmpar.coeffs.data());
+          coeffs_ = wmpar.coeffs;
+          context_->synchronize();
+        } else if (coeffs_ != wmpar.coeffs) {
+          dev_coeffs_->assign(wmpar.coeffs.data());
+          coeffs_ = wmpar.coeffs;
+        }
+
+        kernelModifyWeightsProgNoiseN<T, true><<<nblocks, nthreads, 0, s>>>(
+            size_, d_size_, wmpar.copy_last_column, new_weights, weights, std, n_coeffs,
+            dev_coeffs_->getData(), wmpar.assumed_wmax, amax,
+            context_->getRandomStates(nblocks * nthreads));
+      }
+      done = true;
+    }
+    break;
+  }
+
   case WeightModifierType::DiscretizeAddNormal: {
     if (wmpar.res > 0 || wmpar.std_dev > 0) {
 
       kernelModifyWeightsDiscretizeAddNormal<T><<<nblocks, nthreads, 0, s>>>(
           size_, d_size_, wmpar.copy_last_column, new_weights, weights, wmpar.res, wmpar.sto_round,
           wmpar.std_dev, wmpar.assumed_wmax, amax, context_->getRandomStates(nblocks * nthreads));
       done = true;
@@ -415,14 +466,38 @@
 
     kernelModifyWeightsDropConnections<T><<<nblocks, nthreads, 0, s>>>(
         size_, d_size_, wmpar.copy_last_column, new_weights, new_weights, wmpar.pdrop,
         context_->getRandomStates(nblocks * nthreads));
   }
 }
 
+template <typename T>
+void WeightModifierCuda<T>::dumpExtra(RPU::state_t &extra, const std::string prefix) {
+  RPU::state_t state;
+
+  // don't handle maximizers (no states)
+  RPU::insert(state, "enable_during_test", enable_during_test_);
+  RPU::insert(state, "coeffs", coeffs_);
+  RPU::insert(state, "dev_coeffs", dev_coeffs_);
+
+  RPU::insertWithPrefix(extra, state, prefix);
+}
+
+template <typename T>
+void WeightModifierCuda<T>::loadExtra(
+    const RPU::state_t &extra, const std::string prefix, bool strict) {
+
+  using V = std::vector<T>;
+  auto state = RPU::selectWithPrefix(extra, prefix);
+
+  RPU::load(state, "enable_during_test", enable_during_test_, strict);
+  RPU::load(state, "coeffs", coeffs_, strict);
+  RPU::load(this->context_, state, "dev_coeffs", dev_coeffs_, strict);
+}
+
 template class WeightModifierCuda<float>;
 #ifdef RPU_USE_DOUBLE
 template class WeightModifierCuda<double>;
 #endif
 
 #undef RPU_WM_KERNEL_LOOP
 } // namespace RPU
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/weight_modifier_cuda.h` & `aihwkit-0.8.0/src/rpucuda/cuda/weight_modifier_cuda.h`

 * *Files 10% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -24,20 +24,22 @@
   explicit WeightModifierCuda(CudaContextPtr context, int x_size, int d_size);
   WeightModifierCuda(){};
 
   void apply(T *new_weights, const T *weights, const WeightModifierParameter<T> &wmpar);
 
   inline bool enableDuringTest() { return enable_during_test_; };
 
+  void dumpExtra(RPU::state_t &extra, const std::string prefix);
+  void loadExtra(const RPU::state_t &extra, const std::string prefix, bool strict);
+
 private:
   CudaContextPtr context_ = nullptr;
   int x_size_ = 0;
   int d_size_ = 0;
   int size_ = 0;
-  int max_size_ = 0;
   bool enable_during_test_ = false;
   // no need to copy
   std::unique_ptr<Maximizer<T>> amaximizer_ = nullptr;
   std::unique_ptr<Maximizer<T>> row_amaximizer_ = nullptr;
   std::unique_ptr<Maximizer<T>> row_maximizer_ = nullptr;
   std::unique_ptr<Maximizer<T>> row_minimizer_ = nullptr;
   std::vector<T> coeffs_;
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/weight_remapper_cuda.cu` & `aihwkit-0.8.0/src/rpucuda/cuda/weight_remapper_cuda.cu`

 * *Files 14% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -116,14 +116,63 @@
 
     if (idx < d_size) {
       scales_out[idx] = new_scale;
     }
   }
 }
 
+/*Scale row-wise and shift*/
+template <typename T>
+__global__ void kernelRemapScaleAndShiftRowwise(
+    T *weights_out,
+    T *scales_out, // cannot be in-place otherwise some threads might use new scale...!!!
+    T *biases_out,
+    const T *weights_in,
+    const T *scales,     // NOTE: these need to be initialized !!! should be d_size on per output
+    const T *biases,     // NOTE: these need to be initialized !!!
+    const int x_size_in, //
+    const int d_size_in, // major
+    const float *max_values,    // d_size: eg. max per row
+    const float *neg_max_values // d_size: eg. max of - weights
+) {
+  int x_size = x_size_in;
+  int d_size = d_size_in;
+  int size = x_size * d_size;
+
+  RPU_CUDA_1D_KERNEL_LOOP(idx, size) {
+    T w = weights_in[idx];
+    int didx = idx % d_size;
+    T max_value = (T)max_values[didx]; // shared mem!?? maybe not worth the effort
+    T min_value = -(T)neg_max_values[didx];
+    T b = biases[didx];
+    T s = scales[didx];
+
+    T half_span = (max_value - min_value) / (T)2.0;
+    half_span = half_span > 0 ? half_span : (T)1.0;
+    T new_scale = s * half_span;
+    T new_b = b - new_scale - s * min_value;
+    // T full_w = w*s + b;
+    // T new_full_w = new_w*new_scale + new_b;
+    /* new_full_w == full_w
+       new_w*new_scale + new_b == w*s + b
+       new_w == (w*s + b - new_b)/new_scale
+       new_w == (w*s + new_scale + s*min_value)/new_scale
+       new_w == w*s/new_scale + 1 + s*min_value/new_scale
+       new_w == w/half_span + 1 + min_value/half_span
+    */
+
+    weights_out[idx] = w / half_span + 1 + min_value / half_span;
+
+    if (idx < d_size) {
+      scales_out[didx] = new_scale;
+      biases_out[didx] = new_b;
+    }
+  }
+}
+
 // ctor
 template <typename T>
 WeightRemapperCuda<T>::WeightRemapperCuda(CudaContextPtr context, int x_size, int d_size)
     : context_(context), x_size_(x_size), d_size_(d_size), size_(x_size * d_size),
       max_size_(x_size * d_size) {}
 
 template <typename T> int WeightRemapperCuda<T>::getNBlocks(int nthreads) {
@@ -196,15 +245,14 @@
       kernelRemapScaleRowwiseRange<T><<<getNBlocks(nthreads), nthreads, 0, s>>>(
           weights, scales, scale_buffer_->getData(), weights, x_size_, d_size_,
           row_amaximizer_->getMaxValues(), scale_minimizer_->getMaxValues(), wrmpar.max_scale_range,
           wrmpar.max_scale_ref);
     }
     break;
   }
-
   case WeightRemapType::None: {
     break;
   }
   default:
     RPU_FATAL("Remapping type not implemented.");
   } // switch
 }
```

### Comparing `aihwkit-0.7.1/src/rpucuda/cuda/weight_remapper_cuda.h` & `aihwkit-0.8.0/src/rpucuda/cuda/weight_remapper_cuda.h`

 * *Files 21% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -23,16 +23,19 @@
 public:
   explicit WeightRemapperCuda(CudaContextPtr context, int x_size, int d_size);
   WeightRemapperCuda(){};
 
   void apply(
       T *weights, T current_lr, const WeightRemapParameter &wrmpar, T *scales, T *biases = nullptr);
 
+  void dumpExtra(RPU::state_t &extra, const std::string prefix){};
+  void loadExtra(const RPU::state_t &extra, const std::string prefix, bool strict){};
+
 private:
-  CudaContext *context_ = nullptr;
+  CudaContextPtr context_ = nullptr;
   int x_size_ = 0;
   int d_size_ = 0;
   int size_ = 0;
   int max_size_ = 0;
 
   // no need to copy
   std::unique_ptr<Maximizer<T>> amaximizer_ = nullptr;
```

### Comparing `aihwkit-0.7.1/src/rpucuda/dense_bit_line_maker.cpp` & `aihwkit-0.8.0/src/rpucuda/dense_bit_line_maker.cpp`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/dense_bit_line_maker.h` & `aihwkit-0.8.0/src/rpucuda/dense_bit_line_maker.h`

 * *Files 8% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -51,14 +51,18 @@
       const T lr,
       const T dw_min,
       const PulsedUpdateMetaParameter<T> &up);
 
   void printCounts(int max_n) const;
   bool supports(RPU::PulseType pulse_type) const;
 
+  /* Ignore the buffer / counts, as they will be generated anew each sample.*/
+  void dumpExtra(RPU::state_t &extra, const std::string prefix){};
+  void loadExtra(const RPU::state_t &extra, const std::string prefix, bool strict){};
+
 private:
   void freeContainers();
   void allocateContainers();
   void initialize(int x_size, int d_size);
   inline void generateCoincidences(
       int *coincidences,
       const int *x_counts,
```

### Comparing `aihwkit-0.7.1/src/rpucuda/math_util.cpp` & `aihwkit-0.8.0/src/rpucuda/math_util.cpp`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/math_util.h` & `aihwkit-0.8.0/src/rpucuda/math_util.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/rng.cpp` & `aihwkit-0.8.0/src/rpucuda/rng.cpp`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/rng.h` & `aihwkit-0.8.0/src/rpucuda/rng.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/rpu.cpp` & `aihwkit-0.8.0/src/rpucuda/rpu.cpp`

 * *Files 7% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -205,24 +205,18 @@
   for (int i = 0; i < d_sz; ++i) {
     for (int j = 0; j < x_sz; ++j) {
       weights_[i][j] = (i + 1) * (T)100.0 + (j + 1);
     }
   }
   weights_buffer_ = Array_2D_Get<T>(d_sz, x_sz);
 
-  temp_x_matrix_bias_size_ = 0;
-  temp_x_matrix_bias_ = nullptr;
-
-  temp_tensor_size_ = 0;
-  temp_tensor_ = nullptr;
-
   matrix_indices_ = nullptr;
   matrix_indices_set_ = false;
 
-  temp_x_vector_bias_ = new T[x_sz];
+  temp_x_vector_bias_.resize(x_sz);
 
   rng_ = std::make_shared<RNG<T>>(0);
   rw_rng_ = std::make_shared<RealWorldRNG<T>>(0);
 
   last_update_m_batch_ = 1;
 
   fwd_alpha_ = (T)1.0;
@@ -237,17 +231,14 @@
   DEBUG_CALL(this->disp(););
   DEBUG_OUT("RPUSimple constructed.");
 }
 
 /*********************************************************************************/
 template <typename T> RPUSimple<T>::~RPUSimple() {
 
-  delete[] temp_x_vector_bias_;
-
-  temp_x_vector_bias_ = nullptr;
   rng_ = nullptr;
   rw_rng_ = nullptr;
 
   if (!shared_weights_if_) {
     Array_2D_Free<T>(weights_);
   } else {
     delete[] weights_;
@@ -256,24 +247,14 @@
 
   Array_2D_Free<T>(weights_buffer_);
 
   if (fb_weights_ != nullptr) {
     Array_2D_Free<T>(fb_weights_);
   }
 
-  if (temp_x_matrix_bias_ != nullptr) {
-    delete[] temp_x_matrix_bias_;
-  }
-  temp_x_matrix_bias_ = nullptr;
-
-  if (temp_tensor_ != nullptr) {
-    delete[] temp_tensor_;
-  }
-  temp_tensor_ = nullptr;
-
   matrix_indices_ = nullptr; // memory externally governed
 
   DEBUG_OUT("RPUSimple DESTRUCTED");
 }
 
 /*********************************************************************************/
 // copy constructor
@@ -306,20 +287,14 @@
     RPU::math::copy<T>(this->x_size_ * this->d_size_, other.fb_weights_[0], 1, fb_weights_[0], 1);
     fb_weight_modifier_ =
         RPU::make_unique<WeightModifier<T>>(this->x_size_, this->d_size_); // no copy
   }
 
   use_delayed_update_ = other.use_delayed_update_;
 
-  temp_x_matrix_bias_size_ = 0;
-  temp_x_matrix_bias_ = nullptr; // will be generated a new if needed
-
-  temp_tensor_size_ = 0;
-  temp_tensor_ = nullptr; // will be generated a new if needed
-
   par_ = other.par_;
 
   matrix_indices_ = other.matrix_indices_;
   matrix_indices_set_ = other.matrix_indices_set_;
 
   // note: RNG / temp_values are not copied.
   last_update_m_batch_ = other.last_update_m_batch_;
@@ -344,17 +319,14 @@
 }
 
 // move assignment
 template <typename T> RPUSimple<T> &RPUSimple<T>::operator=(RPUSimple<T> &&other) noexcept {
 
   RPUAbstract<T>::operator=(std::move(other));
 
-  temp_x_vector_bias_ = other.temp_x_vector_bias_;
-  other.temp_x_vector_bias_ = nullptr;
-
   use_delayed_update_ = other.use_delayed_update_;
 
   rng_ = std::move(other.rng_);
   rw_rng_ = std::move(other.rw_rng_);
 
   par_ = other.par_;
 
@@ -369,25 +341,17 @@
   fb_weights_ = other.fb_weights_;
   other.fb_weights_ = nullptr;
 
   delta_weights_extern_ = std::move(other.delta_weights_extern_);
 
   fb_weight_modifier_ = std::move(other.fb_weight_modifier_);
 
+  temp_x_vector_bias_ = other.temp_x_vector_bias_;
   temp_x_matrix_bias_ = other.temp_x_matrix_bias_;
-  other.temp_x_matrix_bias_ = nullptr;
-
-  temp_x_matrix_bias_size_ = other.temp_x_matrix_bias_size_;
-  other.temp_x_matrix_bias_size_ = 0;
-
   temp_tensor_ = other.temp_tensor_;
-  other.temp_tensor_ = nullptr;
-
-  temp_tensor_size_ = other.temp_tensor_size_;
-  other.temp_tensor_size_ = 0;
 
   matrix_indices_ = other.matrix_indices_;
   other.matrix_indices_ = nullptr;
 
   matrix_indices_set_ = other.matrix_indices_set_;
   other.matrix_indices_set_ = false;
 
@@ -399,14 +363,113 @@
   fwd_alpha_ = other.fwd_alpha_;
   bwd_alpha_ = other.bwd_alpha_;
 
   return *this;
 }
 
 /*********************************************************************************/
+/* dump / load state */
+
+template <typename T> void RPUSimple<T>::dumpExtra(RPU::state_t &extra, const std::string prefix) {
+  // This dumps all the temporary buffers needed for setting the
+  // state.  It will not store the meta-parameters and the sizes it
+  // will also not store the buffers that are already handled by
+  // getDeviceParameters
+  using V = std::vector<T>;
+
+  V tmp(this->x_size_ * this->d_size_);
+  RPU::state_t state;
+
+  if (this->fb_weights_) {
+    RPU::math::copy<T>(tmp.size(), this->fb_weights_[0], 1, tmp.data(), 1);
+    RPU::insert(state, "fb_weights", tmp);
+    fb_weight_modifier_->dumpExtra(state, "fb_weight_modifier");
+  }
+
+  RPU::math::copy<T>(tmp.size(), this->weights_buffer_[0], 1, tmp.data(), 1);
+  if (use_delayed_update_) {
+    RPU::insert(state, "weights_buffer", tmp);
+  }
+
+  if (wdrifter_) {
+    wdrifter_->dumpExtra(state, "wdrifter");
+  }
+  if (wclipper_) {
+    wclipper_->dumpExtra(state, "wclipper");
+  }
+  if (wremapper_) {
+    wremapper_->dumpExtra(state, "wremapper");
+  }
+
+  // ignore the temporary buffers
+  // matrix_indices_ not handled... set from outside
+  // RNG (seeds) not copied
+
+  RPU::insert(state, "learning_rate", this->learning_rate_);
+  RPU::insert(state, "use_delayed_update", use_delayed_update_);
+  RPU::insert(state, "last_update_m_batch", last_update_m_batch_);
+  RPU::insert(state, "fwd_alpha", fwd_alpha_);
+  RPU::insert(state, "bwd_alpha", bwd_alpha_);
+
+  RPU::insertWithPrefix(extra, state, prefix);
+}
+
+template <typename T>
+void RPUSimple<T>::loadExtra(const RPU::state_t &extra, const std::string prefix, bool strict) {
+
+  using V = std::vector<T>;
+  auto state = RPU::selectWithPrefix(extra, prefix);
+
+  if (state.count("fb_weights")) {
+    V tmp;
+    RPU::load(state, "fb_weights", tmp, strict);
+    if (tmp.size()) {
+      if (fb_weights_ == nullptr) {
+        fb_weights_ = Array_2D_Get<T>(this->d_size_, this->x_size_);
+        fb_weight_modifier_ = RPU::make_unique<WeightModifier<T>>(this->x_size_, this->d_size_);
+      }
+      RPU::math::copy<T>(tmp.size(), tmp.data(), 1, fb_weights_[0], 1);
+    }
+  }
+
+  if (state.count("wdrifter")) {
+    if (!wdrifter_) {
+      wdrifter_ = RPU::make_unique<WeightDrifter<T>>(this->x_size_ * this->d_size_, getPar().drift);
+    }
+    wdrifter_->loadExtra(state, "wdrifter", strict);
+  }
+
+  if (state.count("wclipper")) {
+    if (!wclipper_) {
+      wclipper_ = RPU::make_unique<WeightClipper<T>>(this->x_size_, this->d_size_);
+    }
+    wclipper_->loadExtra(state, "wclipper", strict);
+  }
+
+  if (state.count("wremapper")) {
+    if (!wremapper_) {
+      wremapper_ = RPU::make_unique<WeightRemapper<T>>(this->x_size_, this->d_size_);
+    }
+    wremapper_->loadExtra(state, "wremapper", strict);
+  }
+
+  RPU::load(state, "learning_rate", this->learning_rate_, strict);
+  RPU::load(state, "use_delayed_update", use_delayed_update_, strict);
+  RPU::load(state, "last_update_m_batch", last_update_m_batch_, strict);
+  RPU::load(state, "fwd_alpha", fwd_alpha_, strict);
+  RPU::load(state, "bwd_alpha", bwd_alpha_, strict);
+
+  if (use_delayed_update_) {
+    V tmp;
+    RPU::load(state, "weights_buffer", tmp, strict);
+    RPU::math::copy<T>(tmp.size(), tmp.data(), 1, weights_buffer_[0], 1);
+  }
+}
+
+/*********************************************************************************/
 /* General forward/backward/update */
 
 template <typename T>
 void RPUSimple<T>::forward(
     const T *X_input,
     T *D_output,
     bool bias,
@@ -447,15 +510,15 @@
     }
   }
 }
 
 template <typename T>
 void RPUSimple<T>::update(
     const T *X_input, const T *D_input, bool bias, int m_batch, bool x_trans, bool d_trans) {
-  last_update_m_batch_ = m_batch; // this is mini-batchsize*reuse_factor !
+  last_update_m_batch_ = m_batch; // this is mini-batchsize * reuse_factor !
 
   // update weights
   if ((m_batch == 1) && (!x_trans) && (!d_trans)) {
     // short-cut for vectors
     if (bias)
       this->updateVectorBias(X_input, D_input);
     else
@@ -557,24 +620,16 @@
   // TODO: use a better way to do this with GEMM LDA etc.
   T *bias_buffer = this->copyToMatrixBiasBuffer(X_input_without_bias, m_batch, x_trans);
   this->updateMatrix(bias_buffer, D_input, m_batch, x_trans, d_trans);
   this->releaseMatrixBiasBuffer();
 }
 
 template <typename T> T *RPUSimple<T>::getMatrixBiasBuffer(int m_batch) {
-
-  if (temp_x_matrix_bias_size_ < m_batch) {
-    DEBUG_OUT("Get new buffer size " << m_batch);
-    if (temp_x_matrix_bias_ != nullptr) {
-      delete[] temp_x_matrix_bias_;
-    }
-    temp_x_matrix_bias_ = new T[(size_t)m_batch * this->x_size_];
-    temp_x_matrix_bias_size_ = m_batch;
-  }
-  return temp_x_matrix_bias_;
+  temp_x_matrix_bias_.resize((size_t)m_batch * this->x_size_);
+  return temp_x_matrix_bias_.data();
 }
 
 template <typename T>
 T *RPUSimple<T>::copyToMatrixBiasBuffer(const T *X_input_without_bias, int m_batch, bool x_trans) {
   T *bias_buffer = getMatrixBiasBuffer(m_batch);
   RPU::math::makeBias<T>(bias_buffer, X_input_without_bias, this->x_size_, m_batch, x_trans);
   return bias_buffer;
@@ -619,22 +674,23 @@
       RPU_FATAL("updateVector for delta weights and xd_inc>1 is not implemented.");
     }
   }
 }
 
 template <typename T>
 void RPUSimple<T>::copyFromVectorBiasBuffer(T *x_output_without_bias, int x_inc) {
-  RPU::math::copy<T>(this->x_size_ - 1, temp_x_vector_bias_, 1, x_output_without_bias, x_inc);
+  RPU::math::copy<T>(
+      this->x_size_ - 1, temp_x_vector_bias_.data(), 1, x_output_without_bias, x_inc);
 }
 
 template <typename T>
 T *RPUSimple<T>::copyToVectorBiasBuffer(const T *x_input_without_bias, int x_inc) {
-  RPU::math::copy<T>(this->x_size_ - 1, x_input_without_bias, x_inc, temp_x_vector_bias_, 1);
+  RPU::math::copy<T>(this->x_size_ - 1, x_input_without_bias, x_inc, temp_x_vector_bias_.data(), 1);
   temp_x_vector_bias_[this->x_size_ - 1] = 1.;
-  return temp_x_vector_bias_;
+  return temp_x_vector_bias_.data();
 }
 
 template <typename T>
 void RPUSimple<T>::forwardVectorBias(
     const T *x_input_without_bias, T *d_output, int x_inc, int d_inc, bool is_test) {
   T *bias_buffer = this->copyToVectorBiasBuffer(x_input_without_bias, x_inc);
   this->forwardVector(bias_buffer, d_output, 1, d_inc, is_test);
@@ -654,30 +710,22 @@
   this->updateVector(bias_buffer, d_input, 1, d_inc);
 }
 
 /*********************************************************************************/
 /* Tensor forward/backward/update */
 
 template <typename T>
-void RPUSimple<T>::getTensorBuffer(T **x_tensor, T **d_tensor, int m_batch, int dim3) {
-  int x_size = this->getXSize();
-  int d_size = this->getDSize();
-
-  int n = (x_size + d_size) * dim3 * m_batch;
-  if (temp_tensor_size_ < n) {
-    if (temp_tensor_ != nullptr) {
-      delete[] temp_tensor_;
-    }
-    temp_tensor_ = new T[n];
-    temp_tensor_size_ = n;
-  }
+void RPUSimple<T>::getTensorBuffer(T **x_tensor_ptr, T **d_tensor_ptr, int m_batch, int dim3) {
+
+  int n = (this->x_size_ + this->d_size_) * dim3 * m_batch;
+  temp_tensor_.resize(n);
 
   // permute 132
-  *x_tensor = temp_tensor_;
-  *d_tensor = &temp_tensor_[(x_size)*dim3 * m_batch];
+  *x_tensor_ptr = temp_tensor_.data();
+  *d_tensor_ptr = temp_tensor_.data() + this->x_size_ * dim3 * m_batch;
 }
 
 template <typename T>
 void RPUSimple<T>::permute132(
     T *out_tensor, const T *in_tensor, int dim1, int dim2, int dim3, bool bias2) {
   // adds bias to original dimension dim2
   math::permute132<T>(out_tensor, in_tensor, dim1, dim2, dim3, bias2);
@@ -688,15 +736,17 @@
     const T *X_input, T *D_output, bool bias, int m_batch, int dim3, bool trans, bool is_test) {
   if ((dim3 == 1) || (!trans))
     this->forward(X_input, D_output, bias, dim3 * m_batch, trans, trans, is_test);
   else {
     int x_size = this->getXSize();
     int d_size = this->getDSize();
 
-    T *x_tensor, *d_tensor;
+    T *x_tensor = nullptr;
+    T *d_tensor = nullptr;
+
     this->getTensorBuffer(&x_tensor, &d_tensor, m_batch, dim3);
 
     this->permute132(x_tensor, X_input, m_batch, x_size, dim3, bias);
 
     this->forwardMatrix(x_tensor, d_tensor, m_batch * dim3, true, true, is_test);
 
     this->permute132(D_output, d_tensor, m_batch, dim3, d_size, false);
@@ -708,15 +758,17 @@
     const T *D_input, T *X_output, bool bias, int m_batch, int dim3, bool trans) {
   if ((dim3 == 1) || (!trans))
     this->backward(D_input, X_output, bias, dim3 * m_batch, trans, trans);
   else {
     int x_size = this->getXSize();
     int d_size = this->getDSize();
 
-    T *x_tensor, *d_tensor;
+    T *x_tensor = nullptr;
+    T *d_tensor = nullptr;
+
     this->getTensorBuffer(&x_tensor, &d_tensor, m_batch, dim3);
 
     this->permute132(d_tensor, D_input, m_batch, d_size, dim3, false);
 
     this->backward(d_tensor, x_tensor, bias, m_batch * dim3, true, true);
 
     int b = (bias) ? 1 : 0;
@@ -729,15 +781,17 @@
     const T *X_input, const T *D_input, bool bias, int m_batch, int dim3, bool trans) {
   if ((dim3 == 1) || (!trans))
     this->update(X_input, D_input, bias, m_batch * dim3, trans, trans);
   else {
     int x_size = this->getXSize();
     int d_size = this->getDSize();
 
-    T *x_tensor, *d_tensor;
+    T *x_tensor = nullptr;
+    T *d_tensor = nullptr;
+
     this->getTensorBuffer(&x_tensor, &d_tensor, m_batch, dim3);
 
     this->permute132(x_tensor, X_input, m_batch, x_size, dim3, bias);
     this->permute132(d_tensor, D_input, m_batch, d_size, dim3, false);
 
     this->update(x_tensor, d_tensor, false, m_batch * dim3, true, true);
   }
@@ -1024,15 +1078,16 @@
     int m_batch,
     int dim3,
     bool trans,
     bool is_test) {
   // EXPECTS forward index to be set properly !!
   // total_input_size is size of X_input
 
-  T *x_tensor, *d_tensor;
+  T *x_tensor = nullptr;
+  T *d_tensor = nullptr;
   this->getTensorBuffer(&x_tensor, &d_tensor, m_batch, dim3);
 
   this->copyIndexedInput(
       x_tensor, X_input, total_input_size, this->getMatrixIndices(), this->getXSize(), m_batch,
       dim3, trans);
   if ((dim3 > 1) && trans) {
     this->forwardMatrix(x_tensor, d_tensor, m_batch * dim3, trans, trans, is_test);
@@ -1049,15 +1104,17 @@
     int total_input_size,
     int m_batch,
     int dim3,
     bool trans,
     int m_batch_slice,
     const int *batch_indices,
     bool is_test) {
-  T *x_tensor, *d_tensor;
+  T *x_tensor = nullptr;
+  T *d_tensor = nullptr;
+
   this->getTensorBuffer(&x_tensor, &d_tensor, m_batch_slice, dim3);
 
   this->copyIndexedInput(
       x_tensor, X_input, total_input_size, this->getMatrixIndices(), this->getXSize(), m_batch,
       dim3, trans, m_batch_slice, batch_indices);
 
   this->forwardMatrix(x_tensor, d_tensor, m_batch_slice * dim3, trans, trans, is_test);
@@ -1069,15 +1126,17 @@
 template <typename T>
 void RPUSimple<T>::backwardIndexed(
     const T *D_input, T *X_output, int total_output_size, int m_batch, int dim3, bool trans) {
   // -- EXPECTS backward index to be set properly !!
   // -- total_output_size is size of X_output
   // -- bias is handled within the indeces
 
-  T *x_tensor, *d_tensor;
+  T *x_tensor = nullptr;
+  T *d_tensor = nullptr;
+
   this->getTensorBuffer(&x_tensor, &d_tensor, m_batch, dim3);
 
   if ((dim3 == 1) || (!trans)) {
     this->backwardMatrix(D_input, x_tensor, m_batch * dim3, trans, trans);
   } else {
     this->permute132(d_tensor, D_input, m_batch, this->getDSize(), dim3, false);
     this->backwardMatrix(d_tensor, x_tensor, m_batch * dim3, trans, trans);
@@ -1097,15 +1156,17 @@
     T *X_output,
     int total_output_size,
     int m_batch,
     int dim3,
     bool trans,
     int m_batch_slice,
     const int *batch_indices) {
-  T *x_tensor, *d_tensor;
+  T *x_tensor = nullptr;
+  T *d_tensor = nullptr;
+
   this->getTensorBuffer(&x_tensor, &d_tensor, m_batch_slice, dim3);
 
   this->copySliceInput(
       d_tensor, D_input, this->getDSize(), m_batch, dim3, trans, m_batch_slice, batch_indices);
 
   this->backwardMatrix(d_tensor, x_tensor, m_batch_slice * dim3, trans, trans);
 
@@ -1113,15 +1174,17 @@
       X_output, x_tensor, total_output_size, this->getMatrixIndices(), this->getXSize(), m_batch,
       dim3, trans, m_batch_slice, batch_indices);
 }
 
 template <typename T>
 void RPUSimple<T>::updateIndexed(
     const T *X_input, const T *D_input, int total_x_input_size, int m_batch, int dim3, bool trans) {
-  T *x_tensor, *d_tensor;
+  T *x_tensor = nullptr;
+  T *d_tensor = nullptr;
+
   this->getTensorBuffer(&x_tensor, &d_tensor, m_batch, dim3);
 
   this->copyIndexedInput(
       x_tensor, X_input, total_x_input_size, this->getMatrixIndices(), this->getXSize(), m_batch,
       dim3, trans);
   if (trans) {
     this->permute132(d_tensor, D_input, m_batch, this->getDSize(), dim3, false);
@@ -1138,15 +1201,17 @@
     int total_x_input_size,
     int m_batch,
     int dim3,
     bool trans,
     int m_batch_slice,
     const int *batch_indices) {
 
-  T *x_tensor, *d_tensor;
+  T *x_tensor = nullptr;
+  T *d_tensor = nullptr;
+
   this->getTensorBuffer(&x_tensor, &d_tensor, m_batch_slice, dim3);
 
   this->copyIndexedInput(
       x_tensor, X_input, total_x_input_size, this->getMatrixIndices(), this->getXSize(), m_batch,
       dim3, trans, m_batch_slice, batch_indices);
 
   this->copySliceInput(
@@ -1496,15 +1561,14 @@
 }
 
 template <typename T> void RPUSimple<T>::clipWeights(const WeightClipParameter &wclpar) {
 
   if (wclipper_ == nullptr) {
     wclipper_ = RPU::make_unique<WeightClipper<T>>(this->x_size_, this->d_size_);
   }
-
   wclipper_->apply(getWeightsPtr()[0], wclpar);
 }
 
 template <typename T> void RPUSimple<T>::diffuseWeights() {
 
   T diffusion = getPar().diffusion;
   if (diffusion > 0.0) {
@@ -1514,15 +1578,14 @@
     for (int i = 0; i < size; ++i) {
       w[i] += diffusion * rng_->sampleGauss();
     }
   }
 }
 
 /*********************************************************************************/
-
 template <typename T>
 void RPUSimple<T>::remapWeights(const WeightRemapParameter &wrmpar, T *scales, T *biases) {
 
   if (wremapper_ == nullptr) {
     wremapper_ = RPU::make_unique<WeightRemapper<T>>(this->x_size_, this->d_size_);
   }
   wremapper_->apply(getWeightsPtr()[0], this->getAlphaLearningRate(), wrmpar, scales, biases);
```

### Comparing `aihwkit-0.7.1/src/rpucuda/rpu.h` & `aihwkit-0.8.0/src/rpucuda/rpu.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -19,14 +19,15 @@
 #include "weight_remapper.h"
 #include <cfenv>
 #include <iostream>
 #include <memory>
 #include <mutex>
 #include <random>
 #include <sstream>
+#include <unordered_map>
 
 // #pragma STDC FENV_ACCESS ON
 
 #define USE_LOOPED_MATRIX_FORWARD(T)                                                               \
   inline void forwardMatrix(                                                                       \
       const T *X_input, T *D_output, int m_batch, bool x_trans, bool d_trans, bool is_test)        \
       override {                                                                                   \
@@ -163,14 +164,16 @@
     printToStream(ss);
     std::cout << ss.str();
   };
 
   RPUSimple<T> *createRPUArray(int x_size, int d_size) {
     auto *rpu = new RPUSimple<T>(x_size, d_size);
     rpu->populateParameter(this);
+    rpu->setWeightsUniformRandom(-0.1, 0.1);
+    rpu->setLearningRate(0.1);
     return rpu;
   };
 };
 
 /******************************************************************************/
 /* RPU Simple */
 
@@ -187,31 +190,27 @@
   RPUSimple<T> &operator=(RPUSimple<T> &&) noexcept;
 
   friend void swap(RPUSimple<T> &a, RPUSimple<T> &b) noexcept {
 
     using std::swap;
     swap(static_cast<RPUAbstract<T> &>(a), static_cast<RPUAbstract<T> &>(b));
 
-    swap(a.temp_x_vector_bias_, b.temp_x_vector_bias_);
-
     swap(a.rng_, b.rng_);
     swap(a.rw_rng_, b.rw_rng_);
     swap(a.par_, b.par_);
 
     swap(a.weights_, b.weights_);
     swap(a.shared_weights_if_, b.shared_weights_if_);
 
     swap(a.weights_buffer_, b.weights_buffer_);
     swap(a.use_delayed_update_, b.use_delayed_update_);
 
+    swap(a.temp_x_vector_bias_, b.temp_x_vector_bias_);
     swap(a.temp_x_matrix_bias_, b.temp_x_matrix_bias_);
-    swap(a.temp_x_matrix_bias_size_, b.temp_x_matrix_bias_size_);
-
     swap(a.temp_tensor_, b.temp_tensor_);
-    swap(a.temp_tensor_size_, b.temp_tensor_size_);
 
     swap(a.matrix_indices_, b.matrix_indices_);
     swap(a.matrix_indices_set_, b.matrix_indices_set_);
 
     swap(a.wdrifter_, b.wdrifter_);
 
     swap(a.wremapper_, b.wremapper_);
@@ -301,14 +300,19 @@
      interface function to get/set the current device parameters,
      which usually are drawn during instantiation of the RPU object
      based on parameters defining their probabilty distributions. */
   virtual void getDeviceParameterNames(std::vector<std::string> &names) const { names.clear(); };
   virtual void getDeviceParameter(std::vector<T *> &data_ptrs){};
   virtual void setDeviceParameter(const std::vector<T *> &data_ptrs){};
 
+  /* These dumps extra state vectors that are not returned by
+     getDeviuceParameters or getWeights*/
+  virtual void dumpExtra(RPU::state_t &extra, const std::string prefix);
+  virtual void loadExtra(const RPU::state_t &extra, const std::string prefix, bool strict);
+
   virtual int getHiddenUpdateIdx() const { return 0; };
   virtual void setHiddenUpdateIdx(int idx){};
 
   /* Decaying the weights once. Alpha can be a factor additionally
      applied to the current decay rate*/
   virtual void decayWeights(bool bias_no_decay);
   virtual void decayWeights(T alpha, bool bias_no_decay);
@@ -373,14 +377,16 @@
      for analog training. This can be called before an update is
      made to use a the given weight pointer as weight storage. beta
      is the GEMM beta: W = alpha*X*D + beta*W. Thus setting beta=1
      means that only DW is stored in w */
   virtual void setDeltaWeights(T *dw_extern);
   virtual T *getDeltaWeights() const { return delta_weights_extern_[0]; };
 
+  virtual void setVerbosityLevel(int verbose){};
+
   /* public interfaces for forward/backward/update. Format is
      expected in x-major order. However, the batch dimension comes
      first iif x_trans or d_trans is set to true */
   void forward(
       const T *X_input,
       T *D_output,
       bool bias = false,
@@ -469,14 +475,15 @@
       int dim3,
       bool trans,
       int m_batch_slice,
       const int *batch_indices);
 
   virtual ContextPtr getContext() const { return nullptr; };
   virtual bool hasInternalContext() const { return true; };
+  virtual std::vector<uint64_t> getPulseCounters() const { return std::vector<uint64_t>(); }
 
 protected:
   /* for specialized forward/backward. To be used in any forward pass
      Note: we do not test with specialized forward/backward
      weights. This is in most cases what one wants, as specialized
      FB weights is meant to be a kind of hardware-aware training and
      during testing one usually assumes that one uses inference
@@ -532,15 +539,15 @@
       int m_batch,
       bool x_trans = false,
       bool d_trans = false) override;
 
   /* when overriding copy methods below, _Vector_Bias can be used in derived */
   virtual T *copyToVectorBiasBuffer(const T *x_input_without_bias, int x_inc);
   virtual void copyFromVectorBiasBuffer(T *x_output_without_bias, int x_inc);
-  virtual T *getVectorBiasBuffer() const { return temp_x_vector_bias_; };
+  virtual T *getVectorBiasBuffer() { return temp_x_vector_bias_.data(); };
 
   void forwardVector(const T *x_input, T *d_output, int x_inc, int d_inc, bool is_test) override;
   void backwardVector(const T *d_input, T *x_output, int d_inc = 1, int x_inc = 1) override;
   void updateVector(const T *x_input, const T *d_input, int x_inc = 1, int d_inc = 1) override;
 
   void forwardVectorBias(
       const T *x_input_without_bias, T *d_output, int x_inc, int d_inc, bool is_test) override;
@@ -562,15 +569,15 @@
       const T *X_input,
       const T *D_input,
       int m_batch,
       bool x_trans = false,
       bool d_trans = false) override;
 
   /* only these need to be overloaded for the Tensor interface */
-  virtual void getTensorBuffer(T **x_tensor, T **d_tensor, int m_batch, int dim3);
+  virtual void getTensorBuffer(T **x_tensor_ptr, T **d_tensor_ptr, int m_batch, int dim3);
   virtual void
   permute132(T *out_tensor, const T *in_tensor, int dim1, int dim2, int dim3, bool bias2);
 
   /* indexed interface: no need to overload, if not some performance benefits possible*/
   virtual void copyIndexedInput(
       T *out_tensor,
       const T *src_tensor,
@@ -645,19 +652,17 @@
 private:
   std::vector<T *> delta_weights_extern_;
 
   void initialize(int x_sz, int d_sz);
 
   SimpleMetaParameter<T> par_;
 
-  T *temp_x_vector_bias_ = nullptr;
-  T *temp_x_matrix_bias_ = nullptr;
-  int temp_x_matrix_bias_size_ = 0;
-  T *temp_tensor_ = nullptr;
-  int temp_tensor_size_ = 0;
+  std::vector<T> temp_x_vector_bias_;
+  std::vector<T> temp_x_matrix_bias_;
+  std::vector<T> temp_tensor_;
 
   std::unique_ptr<WeightDrifter<T>> wdrifter_ = nullptr;
   std::unique_ptr<WeightRemapper<T>> wremapper_ = nullptr;
   std::unique_ptr<WeightClipper<T>> wclipper_ = nullptr;
   std::unique_ptr<WeightModifier<T>> fb_weight_modifier_ = nullptr;
 
   int *matrix_indices_ = nullptr;
```

### Comparing `aihwkit-0.7.1/src/rpucuda/rpu_buffered_transfer_device.cpp` & `aihwkit-0.8.0/src/rpucuda/rpu_buffered_transfer_device.cpp`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/rpu_buffered_transfer_device.h` & `aihwkit-0.8.0/src/rpucuda/rpu_buffered_transfer_device.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/rpu_constantstep_device.cpp` & `aihwkit-0.8.0/src/rpucuda/rpu_constantstep_device.cpp`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/rpu_constantstep_device.h` & `aihwkit-0.8.0/src/rpucuda/rpu_constantstep_device.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/rpu_expstep_device.cpp` & `aihwkit-0.8.0/src/rpucuda/rpu_expstep_device.cpp`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/rpu_expstep_device.h` & `aihwkit-0.8.0/src/rpucuda/rpu_expstep_device.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/rpu_forward_backward_pass.cpp` & `aihwkit-0.8.0/src/rpucuda/rpu_forward_backward_pass.cpp`

 * *Files 12% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -62,14 +62,52 @@
   } else {
     RPU::math::gemv<T>(
         CblasRowMajor, CblasNoTrans, out_size, in_size, alpha, weights[0], in_size, in_values,
         in_inc, beta, out_values, out_inc);
   }
 }
 
+template <typename T>
+void ForwardBackwardPass<T>::dumpExtra(RPU::state_t &extra, const std::string prefix) {
+
+  RPU::state_t state;
+
+  RPU::insert(state, "fwd.v_offset", fb_pars_.fwd.v_offset);
+  RPU::insert(state, "fwd.w_asymmetry", fb_pars_.fwd.w_asymmetry);
+  RPU::insert(state, "fwd.out_nonlinearity", fb_pars_.fwd.out_nonlinearity);
+  RPU::insert(state, "fwd.out_nonlinearity_factor", fb_pars_.fwd.out_nonlinearity_factor);
+  RPU::insert(state, "fwd.out_noise_values", fb_pars_.fwd.out_noise_values);
+
+  RPU::insert(state, "bwd.v_offset", fb_pars_.bwd.v_offset);
+  RPU::insert(state, "bwd.w_asymmetry", fb_pars_.bwd.w_asymmetry);
+  RPU::insert(state, "bwd.out_nonlinearity", fb_pars_.bwd.out_nonlinearity);
+  RPU::insert(state, "bwd.out_nonlinearity_factor", fb_pars_.bwd.out_nonlinearity_factor);
+  RPU::insert(state, "bwd.out_noise_values", fb_pars_.bwd.out_noise_values);
+
+  RPU::insertWithPrefix(extra, state, prefix);
+}
+
+template <typename T>
+void ForwardBackwardPass<T>::loadExtra(
+    const RPU::state_t &extra, const std::string prefix, bool strict) {
+
+  auto state = RPU::selectWithPrefix(extra, prefix);
+  RPU::load(state, "fwd.v_offset", fb_pars_.fwd.v_offset, strict);
+  RPU::load(state, "fwd.w_asymmetry", fb_pars_.fwd.w_asymmetry, strict);
+  RPU::load(state, "fwd.out_nonlinearity", fb_pars_.fwd.out_nonlinearity, strict);
+  RPU::load(state, "fwd.out_nonlinearity_factor", fb_pars_.fwd.out_nonlinearity_factor, strict);
+  RPU::load(state, "fwd.out_noise_values", fb_pars_.fwd.out_noise_values, strict);
+
+  RPU::load(state, "bwd.v_offset", fb_pars_.bwd.v_offset, strict);
+  RPU::load(state, "bwd.w_asymmetry", fb_pars_.bwd.w_asymmetry, strict);
+  RPU::load(state, "bwd.out_nonlinearity", fb_pars_.bwd.out_nonlinearity, strict);
+  RPU::load(state, "bwd.out_nonlinearity_factor", fb_pars_.bwd.out_nonlinearity_factor, strict);
+  RPU::load(state, "bwd.out_noise_values", fb_pars_.bwd.out_noise_values, strict);
+}
+
 template class ForwardBackwardPass<float>;
 #ifdef RPU_USE_DOUBLE
 template class ForwardBackwardPass<double>;
 #endif
 
 /**********************************************/
 /* noise management                          */
@@ -263,14 +301,23 @@
       size_t size = out_size * in_size;
       mv_pars.w_asymmetry.resize(size);
 
       for (size_t i = 0; i < size; i++) {
         mv_pars.w_asymmetry[i] = ((T)1.0 + io.w_read_asymmetry_dtod * rng_->sampleGauss());
       }
     }
+
+    if (io.out_noise_std > (T)0.0) {
+      mv_pars.out_noise_values.resize(out_size);
+
+      for (size_t i = 0; i < out_size; i++) {
+        mv_pars.out_noise_values[i] =
+            std::fabs(io.out_noise * ((T)1.0 + io.out_noise_std * rng_->sampleGauss()));
+      }
+    }
   };
 
   populate(f_io_, this->fb_pars_.fwd, this->x_size_, this->d_size_);
   populate(b_io_, this->fb_pars_.bwd, this->d_size_, this->x_size_);
 }
 
 template <typename T> void ForwardBackwardPassIOManaged<T>::ensureImplemented() {}
@@ -625,19 +672,18 @@
     bool with_asymmetry,
     bool with_bm,
     bool sto_round_if,
     bool with_nonlinearity>
 inline bool finalizeOutputImplStage5(ARGS) {
   int idx = 0;
   bool bound_test_passed = true;
-  T bound = io.out_bound > 0 ? io.out_bound : std::numeric_limits<T>::infinity();
-  T noise = io.out_noise;
-  T asymmetry_scale = ((T)1.0 - io.out_asymmetry);
-  T res = io.out_res;
-  T nlf = mv_pars.out_nonlinearity_factor;
+  const T bound = io.out_bound > 0 ? io.out_bound : std::numeric_limits<T>::infinity();
+  const T asymmetry_scale = ((T)1.0 - io.out_asymmetry);
+  const T res = io.out_res;
+  const T nlf = mv_pars.out_nonlinearity_factor;
 
   PRAGMA_SIMD
   for (int i = 0; i < out_size; ++i) {
 
     T value = out_values[idx];
 
     if (with_nonlinearity) {
@@ -646,15 +692,16 @@
 
     if (with_asymmetry) {
       // after NL (because experimental feature anyway and easier for CUDA)
       value = value < (T)0 ? value * asymmetry_scale : value;
     }
 
     if (with_noise) {
-      value += noise * rng->sampleGauss();
+      const T noise_std = io.out_noise_std > (T)0.0 ? mv_pars.out_noise_values[i] : io.out_noise;
+      value += noise_std * rng->sampleGauss();
     }
 
     value = getDiscretizedValueSR<sto_round_if>(value, res, *rng);
 
     if (with_bm) {
       if (value > bound) {
         value = bound;
@@ -700,15 +747,15 @@
     return finalizeOutputImplStage3<T, with_noise, with_asymmetry, true>(ARGS_CALL);
   } else {
     return finalizeOutputImplStage3<T, with_noise, with_asymmetry, false>(ARGS_CALL);
   }
 }
 
 template <typename T, bool with_asymmetry> inline bool finalizeOutputImplStage1(ARGS) {
-  if (io.out_noise > (T)0.0) {
+  if (io.out_noise > (T)0.0 || io.out_noise_std > (T)0.0) {
     return finalizeOutputImplStage2<T, true, with_asymmetry>(ARGS_CALL);
   } else {
     return finalizeOutputImplStage2<T, true, with_asymmetry>(ARGS_CALL);
   }
 }
 
 #undef ARGS
@@ -979,13 +1026,33 @@
 
   if (scaling || out_scale != 1.0) {
     RPU::math::scal<T>(this->x_size_, out_scale * nm_scale_value, x_output, x_inc);
   }
 };
 #undef CHECK_INPUT_BOUNDS
 
+template <typename T>
+void ForwardBackwardPassIOManaged<T>::dumpExtra(RPU::state_t &extra, const std::string prefix) {
+
+  ForwardBackwardPass<T>::dumpExtra(extra, prefix);
+
+  RPU::state_t state;
+  RPU::insert(state, "aux_nm_value", aux_nm_value_);
+  RPU::insertWithPrefix(extra, state, prefix);
+}
+
+template <typename T>
+void ForwardBackwardPassIOManaged<T>::loadExtra(
+    const RPU::state_t &extra, const std::string prefix, bool strict) {
+
+  ForwardBackwardPass<T>::loadExtra(extra, prefix, strict);
+
+  auto state = RPU::selectWithPrefix(extra, prefix);
+  RPU::load(state, "aux_nm_value", aux_nm_value_, strict);
+}
+
 template class ForwardBackwardPassIOManaged<float>;
 #ifdef RPU_USE_DOUBLE
 template class ForwardBackwardPassIOManaged<double>;
 #endif
 
 } // namespace RPU
```

### Comparing `aihwkit-0.7.1/src/rpucuda/rpu_forward_backward_pass.h` & `aihwkit-0.8.0/src/rpucuda/rpu_forward_backward_pass.h`

 * *Files 9% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -17,14 +17,15 @@
 #include <memory>
 
 namespace RPU {
 
 template <typename T> class MVParameter {
 
 public:
+  std::vector<T> out_noise_values;
   std::vector<T> v_offset;
   std::vector<T> w_asymmetry;
   std::vector<T> out_nonlinearity;
   T out_nonlinearity_factor = 0.0;
 };
 
 template <typename T> class FBParameter {
@@ -76,14 +77,17 @@
       T *out_vales,
       const int out_size,
       const int out_inc,
       const T alpha,
       const T beta,
       const bool transpose);
 
+  virtual void dumpExtra(RPU::state_t &extra, const std::string prefix);
+  virtual void loadExtra(const RPU::state_t &extra, const std::string prefix, bool strict);
+
 protected:
   int x_size_ = 0;
   int d_size_ = 0;
 
   // parameter storage
   FBParameter<T> fb_pars_;
 };
@@ -109,14 +113,17 @@
     swap(a.b_io_, b.b_io_);
     swap(a.checked_implemented_, b.checked_implemented_);
     swap(a.rng_, b.rng_);
 
     // others are tmps so far
   }
 
+  void dumpExtra(RPU::state_t &extra, const std::string prefix) override;
+  void loadExtra(const RPU::state_t &extra, const std::string prefix, bool strict) override;
+
   void forwardVector(
       T **weights,
       const T *x_input,
       const int x_inc,
       T *d_output,
       const int d_inc,
       const T alpha,
```

### Comparing `aihwkit-0.7.1/src/rpucuda/rpu_linearstep_device.cpp` & `aihwkit-0.8.0/src/rpucuda/rpu_linearstep_device.cpp`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/rpu_linearstep_device.h` & `aihwkit-0.8.0/src/rpucuda/rpu_linearstep_device.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/rpu_mixedprec_device.cpp` & `aihwkit-0.8.0/src/rpucuda/rpu_mixedprec_device.cpp`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/rpu_mixedprec_device.h` & `aihwkit-0.8.0/src/rpucuda/rpu_mixedprec_device.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -137,14 +137,17 @@
 protected:
   void populate(const MixedPrecRPUDeviceMetaParameter<T> &par, RealWorldRNG<T> *rng);
 
 private:
   void initialize(int x_size, int d_size);
   void freeContainers();
 
+  // handled in base
+  T **chi_ = nullptr;
+
+  // temporary
   std::vector<T> qx_;
   std::vector<T> qd_;
   std::vector<int> qx_index_;
-  T **chi_ = nullptr;
 };
 
 } // namespace RPU
```

### Comparing `aihwkit-0.7.1/src/rpucuda/rpu_mixedprec_device_base.cpp` & `aihwkit-0.8.0/src/rpucuda/rpu_mixedprec_device_base.cpp`

 * *Files 5% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -237,14 +237,51 @@
     granularity_ = par.granularity;
   }
   if (granularity_ <= 0) {
     RPU_FATAL("Cannot establish granularity from device. Need explicit setting >=0.");
   }
 }
 
+template <typename T>
+void MixedPrecRPUDeviceBase<T>::dumpExtra(RPU::state_t &extra, const std::string prefix) {
+  SimpleRPUDevice<T>::dumpExtra(extra, prefix);
+
+  RPU::state_t state;
+
+  rpu_device_->dumpExtra(state, "rpu_device");
+  transfer_pwu_->dumpExtra(state, "transfer_pwu");
+
+  RPU::insert(state, "granularity", granularity_);
+  RPU::insert(state, "transfer_tmp", transfer_tmp_);
+  RPU::insert(state, "current_row_index", current_row_index_);
+  RPU::insert(state, "current_update_index", current_update_index_);
+  RPU::insert(state, "avg_sparsity", avg_sparsity_);
+
+  // transfer_d_vecs not handled (generated on the fly)
+
+  RPU::insertWithPrefix(extra, state, prefix);
+}
+
+template <typename T>
+void MixedPrecRPUDeviceBase<T>::loadExtra(
+    const RPU::state_t &extra, const std::string prefix, bool strict) {
+  SimpleRPUDevice<T>::loadExtra(extra, prefix, strict);
+
+  auto state = RPU::selectWithPrefix(extra, prefix);
+
+  rpu_device_->loadExtra(state, "rpu_device", strict);
+  transfer_pwu_->loadExtra(state, "transfer_pwu", strict);
+
+  RPU::load(state, "granularity", granularity_, strict);
+  RPU::load(state, "transfer_tmp", transfer_tmp_, strict);
+  RPU::load(state, "current_row_index", current_row_index_, strict);
+  RPU::load(state, "current_update_index", current_update_index_, strict);
+  RPU::load(state, "avg_sparsity", avg_sparsity_, strict);
+}
+
 /*********************************************************************************/
 /* transfer */
 
 template <typename T>
 void MixedPrecRPUDeviceBase<T>::doTransfer(T **weights, const T lr, const int m_batch_info) {
   const auto &par = getPar();
   int every = par.transfer_every * m_batch_info;
```

### Comparing `aihwkit-0.7.1/src/rpucuda/rpu_mixedprec_device_base.h` & `aihwkit-0.8.0/src/rpucuda/rpu_mixedprec_device_base.h`

 * *Files 3% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -116,14 +116,18 @@
 
   void printDP(int x_count, int d_count) const override;
   void getDPNames(std::vector<std::string> &names) const override;
   void getDeviceParameter(T **weights, std::vector<T *> &data_ptrs) override;
   void setDeviceParameter(T **out_weights, const std::vector<T *> &data_ptrs) override;
   int getHiddenWeightsCount() const override;
   void setHiddenWeights(const std::vector<T> &data) override;
+
+  void dumpExtra(RPU::state_t &extra, const std::string prefix) override;
+  void loadExtra(const RPU::state_t &extra, const std::string prefix, bool strict) override;
+
   bool usesUpdateParameter() const override { return true; };
   bool onSetWeights(T **weights) override;
 
   void decayWeights(T **weights, bool bias_no_decay) override;
   void decayWeights(T **weights, T alpha, bool bias_no_decay) override;
   void diffuseWeights(T **weights, RNG<T> &rng) override;
   void clipWeights(T **weights, T clip) override;
```

### Comparing `aihwkit-0.7.1/src/rpucuda/rpu_onesided_device.cpp` & `aihwkit-0.8.0/src/rpucuda/rpu_onesided_device.cpp`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/rpu_onesided_device.h` & `aihwkit-0.8.0/src/rpucuda/rpu_onesided_device.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/rpu_onesided_device_test.cpp` & `aihwkit-0.8.0/src/rpucuda/rpu_onesided_device_test.cpp`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/rpu_piecewisestep_device.cpp` & `aihwkit-0.8.0/src/rpucuda/rpu_piecewisestep_device.cpp`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/rpu_piecewisestep_device.h` & `aihwkit-0.8.0/src/rpucuda/rpu_piecewisestep_device.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/rpu_powstep_device.cpp` & `aihwkit-0.8.0/src/rpucuda/rpu_powstep_device.cpp`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/rpu_powstep_device.h` & `aihwkit-0.8.0/src/rpucuda/rpu_powstep_device.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/rpu_powstep_reference_device.cpp` & `aihwkit-0.8.0/src/rpucuda/rpu_powstep_reference_device.cpp`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/rpu_powstep_reference_device.h` & `aihwkit-0.8.0/src/rpucuda/rpu_powstep_reference_device.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/rpu_pulsed.cpp` & `aihwkit-0.8.0/src/rpucuda/rpu_pulsed.cpp`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -52,14 +52,16 @@
 }
 
 template <typename T>
 RPUPulsed<T> *PulsedMetaParameter<T>::createRPUArray(
     int x_size, int d_size, AbstractRPUDeviceMetaParameter<T> *dp) {
   auto *rpu = new RPUPulsed<T>(x_size, d_size);
   rpu->populateParameter(this, dp);
+  rpu->setWeightsUniformRandom(-0.1, 0.1);
+  rpu->setLearningRate(0.1);
   return rpu;
 };
 
 template <typename T> void PulsedMetaParameter<T>::print() const {
   std::stringstream ss;
   printToStream(ss);
   std::cout << ss.str();
@@ -396,14 +398,40 @@
 
 template <typename T> void RPUPulsed<T>::setFBParameter(FBParameter<T> &fb_pars) {
   CHECK_RPU_DEVICE_INIT;
   fb_pass_->setFBParameter(fb_pars);
 };
 
 /*********************************************************************************/
+/* dump / load state */
+
+template <typename T> void RPUPulsed<T>::dumpExtra(RPU::state_t &extra, const std::string prefix) {
+
+  RPUSimple<T>::dumpExtra(extra, prefix);
+
+  RPU::state_t state;
+
+  pwu_->dumpExtra(state, "pwu");
+  fb_pass_->dumpExtra(state, "fb_pass");
+  rpu_device_->dumpExtra(state, "rpu_device");
+
+  RPU::insertWithPrefix(extra, state, prefix);
+}
+
+template <typename T>
+void RPUPulsed<T>::loadExtra(const RPU::state_t &extra, const std::string prefix, bool strict) {
+
+  RPUSimple<T>::loadExtra(extra, prefix, strict);
+
+  auto state = RPU::selectWithPrefix(extra, prefix);
+
+  pwu_->loadExtra(state, "pwu", strict);
+  fb_pass_->loadExtra(state, "fb_pass", strict);
+  rpu_device_->loadExtra(state, "rpu_device", strict);
+}
 
 /*********************************************************************************/
 
 template <typename T>
 void RPUPulsed<T>::populateParameter(
     PulsedMetaParameter<T> *p, AbstractRPUDeviceMetaParameter<T> *dp) {
   // set parent meta pars (from device pars)
```

### Comparing `aihwkit-0.7.1/src/rpucuda/rpu_pulsed.h` & `aihwkit-0.8.0/src/rpucuda/rpu_pulsed.h`

 * *Files 3% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -77,14 +77,16 @@
   void applyWeightUpdate(T *dw_and_current_weights_out) override;
 
   virtual const PulsedMetaParameter<T> &getMetaPar() const { return par_; };
 
   void getDeviceParameterNames(std::vector<std::string> &names) const override;
   void getDeviceParameter(std::vector<T *> &data_ptrs) override;
   void setDeviceParameter(const std::vector<T *> &data_ptrs) override;
+  void dumpExtra(RPU::state_t &extra, const std::string prefix) override;
+  void loadExtra(const RPU::state_t &extra, const std::string prefix, bool strict) override;
 
   int getHiddenUpdateIdx() const override;
   void setHiddenUpdateIdx(int idx) override;
 
   void setLearningRate(T lrate) override;
   void printToStream(std::stringstream &ss) const override;
   void printParametersToStream(std::stringstream &ss) const override;
```

### Comparing `aihwkit-0.7.1/src/rpucuda/rpu_pulsed_device.cpp` & `aihwkit-0.8.0/src/rpucuda/rpu_pulsed_device.cpp`

 * *Files 6% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -40,16 +40,16 @@
     ss << ", ctoc=" << dw_min_std << ")" << std::endl;
 
     ss << "\t up_down:\t\t" << up_down << "\t(dtod=" << up_down_dtod << ")" << std::endl;
 
     ss << "\t w min:\t\t\t" << w_min << "\t(dtod=" << w_min_dtod << ")" << std::endl;
     ss << "\t w max:\t\t\t" << w_max << "\t(dtod=" << w_max_dtod << ")" << std::endl;
 
-    ss << "\t resets to:\t\t" << reset << "\t(dtod=" << reset_dtod << ", ctoc=" << reset_std << ")"
-       << std::endl;
+    ss << "\t resets to:\t\t" << reset << "\t(dtod=" << reset_dtod << ", ctoc=" << this->reset_std
+       << ")" << std::endl;
 
     if (this->implementsWriteNoise() && write_noise_std > (T)0.0) {
       ss << "\t write noise std:\t" << write_noise_std << std::endl;
     }
 
     if (this->lifetime > 0) {
       ss << "\t lifetime [decay]:\t" << this->lifetime << "\t(dtod=" << lifetime_dtod << ")"
@@ -57,14 +57,19 @@
     }
 
     if (corrupt_devices_prob > 0) {
       ss << "\t corrupt_devices_prob:\t" << corrupt_devices_prob << std::endl;
       ss << "\t corrupt_devices_range:\t" << corrupt_devices_range << std::endl;
     }
 
+    if (adjust_bounds_with_up_down) {
+      ss << "\t adjusted bounds with up/down (dev=" << adjust_bounds_with_up_down_dev << ")"
+         << std::endl;
+    }
+
     if (this->drift.nu > 0) {
       this->drift.printToStream(ss);
     }
 
     if (this->diffusion > 0) {
       ss << "\t diffusion:\t\t" << this->diffusion << "\t(dtod=" << diffusion_dtod << ")"
          << std::endl;
@@ -543,19 +548,23 @@
   PRAGMA_SIMD
   for (int i = 0; i < this->size_; ++i) {
     w[i] = MIN(w[i], max_bound[i]);
     w[i] = MAX(w[i], min_bound[i]);
   }
 
   if (getPar().usesPersistentWeight()) {
+
     PRAGMA_SIMD
     for (int i = 0; i < this->size_; i++) {
       w_persistent_[0][i] = w[i];
+      weights[0][i] = w[i];
+    }
+    if (getPar().apply_write_noise_on_set) {
+      applyUpdateWriteNoise(weights);
     }
-    applyUpdateWriteNoise(weights);
     return true; // modified device thus true
   } else {
     return false; // whether device was changed
   }
 }
 
 template <typename T> void PulsedRPUDevice<T>::applyUpdateWriteNoise(T **weights) {
@@ -632,14 +641,44 @@
         if (w_min_bound_[i][j] > w_max_bound_[i][j]) {
           T m = w_max_bound_[i][j] + (w_min_bound_[i][j] - w_max_bound_[i][j]) / ((T)2.0);
           w_max_bound_[i][j] = m;
           w_min_bound_[i][j] = m;
         }
       }
 
+      // adjust with up_down
+      if (par.adjust_bounds_with_up_down) {
+        // if up_down_deviation is close to zero, up_downetric devices will be
+        // close to one sided. If large, up_down all device bounds will be
+        // symmetric around the 0
+
+        if (w_min_bound_[i][j] > w_max_bound_[i][j]) {
+          std::swap(w_min_bound_[i][j], w_max_bound_[i][j]);
+        }
+
+        T up_down_alpha = w_scale_up_[i][j] / (w_scale_up_[i][j] + w_scale_down_[i][j]);
+        T mm = w_max_bound_[i][j] - w_min_bound_[i][j];
+
+        T new_min_bound = (T)0.0;
+        T up_down_deviation = par.adjust_bounds_with_up_down_dev;
+        if (up_down_deviation > 0) {
+          new_min_bound =
+              -((tanh((up_down_alpha - (T)0.5) / up_down_deviation) + (T)1.0) / ((T)2.0)) * mm;
+        } else {
+          if (up_down_alpha < 0.5)
+            new_min_bound = 0;
+          else if (up_down_alpha == 0.5)
+            new_min_bound = -mm / ((T)2.0);
+          else
+            new_min_bound = -mm;
+        }
+        w_min_bound_[i][j] = new_min_bound;
+        w_max_bound_[i][j] = new_min_bound + mm;
+      }
+
       // corrupt devices
       if (par.corrupt_devices_prob > rng->sampleUniform()) {
         // stuck somewhere in min_max
         T mn = MAX(MIN(w_max_bound_[i][j], w_min_bound_[i][j]), -fabs(par.corrupt_devices_range));
         T mx = MIN(MAX(w_max_bound_[i][j], w_min_bound_[i][j]), fabs(par.corrupt_devices_range));
 
         T value = mn + (mx - mn) * rng->sampleUniform();
```

### Comparing `aihwkit-0.7.1/src/rpucuda/rpu_pulsed_device.h` & `aihwkit-0.8.0/src/rpucuda/rpu_pulsed_device.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -82,18 +82,23 @@
   bool enforce_consistency = false;
   bool perfect_bias = false;
 
   T corrupt_devices_prob = (T)0.0;
   T corrupt_devices_range = std::numeric_limits<T>::max();
 
   T reset = (T)0.0; // mean
-  T reset_std = (T)0.0;
+  // T reset_std = (T)0.0;  from SimpleDevice
   T reset_dtod = (T)0.0;
 
+  bool adjust_bounds_with_up_down = false;
+  T adjust_bounds_with_up_down_dev = (T)0.0;
+
   T write_noise_std = (T)0.0;
+  bool apply_write_noise_on_set = true;
+  bool count_pulses = false; // whether to count the pulses. Some runtime penalty
 
   void printToStream(std::stringstream &ss) const override;
   using SimpleMetaParameter<T>::print;
   std::string getName() const override { return "PulsedRPUDeviceParameter"; };
   PulsedRPUDevice<T> *createDevice(int x_size, int d_size, RealWorldRNG<T> *rng) override {
     RPU_FATAL("Needs implementation");
   };
@@ -106,15 +111,15 @@
 
   void initialize() override {
     PulsedRPUDeviceMetaParameterBase<T>::initialize();
     if (!implementsWriteNoise() && usesPersistentWeight()) {
       RPU_FATAL("Device does not support write noise");
     }
     reset_dtod = MAX(reset_dtod, (T)0.0);
-    reset_std = MAX(reset_std, (T)0.0);
+    this->reset_std = MAX(this->reset_std, (T)0.0);
     reset = MAX(reset, (T)0.0);
   };
 };
 
 template <typename T> class PulsedRPUDeviceBase : public SimpleRPUDevice<T> {
 
 public:
@@ -172,20 +177,39 @@
   virtual T getPulseCountLearningRate(
       T learning_rate, int current_m_batch, const PulsedUpdateMetaParameter<T> &up) {
     UNUSED(up);
     UNUSED(current_m_batch);
     return learning_rate;
   };
 
-  // called from the weight updater before the call to
-  // initUpdateCycle. Can be used to do some additional computation on
-  // the input
+  /* called from the weight updater before the call to
+     initUpdateCycle. Can be used to do some additional computation on
+     the input */
   virtual void
   initWithUpdateInput(const T *x_input, const int x_inc, const T *d_input, const int d_inc){};
 
+  void dumpExtra(RPU::state_t &extra, const std::string prefix) override {
+    SimpleRPUDevice<T>::dumpExtra(extra, prefix);
+
+    RPU::state_t state;
+    RPU::insert(state, "num_states", num_states_);
+    RPU::insert(state, "weight_granularity", weight_granularity_);
+
+    RPU::insertWithPrefix(extra, state, prefix);
+  };
+
+  void loadExtra(const RPU::state_t &extra, const std::string prefix, bool strict) override {
+    SimpleRPUDevice<T>::loadExtra(extra, prefix, strict);
+
+    auto state = RPU::selectWithPrefix(extra, prefix);
+
+    RPU::load(state, "num_states", num_states_, strict);
+    RPU::load(state, "weight_granularity", weight_granularity_, strict);
+  };
+
 protected:
   inline void setWeightGranularity(T weight_granularity) {
     weight_granularity_ = weight_granularity;
   };
   inline void setNumStates(T num_states) { num_states_ = num_states; };
 
   void populate(const PulsedRPUDeviceMetaParameterBase<T> &par, RealWorldRNG<T> *rng) {
@@ -265,14 +289,17 @@
   void clipWeights(T **weights, T add_clip) override;
   bool onSetWeights(T **weights) override;
   void
   resetCols(T **weights, int start_col, int n_cols, T reset_prob, RealWorldRNG<T> &rng) override;
   virtual void resetAtIndices(T **weights, std::vector<int> x_major_indices, RealWorldRNG<T> &rng);
   void copyInvertDeviceParameter(const PulsedRPUDeviceBase<T> *rpu_device) override;
 
+  using PulsedRPUDeviceBase<T>::dumpExtra;
+  using PulsedRPUDeviceBase<T>::loadExtra;
+
 protected:
   void populate(const PulsedRPUDeviceMetaParameter<T> &par, RealWorldRNG<T> *rng);
 
   T **w_max_bound_ = nullptr;
   T **w_min_bound_ = nullptr;
   T **w_scale_up_ = nullptr;
   T **w_scale_down_ = nullptr;
```

### Comparing `aihwkit-0.7.1/src/rpucuda/rpu_pulsed_meta_parameter.cpp` & `aihwkit-0.8.0/src/rpucuda/rpu_pulsed_meta_parameter.cpp`

 * *Files 3% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -75,14 +75,17 @@
 
     if (this->inp_bound <= 0.0) {
       this->inp_bound = std::numeric_limits<T>::infinity();
     }
     if (v_offset_vec.size() > 0 && v_offset_vec.size() != (size_t)d_size) {
       RPU_FATAL("Size mismatch in user-defined v_offsets for forward.");
     }
+    if (v_offset_vec.size() > 0 && v_offset_vec.size() != (size_t)d_size) {
+      RPU_FATAL("Size mismatch in user-defined v_offsets for forward.");
+    }
   }
   UNUSED(x_size);
 }
 
 template <typename T> void IOMetaParameter<T>::initializeForBackward(int x_size, int d_size) {
 
   if (!_par_initialized) {
```

### Comparing `aihwkit-0.7.1/src/rpucuda/rpu_pulsed_meta_parameter.h` & `aihwkit-0.8.0/src/rpucuda/rpu_pulsed_meta_parameter.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -68,14 +68,15 @@
 
   T inp_bound = (T)1.0;
   T inp_res = (T)1.0 / (pow((T)2.0, (T)7.0) - (T)2.0);
   T _inp_res = (T)0.0; // this is the unscaled version saved for output..
   bool inp_sto_round = false;
   T inp_noise = (T)0.0;
   T out_noise = (T)0.06;
+  T out_noise_std = (T)0.0; // systematic variation in percent of out_noise
   T w_noise = (T)0.0;
   T out_bound = (T)12.0;
   T out_res = (T)1.0 / (pow((T)2.0, (T)9.0) - (T)2.0);
   T _out_res = (T)0;
   bool out_sto_round = false;
   T out_scale = (T)1.0;
```

### Comparing `aihwkit-0.7.1/src/rpucuda/rpu_pulsed_test.cpp` & `aihwkit-0.8.0/src/rpucuda/rpu_pulsed_test.cpp`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/rpu_simple_device.h` & `aihwkit-0.8.0/src/rpucuda/rpu_simple_device.h`

 * *Files 3% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -32,22 +32,26 @@
 // register all available devices [better to use registry at some point]
 enum class DeviceUpdateType {
   Undefined,
   FloatingPoint,
   ConstantStep,
   LinearStep,
   SoftBounds,
+  HiddenStep,
   ExpStep,
   Vector,
   OneSided,
   Transfer,
+  BufferedTransfer,
   MixedPrec,
+  MixedPrecInt,
   PowStep,
-  BufferedTransfer,
   PiecewiseStep,
+  ChoppedTransfer,
+  DynamicTransfer,
   SoftBoundsReference,
   PowStepReference
 };
 
 // inherit from Simple
 template <typename T> struct AbstractRPUDeviceMetaParameter : SimpleMetaParameter<T> {
 
@@ -84,24 +88,29 @@
     swap(a._par_initialized, b._par_initialized);
   }
 };
 
 // Simple Device parameter
 template <typename T> struct SimpleRPUDeviceMetaParameter : AbstractRPUDeviceMetaParameter<T> {
 
+  T reset_std = 0.0;
+
   std::string getName() const override { return "SimpleRPUDevice"; };
   SimpleRPUDevice<T> *createDevice(int x_size, int d_size, RealWorldRNG<T> *rng) override {
     return new SimpleRPUDevice<T>(x_size, d_size, *this, rng);
   }
   SimpleRPUDeviceMetaParameter<T> *clone() const override {
     return new SimpleRPUDeviceMetaParameter<T>(*this);
   }
   using SimpleMetaParameter<T>::print;
   void printToStream(std::stringstream &ss) const override {
     SimpleMetaParameter<T>::printToStream(ss);
+    if (reset_std > 0.0) {
+      ss << "\t reset_std: " << reset_std << std::endl;
+    }
   }
   DeviceUpdateType implements() const override { return DeviceUpdateType::FloatingPoint; };
 
   friend void
   swap(SimpleRPUDeviceMetaParameter<T> &a, SimpleRPUDeviceMetaParameter<T> &b) noexcept {
     using std::swap;
     swap(
@@ -124,14 +133,16 @@
   virtual void getDPNames(std::vector<std::string> &names) const = 0;
   virtual void getDeviceParameter(T **weights, std::vector<T *> &data_ptrs) = 0;
   virtual void setDeviceParameter(T **out_weights, const std::vector<T *> &data_ptrs) = 0;
   virtual int getHiddenWeightsCount() const = 0;
   virtual void setHiddenWeights(const std::vector<T> &data) = 0;
   virtual int getHiddenUpdateIdx() const { return 0; };
   virtual void setHiddenUpdateIdx(int idx){};
+  virtual void dumpExtra(RPU::state_t &extra, const std::string prefix) = 0;
+  virtual void loadExtra(const RPU::state_t &extra, const std::string prefix, bool strict) = 0;
 
   virtual void printDP(int x_count, int d_count) const = 0;
   void dispMetaParameter() const {
     std::stringstream ss;
     ss << "\033[0;33m";
     printToStream(ss);
     ss << "\033[0m";
@@ -230,28 +241,28 @@
   void decayWeights(T **weights, bool bias_no_decay) override;
   void decayWeights(T **weights, T alpha, bool bias_no_decay) override;
   void driftWeights(T **weights, T time_since_last_call, RNG<T> &rng) override;
   void diffuseWeights(T **weights, RNG<T> &rng) override;
   void clipWeights(T **weights, T clip) override;
   bool onSetWeights(T **weights) override { return false; };
   void
-  resetCols(T **weights, int start_col, int n_cols, T reset_prob, RealWorldRNG<T> &rng) override {
-    RPU_FATAL("Not supported for Simple devices");
-  }; // maybe support ?
+  resetCols(T **weights, int start_col, int n_cols, T reset_prob, RealWorldRNG<T> &rng) override;
 
   DeviceUpdateType implements() const override { return this->getPar().implements(); };
 
   inline const WeightDrifter<T> *getWDrifter() const {
     if (hasWDrifter()) {
       return &*wdrifter_;
     } else {
       return nullptr;
     }
   };
   inline bool hasWDrifter() const { return wdrifter_ != nullptr; };
+  void dumpExtra(RPU::state_t &extra, const std::string prefix) override;
+  void loadExtra(const RPU::state_t &extra, const std::string prefix, bool strict) override;
 
 protected:
   void populate(const SimpleRPUDeviceMetaParameter<T> &p, RealWorldRNG<T> *rng);
   int x_size_ = 0;
   int d_size_ = 0;
   int size_ = 0;
   std::unique_ptr<WeightDrifter<T>> wdrifter_ = nullptr;
```

### Comparing `aihwkit-0.7.1/src/rpucuda/rpu_softbounds_reference_device.cpp` & `aihwkit-0.8.0/src/rpucuda/rpu_softbounds_reference_device.cpp`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/rpu_softbounds_reference_device.h` & `aihwkit-0.8.0/src/rpucuda/rpu_softbounds_reference_device.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/rpu_transfer_device.cpp` & `aihwkit-0.8.0/src/rpucuda/rpu_transfer_device.cpp`

 * *Files 3% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -659,14 +659,50 @@
   if (fully_hidden_) {
     // weight might have changed because of hidden weight change
     RPU::math::copy<T>(
         this->size_, this->weights_vec_[this->n_devices_ - 1][0], 1, out_weights[0], 1);
   }
 }
 
+template <typename T>
+void TransferRPUDevice<T>::dumpExtra(RPU::state_t &extra, const std::string prefix) {
+  VectorRPUDevice<T>::dumpExtra(extra, prefix);
+
+  RPU::state_t state;
+
+  RPU::insert(state, "current_slice_indices", current_slice_indices_);
+
+  // RPU::insert(state, "transfer_vecs", transfer_vecs_);
+  RPU::insert(state, "transfer_every", transfer_every_);
+
+  transfer_fb_pass_->dumpExtra(state, "transfer_fb_pass");
+  transfer_pwu_->dumpExtra(state, "transfer_pwu");
+
+  RPU::insertWithPrefix(extra, state, prefix);
+}
+
+template <typename T>
+void TransferRPUDevice<T>::loadExtra(
+    const RPU::state_t &extra, const std::string prefix, bool strict) {
+
+  VectorRPUDevice<T>::loadExtra(extra, prefix, strict);
+
+  auto state = RPU::selectWithPrefix(extra, prefix);
+
+  RPU::load(state, "current_slice_indices", current_slice_indices_, strict);
+  if (state.count("transfer_vecs")) {
+    RPU::load(state, "transfer_vecs", transfer_vecs_, strict);
+  }
+  if (state.count("transfer_every")) {
+    RPU::load(state, "transfer_every", transfer_every_, strict);
+  }
+  transfer_fb_pass_->loadExtra(state, "transfer_fb_pass", strict);
+  transfer_pwu_->loadExtra(state, "transfer_pwu", strict);
+}
+
 #undef COMMA
 #undef LOOP_WITH_HIDDEN
 
 template class TransferRPUDevice<float>;
 #ifdef RPU_USE_DOUBLE
 template class TransferRPUDevice<double>;
 #endif
```

### Comparing `aihwkit-0.7.1/src/rpucuda/rpu_transfer_device.h` & `aihwkit-0.8.0/src/rpucuda/rpu_transfer_device.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -209,14 +209,16 @@
       T **weights, int i, const int *x_signed_indices, int x_count, int d_sign, RNG<T> *rng)
       override;
 
   void doDenseUpdate(T **weights, int *coincidences, RNG<T> *rng) override;
   inline const ForwardBackwardPassIOManaged<T> &getTransferFBPass() const {
     return *transfer_fb_pass_;
   };
+  void dumpExtra(RPU::state_t &extra, const std::string prefix) override;
+  void loadExtra(const RPU::state_t &extra, const std::string prefix, bool strict) override;
 
 protected:
   void populate(const TransferRPUDeviceMetaParameter<T> &par, RealWorldRNG<T> *rng);
   void reduceToWeights(T **weights) const override;
   T **getDeviceWeights(int device_idx) const;
   int resetCounters(bool force = false) override;
```

### Comparing `aihwkit-0.7.1/src/rpucuda/rpu_transfer_device_test.cpp` & `aihwkit-0.8.0/src/rpucuda/rpu_transfer_device_test.cpp`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/rpu_vector_device.cpp` & `aihwkit-0.8.0/src/rpucuda/rpu_vector_device.cpp`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -322,14 +322,47 @@
     m++;
   }
   this->reduceToWeights(out_weights); // need to update the weights
   weight_granularity /= rpu_device_vec_.size();
   this->setWeightGranularity(weight_granularity);
 };
 
+template <typename T>
+void VectorRPUDevice<T>::dumpExtra(RPU::state_t &extra, const std::string prefix) {
+
+  PulsedRPUDeviceBase<T>::dumpExtra(extra, prefix);
+
+  RPU::state_t state;
+
+  for (size_t k = 0; k < rpu_device_vec_.size(); k++) {
+    rpu_device_vec_[k]->dumpExtra(state, std::to_string(k));
+  }
+  RPU::insert(state, "reduce_weightening", reduce_weightening_);
+  RPU::insert(state, "current_device_idx", current_device_idx_);
+  RPU::insert(state, "current_update_idx", current_update_idx_);
+
+  RPU::insertWithPrefix(extra, state, prefix);
+}
+
+template <typename T>
+void VectorRPUDevice<T>::loadExtra(
+    const RPU::state_t &extra, const std::string prefix, bool strict) {
+
+  PulsedRPUDeviceBase<T>::loadExtra(extra, prefix, strict);
+
+  auto state = RPU::selectWithPrefix(extra, prefix);
+
+  for (size_t k = 0; k < rpu_device_vec_.size(); k++) {
+    rpu_device_vec_[k]->loadExtra(state, std::to_string(k), strict);
+  }
+  RPU::load(state, "reduce_weightening", reduce_weightening_, strict);
+  RPU::load(state, "current_device_idx", current_device_idx_, strict);
+  RPU::load(state, "current_update_idx", current_update_idx_, strict);
+}
+
 template <typename T> void VectorRPUDevice<T>::printDP(int x_count, int d_count) const {
 
   int x_count1 = x_count;
   int d_count1 = d_count;
   if (x_count < 0 || x_count > this->x_size_)
     x_count1 = this->x_size_;
```

### Comparing `aihwkit-0.7.1/src/rpucuda/rpu_vector_device.h` & `aihwkit-0.8.0/src/rpucuda/rpu_vector_device.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -164,16 +164,18 @@
   void getDPNames(std::vector<std::string> &names) const override;
   void getDeviceParameter(T **weights, std::vector<T *> &data_ptrs) override;
   void setDeviceParameter(T **out_weights, const std::vector<T *> &data_ptrs) override;
   int getHiddenWeightsCount() const override;
   void setHiddenWeights(const std::vector<T> &data) override;
   int getHiddenUpdateIdx() const override;
   void setHiddenUpdateIdx(int idx) override;
+  void dumpExtra(RPU::state_t &extra, const std::string prefix) override;
+  void loadExtra(const RPU::state_t &extra, const std::string prefix, bool strict) override;
 
-  void printDP(int x_cunt, int d_count) const override;
+  void printDP(int x_count, int d_count) const override;
   void printToStream(std::stringstream &ss) const override { this->getPar().printToStream(ss); };
   void disp(std::stringstream &ss) const override {
     ss << this->getPar().getName() << " [" << this->x_size_ << "," << this->d_size_ << "]\n";
   };
 
   VectorRPUDeviceMetaParameter<T> &getPar() const override {
     return static_cast<VectorRPUDeviceMetaParameter<T> &>(SimpleRPUDevice<T>::getPar());
@@ -220,15 +222,15 @@
 
   int n_devices_ = 0;
 
   std::vector<std::unique_ptr<PulsedRPUDeviceBase<T>>> rpu_device_vec_;
   T ***weights_vec_ = nullptr;
   std::vector<T> reduce_weightening_;
   int current_device_idx_ = 0;
-  unsigned long current_update_idx_ = 0;
+  uint64_t current_update_idx_ = 0;
   RealWorldRNG<T> rw_rng_{0};
 
   virtual int resetCounters(bool force = false);
 
 private:
   void freeContainers();
   void allocateContainers(int n_devices);
```

### Comparing `aihwkit-0.7.1/src/rpucuda/rpu_weight_updater.cpp` & `aihwkit-0.8.0/src/rpucuda/rpu_weight_updater.cpp`

 * *Files 8% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -121,14 +121,48 @@
 
   containers_allocated_ = other.containers_allocated_;
 
   return *this;
 }
 
 template <typename T>
+void PulsedRPUWeightUpdater<T>::dumpExtra(RPU::state_t &extra, const std::string prefix) {
+
+  RPUWeightUpdater<T>::dumpExtra(extra, prefix);
+
+  RPU::state_t state;
+
+  if (containers_allocated_) {
+    dblm_->dumpExtra(state, "dblm");
+    sblm_->dumpExtra(state, "sblm");
+  }
+  RPU::insert(state, "containers_allocated", containers_allocated_);
+
+  RPU::insertWithPrefix(extra, state, prefix);
+}
+
+template <typename T>
+void PulsedRPUWeightUpdater<T>::loadExtra(
+    const RPU::state_t &extra, const std::string prefix, bool strict) {
+
+  RPUWeightUpdater<T>::loadExtra(extra, prefix, strict);
+  auto state = RPU::selectWithPrefix(extra, prefix);
+
+  bool was_allocated;
+  RPU::load(state, "containers_allocated", was_allocated, strict);
+  if (!containers_allocated_ && was_allocated) {
+    allocateContainers();
+  }
+  if (containers_allocated_) {
+    dblm_->loadExtra(state, "dblm", strict);
+    sblm_->loadExtra(state, "sblm", strict);
+  }
+}
+
+template <typename T>
 void PulsedRPUWeightUpdater<T>::setUpPar(const PulsedUpdateMetaParameter<T> &up) {
   up_ = up;
   // check the parameters
   up_.initialize();
 }
 
 template <typename T>
```

### Comparing `aihwkit-0.7.1/src/rpucuda/rpu_weight_updater.h` & `aihwkit-0.8.0/src/rpucuda/rpu_weight_updater.h`

 * *Files 5% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -37,14 +37,17 @@
       T **weights,
       const T *x_input,
       const int x_inc,
       const T *d_input,
       const int d_inc,
       const T learning_rate);
 
+  virtual void dumpExtra(RPU::state_t &extra, const std::string prefix){};
+  virtual void loadExtra(const RPU::state_t &extra, const std::string prefix, bool strict){};
+
 protected:
   int x_size_ = 0;
   int d_size_ = 0;
 };
 
 /* RPU stochastic version of the forward pass with noise and management ntechniques*/
 template <typename T> class PulsedRPUWeightUpdater : public RPUWeightUpdater<T> {
@@ -65,14 +68,18 @@
     swap(a.containers_allocated_, b.containers_allocated_);
 
     swap(a.up_, b.up_);
     swap(a.sblm_, b.sblm_);
     swap(a.dblm_, b.dblm_);
     swap(a.rng_, b.rng_);
   }
+
+  void dumpExtra(RPU::state_t &extra, const std::string prefix) override;
+  void loadExtra(const RPU::state_t &extra, const std::string prefix, bool strict) override;
+
   bool checkForFPUpdate(AbstractRPUDevice<T> *rpu_device_in);
 
   virtual void updateVectorWithDevice(
       T **weights,
       const T *x_input,
       const int x_inc,
       const T *d_input,
```

### Comparing `aihwkit-0.7.1/src/rpucuda/sparse_bit_line_maker.cpp` & `aihwkit-0.8.0/src/rpucuda/sparse_bit_line_maker.cpp`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/sparse_bit_line_maker.h` & `aihwkit-0.8.0/src/rpucuda/sparse_bit_line_maker.h`

 * *Files 5% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -64,14 +64,18 @@
       int **&x_indices_p,
       int **&x_indices_n,
       int **&d_indices);
 
   void printCounts(int BL) const;
   bool supports(RPU::PulseType pulse_type) const;
 
+  /* Ignore the buffer / indices, as they will be generated anew each sample.*/
+  void dumpExtra(RPU::state_t &extra, const std::string prefix){};
+  void loadExtra(const RPU::state_t &extra, const std::string prefix, bool strict){};
+
 private:
   void freeContainers();
   void allocateContainers(int max_BL);
   void initialize(int x_size, int d_size, int max_BL);
 
   int x_size_ = 0;
   int d_size_ = 0;
```

### Comparing `aihwkit-0.7.1/src/rpucuda/utility_functions.h` & `aihwkit-0.8.0/src/rpucuda/utility_functions.h`

 * *Files 10% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -11,14 +11,16 @@
  */
 
 #pragma once
 #include <memory>
 #include <sstream>
 #include <stdexcept>
 #include <string.h>
+#include <unordered_map>
+#include <vector>
 
 #define UNUSED(X) (void)X
 
 #ifdef _MSC_VER
 #define PRAGMA(DIRECTIVE) __pragma(DIRECTIVE)
 #define PRAGMA_SIMD
 #else
@@ -87,14 +89,25 @@
 // whether this is the best place [each thread needs it?]. However,
 // default should be anyway to-neareast (for GPU this even cannot be
 // changed apparently) so we should be fine using RINT here.
 #define RPU_ROUNDFUN rint
 
 namespace RPU {
 
+using state_t = std::unordered_map<std::string, std::vector<double>>;
+
+/* state helper functions */
+template <typename T_VEC>
+void load(RPU::state_t &state, std::string key, T_VEC &value, bool strict);
+template <typename T_VEC> void insert(RPU::state_t &state, std::string key, const T_VEC &value);
+
+void insertWithPrefix(RPU::state_t &extra, const RPU::state_t &state, std::string prefix);
+RPU::state_t selectWithPrefix(const RPU::state_t &extra, std::string prefix);
+
+/* Context class */
 class Context {
 public:
   virtual ~Context() = default;
   virtual void synchronize() const {};
 };
 
 typedef Context *ContextPtr;
```

### Comparing `aihwkit-0.7.1/src/rpucuda/weight_clipper.cpp` & `aihwkit-0.8.0/src/rpucuda/weight_clipper.cpp`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
```

### Comparing `aihwkit-0.7.1/src/rpucuda/weight_clipper.h` & `aihwkit-0.8.0/src/rpucuda/weight_clipper.h`

 * *Files 14% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -72,14 +72,17 @@
 public:
   explicit WeightClipper(int x_size, int d_size);
   WeightClipper(){};
 
   /* in-place clipping of weights */
   void apply(T *weights, const WeightClipParameter &wclpar);
 
+  void dumpExtra(RPU::state_t &extra, const std::string prefix){};
+  void loadExtra(const RPU::state_t &extra, const std::string prefix, bool strict){};
+
 private:
   void clip(T *weights, T clip_value);
   std::vector<T> amax_values_;
 
   int x_size_ = 0;
   int d_size_ = 0;
   int size_ = 0;
```

### Comparing `aihwkit-0.7.1/src/rpucuda/weight_drifter.cpp` & `aihwkit-0.8.0/src/rpucuda/weight_drifter.cpp`

 * *Files 16% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -67,27 +67,54 @@
     for (int i = 0; i < size_; i++) {
       nu_[i] = par_.nu + par_.nu_dtod * par_.nu * rng->sampleGauss();
     }
   }
 }
 
 template <typename T>
+void WeightDrifter<T>::dumpExtra(RPU::state_t &extra, const std::string prefix) {
+
+  RPU::state_t state;
+
+  RPU::insert(state, "active", active_);
+  RPU::insert(state, "current_t", current_t_);
+  RPU::insert(state, "previous_weights", previous_weights_);
+  RPU::insert(state, "w0", w0_);
+  RPU::insert(state, "t", t_);
+  RPU::insert(state, "nu", nu_);
+
+  RPU::insertWithPrefix(extra, state, prefix);
+}
+
+template <typename T>
+void WeightDrifter<T>::loadExtra(const RPU::state_t &extra, const std::string prefix, bool strict) {
+
+  auto state = RPU::selectWithPrefix(extra, prefix);
+
+  RPU::load(state, "previous_weights", previous_weights_, strict);
+  RPU::load(state, "w0", w0_, strict);
+  RPU::load(state, "t", t_, strict);
+  RPU::load(state, "nu", nu_, strict);
+  RPU::load(state, "current_t", current_t_, strict);
+  RPU::load(state, "active", active_, strict);
+}
+
+template <typename T>
 void WeightDrifter<T>::saturate(T *weights, const T *min_bounds, const T *max_bounds) {
 
   PRAGMA_SIMD
   for (int i = 0; i < size_; i++) {
     weights[i] = MIN(MAX(min_bounds[i], weights[i]), max_bounds[i]);
     previous_weights_[i] = MIN(MAX(min_bounds[i], previous_weights_[i]), max_bounds[i]);
   }
 }
 
 template <typename T>
 void WeightDrifter<T>::apply(T *weights, T time_since_last_call, RNG<T> &rng) {
 
-  // INIT
   if (previous_weights_.size() != (size_t)size_) {
     initialize(weights);
   }
   if (!par_.isSimpleDrift() && nu_.empty()) {
     RPU_FATAL("Weight drifter needs to be populated first!");
   }
```

### Comparing `aihwkit-0.7.1/src/rpucuda/weight_drifter.h` & `aihwkit-0.8.0/src/rpucuda/weight_drifter.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -125,14 +125,17 @@
 
   void getNu(T *dst) const;
   void setNu(const T *src);
 
   inline const DriftParameter<T> &getPar() const { return par_; };
   inline int getSize() const { return size_; };
 
+  void dumpExtra(RPU::state_t &extra, const std::string prefix);
+  void loadExtra(const RPU::state_t &extra, const std::string prefix, bool strict);
+
 protected:
   int size_ = 0;
   bool active_ = false;
   T current_t_ = 0.0;
 
   DriftParameter<T> par_;
```

### Comparing `aihwkit-0.7.1/src/rpucuda/weight_modifier.cpp` & `aihwkit-0.8.0/src/rpucuda/weight_modifier.cpp`

 * *Files 17% similar despite different names*

```diff
@@ -1,9 +1,10 @@
+
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -33,14 +34,20 @@
   }
 }
 
 template <typename T>
 void WeightModifier<T>::apply(
     T *new_weights, const T *weights, const WeightModifierParameter<T> &wmpar) {
 
+  if (wmpar.type != WeightModifierType::Copy) {
+    if (wmpar.per_batch_sample) {
+      RPU_FATAL("Per batch sample is not implemented in RPUCuda");
+    }
+  }
+
   // just copy always if not in-place [also handles WeightModifierType::Copy]
   if (new_weights != weights) {
     RPU::math::copy<T>(size_, weights, 1, new_weights, 1);
   }
 
   if (wmpar.copy_last_column) {
     saved_bias_.resize(d_size_);
@@ -95,16 +102,16 @@
         T w = new_weights[i];
         new_weights[i] += w * std * rw_rng_.sampleGauss();
       }
     }
     break;
   }
   case WeightModifierType::AddNormal: {
-
     if (wmpar.std_dev > 0) {
+
       const T std = (T)wmpar.std_dev * amax;
       PRAGMA_SIMD
       for (int i = 0; i < size_; i++) {
         new_weights[i] += std * rw_rng_.sampleGauss();
       }
     }
 
@@ -127,14 +134,39 @@
         sig *= std;
         new_weights[i] += amax * sig * rw_rng_.sampleGauss();
       }
     }
     break;
   }
 
+  case WeightModifierType::ProgNoise: {
+
+    if (wmpar.std_dev > 0 && wmpar.coeffs.size() > (size_t)0) {
+      const T std = wmpar.std_dev / wmpar.g_max;
+      PRAGMA_SIMD
+      for (int i = 0; i < size_; i++) {
+        T aw = fabs(new_weights[i]) / amax;
+        T paw = 1;
+        T sig = wmpar.coeffs.at(0);
+        for (size_t j = 1; j < wmpar.coeffs.size(); j++) {
+          paw *= aw;
+          sig += wmpar.coeffs.at(j) * paw;
+        }
+        sig *= std;
+        T w = new_weights[i];
+        if (w < 0.0f) {
+          new_weights[i] = -fabs(w + amax * sig * rw_rng_.sampleGauss());
+        } else {
+          new_weights[i] = fabs(w + amax * sig * rw_rng_.sampleGauss());
+        }
+      }
+    }
+    break;
+  }
+
   case WeightModifierType::DoReFa: {
 
     const T res = (T)wmpar.res;
 
     if (res > 0) {
       const bool sto_round = wmpar.sto_round;
       const T scale = (T)fabs(wmpar.dorefa_clip / tanh(amax));
@@ -179,13 +211,34 @@
   if (wmpar.copy_last_column) {
     for (int j = 0; j < d_size_; j++) {
       new_weights[(j + 1) * x_size_ - 1] = saved_bias_[j];
     }
   }
 }
 
+template <typename T>
+void WeightModifier<T>::dumpExtra(RPU::state_t &extra, const std::string prefix) {
+
+  RPU::state_t state;
+
+  RPU::insert(state, "saved_bias", saved_bias_);
+  RPU::insert(state, "enable_during_test", enable_during_test_);
+
+  RPU::insertWithPrefix(extra, state, prefix);
+}
+
+template <typename T>
+void WeightModifier<T>::loadExtra(
+    const RPU::state_t &extra, const std::string prefix, bool strict) {
+
+  auto state = RPU::selectWithPrefix(extra, prefix);
+
+  RPU::load(state, "saved_bias", saved_bias_, strict);
+  RPU::load(state, "enable_during_test", enable_during_test_, strict);
+}
+
 template class WeightModifier<float>;
 #ifdef RPU_USE_DOUBLE
 template class WeightModifier<double>;
 #endif
 
 } // namespace RPU
```

### Comparing `aihwkit-0.7.1/src/rpucuda/weight_modifier.h` & `aihwkit-0.8.0/src/rpucuda/weight_modifier.h`

 * *Files 10% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -20,27 +20,31 @@
 enum class WeightModifierType {
   Copy, // does nothing, just copy (e.g. for delayed weight update), however, could also drop
   Discretize,
   MultNormal,
   AddNormal,
   DiscretizeAddNormal,
   DoReFa,
-  Poly
+  Poly,
+  DropConnect,
+  ProgNoise,
 };
 
 template <typename T> struct WeightModifierParameter {
   T std_dev = 0.0;
+  bool per_batch_sample = false;
   T res = 0.1;
   bool sto_round = false;
   T dorefa_clip = 0.6;
   T pdrop = 0.0;
   bool enable_during_test = false;
   bool copy_last_column = false;
   bool rel_to_actual_wmax = true;
   T assumed_wmax = 1.0;
+  T g_max = 25.0;
 
   WeightModifierType type = WeightModifierType::Copy;
   std::vector<T> coeffs = {0.26348 / 25.0, 0.0768, -0.001877 * 25.0};
 
   inline std::string getTypeName() const {
     switch (type) {
     case WeightModifierType::Copy:
@@ -53,14 +57,18 @@
       return "AddNormal";
     case WeightModifierType::DoReFa:
       return "DoReFa";
     case WeightModifierType::DiscretizeAddNormal:
       return "DiscretizeAddNormal";
     case WeightModifierType::Poly:
       return "Poly";
+    case WeightModifierType::ProgNoise:
+      return "ProgNoise";
+    case WeightModifierType::DropConnect:
+      return "DropConnect";
     default:
       return "Unknown";
     }
   };
 
   void print() const {
     std::stringstream ss;
@@ -68,15 +76,15 @@
     std::cout << ss.str();
   };
 
   void printToStream(std::stringstream &ss) const {
     ss << "\t weight modifier type:\t" << getTypeName() << std::endl;
     if (type != WeightModifierType::Copy) {
       if (type == WeightModifierType::Poly || type == WeightModifierType::MultNormal ||
-          type == WeightModifierType::AddNormal ||
+          type == WeightModifierType::AddNormal || type == WeightModifierType::ProgNoise ||
           type == WeightModifierType::DiscretizeAddNormal) {
         ss << "\t std_dev:\t\t" << std_dev << std::endl;
       }
       ss << "\t rel_to_actual_wmax:\t" << rel_to_actual_wmax << std::endl;
       ss << "\t assumed_wmax:\t\t" << assumed_wmax << std::endl;
     }
     if (copy_last_column) {
@@ -84,14 +92,18 @@
     }
     if (pdrop > 0.0) {
       ss << "\t pdrop:\t\t\t" << pdrop << std::endl;
     }
     if (type == WeightModifierType::DoReFa) {
       ss << "\t dorefa clip:\t\t" << dorefa_clip << std::endl;
     }
+    if (type == WeightModifierType::ProgNoise) {
+      ss << "\t g_max:\t\t" << g_max << std::endl;
+    }
+
     if (type == WeightModifierType::Poly) {
       for (int i = 0; i < (int)coeffs.size(); i++) {
         ss << "\t coeff [" << i << "]:\t" << coeffs[i] << std::endl;
       }
     }
     if (type == WeightModifierType::Discretize || type == WeightModifierType::DiscretizeAddNormal ||
         type == WeightModifierType::DoReFa) {
@@ -104,15 +116,16 @@
     ss << std::endl;
   }
 
   inline bool usesRandom() {
     return (
         pdrop > 0 || (type == WeightModifierType::Discretize && sto_round) ||
         type == WeightModifierType::MultNormal || type == WeightModifierType::Poly ||
-        type == WeightModifierType::AddNormal || type == WeightModifierType::DiscretizeAddNormal ||
+        type == WeightModifierType::ProgNoise || type == WeightModifierType::AddNormal ||
+        type == WeightModifierType::DiscretizeAddNormal ||
         (type == WeightModifierType::DoReFa && sto_round));
   };
 };
 
 template <typename T> class WeightModifier {
 
 public:
@@ -120,14 +133,17 @@
   WeightModifier(){};
 
   /* buffers the weight changes and redraws the drop connection*/
   void apply(T *new_weights, const T *weights, const WeightModifierParameter<T> &wmpar);
 
   inline bool enableDuringTest() { return enable_during_test_; };
 
+  void dumpExtra(RPU::state_t &extra, const std::string prefix);
+  void loadExtra(const RPU::state_t &extra, const std::string prefix, bool strict);
+
 private:
   void dropConnections(T *weights, T prob);
 
   int x_size_ = 0;
   int d_size_ = 0;
   int size_ = 0;
   std::vector<T> saved_bias_;
```

### Comparing `aihwkit-0.7.1/src/rpucuda/weight_remapper.cpp` & `aihwkit-0.8.0/src/rpucuda/weight_remapper.cpp`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -24,17 +24,14 @@
 WeightRemapper<T>::WeightRemapper(int x_size, int d_size)
     : x_size_(x_size), d_size_(d_size), size_(d_size * x_size) {}
 
 template <typename T>
 void WeightRemapper<T>::apply(
     T *weights, T current_lr, const WeightRemapParameter &wrmpar, T *scales, T *biases) {
 
-  UNUSED(biases);
-  UNUSED(current_lr);
-
   switch (wrmpar.type) {
   case WeightRemapType::LayerwiseSymmetric: {
 
     if (!scales) {
       RPU_FATAL("Expect scales given.");
     }
     T amax = 0.0;
```

### Comparing `aihwkit-0.7.1/src/rpucuda/weight_remapper.h` & `aihwkit-0.8.0/src/rpucuda/weight_remapper.h`

 * *Files 8% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**
- * (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+ * (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
  *
  * This code is licensed under the Apache License, Version 2.0. You may
  * obtain a copy of this license in the LICENSE.txt file in the root directory
  * of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
  *
  * Any modifications or derivative works of this code must retain this
  * copyright notice, and modified files need to carry a notice indicating
@@ -13,15 +13,19 @@
 #pragma once
 
 #include "rng.h"
 #include <memory>
 
 namespace RPU {
 
-enum class WeightRemapType { None, LayerwiseSymmetric, ChannelwiseSymmetric };
+enum class WeightRemapType {
+  None,
+  LayerwiseSymmetric,
+  ChannelwiseSymmetric,
+};
 
 // no template. Just double
 struct WeightRemapParameter {
 
   WeightRemapType type = WeightRemapType::None;
   double max_scale_range = 0.0; // for Symmetric: whether to bound the diversity of scales
   double max_scale_ref = 0.0;   // reference is minimum scale if larger than this
@@ -68,14 +72,17 @@
   explicit WeightRemapper(int x_size, int d_size);
   WeightRemapper(){};
 
   /* in-place remap of weights */
   void apply(
       T *weights, T current_lr, const WeightRemapParameter &wmpar, T *scales, T *biases = nullptr);
 
+  void dumpExtra(RPU::state_t &extra, const std::string prefix){};
+  void loadExtra(const RPU::state_t &extra, const std::string prefix, bool strict){};
+
 private:
   std::vector<T> max_values_;
   std::vector<T> min_values_;
   std::vector<T> old_scales_;
 
   int x_size_ = 0;
   int d_size_ = 0;
```

### Comparing `aihwkit-0.7.1/tests/__init__.py` & `aihwkit-0.8.0/src/aihwkit/cloud/converter/__init__.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
 # that they have been altered from the originals.
 
-"""aihwkit tests."""
+"""Conversion utilities for interacting with the AIHW Composer API."""
```

### Comparing `aihwkit-0.7.1/tests/test_bindings_tiles.py` & `aihwkit-0.8.0/tests/test_bindings_tiles.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -21,65 +21,58 @@
 from torch import Tensor, from_numpy
 from torch.cuda import init
 
 from aihwkit.simulator.rpu_base import tiles
 
 from aihwkit.simulator.configs import FloatingPointRPUConfig, SingleRPUConfig
 from aihwkit.simulator.configs.devices import FloatingPointDevice, ConstantStepDevice, IdealDevice
-from aihwkit.simulator.configs.utils import IOParameters, DriftParameter
+from aihwkit.simulator.parameters.utils import IOParameters, DriftParameter
 
 from .helpers.decorators import parametrize_over_tiles
 from .helpers.testcases import ParametrizedTestCase, SKIP_CUDA_TESTS
-from .helpers.tiles import (FloatingPoint, ConstantStep,
-                            FloatingPointCuda, ConstantStepCuda)
+from .helpers.tiles import FloatingPoint, ConstantStep, FloatingPointCuda, ConstantStepCuda
 
 if not SKIP_CUDA_TESTS:
     init()
 
 
-@parametrize_over_tiles([
-    FloatingPoint,
-    ConstantStep,
-    FloatingPointCuda,
-    ConstantStepCuda
-])
+@parametrize_over_tiles([FloatingPoint, ConstantStep, FloatingPointCuda, ConstantStepCuda])
 class BindingsTilesTest(ParametrizedTestCase):
     """Tests the basic functionality of FloatingPoint and Analog tiles."""
 
     @staticmethod
     def set_init_weights(python_tile):
         """Generate and set the weight init."""
         init_weights = from_numpy(
-            uniform(-0.5, 0.5, size=(python_tile.out_size, python_tile.in_size)))
+            uniform(-0.5, 0.5, size=(python_tile.out_size, python_tile.in_size))
+        )
         python_tile.set_weights(init_weights)
 
     def get_noisefree_tile(self, out_size, in_size):
         """Return a tile of the specified dimensions with noisiness turned off."""
         rpu_config = None
 
-        if 'FloatingPoint' not in self.parameter:
+        if "FloatingPoint" not in self.parameter:
             rpu_config = SingleRPUConfig(
                 forward=IOParameters(is_perfect=True),
                 backward=IOParameters(is_perfect=True),
-                device=IdealDevice()
+                device=IdealDevice(),
             )
 
         python_tile = self.get_tile(out_size, in_size, rpu_config)
         self.set_init_weights(python_tile)
 
         return python_tile
 
     def get_custom_tile(self, out_size, in_size, **parameters):
         """Return a tile with custom parameters for the resistive device."""
-        if 'FloatingPoint' in self.parameter:
-            rpu_config = FloatingPointRPUConfig(
-                device=FloatingPointDevice(**parameters))
+        if "FloatingPoint" in self.parameter:
+            rpu_config = FloatingPointRPUConfig(device=FloatingPointDevice(**parameters))
         else:
-            rpu_config = SingleRPUConfig(
-                device=ConstantStepDevice(**parameters))
+            rpu_config = SingleRPUConfig(device=ConstantStepDevice(**parameters))
 
         python_tile = self.get_tile(out_size, in_size, rpu_config)
         self.set_init_weights(python_tile)
 
         return python_tile
 
     def test_instantiate(self):
@@ -99,68 +92,68 @@
         self.assertAlmostEqual(cpp_tile.get_learning_rate(), 1.23)
 
     def test_decay_weights(self):
         """Check decaying the weights."""
         decay_rate = 0.1
 
         # Use custom parameters for the tile.
-        python_tile = self.get_custom_tile(2, 3, lifetime=1.0/decay_rate)
+        python_tile = self.get_custom_tile(2, 3, lifetime=1.0 / decay_rate)
         cpp_tile = python_tile.tile
 
         init_weights = cpp_tile.get_weights().numpy()
         cpp_tile.decay_weights(1.0)
         weights = cpp_tile.get_weights().numpy()
 
-        assert_array_almost_equal(weights, init_weights*(1.0 - decay_rate))
+        assert_array_almost_equal(weights, init_weights * (1.0 - decay_rate))
 
     def test_diffuse_weights(self):
         """Check diffusing the weights."""
         diffusion_rate = 0.1
 
         # Use custom parameters for the tile.
         python_tile = self.get_custom_tile(100, 122, diffusion=diffusion_rate)
         cpp_tile = python_tile.tile
 
         init_weights = cpp_tile.get_weights().numpy()
         cpp_tile.diffuse_weights()
         weights = cpp_tile.get_weights().numpy()
 
         deviation_std = std((weights - init_weights).flatten())
-        self.assertLess(deviation_std, 1.1*diffusion_rate)
-        self.assertGreater(deviation_std, 0.9*diffusion_rate)
+        self.assertLess(deviation_std, 1.1 * diffusion_rate)
+        self.assertGreater(deviation_std, 0.9 * diffusion_rate)
 
     def test_drift_weights(self):
         """Check drifting the weights."""
         nu = 0.1
         drift_params = DriftParameter(nu=nu, t_0=1.0, nu_dtod=0.0, nu_std=0.0, w_noise_std=0.0)
         delta_t = 2.0
         # Use custom parameters for the tile.
         python_tile = self.get_custom_tile(100, 122, drift=drift_params)
         cpp_tile = python_tile.tile
 
         init_weights = cpp_tile.get_weights()
         cpp_tile.drift_weights(delta_t)
         weights = cpp_tile.get_weights()
 
-        assert_array_almost_equal(weights, init_weights*(delta_t)**(-nu))
+        assert_array_almost_equal(weights, init_weights * (delta_t) ** (-nu))
 
     def test_mimic_rpu_mac(self):
         """Check using the update, forward and backward functions."""
         n_rows = 6  # out_size aka d_size
         n_cols = 5  # in_size aka x_size
         m_batch = 4
         lr = 0.1
 
         python_tile = self.get_noisefree_tile(n_rows, n_cols)
         cpp_tile = python_tile.tile
 
         cpp_tile.set_learning_rate(lr)
 
-        x_t = from_numpy(uniform(-1.2, 1.2, size=(m_batch, n_cols)).astype('float32'))
-        d_t = from_numpy(uniform(-0.1, 0.1, size=(m_batch, n_rows)).astype('float32'))
+        x_t = from_numpy(uniform(-1.2, 1.2, size=(m_batch, n_cols)).astype("float32"))
+        d_t = from_numpy(uniform(-0.1, 0.1, size=(m_batch, n_rows)).astype("float32"))
 
         init_weights = cpp_tile.get_weights()
 
         if python_tile.is_cuda:
             x_t = x_t.cuda()
             d_t = d_t.cuda()
 
@@ -173,76 +166,74 @@
         z = cpp_tile.backward(d_t).cpu()
         self.assertIsInstance(y, Tensor)
         assert_array_almost_equal(z.numpy(), dot(d_t.cpu(), init_weights))
 
         # Perform update.
         cpp_tile.update(x_t, d_t, bias=False)
         post_rank_weights = cpp_tile.get_weights()
-        ref_weights = init_weights - lr*dot(d_t.cpu().T, x_t.cpu())
+        ref_weights = init_weights - lr * dot(d_t.cpu().T, x_t.cpu())
 
         assert_array_almost_equal(post_rank_weights, ref_weights)
 
     def test_cuda_instantiation(self):
         """Test whether cuda weights are copied correctly."""
         if not self.use_cuda or SKIP_CUDA_TESTS:
-            raise SkipTest('not compiled with CUDA support')
+            raise SkipTest("not compiled with CUDA support")
 
         python_tile = self.get_tile(10, 12)
         init_weights = python_tile.tile.get_weights()
 
         cuda_python_tile = python_tile.cuda()
         init_weights_cuda = cuda_python_tile.tile.get_weights()
         assert_array_almost_equal(init_weights, init_weights_cuda)
 
 
-@parametrize_over_tiles([
-    FloatingPoint, FloatingPointCuda
-])
+@parametrize_over_tiles([FloatingPoint, FloatingPointCuda])
 class FloatingPointTileTest(ParametrizedTestCase):
     """Test `rpu_base.FloatingPointTile` functionality."""
 
     def test_setters_weights(self):
         """Check setting and getting the weights."""
         python_tile = self.get_tile(2, 3)
         cpp_tile = python_tile.tile
 
         # Set weights using Tensors.
-        input_weights = Tensor([[1., 2., 3.], [4., 5., 6.]])
+        input_weights = Tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
         cpp_tile.set_weights(input_weights)
         assert_array_equal(cpp_tile.get_weights(), input_weights)
 
         # Set weights using numpy (via python tile).
-        input_weights = array([[1., 2., 3.], [4., 5., 6.]])
+        input_weights = array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
         python_tile.set_weights(input_weights)
         assert_array_equal(cpp_tile.get_weights(), input_weights)
 
         # Set weights using Lists (via python tile).
-        input_weights = [[1., 2., 3.], [4., 5., 6.]]
+        input_weights = [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]
         python_tile.set_weights(input_weights)
         assert_array_equal(cpp_tile.get_weights(), input_weights)
 
     def test_setters_weights_realistic(self):
         """Check setting and getting the weights."""
         python_tile = self.get_tile(2, 3)
         cpp_tile = python_tile.tile
 
         # Set weights using Tensors.
-        input_weights = Tensor([[1., 2., 3.], [4., 5., 6.]])
+        input_weights = Tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
         cpp_tile.set_weights(input_weights)
         assert_array_equal(cpp_tile.get_weights(), input_weights)
 
     def test_n_dim_forward(self):
         """Tests whether forward n-dim inputs work as expected"""
         in_size = 6
         out_size = 5
         add_shape = [2, 3, 4]
 
         python_tile = self.get_tile(out_size, in_size)
         init_weights = python_tile.get_weights()[0]
-        x_t = from_numpy(uniform(-0.1, 0.1, size=add_shape + [in_size]).astype('float32'))
+        x_t = from_numpy(uniform(-0.1, 0.1, size=add_shape + [in_size]).astype("float32"))
 
         if python_tile.is_cuda:
             x_t = x_t.cuda()
 
         y_t = python_tile.forward(x_t).detach().cpu().numpy()
         x_t = x_t.detach().cpu().numpy()
         self.assertEqual(x_t.ndim, y_t.ndim)
@@ -256,15 +247,15 @@
         """Tests whether backward n-dim inputs work as expected"""
         in_size = 6
         out_size = 5
         add_shape = [4, 2]
 
         python_tile = self.get_tile(out_size, in_size)
         init_weights = python_tile.get_weights()[0].cpu().numpy()
-        d_t = from_numpy(uniform(-0.1, 0.1, size=add_shape + [out_size]).astype('float32'))
+        d_t = from_numpy(uniform(-0.1, 0.1, size=add_shape + [out_size]).astype("float32"))
 
         if python_tile.is_cuda:
             d_t = d_t.cuda()
 
         y_t = python_tile.backward(d_t).detach().cpu().numpy()
         d_t = d_t.detach().cpu().numpy()
         self.assertEqual(d_t.ndim, y_t.ndim)
@@ -281,37 +272,35 @@
         lr = 0.3
         add_shape = []
 
         python_tile = self.get_tile(out_size, in_size)
         init_weights = python_tile.get_weights()[0].numpy()
         python_tile.set_learning_rate(lr)
 
-        x_t = from_numpy(uniform(-0.1, 0.1, size=add_shape + [in_size]).astype('float32'))
-        d_t = from_numpy(uniform(-0.1, 0.1, size=add_shape + [out_size]).astype('float32'))
+        x_t = from_numpy(uniform(-0.1, 0.1, size=add_shape + [in_size]).astype("float32"))
+        d_t = from_numpy(uniform(-0.1, 0.1, size=add_shape + [out_size]).astype("float32"))
 
         if python_tile.is_cuda:
             x_t = x_t.cuda()
             d_t = d_t.cuda()
 
         python_tile.update(x_t, d_t)
         updated_weights = python_tile.get_weights()[0]
 
         x_t = x_t.detach().cpu().numpy()
         x_t = reshape(x_t, [-1, in_size])
 
         d_t = d_t.detach().cpu().numpy()
         d_t = reshape(d_t, [-1, out_size])
 
-        ref_weights = init_weights - lr*dot(d_t.T, x_t)
+        ref_weights = init_weights - lr * dot(d_t.T, x_t)
         assert_array_almost_equal(updated_weights, ref_weights)
 
 
-@parametrize_over_tiles([
-    ConstantStep, ConstantStepCuda
-])
+@parametrize_over_tiles([ConstantStep, ConstantStepCuda])
 class AnalogTileTest(ParametrizedTestCase):
     """Test `rpu_base.AnalogTile` functionality."""
 
     def test_setters_weights(self):
         """Check setting and getting the weights."""
         python_tile = self.get_tile(2, 3)
         cpp_tile = python_tile.tile
```

### Comparing `aihwkit-0.7.1/tests/test_client.py` & `aihwkit-0.8.0/tests/test_client.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -27,85 +27,81 @@
 
 class ApiClientTest(AihwkitTestCase):
     """Tests for the AIHW Composer API client."""
 
     def setUp(self) -> None:
         config = ClientConfiguration()
         if not config.token:
-            raise SkipTest('API token not found')
+            raise SkipTest("API token not found")
 
         self.session = ApiSession(config.url, config.token)
         self.api_client = ApiClient(self.session)
 
     def test_experiments_list(self):
         """Test listing experiments."""
         experiments = self.api_client.experiments_list()
 
         self.assertIsInstance(experiments, list)
-        self.assertTrue(all(isinstance(experiment, CloudExperiment)
-                            for experiment in experiments))
+        self.assertTrue(all(isinstance(experiment, CloudExperiment) for experiment in experiments))
 
     def test_experiment_get(self):
         """Test getting an experiment."""
         experiments = self.api_client.experiments_list()
 
         if len(experiments) == 0:
-            raise SkipTest('No experiments found')
+            raise SkipTest("No experiments found")
 
         experiment = experiments[-1]
         fetched_experiment = self.api_client.experiment_get(experiment.id_)
         self.assertEqual(fetched_experiment.id_, experiment.id_)
 
     def test_experiment_input(self):
         """Test getting the input of an experiment."""
         experiments = self.api_client.experiments_list()
         if len(experiments) == 0:
-            raise SkipTest('No experiments found')
+            raise SkipTest("No experiments found")
 
         experiment = experiments[-1]
         input_ = self.api_client.input_get(experiment.input_id)
         self.assertIsInstance(input_, bytes)
 
     def test_experiment_output(self):
         """Test getting the output of an experiment."""
         experiments = self.api_client.experiments_list()
         if len(experiments) == 0:
-            raise SkipTest('No experiments found')
+            raise SkipTest("No experiments found")
 
         for experiment in experiments:
             job_ = self.api_client.job_get(experiment.job.id_)
             if job_.status == CloudJobStatus.COMPLETED:
                 input_ = self.api_client.output_get(job_.output_id)
                 self.assertIsInstance(input_, bytes)
                 break
 
 
-@parametrize_over_experiments([
-    FullyConnectedFashionMNIST
-])
+@parametrize_over_experiments([FullyConnectedFashionMNIST])
 class ApiClientCreateTest(AihwkitTestCase):
     """Tests for the AIHW Composer API client involving experiment creation."""
 
     def setUp(self) -> None:
-        if not os.getenv('TEST_CREATE'):
-            raise SkipTest('TEST_CREATE not set')
+        if not os.getenv("TEST_CREATE"):
+            raise SkipTest("TEST_CREATE not set")
 
         config = ClientConfiguration()
         if not config.token:
-            raise SkipTest('API token not found')
+            raise SkipTest("API token not found")
 
         self.session = ApiSession(config.url, config.token)
         self.api_client = ApiClient(self.session)
 
     def test_create_experiment(self):
         """Test creating a new experiment."""
         experiment = self.get_experiment()
         api_experiment = self.api_client.experiment_create(
-            experiment, name='test_create_experiment',
-            device='cpu'
+            experiment, name="test_create_experiment", device="cpu"
         )
         self.assertIsInstance(api_experiment, CloudExperiment)
 
         # Assert the experiment shows in the list.
         api_experiments = self.api_client.experiments_list()
         api_experiment_ids = [item.id_ for item in api_experiments]
         self.assertIn(api_experiment.id_, api_experiment_ids)
```

### Comparing `aihwkit-0.7.1/tests/test_cloud_runner.py` & `aihwkit-0.8.0/tests/test_cloud_runner.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -28,93 +28,91 @@
 
 class CloudRunnerTest(AihwkitTestCase):
     """Tests for the AIHW Composer CloudRunner."""
 
     def setUp(self) -> None:
         config = ClientConfiguration()
         if not config.token:
-            raise SkipTest('API token not found')
+            raise SkipTest("API token not found")
 
         self.api_url = config.url
         self.api_token = config.token
 
     def test_list_cloud_experiments(self):
         """Test listing cloud experiments."""
         cloud_runner = CloudRunner(self.api_url, self.api_token)
         cloud_experiments = cloud_runner.list_cloud_experiments()
 
         self.assertIsInstance(cloud_experiments, list)
-        self.assertTrue(all(isinstance(experiment, CloudExperiment)
-                            for experiment in cloud_experiments))
+        self.assertTrue(
+            all(isinstance(experiment, CloudExperiment) for experiment in cloud_experiments)
+        )
 
     def test_get_cloud_experiment(self):
         """Test getting an experiment from an execution."""
         cloud_runner = CloudRunner(self.api_url, self.api_token)
         experiments = cloud_runner.list_cloud_experiments()
         if len(experiments) == 0:
-            raise SkipTest('No executions found')
+            raise SkipTest("No executions found")
 
         listed_experiment = experiments[0]
         cloud_experiment = cloud_runner.get_cloud_experiment(listed_experiment.id_)
 
         self.assertIsInstance(cloud_experiment, CloudExperiment)
         self.assertEqual(cloud_experiment.id_, listed_experiment.id_)
 
     def test_get_cloud_experiment_experiment(self):
         """Test getting an experiment from a cloud experiment."""
         cloud_runner = CloudRunner(self.api_url, self.api_token)
         experiments = cloud_runner.list_cloud_experiments()
         if len(experiments) == 0:
-            raise SkipTest('No executions found')
+            raise SkipTest("No executions found")
 
         cloud_experiment = experiments[-1].get_experiment()
-        if 'BasicInferencing' in str(cloud_experiment):
+        if "BasicInferencing" in str(cloud_experiment):
             self.assertIsInstance(cloud_experiment, BasicInferencing)
         else:
             self.assertIsInstance(cloud_experiment, BasicTraining)
 
     def test_get_cloud_experiment_result(self):
         """Test getting the result from a cloud experiment."""
         cloud_runner = CloudRunner(self.api_url, self.api_token)
         cloud_experiments = cloud_runner.list_cloud_experiments()
         if len(cloud_experiments) == 0:
-            raise SkipTest('No executions found')
+            raise SkipTest("No executions found")
 
         for experiment in cloud_experiments:
             if experiment.job.status == CloudJobStatus.COMPLETED:
                 output = experiment.get_result()
                 self.assertIsInstance(output, list)
                 break
         else:
-            raise SkipTest('No completed executions found')
+            raise SkipTest("No completed executions found")
 
 
-@parametrize_over_experiments([
-    FullyConnectedFashionMNIST
-])
+@parametrize_over_experiments([FullyConnectedFashionMNIST])
 class CloudRunnerCreateTest(AihwkitTestCase):
     """Tests for the AIHW Composer CloudRunner involving experiment creation."""
 
     def setUp(self) -> None:
-        if not os.getenv('TEST_CREATE'):
-            raise SkipTest('TEST_CREATE not set')
+        if not os.getenv("TEST_CREATE"):
+            raise SkipTest("TEST_CREATE not set")
 
         config = ClientConfiguration()
         if not config.token:
-            raise SkipTest('API token not found')
+            raise SkipTest("API token not found")
 
         self.api_url = config.url
         self.api_token = config.token
 
     def test_create_experiment(self):
         """Test creating a new experiment."""
         cloud_runner = CloudRunner(self.api_url, self.api_token)
         cloud_experiment = self.get_experiment()
 
-        api_experiment = cloud_runner.run(cloud_experiment,
-                                          device='cpu')
+        api_experiment = cloud_runner.run(cloud_experiment, device="cpu")
         self.assertIsInstance(api_experiment, CloudExperiment)
 
         # Assert the experiment shows in the list.
         api_experiments = cloud_runner.list_cloud_experiments()
         api_experiment_ids = [item.id_ for item in api_experiments]
         self.assertIn(api_experiment.id_, api_experiment_ids)
```

### Comparing `aihwkit-0.7.1/tests/test_experiment_runners.py` & `aihwkit-0.8.0/tests/test_experiment_runners.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -18,60 +18,64 @@
 from unittest.mock import patch
 
 from torch import device as torch_device
 from aihwkit.experiments.runners.local import LocalRunner
 
 from .helpers.decorators import parametrize_over_experiments
 from .helpers.experiments import (
-    FullyConnectedFashionMNIST, FullyConnectedFashionMNISTTikiTaka,
+    FullyConnectedFashionMNIST,
+    FullyConnectedFashionMNISTTikiTaka,
     LeNet5FashionMNIST,
-    Vgg8SVHN, Vgg8SVHNTikiTaka
+    Vgg8SVHN,
+    Vgg8SVHNTikiTaka,
 )
 from .helpers.testcases import AihwkitTestCase, SKIP_CUDA_TESTS
 
 
-@parametrize_over_experiments([
-    FullyConnectedFashionMNIST, FullyConnectedFashionMNISTTikiTaka,
-    LeNet5FashionMNIST,
-    Vgg8SVHN, Vgg8SVHNTikiTaka
-])
+@parametrize_over_experiments(
+    [
+        FullyConnectedFashionMNIST,
+        FullyConnectedFashionMNISTTikiTaka,
+        LeNet5FashionMNIST,
+        Vgg8SVHN,
+        Vgg8SVHNTikiTaka,
+    ]
+)
 class TestLocalRunner(AihwkitTestCase):
     """Test LocalRunner."""
 
     def setUp(self) -> None:
-        if not os.getenv('TEST_DATASET'):
-            raise SkipTest('TEST_DATASET not set')
+        if not os.getenv("TEST_DATASET"):
+            raise SkipTest("TEST_DATASET not set")
 
     def test_run_example_cpu(self):
         """Test running the example using a local runner."""
         training_experiment = self.get_experiment()
-        local_runner = LocalRunner(device=torch_device('cpu'))
-        with patch('sys.stdout', new=StringIO()) as captured_stdout:
-            result = local_runner.run(training_experiment, max_elements_train=10,
-                                      stdout=True)
+        local_runner = LocalRunner(device=torch_device("cpu"))
+        with patch("sys.stdout", new=StringIO()) as captured_stdout:
+            result = local_runner.run(training_experiment, max_elements_train=10, stdout=True)
         # Asserts over stdout.
-        self.assertIn('Epoch: ', captured_stdout.getvalue())
+        self.assertIn("Epoch: ", captured_stdout.getvalue())
 
         # Asserts over the returned results.
         self.assertEqual(len(result), 1)
-        self.assertEqual(result[0]['epoch'], 0)
-        self.assertIn('train_loss', result[0])
-        self.assertIn('accuracy', result[0])
+        self.assertEqual(result[0]["epoch"], 0)
+        self.assertIn("train_loss", result[0])
+        self.assertIn("accuracy", result[0])
 
-    @skipIf(SKIP_CUDA_TESTS, 'not compiled with CUDA support')
+    @skipIf(SKIP_CUDA_TESTS, "not compiled with CUDA support")
     def test_run_example_gpu(self):
         """Test running the example using a local runner."""
         training_experiment = self.get_experiment()
-        local_runner = LocalRunner(device=torch_device('cuda'))
+        local_runner = LocalRunner(device=torch_device("cuda"))
 
-        with patch('sys.stdout', new=StringIO()) as captured_stdout:
-            result = local_runner.run(training_experiment, max_elements_train=10,
-                                      stdout=True)
+        with patch("sys.stdout", new=StringIO()) as captured_stdout:
+            result = local_runner.run(training_experiment, max_elements_train=10, stdout=True)
 
         # Asserts over stdout.
-        self.assertIn('Epoch: ', captured_stdout.getvalue())
+        self.assertIn("Epoch: ", captured_stdout.getvalue())
 
         # Asserts over the returned results.
         self.assertEqual(len(result), 1)
-        self.assertEqual(result[0]['epoch'], 0)
-        self.assertIn('train_loss', result[0])
-        self.assertIn('accuracy', result[0])
+        self.assertEqual(result[0]["epoch"], 0)
+        self.assertIn("train_loss", result[0])
+        self.assertIn("accuracy", result[0])
```

### Comparing `aihwkit-0.7.1/tests/test_experiments.py` & `aihwkit-0.8.0/tests/test_experiments.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,47 +1,52 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
 # that they have been altered from the originals.
 
 """Tests for Experiments."""
 
-import os
+from os import getenv
 from unittest import SkipTest
-
 from aihwkit.cloud.converter.v1.training import BasicTrainingConverter
-from aihwkit.nn.modules.base import AnalogModuleBase
+from aihwkit.nn.modules.base import AnalogLayerBase
 
 from .helpers.decorators import parametrize_over_experiments
 from .helpers.experiments import (
-    FullyConnectedFashionMNIST, FullyConnectedFashionMNISTTikiTaka,
+    FullyConnectedFashionMNIST,
+    FullyConnectedFashionMNISTTikiTaka,
     LeNet5FashionMNIST,
-    Vgg8SVHN, Vgg8SVHNTikiTaka
+    # Vgg8SVHN,
+    # Vgg8SVHNTikiTaka,
 )
 from .helpers.testcases import AihwkitTestCase
 
 
-@parametrize_over_experiments([
-    FullyConnectedFashionMNIST, FullyConnectedFashionMNISTTikiTaka,
-    LeNet5FashionMNIST,
-    Vgg8SVHN, Vgg8SVHNTikiTaka
-])
+@parametrize_over_experiments(
+    [
+        FullyConnectedFashionMNIST,
+        FullyConnectedFashionMNISTTikiTaka,
+        LeNet5FashionMNIST,
+        # Vgg8SVHN,
+        # Vgg8SVHNTikiTaka,
+    ]
+)
 class TestBasicTraining(AihwkitTestCase):
     """Test BasicTraining Experiment."""
 
     def setUp(self) -> None:
-        if not os.getenv('TEST_DATASET'):
-            raise SkipTest('TEST_DATASET not set')
+        if not getenv("TEST_DATASET"):
+            raise SkipTest("TEST_DATASET not set")
 
     def test_conversion_roundtrip(self):
         """Test roundtrip conversion of examples."""
         experiment_original = self.get_experiment()
 
         # Convert to proto.
         converter = BasicTrainingConverter()
@@ -54,15 +59,20 @@
         self.assertEqual(experiment_original.dataset, experiment_converted.dataset)
         self.assertEqual(experiment_original.batch_size, experiment_converted.batch_size)
         self.assertEqual(experiment_original.loss_function, experiment_converted.loss_function)
         self.assertEqual(experiment_original.epochs, experiment_converted.epochs)
 
         # Compare the models by hand, as direct equality comparison is not possible.
         # self.assertEqual(experiment_original.model, experiment_converted.model)
-        self.assertEqual(len(list(experiment_original.model.children())),
-                         len(list(experiment_converted.model.children())))
-        for layer_a, layer_b in zip(experiment_original.model.children(),
-                                    experiment_converted.model.children()):
+        self.assertEqual(
+            len(list(experiment_original.model.children())),
+            len(list(experiment_converted.model.children())),
+        )
+        for layer_a, layer_b in zip(
+            experiment_original.model.children(), experiment_converted.model.children()
+        ):
             self.assertEqual(type(layer_a), type(layer_b))
-            if isinstance(layer_a, AnalogModuleBase):
-                self.assertEqual(type(layer_a.analog_tile.rpu_config.device),
-                                 type(layer_b.analog_tile.rpu_config.device))
+            if isinstance(layer_a, AnalogLayerBase):
+                self.assertEqual(
+                    type(next(layer_a.analog_tiles()).rpu_config.device),
+                    type(next(layer_b.analog_tiles()).rpu_config.device),
+                )
```

### Comparing `aihwkit-0.7.1/tests/test_inference.py` & `aihwkit-0.8.0/tests/test_inference.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,50 +1,51 @@
-
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
 # that they have been altered from the originals.
 
 """Tests for general functionality of layers."""
 
 from torch import randn
 
 from aihwkit.inference import (
-    PCMLikeNoiseModel, StateIndependentNoiseModel, SinglePairConductanceConverter
+    PCMLikeNoiseModel,
+    StateIndependentNoiseModel,
+    SinglePairConductanceConverter,
 )
 
 from .helpers.testcases import AihwkitTestCase
 
 
 class NoiseModelTest(AihwkitTestCase):
     """Noise model tests."""
 
     def test_apply_noise_pcm(self):
         """Test using realistic weights (bias)."""
         weights = randn(10, 35)
 
         noise_model = PCMLikeNoiseModel()
-        t_inference = 100.
+        t_inference = 100.0
         noisy_weights = noise_model.apply_noise(weights, t_inference)
 
         self.assertNotAlmostEqualTensor(noisy_weights, weights)
 
     def test_apply_noise_custom(self):
         """Test using realistic weights (bias)."""
         weights = randn(10, 35)
 
         noise_model = StateIndependentNoiseModel()
-        t_inference = 100.
+        t_inference = 100.0
         noisy_weights = noise_model.apply_noise(weights, t_inference)
 
         self.assertNotAlmostEqualTensor(noisy_weights, weights)
 
 
 class ConductanceConverterTest(AihwkitTestCase):
     """Conductance converter test."""
@@ -61,17 +62,20 @@
         g_lst, params = g_converter.convert_to_conductances(weights)
 
         g_plus = g_lst[0].detach().cpu().numpy()
         g_minus = g_lst[1].detach().cpu().numpy()
 
         tolerance = 1e-6
 
-        self.assertTrue((g_plus > g_max - tolerance).sum()
-                        + (g_minus > g_max - tolerance).sum() > 0)
-        self.assertTrue((g_plus < g_min - tolerance).sum()
-                        + (g_minus < g_min - tolerance).sum() == 0)
-        self.assertTrue((g_plus > g_max + tolerance).sum()
-                        + (g_minus > g_max + tolerance).sum() == 0)
+        self.assertTrue(
+            (g_plus > g_max - tolerance).sum() + (g_minus > g_max - tolerance).sum() > 0
+        )
+        self.assertTrue(
+            (g_plus < g_min - tolerance).sum() + (g_minus < g_min - tolerance).sum() == 0
+        )
+        self.assertTrue(
+            (g_plus > g_max + tolerance).sum() + (g_minus > g_max + tolerance).sum() == 0
+        )
 
         converted_weights = g_converter.convert_back_to_weights(g_lst, params)
 
         self.assertTensorAlmostEqual(weights, converted_weights)
```

### Comparing `aihwkit-0.7.1/tests/test_inference_tiles.py` & `aihwkit-0.8.0/tests/test_inference_tiles.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,65 +1,65 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
 # that they have been altered from the originals.
 
+# pylint: disable=raise-missing-from
+
 """Tests for inference tiles."""
 
 from typing import Optional, List
+from unittest import SkipTest
 
 from parameterized import parameterized
 from torch import ones
 from torch import Tensor
 from torch.nn.functional import mse_loss
 from torch.optim import SGD
 from torch.nn import Linear
 
 from aihwkit.nn import AnalogLinear
 from aihwkit.optim import AnalogSGD
 from aihwkit.simulator.configs import FloatingPointRPUConfig
-from aihwkit.simulator.configs.utils import (
-    WeightNoiseType,
+from aihwkit.simulator.parameters.utils import (
     WeightClipType,
     WeightModifierType,
     WeightModifierParameter,
     WeightRemapType,
 )
 from aihwkit.inference import PCMLikeNoiseModel
+from aihwkit.exceptions import TorchTileConfigError
 
 from .helpers.decorators import parametrize_over_tiles
 from .helpers.testcases import ParametrizedTestCase
-from .helpers.tiles import Inference, InferenceCuda
+from .helpers.tiles import Inference, InferenceCuda, TorchInference, TorchInferenceCuda
 
 
-@parametrize_over_tiles([
-    Inference,
-    InferenceCuda
-])
+@parametrize_over_tiles([Inference, InferenceCuda, TorchInference, TorchInferenceCuda])
 class InferenceTileTest(ParametrizedTestCase):
     """Inference model tests."""
 
     def get_model_and_x(self):
         """Trains a simple model."""
         # Prepare the datasets (input and expected output).
         x = Tensor([[0.1, 0.2, 0.4, 0.3], [0.2, 0.1, 0.1, 0.3]])
         y = Tensor([[1.0, 0.5], [0.7, 0.3]])
 
         # Define a single-layer network, using a constant step device type.
         rpu_config = self.get_rpu_config()
-        rpu_config.forward.out_res = -1.  # Turn off (output) ADC discretization.
-        rpu_config.forward.w_noise_type = WeightNoiseType.ADDITIVE_CONSTANT
-        rpu_config.forward.w_noise = 0.02
+        rpu_config.forward.out_res = -1.0  # Turn off (output) ADC discretization.
+        rpu_config.forward.out_noise = 0.05
+
         rpu_config.noise_model = PCMLikeNoiseModel(g_max=25.0)
 
         model = AnalogLinear(4, 2, bias=True, rpu_config=rpu_config)
 
         # Move the model and tensors to cuda if it is available.
         if self.use_cuda:
             x = x.cuda()
@@ -113,15 +113,14 @@
 
         # Define an analog-aware optimizer, preparing it for using the layers.
         opt = AnalogSGD(model.parameters(), lr=0.1)
         opt_fp = AnalogSGD(model_fp.parameters(), lr=0.1)
         opt_torch = SGD(model_torch.parameters(), lr=0.1)
 
         for _ in range(100):
-
             # inference
             opt.zero_grad()
             pred = model(x)
             loss = mse_loss(pred, y)
             loss.backward()
             opt.step()
 
@@ -152,21 +151,21 @@
         model, x = self.get_model_and_x()
 
         # do inference with drift
         pred_before = model(x)
 
         pred_last = pred_before
         model.eval()
-        for t_inference in [0., 1., 20., 1000., 1e5]:
+        for t_inference in [0.0, 1.0, 20.0, 1000.0, 1e5]:
             model.drift_analog_weights(t_inference)
             pred_drift = model(x)
             self.assertNotAlmostEqualTensor(pred_last, pred_drift)
             pred_last = pred_drift
 
-        self.assertNotAlmostEqualTensor(model.analog_tile.alpha, ones((1,)))
+        self.assertNotAlmostEqualTensor(model.analog_module.alpha, ones((1,)))
 
     def test_post_update_step_clip(self):
         """Tests whether post update diffusion is performed."""
         rpu_config = self.get_rpu_config()
         rpu_config.clip.type = WeightClipType.FIXED_VALUE
         rpu_config.clip.fixed_value = 0.3
         analog_tile = self.get_tile(2, 3, rpu_config=rpu_config, bias=True)
@@ -178,15 +177,20 @@
         analog_tile.set_weights(weights, biases)
 
         analog_tile.post_update_step()
 
         tile_weights, tile_biases = analog_tile.get_weights()
 
         self.assertNotAlmostEqualTensor(tile_weights, weights)
-        self.assertNotAlmostEqualTensor(tile_biases, biases)
+        self.assertAlmostEqual(rpu_config.clip.fixed_value, tile_weights.abs().max().item())
+        if analog_tile.analog_bias:
+            self.assertNotAlmostEqualTensor(tile_biases, biases)
+            self.assertAlmostEqual(rpu_config.clip.fixed_value, tile_biases.abs().max().item())
+        if analog_tile.digital_bias:
+            self.assertTensorAlmostEqual(biases, tile_biases)
 
     def test_post_update_step_remap_layer(self):
         """Tests whether post update remap is performed."""
         rpu_config = self.get_rpu_config()
         rpu_config.mapping.out_scaling_columnwise = False
         rpu_config.mapping.learn_out_scaling = True
         rpu_config.mapping.weight_scaling_omega = 1.0
@@ -199,19 +203,21 @@
         biases = Tensor([-0.1, -0.3])
 
         analog_tile.set_learning_rate(0.123)
         analog_tile.set_weights(weights, biases)
 
         analog_tile.post_update_step()
 
-        tile_weights, tile_biases = analog_tile.get_weights()
+        tile_weights, tile_biases = analog_tile.get_weights(apply_weight_scaling=False)
 
         self.assertTensorAlmostEqual(tile_weights, weights / 0.6)
-        self.assertTensorAlmostEqual(tile_biases, biases / 0.6)
-
+        if analog_tile.analog_bias:
+            self.assertTensorAlmostEqual(tile_biases, biases / 0.6)
+        if analog_tile.digital_bias:
+            self.assertTensorAlmostEqual(tile_biases, biases)
         scales = analog_tile.get_mapping_scales()
         self.assertTensorAlmostEqual(scales, Tensor([0.6, 0.6]))
 
     def test_post_update_step_remap_column(self):
         """Tests whether post update remap is performed."""
         rpu_config = self.get_rpu_config()
         rpu_config.mapping.out_scaling_columnwise = True
@@ -222,95 +228,107 @@
         rpu_config.remap.type = WeightRemapType.CHANNELWISE_SYMMETRIC
         analog_tile = self.get_tile(2, 3, rpu_config=rpu_config, bias=True)
 
         weights = Tensor([[-0.7, 0.2, 0.3], [0.4, 0.5, 0.6]])
         biases = Tensor([-0.1, -0.3])
 
         analog_tile.set_learning_rate(0.123)
-        analog_tile.set_weights(weights, biases)
+        analog_tile.set_weights(weights, biases, apply_weight_scaling=False)
 
         analog_tile.post_update_step()
 
-        tile_weights, tile_biases = analog_tile.get_weights()
+        tile_weights, tile_biases = analog_tile.get_weights(apply_weight_scaling=False)
 
         expected_scales = Tensor([0.7, 0.6])
         self.assertTensorAlmostEqual(tile_weights, weights / expected_scales.view(-1, 1))
-        self.assertTensorAlmostEqual(tile_biases, biases / expected_scales)
-
+        if analog_tile.analog_bias:
+            self.assertTensorAlmostEqual(tile_biases, biases / expected_scales)
+        if analog_tile.digital_bias:
+            self.assertTensorAlmostEqual(tile_biases, biases)
         scales = analog_tile.get_mapping_scales()
         self.assertTensorAlmostEqual(scales, expected_scales)
 
-    @parameterized.expand([
-        ('none', None,),
-        ('dorefa', WeightModifierType.DOREFA,),
-        ('mult_normal', WeightModifierType.MULT_NORMAL,),
-        ('poly', WeightModifierType.POLY, [1., 3.]),
-        ('polyN', WeightModifierType.POLY, [0.1, 0.2, 0.2, 0.3]),
-        ('discretize', WeightModifierType.DISCRETIZE,),
-        ('add_normal', WeightModifierType.DISCRETIZE_ADD_NORMAL,),
-    ])
-    def test_post_forward_modifier_types(self, _,
-                                         modifier_type: 'WeightModifierType',
-                                         coeffs: Optional[List] = None):
+    @parameterized.expand(
+        [
+            ("none", None),
+            ("dorefa", WeightModifierType.DOREFA),
+            ("mult_normal", WeightModifierType.MULT_NORMAL),
+            ("add_normal", WeightModifierType.ADD_NORMAL),
+            ("poly", WeightModifierType.POLY, [1.0, 3.0]),
+            ("polyN", WeightModifierType.POLY, [0.1, 0.2, 0.2, 0.3]),
+            ("progN", WeightModifierType.PROG_NOISE, [0.1, 0.2, 0.2, 0.3]),
+            ("discretize", WeightModifierType.DISCRETIZE),
+            ("discretize_add_normal", WeightModifierType.DISCRETIZE_ADD_NORMAL),
+        ]
+    )
+    def test_post_forward_modifier_types(
+        self, _, modifier_type: "WeightModifierType", coeffs: Optional[List] = None
+    ):
         """Tests whether modifier is performed."""
         rpu_config = self.get_rpu_config()
         rpu_config.drift_compensation = None
         rpu_config.forward.is_perfect = True
         rpu_config.forward.out_noise = 0.0
         rpu_config.forward.inp_noise = 0.0
 
         modifier = self.get_modifier(modifier_type, coeffs)
         if modifier is not None:
             rpu_config.modifier = modifier
 
-        analog_tile = self.get_tile(2, 3, rpu_config=rpu_config, bias=True)
+        try:
+            analog_tile = self.get_tile(2, 3, rpu_config=rpu_config, bias=True)
+        except TorchTileConfigError as exc:
+            raise SkipTest("Not supported by TorchTile: {}".format(str(exc)))
+
         x_input = Tensor([[0.1, 0.2, 0.3]])
         weights = Tensor([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])
         biases = Tensor([-0.1, -0.2])
 
         if self.use_cuda:
             x_input = x_input.cuda()
 
         analog_tile.set_learning_rate(0.123)
         analog_tile.set_weights(weights, biases)
 
-        x_output = analog_tile.forward(x_input, is_test=True)
+        analog_tile.eval()
+        x_output = analog_tile(x_input)
         analog_tile.post_update_step()
-        x_output_post = analog_tile.forward(x_input, is_test=False)
-        x_output_post_true = analog_tile.forward(x_input, is_test=True)
+        analog_tile.train()
+        x_output_post = analog_tile(x_input)
+        analog_tile.eval()
+        x_output_post_true = analog_tile(x_input)
         tile_weights, tile_biases = analog_tile.get_weights()
 
         self.assertTensorAlmostEqual(tile_weights, weights)
         self.assertTensorAlmostEqual(tile_biases, biases)
 
         if modifier is None:
             self.assertTensorAlmostEqual(x_output, x_output_post)
         else:
             self.assertNotAlmostEqualTensor(x_output, x_output_post)
 
         self.assertTensorAlmostEqual(x_output, x_output_post_true)
 
     @staticmethod
     def get_modifier(
-            modifier_type: Optional[WeightModifierType],
-            coeffs: Optional[List] = None,
+        modifier_type: Optional[WeightModifierType], coeffs: Optional[List] = None
     ) -> Optional[WeightModifierParameter]:
         """Returns the modifier parameter."""
         if modifier_type is None:
             return None
         if coeffs is None:
             coeffs = [1.0, 0.1]
 
         modifier = WeightModifierParameter(
             type=modifier_type,
             std_dev=1.0,
             enable_during_test=False,
             res=0.132,
             coeffs=coeffs,
             rel_to_actual_wmax=False,
-            assumed_wmax=1.0
+            assumed_wmax=1.0,
         )
 
-        if modifier_type == WeightModifierType.COPY:
+        if modifier_type == WeightModifierType.DROP_CONNECT:
             modifier.pdrop = 0.5
 
         return modifier
```

### Comparing `aihwkit-0.7.1/tests/test_layers.py` & `aihwkit-0.8.0/tests/test_layers.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -12,393 +12,372 @@
 
 """Tests for general functionality of layers."""
 
 # pylint: disable=too-few-public-methods
 from tempfile import TemporaryFile
 from unittest import SkipTest
 
-from torch import Tensor, device, load, save
+from torch import Tensor, device, load, save, zeros
 from torch.cuda import current_device, device_count
 
 from aihwkit.nn import AnalogSequential
 from aihwkit.simulator.configs import SingleRPUConfig
 from aihwkit.simulator.rpu_base import tiles
 from aihwkit.simulator.tiles import AnalogTile
 
 from .helpers.decorators import parametrize_over_layers
 from .helpers.layers import (
-    Linear, Conv1d, Conv2d, Conv3d,
-    LinearCuda, Conv1dCuda, Conv2dCuda, Conv3dCuda
+    Linear,
+    Conv1d,
+    Conv2d,
+    Conv3d,
+    LinearCuda,
+    Conv1dCuda,
+    Conv2dCuda,
+    Conv3dCuda,
 )
 from .helpers.testcases import ParametrizedTestCase
 from .helpers.tiles import ConstantStep, Inference
 from .helpers.testcases import SKIP_CUDA_TESTS
 
 
 @parametrize_over_layers(
-    layers=[Linear, Conv1d, Conv2d, Conv3d,
-            LinearCuda, Conv1dCuda, Conv2dCuda, Conv3dCuda],
-    tiles=[ConstantStep],
-    biases=['analog', None]
+    layers=[Linear, Conv2d, LinearCuda, Conv2dCuda], tiles=[ConstantStep], biases=[None]
 )
 class AnalogLayerTest(ParametrizedTestCase):
     """Analog layers abstraction tests."""
+
     # pylint: disable=no-member
 
     def test_realistic_weights(self):
         """Test using realistic weights."""
-        layer = self.get_layer(realistic_read_write=True)
-
-        shape = layer.weight.shape
-        # Check that the tile weights are equal from the layer weights, as
-        # the weights are synced after being set.
-        tile_weights, tile_biases = layer.analog_tile.get_weights()
-        self.assertTensorAlmostEqual(layer.weight, tile_weights.reshape(shape))
-        if self.analog_bias:
-            self.assertTensorAlmostEqual(layer.bias, tile_biases)
+        layer = self.get_layer()
 
-        # 1. Set the layer weights and biases.
+        # 1. Set the layer weights and biases. (Exact writing / reading)
         user_weights = Tensor(layer.out_features, layer.in_features).uniform_(-0.5, 0.5)
         user_biases = Tensor(layer.out_features).uniform_(-0.5, 0.5)
-        layer.set_weights(user_weights, user_biases)
+        layer.set_weights(user_weights, user_biases, realistic=False)
+        tile_weights, tile_biases = layer.get_weights()
 
-        # Check that the tile weights are equal from the layer weights, as
-        # the weights are synced after being set.
-        tile_weights, tile_biases = layer.analog_tile.get_weights()
-        self.assertTensorAlmostEqual(layer.weight, tile_weights.reshape(shape))
+        # Check that the tile weights are equal
+        self.assertTensorAlmostEqual(user_weights, tile_weights)
         if self.analog_bias:
-            self.assertTensorAlmostEqual(layer.bias, tile_biases)
+            self.assertTensorAlmostEqual(user_biases, tile_biases)
 
-        # Check that the tile weights are different than the user-specified
-        # weights, as it is realistic.
-        self.assertNotAlmostEqualTensor(user_weights, tile_weights.reshape(shape))
+        # 2. Realistic writing
+        layer.set_weights(user_weights, user_biases, realistic=True)
+        tile_weights, tile_biases = layer.get_weights()
+
+        # Check that the tile weights are not equal
+        self.assertNotAlmostEqualTensor(user_weights, tile_weights)
+        # but approximately correct
+        self.assertTensorAlmostEqual((user_weights - tile_weights).mean(), zeros(1), decimal=1)
         if self.analog_bias:
             self.assertNotAlmostEqualTensor(user_biases, tile_biases)
 
-        # 2. Get the layer weights and biases.
-        gotten_weights, gotten_biases = layer.get_weights()
-
-        # Check that the tile weights are different than the gotten
-        # weights, as it is realistic.
-        self.assertNotAlmostEqualTensor(gotten_weights, tile_weights.reshape(shape))
-        if self.analog_bias:
-            self.assertNotAlmostEqualTensor(gotten_biases, tile_biases)
-
-    def test_not_realistic_weights(self):
-        """Test using non realistic weights."""
-        layer = self.get_layer(realistic_read_write=False)
-
-        shape = layer.weight.shape
-        # Check that the tile weights are equal from the layer weights, as
-        # the weights are synced after being set.
-        tile_weights, tile_biases = layer.analog_tile.get_weights()
-        self.assertTensorAlmostEqual(layer.weight, tile_weights.reshape(shape))
-        if self.analog_bias:
-            self.assertTensorAlmostEqual(layer.bias, tile_biases)
-
-        # 1. Set the layer weights and biases.
-        user_weights = Tensor(layer.out_features, layer.in_features).uniform_(-0.5, 0.5)
-        user_biases = Tensor(layer.out_features).uniform_(-0.5, 0.5)
-        layer.set_weights(user_weights, user_biases)
+        # 2. Realistic reading
+        layer.set_weights(user_weights, user_biases, realistic=False)
+        tile_weights, tile_biases = layer.get_weights(realistic=True)
 
-        # Check that the tile weights are equal from the layer weights, as
-        # the weights are synced after being set.
-        tile_weights, tile_biases = layer.analog_tile.get_weights()
-        self.assertTensorAlmostEqual(layer.weight, tile_weights.reshape(shape))
+        # Check that the tile weights are not equal
+        self.assertNotAlmostEqualTensor(user_weights, tile_weights)
         if self.analog_bias:
-            self.assertTensorAlmostEqual(layer.bias, tile_biases)
-
-        # Check that the tile weights are equal to the user-specified
-        # weights, as it is not realistic.
-        self.assertTensorAlmostEqual(user_weights, tile_weights)
-        if self.analog_bias:
-            self.assertTensorAlmostEqual(user_biases, tile_biases)
-
-        # 2. Get the layer weights and biases.
-        gotten_weights, gotten_biases = layer.get_weights()
-
-        # Check that the tile weights are equal than the gotten
-        # weights, as it is not realistic.
-        self.assertTensorAlmostEqual(gotten_weights, tile_weights)
-        if self.analog_bias:
-            self.assertTensorAlmostEqual(gotten_biases, tile_biases)
+            self.assertNotAlmostEqualTensor(user_biases, tile_biases)
 
 
 @parametrize_over_layers(
-    layers=[Linear, Conv1d, Conv2d, Conv3d,
-            LinearCuda, Conv1dCuda, Conv2dCuda, Conv3dCuda],
+    layers=[Linear, Conv1d, Conv2d, Conv3d, LinearCuda, Conv1dCuda, Conv2dCuda, Conv3dCuda],
     tiles=[ConstantStep, Inference],
-    biases=['analog', None]
+    biases=["analog", None],
 )
 class AnalogLayerMoveTest(ParametrizedTestCase):
     """Analog layers abstraction tests."""
 
     def test_sequential_move_to_cuda(self):
         """Test moving AnalogSequential to cuda (from CPU)."""
         if SKIP_CUDA_TESTS:
-            raise SkipTest('not compiled with CUDA support')
+            raise SkipTest("not compiled with CUDA support")
 
         # Map the original tile classes to the expected ones after `cuda()`.
         tile_classes = {
             tiles.AnalogTile: tiles.CudaAnalogTile,
-            tiles.CudaAnalogTile: tiles.CudaAnalogTile
+            tiles.CudaAnalogTile: tiles.CudaAnalogTile,
         }
 
         layer = self.get_layer()
-        expected_class = tile_classes[layer.analog_tile.tile.__class__]
-        expected_device = device('cuda', current_device())
+        analog_tile = next(layer.analog_tiles())
+        expected_class = tile_classes[analog_tile.tile.__class__]
+        expected_device = device("cuda", current_device())
 
         # Create a container and move to cuda.
         model = AnalogSequential(layer)
         model.cuda()
 
-        analog_tile = layer.analog_tile
         self.assertEqual(analog_tile.device, expected_device)
         self.assertEqual(analog_tile.get_analog_ctx().data.device, expected_device)
         if analog_tile.shared_weights is not None:
             self.assertEqual(analog_tile.shared_weights.data.device, expected_device)
-            self.assertEqual(analog_tile.shared_weights.data.size()[0],
-                             analog_tile.tile.get_x_size())
-            self.assertEqual(analog_tile.shared_weights.data.size()[1],
-                             analog_tile.tile.get_d_size())
+            self.assertEqual(
+                analog_tile.shared_weights.data.size()[0], analog_tile.tile.get_x_size()
+            )
+            self.assertEqual(
+                analog_tile.shared_weights.data.size()[1], analog_tile.tile.get_d_size()
+            )
 
         # Assert the tile has been moved to cuda.
-        self.assertIsInstance(layer.analog_tile.tile, expected_class)
+        self.assertIsInstance(analog_tile.tile, expected_class)
 
     def test_sequential_move_to_cuda_via_to(self):
         """Test moving AnalogSequential to cuda (from CPU), using ``.to()``."""
         if SKIP_CUDA_TESTS:
-            raise SkipTest('not compiled with CUDA support')
+            raise SkipTest("not compiled with CUDA support")
 
         # Map the original tile classes to the expected ones after `cuda()`.
         tile_classes = {
             tiles.AnalogTile: tiles.CudaAnalogTile,
-            tiles.CudaAnalogTile: tiles.CudaAnalogTile
+            tiles.CudaAnalogTile: tiles.CudaAnalogTile,
         }
 
         layer = self.get_layer()
-        expected_class = tile_classes[layer.analog_tile.tile.__class__]
-        expected_device = device('cuda', current_device())
+        analog_tile = next(layer.analog_tiles())
+        expected_class = tile_classes[analog_tile.tile.__class__]
+        expected_device = device("cuda", current_device())
 
         # Create a container and move to cuda.
         model = AnalogSequential(layer)
-        model.to(device('cuda'))
+        model.to(device("cuda"))
 
-        analog_tile = layer.analog_tile
         self.assertEqual(analog_tile.device, expected_device)
         self.assertEqual(analog_tile.get_analog_ctx().data.device, expected_device)
         if analog_tile.shared_weights is not None:
             self.assertEqual(analog_tile.shared_weights.data.device, expected_device)
-            self.assertEqual(analog_tile.shared_weights.data.size()[0],
-                             analog_tile.tile.get_x_size())
-            self.assertEqual(analog_tile.shared_weights.data.size()[1],
-                             analog_tile.tile.get_d_size())
+            self.assertEqual(
+                analog_tile.shared_weights.data.size()[0], analog_tile.tile.get_x_size()
+            )
+            self.assertEqual(
+                analog_tile.shared_weights.data.size()[1], analog_tile.tile.get_d_size()
+            )
 
         # Assert the tile has been moved to cuda.
-        self.assertIsInstance(layer.analog_tile.tile, expected_class)
+        self.assertIsInstance(analog_tile.tile, expected_class)
 
     def test_sequential_move_to_cuda_via_to_multiple_gpus(self):
         """Test moving AnalogSequential to cuda (from CPU), using ``.to()``."""
         if SKIP_CUDA_TESTS:
-            raise SkipTest('not compiled with CUDA support')
+            raise SkipTest("not compiled with CUDA support")
         if device_count() < 2:
-            raise SkipTest('Need at least two devices for this test')
+            raise SkipTest("Need at least two devices for this test")
 
         # Map the original tile classes to the expected ones after `cuda()`.
         tile_classes = {
             tiles.AnalogTile: tiles.CudaAnalogTile,
-            tiles.CudaAnalogTile: tiles.CudaAnalogTile
+            tiles.CudaAnalogTile: tiles.CudaAnalogTile,
         }
 
         # Test whether it can move to GPU with index 1
         expected_device_num = 1
 
         layer = self.get_layer()
-        if isinstance(layer.analog_tile.tile.__class__, (tiles.CudaAnalogTile,
-                                                         tiles.CudaFloatingPointTile)):
-            raise SkipTest('Layer is already on CUDA')
+        analog_tile = next(layer.analog_tiles())
+        if isinstance(analog_tile.tile, (tiles.CudaAnalogTile, tiles.CudaFloatingPointTile)):
+            raise SkipTest("Layer is already on CUDA")
 
-        expected_class = tile_classes[layer.analog_tile.tile.__class__]
-        expected_device = device('cuda', expected_device_num)
+        expected_class = tile_classes[analog_tile.tile.__class__]
+        expected_device = device("cuda", expected_device_num)
 
         # Create a container and move to cuda.
         model = AnalogSequential(layer)
-        model.to(device('cuda', expected_device_num))
+        model.to(device("cuda", expected_device_num))
 
-        analog_tile = layer.analog_tile
         self.assertEqual(analog_tile.device, expected_device)
         self.assertEqual(analog_tile.get_analog_ctx().data.device, expected_device)
         if analog_tile.shared_weights is not None:
             self.assertEqual(analog_tile.shared_weights.data.device, expected_device)
-            self.assertEqual(analog_tile.shared_weights.data.size()[0],
-                             analog_tile.tile.get_x_size())
-            self.assertEqual(analog_tile.shared_weights.data.size()[1],
-                             analog_tile.tile.get_d_size())
+            self.assertEqual(
+                analog_tile.shared_weights.data.size()[0], analog_tile.tile.get_x_size()
+            )
+            self.assertEqual(
+                analog_tile.shared_weights.data.size()[1], analog_tile.tile.get_d_size()
+            )
 
         # Assert the tile has been moved to cuda.
-        self.assertIsInstance(layer.analog_tile.tile, expected_class)
+        self.assertIsInstance(analog_tile.tile, expected_class)
 
     def test_sequential_move_to_cuda_multiple_gpus(self):
         """Test moving AnalogSequential to cuda (from CPU), using ``.to()``."""
         if SKIP_CUDA_TESTS:
-            raise SkipTest('not compiled with CUDA support')
+            raise SkipTest("not compiled with CUDA support")
         if device_count() < 2:
-            raise SkipTest('Need at least two devices for this test')
+            raise SkipTest("Need at least two devices for this test")
 
         # Map the original tile classes to the expected ones after `cuda()`.
         tile_classes = {
             tiles.AnalogTile: tiles.CudaAnalogTile,
-            tiles.CudaAnalogTile: tiles.CudaAnalogTile
+            tiles.CudaAnalogTile: tiles.CudaAnalogTile,
         }
 
         # Test whether it can move to GPU with index 1
         expected_device_num = 1
 
         layer = self.get_layer()
-        if isinstance(layer.analog_tile.tile.__class__, (tiles.CudaAnalogTile,
-                                                         tiles.CudaFloatingPointTile)):
-            raise SkipTest('Layer is already on CUDA')
+        analog_tile = next(layer.analog_tiles())
+
+        if isinstance(analog_tile.tile, (tiles.CudaAnalogTile, tiles.CudaFloatingPointTile)):
+            raise SkipTest("Layer is already on CUDA")
 
-        expected_class = tile_classes[layer.analog_tile.tile.__class__]
-        expected_device = device('cuda', expected_device_num)
+        expected_class = tile_classes[analog_tile.tile.__class__]
+        expected_device = device("cuda", expected_device_num)
 
         # Create a container and move to cuda.
         model = AnalogSequential(layer)
-        model.cuda(device('cuda', expected_device_num))
+        model.cuda(device("cuda", expected_device_num))
 
-        analog_tile = layer.analog_tile
         self.assertEqual(analog_tile.device, expected_device)
         self.assertEqual(analog_tile.get_analog_ctx().data.device, expected_device)
         if analog_tile.shared_weights is not None:
             self.assertEqual(analog_tile.shared_weights.data.device, expected_device)
-            self.assertEqual(analog_tile.shared_weights.data.size()[0],
-                             analog_tile.tile.get_x_size())
-            self.assertEqual(analog_tile.shared_weights.data.size()[1],
-                             analog_tile.tile.get_d_size())
+            self.assertEqual(
+                analog_tile.shared_weights.data.size()[0], analog_tile.tile.get_x_size()
+            )
+            self.assertEqual(
+                analog_tile.shared_weights.data.size()[1], analog_tile.tile.get_d_size()
+            )
 
         # Assert the tile has been moved to cuda.
-        self.assertIsInstance(layer.analog_tile.tile, expected_class)
+        self.assertIsInstance(analog_tile.tile, expected_class)
 
     def test_save_with_cuda(self):
         """Whether model is correctly reconstructed after saving"""
         if SKIP_CUDA_TESTS:
-            raise SkipTest('not compiled with CUDA support')
+            raise SkipTest("not compiled with CUDA support")
 
         # Map the original tile classes to the expected ones after `cuda()`.
         tile_classes = {
             tiles.AnalogTile: tiles.CudaAnalogTile,
-            tiles.CudaAnalogTile: tiles.CudaAnalogTile
+            tiles.CudaAnalogTile: tiles.CudaAnalogTile,
         }
 
         layer = self.get_layer()
+
         model = AnalogSequential(layer)
         model.cuda()
+        analog_tile = next(model.analog_tiles())
+        expected_class = tile_classes[analog_tile.tile.__class__]
+
         with TemporaryFile() as file:
             save(model.state_dict(), file)
             # Create a new model and load its state dict.
             file.seek(0)
             checkpoint = load(file)
         model.load_state_dict(checkpoint)
 
-        expected_device = device('cuda', current_device())
-        expected_class = tile_classes[layer.analog_tile.tile.__class__]
+        expected_device = device("cuda", current_device())
+        expected_class = tile_classes[analog_tile.tile.__class__]
+        analog_tile = next(model.analog_tiles())
 
-        analog_tile = model[0].analog_tile
         self.assertEqual(analog_tile.device, expected_device)
         self.assertEqual(analog_tile.get_analog_ctx().data.device, expected_device)
         if analog_tile.shared_weights is not None:
             self.assertEqual(analog_tile.shared_weights.data.device, expected_device)
-            self.assertEqual(analog_tile.shared_weights.data.size()[0],
-                             analog_tile.tile.get_x_size())
-            self.assertEqual(analog_tile.shared_weights.data.size()[1],
-                             analog_tile.tile.get_d_size())
+            self.assertEqual(
+                analog_tile.shared_weights.data.size()[0], analog_tile.tile.get_x_size()
+            )
+            self.assertEqual(
+                analog_tile.shared_weights.data.size()[1], analog_tile.tile.get_d_size()
+            )
 
         # Assert the tile has been moved to cuda.
-        self.assertIsInstance(layer.analog_tile.tile, expected_class)
+        self.assertIsInstance(analog_tile.tile, expected_class)
 
 
 @parametrize_over_layers(
     layers=[Linear, Conv1d, Conv2d, Conv3d],
     tiles=[ConstantStep, Inference],
-    biases=['analog', 'digital', None]
+    biases=["analog", "digital", None],
 )
 class CpuAnalogLayerTest(ParametrizedTestCase):
     """Analog layers tests using CPU tiles as the source."""
 
     def test_sequential_move_to_cpu(self):
         """Test moving AnalogSequential to CPU (from CPU)."""
         layer = self.get_layer()
 
         # Create a container and move to cuda.
         model = AnalogSequential(layer)
         model.cpu()
 
-        analog_tile = layer.analog_tile
-        self.assertEqual(analog_tile.device, device('cpu'))
-        self.assertEqual(analog_tile.get_analog_ctx().data.device, device('cpu'))
+        analog_tile = next(layer.analog_tiles())
+        self.assertEqual(analog_tile.device, device("cpu"))
+        self.assertEqual(analog_tile.get_analog_ctx().data.device, device("cpu"))
 
         if analog_tile.shared_weights is not None:
-            self.assertEqual(analog_tile.shared_weights.data.device, device('cpu'))
-            self.assertEqual(analog_tile.shared_weights.data.size()[0],
-                             analog_tile.tile.get_d_size())
-            self.assertEqual(analog_tile.shared_weights.data.size()[1],
-                             analog_tile.tile.get_x_size())
+            self.assertEqual(analog_tile.shared_weights.data.device, device("cpu"))
+            self.assertEqual(
+                analog_tile.shared_weights.data.size()[0], analog_tile.tile.get_d_size()
+            )
+            self.assertEqual(
+                analog_tile.shared_weights.data.size()[1], analog_tile.tile.get_x_size()
+            )
 
         # Assert the tile is still on CPU.
-        self.assertIsInstance(layer.analog_tile.tile, tiles.AnalogTile)
+        self.assertIsInstance(analog_tile.tile, tiles.AnalogTile)
 
     def test_sequential_move_to_cpu_via_to(self):
         """Test moving AnalogSequential to CPU (from CPU), using ``.to()``."""
         layer = self.get_layer()
 
-        expected_device = device('cpu')
+        expected_device = device("cpu")
         # Create a container and move to cuda.
         model = AnalogSequential(layer)
-        model.to(device('cpu'))
+        model.to(device("cpu"))
 
-        analog_tile = layer.analog_tile
+        analog_tile = next(layer.analog_tiles())
         self.assertEqual(analog_tile.device, expected_device)
         self.assertEqual(analog_tile.get_analog_ctx().data.device, expected_device)
 
         if analog_tile.shared_weights is not None:
             self.assertEqual(analog_tile.shared_weights.data.device, expected_device)
-            self.assertEqual(analog_tile.shared_weights.data.size()[0],
-                             analog_tile.tile.get_d_size())
-            self.assertEqual(analog_tile.shared_weights.data.size()[1],
-                             analog_tile.tile.get_x_size())
+            self.assertEqual(
+                analog_tile.shared_weights.data.size()[0], analog_tile.tile.get_d_size()
+            )
+            self.assertEqual(
+                analog_tile.shared_weights.data.size()[1], analog_tile.tile.get_x_size()
+            )
 
         # Assert the tile is still on CPU.
-        self.assertIsInstance(layer.analog_tile.tile, tiles.AnalogTile)
+        self.assertIsInstance(analog_tile.tile, tiles.AnalogTile)
 
 
 class CustomAnalogTile(AnalogTile):
     """Helper tile for ``CustomTileTest``."""
 
 
 class CustomRPUConfig(SingleRPUConfig):
     """Helper rpu config for ``CustomTileTest``."""
+
     tile_class = CustomAnalogTile
 
 
 class CustomTileTestHelper:
     """Helper tile for parametrizing during ``CustomTileTest``."""
 
     def get_rpu_config(self):
         """Return a RPU Config."""
         return CustomRPUConfig()
 
 
 @parametrize_over_layers(
     layers=[Linear, Conv1d, Conv2d, Conv3d],
     tiles=[CustomTileTestHelper],
-    biases=['analog', 'digital', None]
+    biases=["analog", "digital", None],
 )
 class CustomTileTest(ParametrizedTestCase):
     """Test for analog layers using custom tiles."""
 
     def test_custom_tile(self):
         """Test using a custom tile with analog layers."""
         # Create the layer, which uses `CustomRPUConfig`.
         layer = self.get_layer()
 
         # Assert that the internal analog tile is `CustomAnalogTile`.
-        self.assertIsInstance(layer.analog_tile, CustomAnalogTile)
+        analog_tile = next(layer.analog_tiles())
+        self.assertIsInstance(analog_tile, CustomAnalogTile)
```

### Comparing `aihwkit-0.7.1/tests/test_layers_convolution.py` & `aihwkit-0.8.0/tests/test_layers_convolution.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,78 +1,92 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
 # that they have been altered from the originals.
 
 """Tests for layer abstractions."""
 
+from unittest import SkipTest
+
 from torch import randn
-from torch.nn import (Conv1d as torch_Conv1d, Conv2d as torch_Conv2d,
-                      Conv3d as torch_Conv3d, Sequential)
+from torch.nn import (
+    Conv1d as torch_Conv1d,
+    Conv2d as torch_Conv2d,
+    Conv3d as torch_Conv3d,
+    Sequential,
+)
 from torch.nn.functional import mse_loss
 
 from aihwkit.optim import AnalogSGD
 from aihwkit.simulator.configs.configs import InferenceRPUConfig
-from aihwkit.simulator.configs.utils import (
-    MappingParameter, IOParameters, WeightModifierParameter, WeightModifierType
+from aihwkit.simulator.parameters.utils import (
+    MappingParameter,
+    IOParameters,
+    WeightModifierParameter,
+    WeightModifierType,
 )
 from aihwkit.inference.compensation.drift import GlobalDriftCompensation
 from aihwkit.inference.noise.custom import StateIndependentNoiseModel
 from aihwkit.nn.conversion import convert_to_analog
 
 from .helpers.decorators import parametrize_over_layers
 from .helpers.layers import Conv1d, Conv1dCuda, Conv2d, Conv2dCuda, Conv3d, Conv3dCuda
 from .helpers.testcases import ParametrizedTestCase
-from .helpers.tiles import FloatingPoint, Inference
+from .helpers.tiles import FloatingPoint, Inference, TorchInference, Custom
 
 
 class ConvolutionLayerTest(ParametrizedTestCase):
     """Generic class for helping testing analog convolution layers."""
 
     digital_layer_cls = torch_Conv1d
 
     def get_digital_layer(self, in_channels=2, out_channels=3, kernel_size=4, padding=2):
         """Return a digital layer."""
-        layer = self.digital_layer_cls(in_channels=in_channels,
-                                       out_channels=out_channels,
-                                       kernel_size=kernel_size,
-                                       padding=padding,
-                                       bias=self.bias)
+        layer = self.digital_layer_cls(
+            in_channels=in_channels,
+            out_channels=out_channels,
+            kernel_size=kernel_size,
+            padding=padding,
+            bias=self.bias,
+        )
         if self.use_cuda:
             layer = layer.cuda()
 
         return layer
 
     def set_weights_from_digital_model(self, analog_model, digital_model):
         """Set the analog model weights based on the digital model."""
         weights, biases = self.get_weights_from_digital_model(analog_model, digital_model)
-        analog_model.set_weights(weights, biases, force_exact=True)
+        analog_model.set_weights(weights, biases)
 
     @staticmethod
     def get_weights_from_digital_model(analog_model, digital_model):
         """Set the analog model weights based on the digital model."""
-        weights = digital_model.weight.data.detach().reshape(
-            [analog_model.out_features, analog_model.in_features]).cpu()
+        weights = (
+            digital_model.weight.data.detach()
+            .reshape([analog_model.out_features, analog_model.in_features])
+            .cpu()
+        )
         biases = None
         if digital_model.bias is not None:
             biases = digital_model.bias.data.detach().cpu()
 
         return weights, biases
 
     @staticmethod
     def get_weights_from_analog_model(analog_model):
         """Set the analog model weights based on the digital model."""
-        weights, biases = analog_model.get_weights(force_exact=True)
+        weights, biases = analog_model.get_weights()
         return weights, biases
 
     @staticmethod
     def train_model(model, loss_func, x_b, y_b):
         """Train the model."""
         opt = AnalogSGD(model.parameters(), lr=0.1)
 
@@ -81,27 +95,22 @@
             opt.zero_grad()
             pred = model(x_b)
             loss = loss_func(pred, y_b)
             loss.backward()
             opt.step()
 
     def base_test_inference_modifier(self, torch_model, x_b):
-        """ tests whether modifier are used """
+        """tests whether modifier are used"""
 
         rpu_config = InferenceRPUConfig(
             mapping=MappingParameter(
-                weight_scaling_omega=0.0,
-                learn_out_scaling=False,
-                weight_scaling_columnwise=False
-            ),
-            modifier=WeightModifierParameter(
-                type=WeightModifierType.ADD_NORMAL,
-                std_dev=1.0,
+                weight_scaling_omega=0.0, learn_out_scaling=False, weight_scaling_columnwise=False
             ),
-            forward=IOParameters(is_perfect=True)
+            modifier=WeightModifierParameter(type=WeightModifierType.ADD_NORMAL, std_dev=1.0),
+            forward=IOParameters(is_perfect=True),
         )
 
         model = convert_to_analog(torch_model, rpu_config)
 
         if self.use_cuda:
             x_b = x_b.cuda()
             model = model.cuda()
@@ -118,62 +127,57 @@
         opt.step()
         y_train2 = model(x_b)
 
         self.assertNotAlmostEqualTensor(y_train1, y_train2)
         self.assertTensorAlmostEqual(y_eval2, y_eval1)
 
     def base_test_drift_compensation(self, torch_model, x_b):
-        """ tests whether drift compensation is performed """
+        """tests whether drift compensation is performed"""
 
         rpu_config = InferenceRPUConfig(
             mapping=MappingParameter(
-                weight_scaling_omega=0.0,
-                learn_out_scaling=False,
-                weight_scaling_columnwise=False
+                weight_scaling_omega=0.0, learn_out_scaling=False, weight_scaling_columnwise=False
             ),
             forward=IOParameters(is_perfect=True),
             drift_compensation=GlobalDriftCompensation(),
             noise_model=StateIndependentNoiseModel(
-                prog_noise_scale=0.0,
-                read_noise_scale=0.0,
-                drift_nu_std=0.0,
-                drift_nu_mean=0.1,
-            )
+                prog_noise_scale=0.0, read_noise_scale=0.0, drift_nu_std=0.0, drift_nu_mean=0.1
+            ),
         )
 
         model = convert_to_analog(torch_model, rpu_config)
 
         rpu_config.drift_compensation = None
         model_without = convert_to_analog(torch_model, rpu_config)
 
         if self.use_cuda:
             x_b = x_b.cuda()
             model = model.cuda()
             model_without = model_without.cuda()
 
         model.eval()
         y_before = model(x_b)
-        model.drift_analog_weights(1000.)
+        model.drift_analog_weights(1000.0)
         y_after = model(x_b)
 
         model_without.eval()
         y_without_before = model_without(x_b)
-        model_without.drift_analog_weights(1000.)
+        model_without.drift_analog_weights(1000.0)
         y_without_after = model_without(x_b)
 
         self.assertTensorAlmostEqual(y_before, y_without_before)
         self.assertTensorAlmostEqual(y_before, y_after)
         self.assertNotAlmostEqualTensor(y_after, y_without_after)
         self.assertNotAlmostEqualTensor(y_without_before, y_without_after)
 
 
 @parametrize_over_layers(
     layers=[Conv1d, Conv1dCuda],
     tiles=[FloatingPoint, Inference],
-    biases=['analog', 'digital', None]
+    biases=["analog", "digital", None],
 )
 class Convolution1dLayerTest(ConvolutionLayerTest):
     """Tests for AnalogConv1d layer."""
 
     digital_layer_cls = torch_Conv1d
 
     def test_torch_original_layer(self):
@@ -218,20 +222,20 @@
         if self.bias:
             self.assertTensorAlmostEqual(bias_analog, bias)
 
     def test_torch_train_original_layer_multiple(self):
         """Test the backward pass, having the digital layer as reference."""
         model = Sequential(
             self.get_digital_layer(in_channels=2, out_channels=2, kernel_size=4, padding=2),
-            self.get_digital_layer(in_channels=2, out_channels=3, kernel_size=4, padding=2)
+            self.get_digital_layer(in_channels=2, out_channels=3, kernel_size=4, padding=2),
         )
 
         analog_model = Sequential(
             self.get_layer(in_channels=2, out_channels=2, kernel_size=4, padding=2),
-            self.get_layer(in_channels=2, out_channels=3, kernel_size=4, padding=2)
+            self.get_layer(in_channels=2, out_channels=3, kernel_size=4, padding=2),
         )
 
         for analog_layer, layer in zip(analog_model.children(), model.children()):
             self.set_weights_from_digital_model(analog_layer, layer)
 
         loss_func = mse_loss
         y_b = randn(3, 3, 6)
@@ -250,125 +254,134 @@
 
             self.assertTensorAlmostEqual(weight_analog, weight)
             if self.bias:
                 self.assertTensorAlmostEqual(bias_analog, bias)
 
     def test_out_scaling_learning(self):
         """Check if out scaling are learning."""
-        rpu_config = InferenceRPUConfig(mapping=MappingParameter(
-            learn_out_scaling=True,
-            out_scaling_columnwise=False))
+        rpu_config = InferenceRPUConfig(
+            mapping=MappingParameter(learn_out_scaling=True, out_scaling_columnwise=False)
+        )
 
         analog_model = Sequential(
-            self.get_layer(in_channels=2, out_channels=2, kernel_size=4,
-                           padding=2, rpu_config=rpu_config),
-            self.get_layer(in_channels=2, out_channels=3, kernel_size=4,
-                           padding=2, rpu_config=rpu_config)
+            self.get_layer(
+                in_channels=2, out_channels=2, kernel_size=4, padding=2, rpu_config=rpu_config
+            ),
+            self.get_layer(
+                in_channels=2, out_channels=3, kernel_size=4, padding=2, rpu_config=rpu_config
+            ),
         )
 
         loss_func = mse_loss
         y_b = randn(3, 3, 6)
         x_b = randn(3, 2, 4)
 
         if self.use_cuda:
             y_b = y_b.cuda()
             x_b = x_b.cuda()
 
-        initial_out_scaling_0 = analog_model[0].analog_tile.get_learned_out_scales().clone()
-        initial_out_scaling_1 = analog_model[1].analog_tile.get_learned_out_scales().clone()
+        analog_tile_0 = next(analog_model[0].analog_tiles())
+        analog_tile_1 = next(analog_model[1].analog_tiles())
+
+        initial_out_scaling_0 = analog_tile_0.get_learned_out_scales().clone()
+        initial_out_scaling_1 = analog_tile_1.get_learned_out_scales().clone()
         self.assertEqual(initial_out_scaling_0.numel(), 1)
         self.assertEqual(initial_out_scaling_1.numel(), 1)
 
         self.train_model(analog_model, loss_func, x_b, y_b)
 
-        learned_out_scaling_0 = analog_model[0].analog_tile.get_learned_out_scales().clone()
-        learned_out_scaling_1 = analog_model[1].analog_tile.get_learned_out_scales().clone()
+        learned_out_scaling_0 = analog_tile_0.get_learned_out_scales().clone()
+        learned_out_scaling_1 = analog_tile_1.get_learned_out_scales().clone()
 
-        self.assertIsNotNone(analog_model[0].analog_tile.get_learned_out_scales().grad)
+        self.assertIsNotNone(analog_tile_0.get_learned_out_scales().grad)
         self.assertNotAlmostEqualTensor(initial_out_scaling_0, learned_out_scaling_0)
-        self.assertIsNotNone(analog_model[1].analog_tile.get_learned_out_scales().grad)
+        self.assertIsNotNone(analog_tile_1.get_learned_out_scales().grad)
         self.assertNotAlmostEqualTensor(initial_out_scaling_1, learned_out_scaling_1)
 
     def test_out_scaling_learning_columnwise(self):
         """Check if out scaling alpha are learning."""
-        rpu_config = InferenceRPUConfig(mapping=MappingParameter(
-            weight_scaling_omega=0.6,
-            learn_out_scaling=True,
-            weight_scaling_columnwise=True))
+        rpu_config = InferenceRPUConfig(
+            mapping=MappingParameter(
+                weight_scaling_omega=0.6, learn_out_scaling=True, weight_scaling_columnwise=True
+            )
+        )
 
         analog_model = Sequential(
-            self.get_layer(in_channels=2, out_channels=2, kernel_size=4,
-                           padding=2, rpu_config=rpu_config),
-            self.get_layer(in_channels=2, out_channels=3, kernel_size=4,
-                           padding=2, rpu_config=rpu_config)
+            self.get_layer(
+                in_channels=2, out_channels=2, kernel_size=4, padding=2, rpu_config=rpu_config
+            ),
+            self.get_layer(
+                in_channels=2, out_channels=3, kernel_size=4, padding=2, rpu_config=rpu_config
+            ),
         )
 
         loss_func = mse_loss
         y_b = randn(3, 3, 6)
         x_b = randn(3, 2, 4)
 
         if self.use_cuda:
             y_b = y_b.cuda()
             x_b = x_b.cuda()
 
-        initial_out_scaling_0 = analog_model[0].analog_tile.get_learned_out_scales().clone()
-        initial_out_scaling_1 = analog_model[1].analog_tile.get_learned_out_scales().clone()
+        analog_tile_0 = next(analog_model[0].analog_tiles())
+        analog_tile_1 = next(analog_model[1].analog_tiles())
+
+        initial_out_scaling_0 = analog_tile_0.get_learned_out_scales().clone()
+        initial_out_scaling_1 = analog_tile_1.get_learned_out_scales().clone()
 
         self.train_model(analog_model, loss_func, x_b, y_b)
 
-        learned_out_scaling_0 = analog_model[0].analog_tile.get_learned_out_scales().clone()
-        learned_out_scaling_1 = analog_model[1].analog_tile.get_learned_out_scales().clone()
+        learned_out_scaling_0 = analog_tile_0.get_learned_out_scales().clone()
+        learned_out_scaling_1 = analog_tile_1.get_learned_out_scales().clone()
 
-        self.assertIsNotNone(analog_model[0].analog_tile.get_learned_out_scales().grad)
+        self.assertIsNotNone(analog_tile_0.get_learned_out_scales().grad)
         self.assertNotAlmostEqualTensor(initial_out_scaling_0, learned_out_scaling_0)
-        self.assertIsNotNone(analog_model[1].analog_tile.get_learned_out_scales().grad)
+        self.assertIsNotNone(analog_tile_1.get_learned_out_scales().grad)
         self.assertNotAlmostEqualTensor(initial_out_scaling_1, learned_out_scaling_1)
 
     def test_layer_instantiation(self):
         """Test AnalogConv2d layer instantiation."""
         model = self.get_layer(in_channels=2, out_channels=3, kernel_size=4)
 
         # Assert the number of elements of the weights.
-        tile_weights, tile_biases = model.analog_tile.get_weights()
+        tile_weights, tile_biases = model.get_weights()
 
-        self.assertEqual(tile_weights.numel(), 2*3*4)
-        if model.analog_bias:
+        self.assertEqual(tile_weights.numel(), 2 * 3 * 4)
+        if next(model.analog_tiles()).analog_bias:
             self.assertEqual(tile_biases.numel(), 3)
 
 
-@parametrize_over_layers(
-    layers=[Conv1d, Conv1dCuda],
-    tiles=[Inference],
-    biases=['digital']
-)
+@parametrize_over_layers(layers=[Conv1d, Conv1dCuda], tiles=[Inference], biases=["digital"])
 class Convolution1dLayerTestInference(ConvolutionLayerTest):
     """Tests for AnalogConv1d layer specific for inference."""
 
     digital_layer_cls = torch_Conv1d
 
     def test_drift_compensation(self):
-        """ tests the drift compensation """
+        """tests the drift compensation"""
 
         x_b = randn(3, 2, 4)
-        torch_model = self.get_digital_layer(in_channels=2, out_channels=2, kernel_size=4,
-                                             padding=2)
+        torch_model = self.get_digital_layer(
+            in_channels=2, out_channels=2, kernel_size=4, padding=2
+        )
         self.base_test_drift_compensation(torch_model, x_b)
 
     def test_inference_modifier(self):
-        """ tests the modifier function """
+        """tests the modifier function"""
         x_b = randn(3, 2, 4)
-        torch_model = self.get_digital_layer(in_channels=2, out_channels=2, kernel_size=4,
-                                             padding=2)
+        torch_model = self.get_digital_layer(
+            in_channels=2, out_channels=2, kernel_size=4, padding=2
+        )
         self.base_test_inference_modifier(torch_model, x_b)
 
 
 @parametrize_over_layers(
     layers=[Conv2d, Conv2dCuda],
-    tiles=[FloatingPoint, Inference],
-    biases=['analog', 'digital', None]
+    tiles=[FloatingPoint, Inference, TorchInference, Custom],
+    biases=["analog", "digital", None],
 )
 class Convolution2dLayerTest(ConvolutionLayerTest):
     """Tests for AnalogConv2d layer."""
 
     digital_layer_cls = torch_Conv2d
 
     def test_torch_original_layer(self):
@@ -387,14 +400,17 @@
 
         y_analog = analog_model(x)
         self.assertTensorAlmostEqual(y_analog, y)
 
     def test_torch_original_layer_indexed(self):
         """Test a single layer, having the digital layer as reference."""
         # This tests the forward pass
+        if not self.get_rpu_config().tile_class.supports_indexed:
+            raise SkipTest("Indexed not supported")
+
         model = self.get_digital_layer(in_channels=2, out_channels=3, kernel_size=4, padding=2)
         x = randn(3, 2, 4, 4)
 
         if self.use_cuda:
             x = x.cuda()
 
         y = model(x)
@@ -448,20 +464,20 @@
         if self.bias:
             self.assertTensorAlmostEqual(bias_analog, bias)
 
     def test_torch_train_original_layer_multiple(self):
         """Test the backward pass, having the digital layer as reference."""
         model = Sequential(
             self.get_digital_layer(in_channels=2, out_channels=2, kernel_size=4, padding=2),
-            self.get_digital_layer(in_channels=2, out_channels=3, kernel_size=4, padding=2)
+            self.get_digital_layer(in_channels=2, out_channels=3, kernel_size=4, padding=2),
         )
 
         analog_model = Sequential(
             self.get_layer(in_channels=2, out_channels=2, kernel_size=4, padding=2),
-            self.get_layer(in_channels=2, out_channels=3, kernel_size=4, padding=2)
+            self.get_layer(in_channels=2, out_channels=3, kernel_size=4, padding=2),
         )
 
         for analog_layer, layer in zip(analog_model.children(), model.children()):
             self.set_weights_from_digital_model(analog_layer, layer)
 
         loss_func = mse_loss
         y_b = randn(3, 3, 6, 6)
@@ -481,122 +497,133 @@
 
             self.assertTensorAlmostEqual(weight_analog, weight)
             if self.bias:
                 self.assertTensorAlmostEqual(bias_analog, bias)
 
     def test_out_scaling_learning(self):
         """Check if out scaling alpha are learning."""
-        rpu_config = InferenceRPUConfig(mapping=MappingParameter(
-            weight_scaling_omega=0.6,
-            learn_out_scaling=True))
+        rpu_config = InferenceRPUConfig(
+            mapping=MappingParameter(weight_scaling_omega=0.6, learn_out_scaling=True)
+        )
 
         analog_model = Sequential(
-            self.get_layer(in_channels=2, out_channels=2, kernel_size=4,
-                           padding=2, rpu_config=rpu_config),
-            self.get_layer(in_channels=2, out_channels=3, kernel_size=4,
-                           padding=2, rpu_config=rpu_config)
+            self.get_layer(
+                in_channels=2, out_channels=2, kernel_size=4, padding=2, rpu_config=rpu_config
+            ),
+            self.get_layer(
+                in_channels=2, out_channels=3, kernel_size=4, padding=2, rpu_config=rpu_config
+            ),
         )
 
         loss_func = mse_loss
         y_b = randn(3, 3, 6, 6)
         x_b = randn(3, 2, 4, 4)
 
         if self.use_cuda:
             y_b = y_b.cuda()
             x_b = x_b.cuda()
 
-        initial_out_scaling_0 = analog_model[0].analog_tile.get_learned_out_scales().clone()
-        initial_out_scaling_1 = analog_model[1].analog_tile.get_learned_out_scales().clone()
+        analog_tile_0 = next(analog_model[0].analog_tiles())
+        analog_tile_1 = next(analog_model[1].analog_tiles())
+
+        initial_out_scaling_0 = analog_tile_0.get_learned_out_scales().clone()
+        initial_out_scaling_1 = analog_tile_1.get_learned_out_scales().clone()
 
         self.train_model(analog_model, loss_func, x_b, y_b)
 
-        learned_out_scaling_0 = analog_model[0].analog_tile.get_learned_out_scales().clone()
-        learned_out_scaling_1 = analog_model[1].analog_tile.get_learned_out_scales().clone()
+        learned_out_scaling_0 = analog_tile_0.get_learned_out_scales().clone()
+        learned_out_scaling_1 = analog_tile_1.get_learned_out_scales().clone()
 
-        self.assertIsNotNone(analog_model[0].analog_tile.get_learned_out_scales().grad)
+        self.assertIsNotNone(analog_tile_0.get_learned_out_scales().grad)
         self.assertNotAlmostEqualTensor(initial_out_scaling_0, learned_out_scaling_0)
-        self.assertIsNotNone(analog_model[1].analog_tile.get_learned_out_scales().grad)
+        self.assertIsNotNone(analog_tile_1.get_learned_out_scales().grad)
         self.assertNotAlmostEqualTensor(initial_out_scaling_1, learned_out_scaling_1)
 
     def test_out_scaling_learning_columnwise(self):
         """Check if out scaling alpha are learning."""
-        rpu_config = InferenceRPUConfig(mapping=MappingParameter(
-            weight_scaling_omega=0.6,
-            learn_out_scaling=True,
-            weight_scaling_columnwise=True))
+        rpu_config = InferenceRPUConfig(
+            mapping=MappingParameter(
+                weight_scaling_omega=0.6, learn_out_scaling=True, weight_scaling_columnwise=True
+            )
+        )
 
         analog_model = Sequential(
-            self.get_layer(in_channels=2, out_channels=2, kernel_size=4,
-                           padding=2, rpu_config=rpu_config),
-            self.get_layer(in_channels=2, out_channels=3, kernel_size=4,
-                           padding=2, rpu_config=rpu_config)
+            self.get_layer(
+                in_channels=2, out_channels=2, kernel_size=4, padding=2, rpu_config=rpu_config
+            ),
+            self.get_layer(
+                in_channels=2, out_channels=3, kernel_size=4, padding=2, rpu_config=rpu_config
+            ),
         )
 
         loss_func = mse_loss
         y_b = randn(3, 3, 6, 6)
         x_b = randn(3, 2, 4, 4)
 
         if self.use_cuda:
             y_b = y_b.cuda()
             x_b = x_b.cuda()
 
-        initial_out_scaling_0 = analog_model[0].analog_tile.get_learned_out_scales().clone()
-        initial_out_scaling_1 = analog_model[1].analog_tile.get_learned_out_scales().clone()
+        analog_tile_0 = next(analog_model[0].analog_tiles())
+        analog_tile_1 = next(analog_model[1].analog_tiles())
+
+        initial_out_scaling_0 = analog_tile_0.get_learned_out_scales().clone()
+        initial_out_scaling_1 = analog_tile_1.get_learned_out_scales().clone()
 
         self.train_model(analog_model, loss_func, x_b, y_b)
 
-        learned_out_scaling_0 = analog_model[0].analog_tile.get_learned_out_scales().clone()
-        learned_out_scaling_1 = analog_model[1].analog_tile.get_learned_out_scales().clone()
+        learned_out_scaling_0 = analog_tile_0.get_learned_out_scales().clone()
+        learned_out_scaling_1 = analog_tile_1.get_learned_out_scales().clone()
 
-        self.assertIsNotNone(analog_model[0].analog_tile.get_learned_out_scales().grad)
+        self.assertIsNotNone(analog_tile_0.get_learned_out_scales().grad)
         self.assertNotAlmostEqualTensor(initial_out_scaling_0, learned_out_scaling_0)
-        self.assertIsNotNone(analog_model[1].analog_tile.get_learned_out_scales().grad)
+        self.assertIsNotNone(analog_tile_1.get_learned_out_scales().grad)
         self.assertNotAlmostEqualTensor(initial_out_scaling_1, learned_out_scaling_1)
 
     def test_layer_instantiation(self):
         """Test AnalogConv2d layer instantiation."""
         model = self.get_layer(in_channels=2, out_channels=3, kernel_size=4)
 
         # Assert the number of elements of the weights.
-        tile_weights, tile_biases = model.analog_tile.get_weights()
+        tile_weights, tile_biases = model.get_weights()
 
-        self.assertEqual(tile_weights.numel(), 2*3*4*4)
-        if model.analog_bias:
+        self.assertEqual(tile_weights.numel(), 2 * 3 * 4 * 4)
+        if next(model.analog_tiles()).analog_bias:
             self.assertEqual(tile_biases.numel(), 3)
 
 
 @parametrize_over_layers(
-    layers=[Conv2d, Conv2dCuda],
-    tiles=[Inference],
-    biases=['digital']
+    layers=[Conv2d, Conv2dCuda], tiles=[Inference, TorchInference], biases=["digital"]
 )
 class Convolution2dLayerTestInference(ConvolutionLayerTest):
     """Tests for AnalogConv2d layer specific for infernence."""
 
     digital_layer_cls = torch_Conv2d
 
     def test_drift_compensation(self):
-        """ tests the drift compensation """
+        """tests the drift compensation"""
         x_b = randn(3, 2, 4, 4)
-        torch_model = self.get_digital_layer(in_channels=2, out_channels=2, kernel_size=4,
-                                             padding=2)
+        torch_model = self.get_digital_layer(
+            in_channels=2, out_channels=2, kernel_size=4, padding=2
+        )
         self.base_test_drift_compensation(torch_model, x_b)
 
     def test_inference_modifier(self):
-        """ tests the modifier function """
+        """tests the modifier function"""
         x_b = randn(3, 2, 4, 4)
-        torch_model = self.get_digital_layer(in_channels=2, out_channels=2, kernel_size=4,
-                                             padding=2)
+        torch_model = self.get_digital_layer(
+            in_channels=2, out_channels=2, kernel_size=4, padding=2
+        )
         self.base_test_inference_modifier(torch_model, x_b)
 
 
 @parametrize_over_layers(
     layers=[Conv3d, Conv3dCuda],
     tiles=[FloatingPoint, Inference],
-    biases=['analog', 'digital', None]
+    biases=["analog", "digital", None],
 )
 class Convolution3dLayerTest(ConvolutionLayerTest):
     """Tests for AnalogConv3d layer."""
 
     digital_layer_cls = torch_Conv3d
 
     def test_torch_original_layer(self):
@@ -640,20 +667,20 @@
         if self.bias:
             self.assertTensorAlmostEqual(bias_analog, bias)
 
     def test_torch_train_original_layer_multiple(self):
         """Test the backward pass, having the digital layer as reference."""
         model = Sequential(
             self.get_digital_layer(in_channels=2, out_channels=2, kernel_size=4, padding=2),
-            self.get_digital_layer(in_channels=2, out_channels=3, kernel_size=4, padding=2)
+            self.get_digital_layer(in_channels=2, out_channels=3, kernel_size=4, padding=2),
         )
 
         analog_model = Sequential(
             self.get_layer(in_channels=2, out_channels=2, kernel_size=4, padding=2),
-            self.get_layer(in_channels=2, out_channels=3, kernel_size=4, padding=2)
+            self.get_layer(in_channels=2, out_channels=3, kernel_size=4, padding=2),
         )
 
         for analog_layer, layer in zip(analog_model.children(), model.children()):
             self.set_weights_from_digital_model(analog_layer, layer)
 
         loss_func = mse_loss
         y_b = randn(3, 3, 6, 6, 6)
@@ -672,114 +699,122 @@
 
             self.assertTensorAlmostEqual(weight_analog, weight)
             if self.bias:
                 self.assertTensorAlmostEqual(bias_analog, bias)
 
     def test_out_scaling_learning(self):
         """Check if out scaling alpha are learning."""
-        rpu_config = InferenceRPUConfig(mapping=MappingParameter(
-            learn_out_scaling=True,
-            out_scaling_columnwise=False))
+        rpu_config = InferenceRPUConfig(
+            mapping=MappingParameter(learn_out_scaling=True, out_scaling_columnwise=False)
+        )
 
         analog_model = Sequential(
-            self.get_layer(in_channels=2, out_channels=2, kernel_size=4,
-                           padding=2, rpu_config=rpu_config),
-            self.get_layer(in_channels=2, out_channels=3, kernel_size=4,
-                           padding=2, rpu_config=rpu_config)
+            self.get_layer(
+                in_channels=2, out_channels=2, kernel_size=4, padding=2, rpu_config=rpu_config
+            ),
+            self.get_layer(
+                in_channels=2, out_channels=3, kernel_size=4, padding=2, rpu_config=rpu_config
+            ),
         )
 
         loss_func = mse_loss
         y_b = randn(3, 3, 6, 6, 6)
         x_b = randn(3, 2, 4, 4, 4)
 
         if self.use_cuda:
             y_b = y_b.cuda()
             x_b = x_b.cuda()
 
-        initial_out_scaling_0 = analog_model[0].analog_tile.get_learned_out_scales().clone()
-        initial_out_scaling_1 = analog_model[1].analog_tile.get_learned_out_scales().clone()
+        analog_tile_0 = next(analog_model[0].analog_tiles())
+        analog_tile_1 = next(analog_model[1].analog_tiles())
+
+        initial_out_scaling_0 = analog_tile_0.get_learned_out_scales().clone()
+        initial_out_scaling_1 = analog_tile_1.get_learned_out_scales().clone()
 
         self.train_model(analog_model, loss_func, x_b, y_b)
 
-        learned_out_scaling_0 = analog_model[0].analog_tile.get_learned_out_scales().clone()
-        learned_out_scaling_1 = analog_model[1].analog_tile.get_learned_out_scales().clone()
+        learned_out_scaling_0 = analog_tile_0.get_learned_out_scales().clone()
+        learned_out_scaling_1 = analog_tile_1.get_learned_out_scales().clone()
 
         self.assertEqual(initial_out_scaling_0.numel(), 1)
-        self.assertIsNotNone(analog_model[0].analog_tile.get_learned_out_scales().grad)
+        self.assertIsNotNone(analog_tile_0.get_learned_out_scales().grad)
         self.assertNotAlmostEqualTensor(initial_out_scaling_0, learned_out_scaling_0)
 
         self.assertEqual(initial_out_scaling_1.numel(), 1)
-        self.assertIsNotNone(analog_model[1].analog_tile.get_learned_out_scales().grad)
+        self.assertIsNotNone(analog_tile_1.get_learned_out_scales().grad)
         self.assertNotAlmostEqualTensor(initial_out_scaling_1, learned_out_scaling_1)
 
     def test_out_scaling_learning_columnwise(self):
         """Check if out scaling alpha are learning."""
-        rpu_config = InferenceRPUConfig(mapping=MappingParameter(
-            learn_out_scaling=True,
-            out_scaling_columnwise=True))
+        rpu_config = InferenceRPUConfig(
+            mapping=MappingParameter(learn_out_scaling=True, out_scaling_columnwise=True)
+        )
 
         analog_model = Sequential(
-            self.get_layer(in_channels=2, out_channels=2, kernel_size=4,
-                           padding=2, rpu_config=rpu_config),
-            self.get_layer(in_channels=2, out_channels=3, kernel_size=4,
-                           padding=2, rpu_config=rpu_config)
+            self.get_layer(
+                in_channels=2, out_channels=2, kernel_size=4, padding=2, rpu_config=rpu_config
+            ),
+            self.get_layer(
+                in_channels=2, out_channels=3, kernel_size=4, padding=2, rpu_config=rpu_config
+            ),
         )
 
         loss_func = mse_loss
         y_b = randn(3, 3, 6, 6, 6)
         x_b = randn(3, 2, 4, 4, 4)
 
         if self.use_cuda:
             y_b = y_b.cuda()
             x_b = x_b.cuda()
 
-        initial_out_scaling_0 = analog_model[0].analog_tile.get_learned_out_scales().clone()
-        initial_out_scaling_1 = analog_model[1].analog_tile.get_learned_out_scales().clone()
+        analog_tile_0 = next(analog_model[0].analog_tiles())
+        analog_tile_1 = next(analog_model[1].analog_tiles())
+
+        initial_out_scaling_0 = analog_tile_0.get_learned_out_scales().clone()
+        initial_out_scaling_1 = analog_tile_1.get_learned_out_scales().clone()
 
         self.train_model(analog_model, loss_func, x_b, y_b)
 
-        learned_out_scaling_0 = analog_model[0].analog_tile.get_learned_out_scales().clone()
-        learned_out_scaling_1 = analog_model[1].analog_tile.get_learned_out_scales().clone()
+        learned_out_scaling_0 = analog_tile_0.get_learned_out_scales().clone()
+        learned_out_scaling_1 = analog_tile_1.get_learned_out_scales().clone()
 
         self.assertGreaterEqual(initial_out_scaling_0.numel(), 1)
-        self.assertIsNotNone(analog_model[0].analog_tile.get_learned_out_scales().grad)
+        self.assertIsNotNone(analog_tile_0.get_learned_out_scales().grad)
         self.assertNotAlmostEqualTensor(initial_out_scaling_0, learned_out_scaling_0)
 
         self.assertGreaterEqual(initial_out_scaling_1.numel(), 1)
-        self.assertIsNotNone(analog_model[1].analog_tile.get_learned_out_scales().grad)
+        self.assertIsNotNone(analog_tile_1.get_learned_out_scales().grad)
         self.assertNotAlmostEqualTensor(initial_out_scaling_1, learned_out_scaling_1)
 
     def test_layer_instantiation(self):
         """Test AnalogConv2d layer instantiation."""
         model = self.get_layer(in_channels=2, out_channels=3, kernel_size=4)
 
         # Assert the number of elements of the weights.
-        tile_weights, tile_biases = model.analog_tile.get_weights()
+        tile_weights, tile_biases = model.get_weights()
 
-        self.assertEqual(tile_weights.numel(), 2*3*4*4*4)
-        if model.analog_bias:
+        self.assertEqual(tile_weights.numel(), 2 * 3 * 4 * 4 * 4)
+        if next(model.analog_tiles()).analog_bias:
             self.assertEqual(tile_biases.numel(), 3)
 
 
-@parametrize_over_layers(
-    layers=[Conv3d, Conv3dCuda],
-    tiles=[Inference],
-    biases=['digital']
-)
+@parametrize_over_layers(layers=[Conv3d, Conv3dCuda], tiles=[Inference], biases=["digital"])
 class Convolution3dLayerTestInference(ConvolutionLayerTest):
     """Tests for AnalogConv2d layer specific for infernence."""
 
     digital_layer_cls = torch_Conv3d
 
     def test_drift_compensation(self):
-        """ tests the drift compensation """
+        """tests the drift compensation"""
         x_b = randn(3, 2, 4, 4, 4)
-        torch_model = self.get_digital_layer(in_channels=2, out_channels=2, kernel_size=4,
-                                             padding=2)
+        torch_model = self.get_digital_layer(
+            in_channels=2, out_channels=2, kernel_size=4, padding=2
+        )
         self.base_test_drift_compensation(torch_model, x_b)
 
     def test_inference_modifier(self):
-        """ tests the modifier function """
+        """tests the modifier function"""
         x_b = randn(3, 2, 4, 4, 4)
-        torch_model = self.get_digital_layer(in_channels=2, out_channels=2, kernel_size=4,
-                                             padding=2)
+        torch_model = self.get_digital_layer(
+            in_channels=2, out_channels=2, kernel_size=4, padding=2
+        )
         self.base_test_inference_modifier(torch_model, x_b)
```

### Comparing `aihwkit-0.7.1/tests/test_layers_linear.py` & `aihwkit-0.8.0/tests/test_layers_linear.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,58 +1,62 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
 # that they have been altered from the originals.
 
 """Tests for linear layer."""
 
-from numpy.testing import assert_array_almost_equal
+from unittest import SkipTest
 
+from numpy.testing import assert_array_almost_equal
 from torch import Tensor, manual_seed
 from torch.nn import Sequential, Linear as torchLinear
 from torch.nn.functional import mse_loss
 from torch.optim import SGD
 
 from aihwkit.optim import AnalogSGD
 from aihwkit.simulator.configs.configs import InferenceRPUConfig, FloatingPointRPUConfig
-from aihwkit.simulator.configs.utils import (
-    MappingParameter, WeightModifierType, WeightModifierParameter, IOParameters,
-    WeightRemapType
+from aihwkit.simulator.parameters.utils import (
+    MappingParameter,
+    WeightModifierType,
+    WeightModifierParameter,
+    IOParameters,
+    WeightRemapType,
 )
 from aihwkit.inference.compensation.drift import GlobalDriftCompensation
 from aihwkit.inference.noise.custom import StateIndependentNoiseModel
 
 
 from aihwkit.nn import AnalogSequential, AnalogLinear
 
 from .helpers.decorators import parametrize_over_layers
 from .helpers.layers import Linear, LinearCuda
 from .helpers.testcases import ParametrizedTestCase
-from .helpers.tiles import FloatingPoint, IdealizedConstantStep, Inference
+from .helpers.tiles import FloatingPoint, IdealizedConstantStep, Inference, TorchInference, Custom
 
 
 @parametrize_over_layers(
     layers=[Linear, LinearCuda],
-    tiles=[FloatingPoint, IdealizedConstantStep, Inference],
-    biases=['analog', 'digital', None]
+    tiles=[FloatingPoint, IdealizedConstantStep, Inference, TorchInference, Custom],
+    biases=["analog", "digital", None],
 )
 class LinearLayerTest(ParametrizedTestCase):
     """Linear layer abstractions tests."""
 
     @staticmethod
     def train_model(model, loss_func, x_b, y_b, **kwargs):
         """Train the model."""
-        opt = AnalogSGD(model.parameters(), lr=0.5,  **kwargs)
+        opt = AnalogSGD(model.parameters(), lr=0.5, **kwargs)
         opt.regroup_param_groups(model)
 
         epochs = 100
         for _ in range(epochs):
             opt.zero_grad()
             pred = model(x_b)
             loss = loss_func(pred, y_b)
@@ -129,71 +133,62 @@
         """Check using a several analog layers."""
         loss_func = mse_loss
 
         x_b = Tensor([[0.1, 0.2, 0.3, 0.4], [0.2, 0.4, 0.3, 0.1]])
         y_b = Tensor([[0.3], [0.6]])
 
         manual_seed(4321)
-        model = Sequential(
-            self.get_layer(4, 2),
-            self.get_layer(2, 1)
-        )
+        model = Sequential(self.get_layer(4, 2), self.get_layer(2, 1))
         if self.use_cuda:
             x_b = x_b.cuda()
             y_b = y_b.cuda()
-            model = model.cuda()
+            model.cuda()
 
         initial_loss = loss_func(model(x_b), y_b)
         self.train_model(model, loss_func, x_b, y_b)
         self.assertLess(loss_func(model(x_b), y_b), initial_loss)
 
     def test_analog_and_digital(self):
         """Check mixing analog and digital layers."""
         loss_func = mse_loss
 
         x_b = Tensor([[0.1, 0.2, 0.3, 0.4], [0.2, 0.4, 0.3, 0.1]])
         y_b = Tensor([[0.3], [0.6]])
 
         manual_seed(4321)
-        model = Sequential(
-            self.get_layer(4, 3),
-            torchLinear(3, 3),
-            self.get_layer(3, 1)
-        )
+        model = Sequential(self.get_layer(4, 3), torchLinear(3, 3), self.get_layer(3, 1))
         if self.use_cuda:
             x_b = x_b.cuda()
             y_b = y_b.cuda()
-            model = model.cuda()
+            model.cuda()
 
         initial_loss = loss_func(model(x_b), y_b)
         self.train_model(model, loss_func, x_b, y_b)
         self.assertLess(loss_func(model(x_b), y_b), initial_loss)
 
     def test_analog_torch_optimizer(self):
         """Check analog layers with torch SGD for inference."""
         loss_func = mse_loss
 
         x_b = Tensor([[0.1, 0.2, 0.3, 0.4], [0.2, 0.4, 0.3, 0.1]])
         y_b = Tensor([[0.3], [0.6]])
 
         manual_seed(4321)
-        model = Sequential(
-            self.get_layer(4, 3),
-            self.get_layer(3, 1),
-        )
-        if not isinstance(model[0].analog_tile.rpu_config, InferenceRPUConfig):
+        model = Sequential(self.get_layer(4, 3), self.get_layer(3, 1))
+        if not isinstance(model[0].analog_module.rpu_config, InferenceRPUConfig):
             return
 
-        manual_seed(4321)
         rpu_config = FloatingPointRPUConfig(
             mapping=MappingParameter(digital_bias=self.digital_bias)
         )
+
+        manual_seed(4321)
         model2 = AnalogSequential(
             AnalogLinear(4, 3, rpu_config=rpu_config, bias=self.bias),
-            AnalogLinear(3, 1, rpu_config=rpu_config, bias=self.bias)
+            AnalogLinear(3, 1, rpu_config=rpu_config, bias=self.bias),
         )
 
         if self.use_cuda:
             x_b = x_b.cuda()
             y_b = y_b.cuda()
             model = model.cuda()
             model2 = model2.cuda()
@@ -229,127 +224,129 @@
             model = model.cuda()
         opt = AnalogSGD(model.parameters(), lr=0.5)
         opt.regroup_param_groups(model)
         opt.zero_grad()
 
         new_lr = 0.07
         for param_group in opt.param_groups:
-            param_group['lr'] = new_lr
+            param_group["lr"] = new_lr
 
         pred = model(x_b)
         loss = loss_func(pred, y_b)
         loss.backward()
         opt.step()
 
-        if not layer1.analog_tile.get_analog_ctx().use_torch_update:
-            self.assertAlmostEqual(layer1.analog_tile.get_learning_rate(), new_lr)
+        if not layer1.analog_module.get_analog_ctx().use_torch_update:
+            self.assertAlmostEqual(layer1.analog_module.get_learning_rate(), new_lr)
 
     def test_learning_rate_update_fn(self):
         """Check the learning rate update is applied to tile."""
+
         layer1 = self.get_layer(2, 3)
         layer2 = self.get_layer(3, 1)
 
         model = Sequential(layer1, layer2)
         if self.use_cuda:
             model = model.cuda()
         opt = AnalogSGD(model.parameters(), lr=0.5)
         opt.regroup_param_groups(model)
         opt.zero_grad()
 
         new_lr = 0.07
 
         opt.set_learning_rate(new_lr)
 
-        self.assertAlmostEqual(layer1.analog_tile.get_learning_rate(), new_lr)
-        self.assertAlmostEqual(layer2.analog_tile.get_learning_rate(), new_lr)
+        if layer1.analog_module.analog_ctx.use_torch_update:
+            raise SkipTest("Not supported")
+
+        self.assertAlmostEqual(layer1.analog_module.get_learning_rate(), new_lr)
+        self.assertAlmostEqual(layer2.analog_module.get_learning_rate(), new_lr)
 
     def test_out_scaling_learning(self):
         """Check if out scales are learning."""
         loss_func = mse_loss
 
         x_b = Tensor([[0.1, 0.2, 0.3, 0.4], [0.2, 0.4, 0.3, 0.1]])
         y_b = Tensor([[0.3], [0.6]])
 
         manual_seed(4321)
 
-        rpu_config = InferenceRPUConfig(mapping=MappingParameter(
-            weight_scaling_omega=0.6,
-            learn_out_scaling=True,
-            out_scaling_columnwise=False))
+        rpu_config = InferenceRPUConfig(
+            mapping=MappingParameter(
+                weight_scaling_omega=0.6, learn_out_scaling=True, out_scaling_columnwise=False
+            )
+        )
 
         model = Sequential(
-            self.get_layer(4, 2, rpu_config=rpu_config),
-            self.get_layer(2, 1, rpu_config=rpu_config)
+            self.get_layer(4, 2, rpu_config=rpu_config), self.get_layer(2, 1, rpu_config=rpu_config)
         )
         if self.use_cuda:
             x_b = x_b.cuda()
             y_b = y_b.cuda()
             model = model.cuda()
 
-        initial_out_scaling_0 = model[0].analog_tile.get_learned_out_scales().clone()
-        initial_out_scaling_1 = model[1].analog_tile.get_learned_out_scales().clone()
+        initial_out_scaling_0 = model[0].analog_module.get_learned_out_scales().clone()
+        initial_out_scaling_1 = model[1].analog_module.get_learned_out_scales().clone()
 
         self.train_model(model, loss_func, x_b, y_b)
 
-        learned_out_scaling_0 = model[0].analog_tile.get_learned_out_scales().data.clone()
-        learned_out_scaling_1 = model[1].analog_tile.get_learned_out_scales().data.clone()
+        learned_out_scaling_0 = model[0].analog_module.get_learned_out_scales().data.clone()
+        learned_out_scaling_1 = model[1].analog_module.get_learned_out_scales().data.clone()
 
         self.assertEqual(initial_out_scaling_0.numel(), 1)
-        self.assertIsNotNone(model[0].analog_tile.get_learned_out_scales().grad)
+        self.assertIsNotNone(model[0].analog_module.get_learned_out_scales().grad)
         self.assertNotAlmostEqualTensor(initial_out_scaling_0, learned_out_scaling_0)
 
         self.assertEqual(initial_out_scaling_0.numel(), 1)
-        self.assertIsNotNone(model[1].analog_tile.get_learned_out_scales().grad)
+        self.assertIsNotNone(model[1].analog_module.get_learned_out_scales().grad)
         self.assertNotAlmostEqualTensor(initial_out_scaling_1, learned_out_scaling_1)
         self.assertEqual(initial_out_scaling_0.numel(), 1)
 
     def test_out_scaling_learning_columnwise(self):
         """Check if out scaling alpha are learning when columnwise is True."""
         loss_func = mse_loss
 
         x_b = Tensor([[0.1, 0.2, 0.3, 0.4], [0.2, 0.4, 0.3, 0.1]])
         y_b = Tensor([[0.3], [0.6]])
 
         manual_seed(4321)
 
-        rpu_config = InferenceRPUConfig(mapping=MappingParameter(
-            weight_scaling_omega=0.6,
-            learn_out_scaling=True,
-            weight_scaling_columnwise=True))
+        rpu_config = InferenceRPUConfig(
+            mapping=MappingParameter(
+                weight_scaling_omega=0.6, learn_out_scaling=True, weight_scaling_columnwise=True
+            )
+        )
 
         model = Sequential(
-            self.get_layer(4, 2, rpu_config=rpu_config),
-            self.get_layer(2, 1, rpu_config=rpu_config)
+            self.get_layer(4, 2, rpu_config=rpu_config), self.get_layer(2, 1, rpu_config=rpu_config)
         )
         if self.use_cuda:
             x_b = x_b.cuda()
             y_b = y_b.cuda()
             model = model.cuda()
 
-        initial_out_scaling_0 = model[0].analog_tile.get_learned_out_scales().clone()
-        initial_out_scaling_1 = model[1].analog_tile.get_learned_out_scales().clone()
+        initial_out_scaling_0 = model[0].analog_module.get_learned_out_scales().clone()
+        initial_out_scaling_1 = model[1].analog_module.get_learned_out_scales().clone()
 
         self.train_model(model, loss_func, x_b, y_b)
 
-        learned_out_scaling_0 = model[0].analog_tile.get_learned_out_scales().clone()
-        learned_out_scaling_1 = model[1].analog_tile.get_learned_out_scales().clone()
+        learned_out_scaling_0 = model[0].analog_module.get_learned_out_scales().clone()
+        learned_out_scaling_1 = model[1].analog_module.get_learned_out_scales().clone()
 
         self.assertGreaterEqual(initial_out_scaling_0.numel(), 1)
-        self.assertIsNotNone(model[0].analog_tile.get_learned_out_scales().grad)
+        self.assertIsNotNone(model[0].analog_module.get_learned_out_scales().grad)
         self.assertNotAlmostEqualTensor(initial_out_scaling_0, learned_out_scaling_0)
 
         self.assertGreaterEqual(initial_out_scaling_1.numel(), 1)
-        self.assertIsNotNone(model[1].analog_tile.get_learned_out_scales().grad)
+        self.assertIsNotNone(model[1].analog_module.get_learned_out_scales().grad)
         self.assertNotAlmostEqualTensor(initial_out_scaling_1, learned_out_scaling_1)
 
 
 @parametrize_over_layers(
-    layers=[Linear, LinearCuda],
-    tiles=[Inference],
-    biases=['digital']
+    layers=[Linear, LinearCuda], tiles=[Inference, TorchInference], biases=["digital"]
 )
 class LinearLayerInferenceTest(ParametrizedTestCase):
     """Linear layer abstractions tests for inference."""
 
     def test_remapping_learning(self):
         """Check analog layers with torch SGD for inference."""
         loss_func = mse_loss
@@ -363,75 +360,67 @@
         rpu_config.forward.is_perfect = True
 
         rpu_config.mapping.learn_out_scaling = False
         rpu_config.mapping.weight_scaling_omega = 1.0
         rpu_config.mapping.weight_scaling_columnwise = False
 
         analog_model = Sequential(
-            self.get_layer(4, 3, rpu_config=rpu_config),
-            self.get_layer(3, 1, rpu_config=rpu_config),
+            self.get_layer(4, 3, rpu_config=rpu_config), self.get_layer(3, 1, rpu_config=rpu_config)
         )
 
         manual_seed(4321)
         torch_model = Sequential(
-            torchLinear(4, 3, bias=self.bias),
-            torchLinear(3, 1, bias=self.bias)
+            torchLinear(4, 3, bias=self.bias), torchLinear(3, 1, bias=self.bias)
         )
 
         if self.use_cuda:
             x_b = x_b.cuda()
             y_b = y_b.cuda()
             analog_model = analog_model.cuda()
             torch_model = torch_model.cuda()
 
         initial_loss = loss_func(analog_model(x_b), y_b).detach().cpu().numpy()
 
         # train analog model with AnalogSGD
         LinearLayerTest.train_model(analog_model, loss_func, x_b, y_b)
-        self.assertLess(loss_func(analog_model(x_b), y_b).detach().cpu().numpy(),
-                        initial_loss)
+        self.assertLess(loss_func(analog_model(x_b), y_b).detach().cpu().numpy(), initial_loss)
 
         # check remapping
         tile_weights = analog_model[0].get_weights(apply_weight_scaling=False)[0]
         self.assertAlmostEqual(tile_weights.abs().flatten().max().item(), 1.0)
 
         # train torch model with SGD
         initial_loss2 = loss_func(torch_model(x_b), y_b).detach().cpu().numpy()
         LinearLayerTest.train_model_torch(torch_model, loss_func, x_b, y_b)
-        self.assertLess(loss_func(torch_model(x_b), y_b).detach().cpu().numpy(),
-                        initial_loss)
+        self.assertLess(loss_func(torch_model(x_b), y_b).detach().cpu().numpy(), initial_loss)
 
         # should be same
         final_loss = loss_func(analog_model(x_b), y_b).detach().cpu().numpy()
         final_loss2 = loss_func(torch_model(x_b), y_b).detach().cpu().numpy()
 
         assert_array_almost_equal(initial_loss, initial_loss2)
         assert_array_almost_equal(final_loss, final_loss2)
 
     def test_inference_modifier(self):
-        """ tests whether modifier are used """
+        """tests whether modifier are used"""
 
         x_b = Tensor([[0.1, 0.2, 0.3, 0.4], [0.2, 0.4, 0.3, 0.1]])
 
-        rpu_config = InferenceRPUConfig(
-            mapping=MappingParameter(
-                weight_scaling_omega=0.0,
-                learn_out_scaling=False,
-                weight_scaling_columnwise=False
-            ),
-            modifier=WeightModifierParameter(
-                type=WeightModifierType.ADD_NORMAL,
-                std_dev=1.0,
-            ),
-            forward=IOParameters(is_perfect=True)
+        rpu_config = self.get_rpu_config()
+
+        rpu_config.mapping.weight_scaling_omega = 0.0
+        rpu_config.mapping.learn_out_scaling = False
+        rpu_config.mapping.weight_scaling_columnwise = False
+        rpu_config.modifier = WeightModifierParameter(
+            type=WeightModifierType.ADD_NORMAL, std_dev=1.0
         )
+        rpu_config.forward = IOParameters(is_perfect=True)
 
         model = AnalogSequential(
-            self.get_layer(4, 2, rpu_config=rpu_config),
-            self.get_layer(2, 1, rpu_config=rpu_config)
+            self.get_layer(4, 2, rpu_config=rpu_config), self.get_layer(2, 1, rpu_config=rpu_config)
         )
         if self.use_cuda:
             x_b = x_b.cuda()
             model = model.cuda()
 
         opt = AnalogSGD(model.parameters(), lr=0.0)
         opt.step()
@@ -448,59 +437,52 @@
         opt.step()
         y_train2 = model(x_b)
 
         self.assertNotAlmostEqualTensor(y_train1, y_train2)
         self.assertTensorAlmostEqual(y_eval2, y_eval1)
 
     def test_drift_compensation(self):
-        """ tests whether drift compensation is performed """
+        """tests whether drift compensation is performed"""
 
         x_b = Tensor([[0.1, 0.2, 0.3, 0.4], [0.2, 0.4, 0.3, 0.1]])
 
-        rpu_config = InferenceRPUConfig(
-            mapping=MappingParameter(
-                weight_scaling_omega=0.0,
-                learn_out_scaling=False,
-                weight_scaling_columnwise=False
-            ),
-            forward=IOParameters(is_perfect=True),
-            drift_compensation=GlobalDriftCompensation(),
-            noise_model=StateIndependentNoiseModel(
-                prog_noise_scale=0.0,
-                read_noise_scale=0.0,
-                drift_nu_std=0.0,
-                drift_nu_mean=0.1,
-            )
+        rpu_config = self.get_rpu_config()
+
+        rpu_config.mapping.weight_scaling_omega = 0.0
+        rpu_config.mapping.learn_out_scaling = False
+        rpu_config.mapping.weight_scaling_columnwise = False
+        rpu_config.forward = IOParameters(is_perfect=True)
+        rpu_config.drift_compensation = GlobalDriftCompensation()
+        rpu_config.noise_model = StateIndependentNoiseModel(
+            prog_noise_scale=0.0, read_noise_scale=0.0, drift_nu_std=0.0, drift_nu_mean=0.1
         )
 
         model = AnalogSequential(
-            self.get_layer(4, 2, rpu_config=rpu_config),
-            self.get_layer(2, 1, rpu_config=rpu_config)
+            self.get_layer(4, 2, rpu_config=rpu_config), self.get_layer(2, 1, rpu_config=rpu_config)
         )
 
         rpu_config.drift_compensation = None
         model_without = AnalogSequential(
-            self.get_layer(4, 2, rpu_config=rpu_config),
-            self.get_layer(2, 1, rpu_config=rpu_config)
+            self.get_layer(4, 2, rpu_config=rpu_config), self.get_layer(2, 1, rpu_config=rpu_config)
         )
 
         model_without.load_state_dict(model.state_dict(), load_rpu_config=False)
 
         if self.use_cuda:
             x_b = x_b.cuda()
-            model = model.cuda()
-            model_without = model_without.cuda()
+            model_without.cuda()
+            model.cuda()
 
         model.eval()
         y_before = model(x_b)
-        model.drift_analog_weights(1000.)
+        model.drift_analog_weights(1000.0)
         y_after = model(x_b)
 
         model_without.eval()
         y_without_before = model_without(x_b)
-        model_without.drift_analog_weights(1000.)
+        model_without.drift_analog_weights(1000.0)
         y_without_after = model_without(x_b)
 
         self.assertTensorAlmostEqual(y_before, y_without_before)
         self.assertTensorAlmostEqual(y_before, y_after)
         self.assertNotAlmostEqualTensor(y_after, y_without_after)
         self.assertNotAlmostEqualTensor(y_without_before, y_without_after)
```

### Comparing `aihwkit-0.7.1/tests/test_layers_mapped.py` & `aihwkit-0.8.0/tests/test_layers_mapped.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -21,66 +21,68 @@
 
 # Imports from PyTorch.
 from torch import randn, load, save, manual_seed
 from torch.nn.functional import mse_loss
 
 # Imports from aihwkit.
 from aihwkit.nn import (
-    AnalogLinearMapped, AnalogConv1dMapped,
-    AnalogConv2dMapped,  AnalogConv3dMapped
+    AnalogLinearMapped,
+    AnalogConv1dMapped,
+    AnalogConv2dMapped,
+    AnalogConv3dMapped,
 )
 from aihwkit.optim import AnalogSGD
 
 from .helpers.decorators import parametrize_over_layers
 from .helpers.testcases import ParametrizedTestCase
 from .helpers.tiles import FloatingPoint, Inference, Ideal
-from .helpers.layers import (
-    Linear, LinearCuda, Conv1d, Conv1dCuda, Conv2d,
-    Conv2dCuda, Conv3dCuda
-)
+from .helpers.layers import Linear, LinearCuda, Conv1d, Conv1dCuda, Conv2d, Conv2dCuda, Conv3dCuda
 
 DECIMAL = 4
 
 
 @parametrize_over_layers(
     layers=[Linear, LinearCuda, Conv1d, Conv2d, Conv1dCuda, Conv2dCuda, Conv3dCuda],
     tiles=[FloatingPoint, Inference, Ideal],
-    biases=['digital', None])
+    biases=["digital", None],
+)
 class MappedLayerLinearTest(ParametrizedTestCase):
-    """Tests for the AnalogMappedLayer functionality """
+    """Tests for the AnalogMappedLayer functionality"""
 
     def get_mapped_class(self, model):
         """Returns the mapped class"""
         name = model.__class__.__name__
-        if name == 'AnalogLinear':
+        if name == "AnalogLinear":
             return AnalogLinearMapped
-        if name == 'AnalogConv1d':
+        if name == "AnalogConv1d":
             return AnalogConv1dMapped
-        if name == 'AnalogConv2d':
+        if name == "AnalogConv2d":
             return AnalogConv2dMapped
-        if name == 'AnalogConv3d':
+        if name == "AnalogConv3d":
             return AnalogConv3dMapped
 
         raise RuntimeError("Cannot find mapped module.")
 
     def get_mapped_model(self, model, rpu_config):
         """Returns the mapped model"""
-        mapped_model = self.get_mapped_class(model).from_digital(
-            model, rpu_config=rpu_config)
+        weight, bias = model.weight, model.bias
+        model.weight, model.bias = model.get_weights()
+        mapped_model = self.get_mapped_class(model).from_digital(model, rpu_config=rpu_config)
         mapped_model.reset_parameters()  # should set it explicitly below
+        model.weight, model.bias = weight, bias
         return mapped_model
 
     def get_image_size(self, model):
         """Returns the image size"""
-        if not hasattr(model, 'kernel_size'):
+        if not hasattr(model, "kernel_size"):
             return []
         return (array(model.kernel_size) * 2).tolist()
 
     def train_model(self, model, in_vectors, out_vectors):
-        """Trains a model """
+        """Trains a model"""
 
         opt = AnalogSGD(model.parameters(), lr=0.1)
 
         for _ in range(10):
             opt.zero_grad()
 
             # Add the training Tensor to the model (input).
@@ -90,15 +92,15 @@
             # Run training (backward propagation).
             loss_value.backward()
             opt.step()
 
         return loss_value
 
     def test_construction(self):
-        """ Test construction of a mapped layer"""
+        """Test construction of a mapped layer"""
         manual_seed(123)
         in_features = 14
         out_features = 5
 
         rpu_config = self.get_rpu_config()
         rpu_config.mapping.max_input_size = 10
         rpu_config.mapping.max_output_size = 4
@@ -120,78 +122,80 @@
         mapped_weights, mapped_bias = mapped_model.get_weights()
 
         self.assertTensorAlmostEqual(weight, mapped_weights, decimal=DECIMAL)
         if self.bias:
             self.assertTensorAlmostEqual(bias, mapped_bias, decimal=DECIMAL)
 
     def test_training(self):
-        """ Test of training of a mapped linear layer"""
+        """Test of training of a mapped linear layer"""
         manual_seed(123)
 
         in_features = 12
         out_features = 13
         batch_size = 10
 
         rpu_config = self.get_rpu_config()
-        rpu_config.mapping.max_input_size = 10
-        rpu_config.mapping.max_output_size = 6
 
         model = self.get_layer(in_features, out_features, rpu_config=rpu_config)
         weight, bias = model.get_weights()
 
         weight = randn(*weight.shape)
         if self.bias:
             bias = randn(*bias.shape)
         model.set_weights(weight, bias)
 
+        rpu_config.mapping.max_input_size = 10
+        rpu_config.mapping.max_output_size = 6
+
         mapped_model = self.get_mapped_model(model, rpu_config)
         mapped_model.set_weights(weight, bias)
 
         in_vectors = randn(*([batch_size, in_features] + self.get_image_size(model)))
 
         if self.use_cuda:
             in_vectors = in_vectors.cuda()
             mapped_model = mapped_model.cuda()
 
         out_vectors = randn(*model(in_vectors).shape)
         if self.use_cuda:
             out_vectors = out_vectors.cuda()
 
-        # compare predictions for analog linear and analog spit linear layers
+        # compare predictions for analog linear and analog split linear layers
         self.assertTensorAlmostEqual(model(in_vectors), mapped_model(in_vectors), decimal=DECIMAL)
 
         # Define an analog-aware optimizer, preparing it for using the layers.
         loss = self.train_model(model, in_vectors, out_vectors)
         mapped_loss = self.train_model(mapped_model, in_vectors, out_vectors)
 
         self.assertTensorAlmostEqual(loss, mapped_loss, decimal=DECIMAL)
 
         # Make sure that the train model produces the same forward pass
         self.assertTensorAlmostEqual(model(in_vectors), mapped_model(in_vectors), decimal=DECIMAL)
 
     def test_training_after_save(self):
-        """ Test training after it was saved """
+        """Test training after it was saved"""
         manual_seed(123)
 
         in_features = 11
         out_features = 11
         batch_size = 10
 
         rpu_config = self.get_rpu_config()
-        rpu_config.mapping.max_input_size = 10
-        rpu_config.mapping.max_output_size = 4
 
         model = self.get_layer(in_features, out_features, rpu_config=rpu_config)
         weight, bias = model.get_weights()
 
         weight = randn(*weight.shape)
         if self.bias:
             bias = randn(*bias.shape)
         model.set_weights(weight, bias)
 
+        rpu_config.mapping.max_input_size = 10
+        rpu_config.mapping.max_output_size = 4
+
         mapped_model = self.get_mapped_model(model, rpu_config)
         mapped_model.set_weights(weight, bias)
 
         in_vectors = randn(*([batch_size, in_features] + self.get_image_size(model)))
         if self.use_cuda:
             in_vectors = in_vectors.cuda()
             mapped_model = mapped_model.cuda()
@@ -211,19 +215,18 @@
             new_model.load_state_dict(load(file))
 
         new_weight, new_bias = new_model.get_weights()
 
         self.assertTensorAlmostEqual(new_weight, weight, decimal=DECIMAL)
         if self.bias:
             self.assertTensorAlmostEqual(new_bias, bias, decimal=DECIMAL)
-            self.assertTensorAlmostEqual(model.bias, bias, decimal=DECIMAL)
-            self.assertTensorAlmostEqual(mapped_model.bias, bias, decimal=DECIMAL)
 
-        for new_tile, tile in zip(list(new_model.analog_tiles()),
-                                  list(mapped_model.analog_tiles())):
+        for new_tile, tile in zip(
+            list(new_model.analog_tiles()), list(mapped_model.analog_tiles())
+        ):
             new_tile_weight, _ = new_tile.get_weights()
             tile_weight, _ = tile.get_weights()
             self.assertTensorAlmostEqual(tile_weight, new_tile_weight, decimal=DECIMAL)
 
         # compare predictions for analog linear and analog spit linear layers
         self.assertTensorAlmostEqual(model(in_vectors), new_model(in_vectors), decimal=DECIMAL)
```

### Comparing `aihwkit-0.7.1/tests/test_layers_rnn.py` & `aihwkit-0.8.0/tests/test_layers_rnn.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -15,43 +15,51 @@
 from torch.nn import MSELoss
 from numpy.testing import assert_array_almost_equal, assert_raises
 
 from aihwkit.optim import AnalogSGD
 from aihwkit.optim.context import AnalogContext
 
 from .helpers.decorators import parametrize_over_layers
-from .helpers.layers import LSTM, LSTMCuda, GRU, GRUCuda, VanillaRNN, VanillaRNNCuda, \
-                            LSTMCombinedWeight, LSTMCombinedWeightCuda
+from .helpers.layers import (
+    LSTM,
+    LSTMCuda,
+    GRU,
+    GRUCuda,
+    VanillaRNN,
+    VanillaRNNCuda,
+    LSTMCombinedWeight,
+    LSTMCombinedWeightCuda,
+)
 from .helpers.testcases import ParametrizedTestCase
-from .helpers.tiles import FloatingPoint, Inference
+from .helpers.tiles import FloatingPoint, Inference, TorchInference, Custom
 
 
 @parametrize_over_layers(
     layers=[LSTM, VanillaRNN, GRU, LSTMCuda, GRUCuda, VanillaRNNCuda],
-    tiles=[FloatingPoint, Inference],
-    biases=['analog', 'digital', None]
+    tiles=[FloatingPoint, Inference, TorchInference, Custom],
+    biases=["analog", "digital", None],
 )
 class RNNLayerTest(ParametrizedTestCase):
-    """ Base test for RNNs"""
+    """Base test for RNNs"""
 
     @staticmethod
     def train_once(model, y_in, y_out, analog_if, lr=0.5, digital_bias_lr_scale=1.0):
         """Train once."""
 
         criterion = MSELoss()
         optimizer = AnalogSGD(model.parameters(), lr=lr, momentum=0.0, nesterov=0.0)
         optimizer.regroup_param_groups()
         batch_size = y_in.size()[1]
 
         if analog_if:
             for param_group in optimizer.param_groups:
-                if isinstance(param_group['params'][0], AnalogContext):
-                    param_group['lr'] = lr
+                if isinstance(param_group["params"][0], AnalogContext):
+                    param_group["lr"] = lr
                 else:
-                    param_group['lr'] = lr * digital_bias_lr_scale
+                    param_group["lr"] = lr * digital_bias_lr_scale
             states = model.get_zero_state(batch_size)
 
         else:
             states = None
 
         for _ in range(2):
             optimizer.zero_grad()
@@ -85,58 +93,57 @@
 
     def test_layer_instantiation(self):
         """Test AnalogLSTM layer instantiation."""
         input_size = 2
         hidden_size = 3
         num_layers = 4
 
-        model = self.get_layer(input_size=input_size,
-                               hidden_size=hidden_size,
-                               num_layers=num_layers)
+        model = self.get_layer(
+            input_size=input_size, hidden_size=hidden_size, num_layers=num_layers
+        )
 
         # Assert over the stacked layers.
         self.assertEqual(len(model.rnn.layers), num_layers)
         for i, layer in enumerate(model.rnn.layers):
             # Assert over the size of weight_ih.
             if i == 0:
                 self.assertEqual(layer.cell.weight_ih.in_features, input_size)
             else:
                 self.assertEqual(layer.cell.weight_ih.in_features, hidden_size)
             self.assertEqual(layer.cell.weight_hh.in_features, hidden_size)
             # Assert over the rpu_config.
-            analog_tile_ih = list(layer.cell.weight_ih.analog_tiles())[0]
-            analog_tile_hh = list(layer.cell.weight_hh.analog_tiles())[0]
-            self.assertEqual(analog_tile_ih.rpu_config.__class__,
-                             self.get_rpu_config().__class__)
-            self.assertEqual(analog_tile_hh.rpu_config.__class__,
-                             self.get_rpu_config().__class__)
+            analog_tile_ih = next(layer.cell.weight_ih.analog_tiles())
+            analog_tile_hh = next(layer.cell.weight_hh.analog_tiles())
+            self.assertEqual(analog_tile_ih.rpu_config.__class__, self.get_rpu_config().__class__)
+            self.assertEqual(analog_tile_hh.rpu_config.__class__, self.get_rpu_config().__class__)
 
     def get_native_layer_comparison(self, *args, **kwargs):
-        """ Returns the torch native model """
+        """Returns the torch native model"""
         raise NotImplementedError
 
     def test_layer_training(self):
         """Test AnalogLSTM layer training."""
+
         # pylint: disable=too-many-locals, too-many-statements
         def get_parameters(model, analog_if) -> dict:
             """Returns the parameter in an dict."""
 
             dic = {}
             for name, param in model.named_parameters():
                 if isinstance(param, AnalogContext):
                     weight, bias = param.analog_tile.get_weights()
-                    splits = name.split('.')
-                    add_on = '_' + splits[-2].split('_')[-1] + '_l' + splits[2]
-                    dic['weight' + add_on] = weight
+                    splits = name.split(".")
+                    add_on = "_" + splits[-3].split("_")[-1] + "_l" + splits[2]
+                    dic["weight" + add_on] = weight
                     if bias is not None:
-                        dic['bias' + add_on] = bias
-                elif analog_if and name.endswith('bias'):  # digital bias
-                    splits = name.split('.')
-                    add_on = '_' + splits[-2].split('_')[-1] + '_l' + splits[2]
-                    dic['bias' + add_on] = param
+                        dic["bias" + add_on] = bias
+                elif analog_if and name.endswith("bias"):  # digital bias
+                    splits = name.split(".")
+                    add_on = "_" + splits[-3].split("_")[-1] + "_l" + splits[2]
+                    dic["bias" + add_on] = param
                 else:
                     dic[name] = param
 
             return dic
 
         input_size = 2
         hidden_size = 2
@@ -145,31 +152,27 @@
         batch_size = 3
         test_for_update = False  # For debugging. Does test whether all weights are updated.
 
         # Make dataset (just random).
         y_in = randn(seq_length, batch_size, input_size)
         y_out = ones(seq_length, batch_size, 1)
 
-        rnn_analog = self.get_layer(input_size=input_size,
-                                    hidden_size=hidden_size,
-                                    num_layers=num_layers,
-                                    realistic_read_write=False,
-                                    dropout=0.0)
-
-        rnn = self.get_native_layer_comparison(input_size=input_size,
-                                               hidden_size=hidden_size,
-                                               num_layers=num_layers,
-                                               dropout=0.0,
-                                               bias=self.bias)
-
-        weights_org = []
-
-        # pylint: disable=protected-access
-        rnn_analog._apply_to_analog(lambda lay: weights_org.append(
-            lay.analog_tile.tile.get_weights()))
+        rnn_analog = self.get_layer(
+            input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=0.0
+        )
+
+        rnn = self.get_native_layer_comparison(
+            input_size=input_size,
+            hidden_size=hidden_size,
+            num_layers=num_layers,
+            dropout=0.0,
+            bias=self.bias,
+        )
+
+        weights_org = rnn_analog.get_weights()
 
         rnn_pars0 = get_parameters(rnn, False)
         rnn_analog_pars0 = get_parameters(rnn_analog, True)
 
         for par_name, par_item in rnn_pars0.items():
             par_item.data = rnn_analog_pars0[par_name].detach().clone()
 
@@ -179,70 +182,67 @@
         if self.use_cuda:
             y_in = y_in.cuda()
             y_out = y_out.cuda()
             rnn_analog.cuda()
             rnn.cuda()
 
         with no_grad():
-            assert_array_almost_equal(rnn(y_in)[0].detach().clone().cpu(),
-                                      rnn_analog(y_in)[0].detach().clone().cpu())
+            self.assertTensorAlmostEqual(rnn(y_in)[0], rnn_analog(y_in)[0])
 
         # First train analog and make sure weights differ.
         pred_analog = self.train_once(rnn_analog, y_in, y_out, True)
 
-        analog_weights = []
-        rnn_analog._apply_to_analog(lambda lay: analog_weights.append(
-            lay.analog_tile.tile.get_weights()))
+        analog_weights = rnn_analog.get_weights()
 
         if test_for_update:
-            for weight, weight_org in zip(analog_weights, weights_org):
-                assert_raises(AssertionError, assert_array_almost_equal, weight, weight_org)
+            for weight, weight_org in zip(analog_weights.values(), weights_org.values()):
+                assert_raises(AssertionError, assert_array_almost_equal, weight[0], weight_org[0])
 
         # Compare with RNN.
         pred = self.train_once(rnn, y_in, y_out, False)
         assert_array_almost_equal(pred, pred_analog)
 
-        rnn_analog._apply_to_analog(lambda lay: lay._sync_weights_from_tile())
-
         rnn_pars = get_parameters(rnn, False)
         rnn_analog_pars = get_parameters(rnn_analog, True)
 
         if test_for_update:
             for par_name, par_item in rnn_pars.items():
                 par0 = rnn_pars0[par_name].detach().cpu().numpy()
                 par = par_item.detach().cpu().numpy()
                 assert_raises(AssertionError, assert_array_almost_equal, par, par0)
 
         for par_name, par_item in rnn_pars.items():
-            assert_array_almost_equal(par_item.detach().cpu().numpy(),
-                                      rnn_analog_pars[par_name].detach().cpu().numpy())
+            assert_array_almost_equal(
+                par_item.detach().cpu().numpy(), rnn_analog_pars[par_name].detach().cpu().numpy()
+            )
 
     def test_bidir_layer_training(self):
         """Test AnalogLSTM bidirectional layer training."""
+
         # pylint: disable=too-many-locals, too-many-statements
         def get_parameters(model, analog_if) -> dict:
             """Returns the parameter in an dict."""
             dic = {}
             for name, param in model.named_parameters():
                 if isinstance(param, AnalogContext):
                     weight, bias = param.analog_tile.get_weights()
-                    splits = name.split('.')
-                    add_on = '_' + splits[-2].split('_')[-1] + '_l' + splits[2]
-                    if splits[4] == '1':
-                        add_on += '_reverse'
+                    splits = name.split(".")
+                    add_on = "_" + splits[-3].split("_")[-1] + "_l" + splits[2]
+                    if splits[4] == "1":
+                        add_on += "_reverse"
 
-                    dic['weight' + add_on] = weight
+                    dic["weight" + add_on] = weight
                     if bias is not None:
-                        dic['bias' + add_on] = bias
-                elif analog_if and name.endswith('bias'):  # digital bias
-                    splits = name.split('.')
-                    add_on = '_' + splits[-2].split('_')[-1] + '_l' + splits[2]
-                    if splits[4] == '1':
-                        add_on += '_reverse'
-                    dic['bias' + add_on] = param
+                        dic["bias" + add_on] = bias
+                elif analog_if and name.endswith("bias"):  # digital bias
+                    splits = name.split(".")
+                    add_on = "_" + splits[-3].split("_")[-1] + "_l" + splits[2]
+                    if splits[4] == "1":
+                        add_on += "_reverse"
+                    dic["bias" + add_on] = param
                 else:
                     dic[name] = param
 
             return dic
 
         input_size = 4
         hidden_size = 5
@@ -251,33 +251,32 @@
         batch_size = 3
         test_for_update = False  # For debugging. Does test whether all weights are updated.
 
         # Make dataset (just random).
         y_in = randn(seq_length, batch_size, input_size)
         y_out = ones(seq_length, batch_size, 1)
 
-        rnn_analog = self.get_layer(input_size=input_size,
-                                    hidden_size=hidden_size,
-                                    num_layers=num_layers,
-                                    realistic_read_write=False,
-                                    dropout=0.0,
-                                    bidir=True)
-
-        rnn = self.get_native_layer_comparison(input_size=input_size,
-                                               hidden_size=hidden_size,
-                                               num_layers=num_layers,
-                                               dropout=0.0,
-                                               bias=self.bias,
-                                               bidirectional=True)
-
-        weights_org = []
-
-        # pylint: disable=protected-access
-        rnn_analog._apply_to_analog(lambda lay: weights_org.append(
-            lay.analog_tile.tile.get_weights()))
+        rnn_analog = self.get_layer(
+            input_size=input_size,
+            hidden_size=hidden_size,
+            num_layers=num_layers,
+            dropout=0.0,
+            bidir=True,
+        )
+
+        rnn = self.get_native_layer_comparison(
+            input_size=input_size,
+            hidden_size=hidden_size,
+            num_layers=num_layers,
+            dropout=0.0,
+            bias=self.bias,
+            bidirectional=True,
+        )
+
+        weights_org = rnn_analog.get_weights()
 
         rnn_pars0 = get_parameters(rnn, False)
         rnn_analog_pars0 = get_parameters(rnn_analog, True)
 
         for par_name, par_item in rnn_pars0.items():
             par_item.data = rnn_analog_pars0[par_name].detach().clone()
 
@@ -287,111 +286,107 @@
         if self.use_cuda:
             y_in = y_in.cuda()
             y_out = y_out.cuda()
             rnn_analog.cuda()
             rnn.cuda()
 
         with no_grad():
-            assert_array_almost_equal(rnn(y_in)[0].detach().clone().cpu(),
-                                      rnn_analog(y_in)[0].detach().clone().cpu())
+            self.assertTensorAlmostEqual(rnn(y_in)[0], rnn_analog(y_in)[0])
 
         # First train analog and make sure weights differ.
         pred_analog = self.train_once_bidir(rnn_analog, y_in, y_out, True)
 
-        analog_weights = []
-        rnn_analog._apply_to_analog(lambda lay: analog_weights.append(
-            lay.analog_tile.tile.get_weights()))
+        analog_weights = rnn_analog.get_weights()
 
         if test_for_update:
-            for weight, weight_org in zip(analog_weights, weights_org):
-                assert_raises(AssertionError, assert_array_almost_equal, weight, weight_org)
+            for weight, weight_org in zip(analog_weights.values(), weights_org.values()):
+                self.assertNotAlmostEqualTensor(weight[0], weight_org[0])
 
         # Compare with RNN.
         pred = self.train_once_bidir(rnn, y_in, y_out, False)
         assert_array_almost_equal(pred, pred_analog)
 
-        rnn_analog._apply_to_analog(lambda lay: lay._sync_weights_from_tile())
-
         rnn_pars = get_parameters(rnn, False)
         rnn_analog_pars = get_parameters(rnn_analog, True)
 
         if test_for_update:
             for par_name, par_item in rnn_pars.items():
                 par0 = rnn_pars0[par_name].detach().cpu().numpy()
                 par = par_item.detach().cpu().numpy()
                 assert_raises(AssertionError, assert_array_almost_equal, par, par0)
 
         for par_name, par_item in rnn_pars.items():
-            assert_array_almost_equal(par_item.detach().cpu().numpy(),
-                                      rnn_analog_pars[par_name].detach().cpu().numpy())
+            assert_array_almost_equal(
+                par_item.detach().cpu().numpy(), rnn_analog_pars[par_name].detach().cpu().numpy()
+            )
 
 
 @parametrize_over_layers(
     layers=[LSTMCombinedWeight, LSTMCombinedWeightCuda],
     tiles=[FloatingPoint],
-    biases=['digital', None]
+    biases=["digital", None],
 )
 class LSTMCombinedWeightTest(RNNLayerTest):
-    """ Base test for AnalogLSTMCombinedWeight"""
+    """Base test for AnalogLSTMCombinedWeight"""
 
     def test_layer_instantiation(self):
         """Test AnalogLSTM layer instantiation."""
         input_size = 2
         hidden_size = 3
         num_layers = 4
 
-        model = self.get_layer(input_size=input_size,
-                               hidden_size=hidden_size,
-                               num_layers=num_layers)
+        model = self.get_layer(
+            input_size=input_size, hidden_size=hidden_size, num_layers=num_layers
+        )
 
         # Assert over the stacked layers.
         self.assertEqual(len(model.rnn.layers), num_layers)
         for i, layer in enumerate(model.rnn.layers):
             # Assert over the size of weight_ih.
             if i == 0:
-                self.assertEqual(layer.cell.weight.in_features, input_size+hidden_size)
+                self.assertEqual(layer.cell.weight.in_features, input_size + hidden_size)
             else:
                 self.assertEqual(layer.cell.weight.in_features, 2 * hidden_size)
             self.assertEqual(layer.cell.weight.out_features, 4 * hidden_size)
             # Assert over the rpu_config.
             analog_tile = list(layer.cell.weight.analog_tiles())[0]
-            self.assertEqual(analog_tile.rpu_config.__class__,
-                             self.get_rpu_config().__class__)
+            self.assertEqual(analog_tile.rpu_config.__class__, self.get_rpu_config().__class__)
 
     def get_native_layer_comparison(self, *args, **kwargs):
-        """ Returns the torch native model """
+        """Returns the torch native model"""
         raise NotImplementedError
 
     def test_layer_training(self):
         """Test AnalogLSTM layer training."""
+
         # pylint: disable=too-many-locals, too-many-statements
         def get_parameters(model, analog_if) -> dict:
             """Returns the parameter in an dict."""
 
             dic = {}
             for name, param in model.named_parameters():
                 if isinstance(param, AnalogContext):
                     weight, bias = param.analog_tile.get_weights()
-                    lay = int(name.split('.')[2])
-                    add_on = '_l' + str(lay)
+                    lay = int(name.split(".")[2])
+                    add_on = "_l" + str(lay)
                     if lay == 0:
                         in_dim = input_size
                     else:
                         in_dim = hidden_size
-                    dic['weight_ih' + add_on] = weight[..., :in_dim]
-                    dic['weight_hh' + add_on] = weight[..., in_dim:]
+                    dic["weight_ih" + add_on] = weight[..., :in_dim]
+                    dic["weight_hh" + add_on] = weight[..., in_dim:]
 
                     if bias is not None:
-                        dic['bias_ih' + add_on] = bias
-                        dic['bias_hh' + add_on] = 0.0 * bias
+                        dic["bias_ih" + add_on] = bias
+                        dic["bias_hh" + add_on] = 0.0 * bias
 
-                elif analog_if and name.endswith('bias'):  # digital bias
-                    add_on = '_l' + name.split('.')[2]
-                    dic['bias_ih' + add_on] = param
-                    dic['bias_hh' + add_on] = 0.0 * param
+                elif analog_if and name.endswith("bias"):  # digital bias
+                    add_on = "_l" + name.split(".")[2]
+                    dic["bias_ih" + add_on] = param
+                    dic["bias_hh" + add_on] = 0.0 * param
                 else:
                     dic[name] = param
 
             return dic
 
         input_size = 5
         hidden_size = 3
@@ -400,57 +395,53 @@
         batch_size = 4
         test_for_update = True  # For debugging. Does test whether all weights are updated.
 
         # Make dataset (just random).
         y_in = randn(seq_length, batch_size, input_size)
         y_out = ones(seq_length, batch_size, 1)
 
-        rnn_analog = self.get_layer(input_size=input_size,
-                                    hidden_size=hidden_size,
-                                    num_layers=num_layers,
-                                    realistic_read_write=False,
-                                    dropout=0.0)
-
-        rnn = self.get_native_layer_comparison(input_size=input_size,
-                                               hidden_size=hidden_size,
-                                               num_layers=num_layers,
-                                               dropout=0.0,
-                                               bias=self.bias)
+        rnn_analog = self.get_layer(
+            input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=0.0
+        )
+
+        rnn = self.get_native_layer_comparison(
+            input_size=input_size,
+            hidden_size=hidden_size,
+            num_layers=num_layers,
+            dropout=0.0,
+            bias=self.bias,
+        )
 
         rnn_pars0 = get_parameters(rnn, False)
         rnn_analog_pars0 = get_parameters(rnn_analog, True)
 
         for par_name, par_item in rnn_pars0.items():
             par_item.data = rnn_analog_pars0[par_name].detach().clone()
 
         if test_for_update:
-            weights_org = []
-            # pylint: disable=protected-access
-            rnn_analog._apply_to_analog(lambda lay: weights_org.append(
-                lay.get_weights()))
+            weights_org = rnn_analog.get_weights()
 
         if self.use_cuda:
             y_in = y_in.cuda()
             y_out = y_out.cuda()
             rnn_analog.cuda()
             rnn.cuda()
 
         with no_grad():
-            assert_array_almost_equal(rnn(y_in)[0].detach().clone().cpu(),
-                                      rnn_analog(y_in)[0].detach().clone().cpu())
+            assert_array_almost_equal(
+                rnn(y_in)[0].detach().clone().cpu(), rnn_analog(y_in)[0].detach().clone().cpu()
+            )
 
         # First train analog and make sure weights differ.
         # since there is only one bias the LR of the bias is changed
         pred_analog = self.train_once(rnn_analog, y_in, y_out, True, digital_bias_lr_scale=2.0)
 
         if test_for_update:
-            analog_weights = []
-            # pylint: disable=protected-access
-            rnn_analog._apply_to_analog(lambda lay: analog_weights.append(
-                lay.get_weights()))
-            for weight, weight_org in zip(analog_weights, weights_org):
-                assert_raises(AssertionError, assert_array_almost_equal, weight[0], weight_org[0])
+            analog_weights = rnn_analog.get_weights()
+
+            for weight, weight_org in zip(analog_weights.values(), weights_org.values()):
+                self.assertNotAlmostEqualTensor(weight[0], weight_org[0])
 
         # Compare with RNN.
         pred = self.train_once(rnn, y_in, y_out, False)
 
         assert_array_almost_equal(pred, pred_analog)
```

### Comparing `aihwkit-0.7.1/tests/test_optimizers.py` & `aihwkit-0.8.0/tests/test_optimizers.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -47,15 +47,15 @@
         # Assert that a new subclass is created, with both the wrapped
         # optimizer and the AnalogInferenceOptimizer as parents.
         self.assertIsInstance(optimizer, AnalogOptimizer)
         self.assertIsInstance(optimizer, SGD)
         self.assertIsNot(type(optimizer), AnalogOptimizer)
 
         # Assert over specific wrapped optimizer parameters.
-        self.assertEqual(optimizer.defaults['lr'], 0.123)
+        self.assertEqual(optimizer.defaults["lr"], 0.123)
 
     def test_train_digital_sgd(self):
         """Test training digital layer with analog optimizer (SGD)."""
         loss_func = mse_loss
 
         x_b = Tensor([[0.1, 0.2, 0.3, 0.1], [0.2, 0.4, 0.3, 0.6]])
         y_b = Tensor([[0.3], [0.6]])
```

### Comparing `aihwkit-0.7.1/tests/test_rpu_configurations.py` & `aihwkit-0.8.0/tests/test_rpu_configurations.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,97 +1,113 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
 # that they have been altered from the originals.
 
 """Tests for the high level simulator devices functionality."""
 from sys import version_info
 from unittest import SkipTest
 
 from aihwkit.exceptions import ConfigError
-from aihwkit.simulator.configs.utils import (
-    IOParameters, UpdateParameters
-)
+from aihwkit.simulator.parameters.utils import IOParameters, UpdateParameters
 
 from .helpers.decorators import parametrize_over_tiles
 from .helpers.testcases import ParametrizedTestCase
 from .helpers.tiles import (
-    FloatingPoint, Ideal, ConstantStep, LinearStep,
-    ExpStep, SoftBounds, SoftBoundsPmax, PowStep, PiecewiseStep, Vector, OneSided,
-    Transfer, MixedPrecision,
-    FloatingPointCuda, IdealCuda, ConstantStepCuda, LinearStepCuda,
-    ExpStepCuda, SoftBoundsCuda, SoftBoundsPmaxCuda, PowStepCuda, PiecewiseStepCuda,
-    VectorCuda, OneSidedCuda, TransferCuda, MixedPrecisionCuda
-
-)
-
-
-@parametrize_over_tiles([
     FloatingPoint,
-    FloatingPointCuda
-])
-class RPUConfigurationsFloatingPointTest(ParametrizedTestCase):
-    """Tests related to resistive processing unit configurations (floating point)."""
-
-    def test_create_array(self):
-        """Test creating an array using the mappings to bindings."""
-        rpu_config = self.get_rpu_config()
-
-        tile_params = rpu_config.device.as_bindings()
-
-        _ = tile_params.create_array(10, 20)
-
-    def test_config_device_parameters(self):
-        """Test modifying the device parameters."""
-        rpu_config = self.get_rpu_config()
-
-        rpu_config.device.diffusion = 1.23
-        rpu_config.device.lifetime = 4.56
-
-        tile = self.get_tile(11, 22, rpu_config).tile
-
-        # Assert over the parameters in the binding objects.
-        parameters = tile.get_parameters()
-        self.assertAlmostEqual(parameters.diffusion, 1.23, places=4)
-        self.assertAlmostEqual(parameters.lifetime, 4.56, places=4)
-
-
-@parametrize_over_tiles([
     Ideal,
     ConstantStep,
     LinearStep,
     ExpStep,
     SoftBounds,
     SoftBoundsPmax,
     PowStep,
     PiecewiseStep,
     Vector,
     OneSided,
     Transfer,
     MixedPrecision,
+    FloatingPointCuda,
     IdealCuda,
     ConstantStepCuda,
     LinearStepCuda,
     ExpStepCuda,
     SoftBoundsCuda,
     SoftBoundsPmaxCuda,
     PowStepCuda,
     PiecewiseStepCuda,
     VectorCuda,
     OneSidedCuda,
     TransferCuda,
     MixedPrecisionCuda,
-])
+)
+
+
+@parametrize_over_tiles([FloatingPoint, FloatingPointCuda])
+class RPUConfigurationsFloatingPointTest(ParametrizedTestCase):
+    """Tests related to resistive processing unit configurations (floating point)."""
+
+    def test_create_array(self):
+        """Test creating an array using the mappings to bindings."""
+        rpu_config = self.get_rpu_config()
+
+        tile_params = rpu_config.device.as_bindings()
+
+        _ = tile_params.create_array(10, 20)
+
+    def test_config_device_parameters(self):
+        """Test modifying the device parameters."""
+        rpu_config = self.get_rpu_config()
+
+        rpu_config.device.diffusion = 1.23
+        rpu_config.device.lifetime = 4.56
+
+        tile = self.get_tile(11, 22, rpu_config).tile
+
+        # Assert over the parameters in the binding objects.
+        config = tile.get_meta_parameters()
+        self.assertAlmostEqual(config.diffusion, 1.23, places=4)
+        self.assertAlmostEqual(config.lifetime, 4.56, places=4)
+
+
+@parametrize_over_tiles(
+    [
+        Ideal,
+        ConstantStep,
+        LinearStep,
+        ExpStep,
+        SoftBounds,
+        SoftBoundsPmax,
+        PowStep,
+        PiecewiseStep,
+        Vector,
+        OneSided,
+        Transfer,
+        MixedPrecision,
+        IdealCuda,
+        ConstantStepCuda,
+        LinearStepCuda,
+        ExpStepCuda,
+        SoftBoundsCuda,
+        SoftBoundsPmaxCuda,
+        PowStepCuda,
+        PiecewiseStepCuda,
+        VectorCuda,
+        OneSidedCuda,
+        TransferCuda,
+        MixedPrecisionCuda,
+    ]
+)
 class RPUConfigurationsTest(ParametrizedTestCase):
     """Tests related to resistive processing unit configurations."""
 
     def test_create_array(self):
         """Test creating an array using the mappings to bindings."""
         rpu_config = self.get_rpu_config()
 
@@ -115,33 +131,33 @@
         """Test modifying the device parameters."""
         rpu_config = self.get_rpu_config()
 
         rpu_config.device.diffusion = 1.23
         rpu_config.device.lifetime = 4.56
         rpu_config.device.construction_seed = 192
 
-        # TODO: don't assert over tile.get_parameters() as some of them might
+        # TODO: don't assert over tile.get_meta_parameters() as some of them might
         # not be present.
         _ = self.get_tile(11, 22, rpu_config).tile
 
     def test_config_tile_parameters(self):
         """Test modifying the tile parameters."""
         rpu_config = self.get_rpu_config()
 
         rpu_config.forward = IOParameters(inp_noise=0.321)
         rpu_config.backward = IOParameters(inp_noise=0.456)
         rpu_config.update = UpdateParameters(desired_bl=78)
 
         tile = self.get_tile(11, 22, rpu_config).tile
 
         # Assert over the parameters in the binding objects.
-        parameters = tile.get_parameters()
-        self.assertAlmostEqual(parameters.forward_io.inp_noise, 0.321)
-        self.assertAlmostEqual(parameters.backward_io.inp_noise, 0.456)
-        self.assertAlmostEqual(parameters.update.desired_bl, 78)
+        config = tile.get_meta_parameters()
+        self.assertAlmostEqual(config.forward_io.inp_noise, 0.321)
+        self.assertAlmostEqual(config.backward_io.inp_noise, 0.456)
+        self.assertAlmostEqual(config.update.desired_bl, 78)
 
     def test_construction_seed(self):
         """Test the construction seed leads to the same tile values."""
         rpu_config = self.get_rpu_config()
 
         # Set the seed.
         rpu_config.device.construction_seed = 10
@@ -149,14 +165,12 @@
         tile_1 = self.get_tile(3, 4, rpu_config)
         tile_2 = self.get_tile(3, 4, rpu_config)
 
         hidden_parameters_1 = tile_1.get_hidden_parameters()
         hidden_parameters_2 = tile_2.get_hidden_parameters()
 
         # Compare old and new hidden parameters tensors.
-        for (field, old), (_, new) in zip(hidden_parameters_1.items(),
-                                          hidden_parameters_2.items()):
-
-            if 'weights' in field:
+        for (field, old), (_, new) in zip(hidden_parameters_1.items(), hidden_parameters_2.items()):
+            if "weights" in field:
                 # exclude weights as these are not governed by construction seed
                 continue
             self.assertTrue(old.allclose(new))
```

### Comparing `aihwkit-0.7.1/tests/test_simulator_tiles.py` & `aihwkit-0.8.0/tests/test_simulator_tiles.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -12,84 +12,114 @@
 
 # pylint: disable=too-many-locals
 
 """Tests for the high level simulator devices functionality."""
 
 from unittest import SkipTest
 
-from torch import Tensor, zeros, ones
+from torch import Tensor, zeros, ones, manual_seed
 
+from aihwkit.exceptions import ArgumentError
 from aihwkit.simulator.configs.configs import UnitCellRPUConfig
-from aihwkit.simulator.configs.compounds import (
-    VectorUnitCell, ReferenceUnitCell
+from aihwkit.simulator.configs.compounds import VectorUnitCell, ReferenceUnitCell
+from aihwkit.simulator.parameters.enums import (
+    VectorUnitCellUpdatePolicy,
+    NoiseManagementType,
+    BoundManagementType,
 )
-from aihwkit.simulator.configs.enums import (
-    VectorUnitCellUpdatePolicy, NoiseManagementType, BoundManagementType
-)
-from aihwkit.simulator.configs.utils import PrePostProcessingRPU
+from aihwkit.simulator.configs.configs import PrePostProcessingRPU
 
 from .helpers.decorators import parametrize_over_tiles
 from .helpers.testcases import ParametrizedTestCase
 from .helpers.tiles import (
-    FloatingPoint, Ideal, ConstantStep, LinearStep, SoftBounds,
-    ExpStep, Vector, OneSided, Transfer, BufferedTransfer, MixedPrecision,
-    PiecewiseStep, PiecewiseStepCuda,
-    Inference, Reference, FloatingPointCuda, IdealCuda, ConstantStepCuda,
-    LinearStepCuda, SoftBoundsCuda, ExpStepCuda, VectorCuda, OneSidedCuda,
-    TransferCuda, BufferedTransferCuda, InferenceCuda, ReferenceCuda,
-    MixedPrecisionCuda, PowStep, PowStepCuda,
-    PowStepReference, PowStepReferenceCuda,
-    SoftBoundsReference, SoftBoundsReferenceCuda,
-)
-from .helpers.testcases import SKIP_CUDA_TESTS
-
-TOL = 1e-6
-
-
-@parametrize_over_tiles([
     FloatingPoint,
     Ideal,
     ConstantStep,
     LinearStep,
-    ExpStep,
     SoftBounds,
-    SoftBoundsReference,
+    ExpStep,
     Vector,
     OneSided,
     Transfer,
     BufferedTransfer,
     MixedPrecision,
-    Inference,
-    Reference,
-    PowStep,
-    PowStepReference,
     PiecewiseStep,
     PiecewiseStepCuda,
+    Inference,
+    Reference,
     FloatingPointCuda,
     IdealCuda,
     ConstantStepCuda,
     LinearStepCuda,
-    ExpStepCuda,
-    PowStepCuda,
-    PowStepReferenceCuda,
-    PiecewiseStepCuda,
     SoftBoundsCuda,
-    SoftBoundsReferenceCuda,
+    ExpStepCuda,
     VectorCuda,
     OneSidedCuda,
     TransferCuda,
     BufferedTransferCuda,
-    MixedPrecisionCuda,
     InferenceCuda,
     ReferenceCuda,
-])
+    MixedPrecisionCuda,
+    PowStep,
+    PowStepCuda,
+    PowStepReference,
+    PowStepReferenceCuda,
+    SoftBoundsReference,
+    SoftBoundsReferenceCuda,
+    TorchInference,
+    TorchInferenceCuda,
+)
+from .helpers.testcases import SKIP_CUDA_TESTS
+
+TOL = 1e-6
+
+
+@parametrize_over_tiles(
+    [
+        FloatingPoint,
+        Ideal,
+        ConstantStep,
+        LinearStep,
+        ExpStep,
+        SoftBounds,
+        SoftBoundsReference,
+        Vector,
+        OneSided,
+        Transfer,
+        BufferedTransfer,
+        MixedPrecision,
+        Inference,
+        Reference,
+        PowStep,
+        PowStepReference,
+        PiecewiseStep,
+        PiecewiseStepCuda,
+        FloatingPointCuda,
+        IdealCuda,
+        ConstantStepCuda,
+        LinearStepCuda,
+        ExpStepCuda,
+        PowStepCuda,
+        PowStepReferenceCuda,
+        PiecewiseStepCuda,
+        SoftBoundsCuda,
+        SoftBoundsReferenceCuda,
+        VectorCuda,
+        OneSidedCuda,
+        TransferCuda,
+        BufferedTransferCuda,
+        MixedPrecisionCuda,
+        InferenceCuda,
+        ReferenceCuda,
+    ]
+)
 class TileTest(ParametrizedTestCase):
     """Test floating point tile."""
 
-    def test_bias(self):
+    def test_bias(self) -> None:
         """Test instantiating a tile."""
         out_size = 2
         in_size = 3
 
         analog_tile = self.get_tile(out_size, in_size, bias=True)
 
         self.assertIsInstance(analog_tile.tile, self.simulator_tile_class)
@@ -101,25 +131,26 @@
 
         # Set some properties in the simulators.Tile.
         analog_tile.set_learning_rate(0.123)
         analog_tile.set_weights(weights, biases)
 
         # Assert over learning rate.
         self.assertAlmostEqual(analog_tile.get_learning_rate(), learning_rate)
-        self.assertAlmostEqual(analog_tile.get_learning_rate(),
-                               analog_tile.tile.get_learning_rate())
+        self.assertAlmostEqual(
+            analog_tile.get_learning_rate(), analog_tile.tile.get_learning_rate()
+        )
 
         # Assert over weights and biases.
         tile_weights, tile_biases = analog_tile.get_weights()
         self.assertEqual(tile_weights.shape, (out_size, in_size))
         self.assertEqual(tile_biases.shape, (out_size,))
         self.assertTensorAlmostEqual(tile_weights, weights)
         self.assertTensorAlmostEqual(tile_biases, biases)
 
-    def test_no_bias(self):
+    def test_no_bias(self) -> None:
         """Test instantiating a floating point tile."""
         out_size = 2
         in_size = 3
 
         analog_tile = self.get_tile(out_size, in_size, bias=False)
         self.assertIsInstance(analog_tile.tile, self.simulator_tile_class)
 
@@ -130,24 +161,25 @@
 
         # Set some properties in the simulators.Tile.
         analog_tile.set_learning_rate(0.123)
         analog_tile.set_weights(weights)
 
         # Assert over learning rate.
         self.assertAlmostEqual(analog_tile.get_learning_rate(), learning_rate)
-        self.assertAlmostEqual(analog_tile.get_learning_rate(),
-                               analog_tile.tile.get_learning_rate())
+        self.assertAlmostEqual(
+            analog_tile.get_learning_rate(), analog_tile.tile.get_learning_rate()
+        )
 
         # Assert over weights and biases.
         tile_weights, tile_biases = analog_tile.get_weights()
         self.assertEqual(tuple(tile_weights.shape), (out_size, in_size))
         self.assertEqual(tile_biases, None)
         self.assertTensorAlmostEqual(tile_weights, weights)
 
-    def test_get_hidden_parameters(self):
+    def test_get_hidden_parameters(self) -> None:
         """Test getting hidden parameters."""
         analog_tile = self.get_tile(4, 5)
 
         hidden_parameters = analog_tile.get_hidden_parameters()
         field = self.first_hidden_field
 
         # Check that there are hidden parameters.
@@ -155,20 +187,18 @@
             self.assertGreater(len(hidden_parameters), 0)
         else:
             self.assertEqual(len(hidden_parameters), 0)
 
         if field:
             # Check that one of the parameters is correct.
             self.assertIn(field, hidden_parameters.keys())
-            self.assertEqual(hidden_parameters[field].shape,
-                             (4, 5))
-            self.assertTrue(all(abs(val - 0.6) < TOL for val in
-                                hidden_parameters[field].flatten()))
+            self.assertEqual(hidden_parameters[field].shape, (4, 5))
+            self.assertTrue(all(abs(val - 0.6) < TOL for val in hidden_parameters[field].flatten()))
 
-    def test_set_hidden_parameters(self):
+    def test_set_hidden_parameters(self) -> None:
         """Test setting hidden parameters."""
         analog_tile = self.get_tile(3, 4)
 
         hidden_parameters = analog_tile.get_hidden_parameters()
         field = self.first_hidden_field
 
         # Update one of the values of the hidden parameters.
@@ -178,31 +208,31 @@
 
         analog_tile.set_hidden_parameters(hidden_parameters)
 
         # Check that the change was propagated to the tile.
         new_hidden_parameters = analog_tile.get_hidden_parameters()
 
         # Compare old and new hidden parameters tensors.
-        for (_, old), (_, new) in zip(hidden_parameters.items(),
-                                      new_hidden_parameters.items()):
+        for (_, old), (_, new) in zip(hidden_parameters.items(), new_hidden_parameters.items()):
             self.assertTrue(old.allclose(new))
 
         if field:
             self.assertEqual(new_hidden_parameters[field][1][1], 0.8)
 
-    def test_post_update_step_diffuse(self):
+    def test_post_update_step_diffuse(self) -> None:
         """Tests whether post update diffusion is performed"""
         rpu_config = self.get_rpu_config()
 
-        if not hasattr(rpu_config.device, 'diffusion'):
-            if hasattr(rpu_config.device, 'unit_cell_devices') \
-               and hasattr(rpu_config.device.unit_cell_devices[-1], 'diffusion'):
+        if not hasattr(rpu_config.device, "diffusion"):
+            if hasattr(rpu_config.device, "unit_cell_devices") and hasattr(
+                rpu_config.device.unit_cell_devices[-1], "diffusion"
+            ):
                 rpu_config.device.unit_cell_devices[-1].diffusion = 0.323
             else:
-                raise SkipTest('This device does not support diffusion')
+                raise SkipTest("This device does not support diffusion")
         else:
             rpu_config.device.diffusion = 0.323
 
         analog_tile = self.get_tile(2, 3, rpu_config=rpu_config, bias=True)
 
         weights = Tensor([[0.1, 0.2, 0.3], [0.4, -0.5, -0.6]])
         biases = Tensor([-0.1, 0.2])
@@ -211,51 +241,59 @@
         analog_tile.set_weights(weights, biases)
 
         analog_tile.post_update_step()
 
         tile_weights, tile_biases = analog_tile.get_weights()
 
         self.assertNotAlmostEqualTensor(tile_weights, weights)
-        self.assertNotAlmostEqualTensor(tile_biases, biases)
+        if analog_tile.analog_bias:
+            self.assertNotAlmostEqualTensor(tile_biases, biases)
+        else:
+            self.assertTensorAlmostEqual(tile_biases, biases)
 
-    def test_post_update_step_lifetime(self):
+    def test_post_update_step_lifetime(self) -> None:
         """Tests whether post update decay is performed"""
         rpu_config = self.get_rpu_config()
 
-        if not hasattr(rpu_config.device, 'lifetime'):
-            if hasattr(rpu_config.device, 'unit_cell_devices') \
-               and hasattr(rpu_config.device.unit_cell_devices[-1], 'lifetime'):
+        if not hasattr(rpu_config.device, "lifetime"):
+            if hasattr(rpu_config.device, "unit_cell_devices") and hasattr(
+                rpu_config.device.unit_cell_devices[-1], "lifetime"
+            ):
                 for idx, _ in enumerate(rpu_config.device.unit_cell_devices):
-                    rpu_config.device.unit_cell_devices[idx].lifetime = 100.
+                    rpu_config.device.unit_cell_devices[idx].lifetime = 100.0
             else:
-                raise SkipTest('This device does not support lifetime')
+                raise SkipTest("This device does not support lifetime")
         else:
-            rpu_config.device.lifetime = 100.
+            rpu_config.device.lifetime = 100.0
 
         analog_tile = self.get_tile(2, 3, rpu_config=rpu_config, bias=True)
 
         weights = Tensor([[0.1, 0.2, 0.3], [0.4, -0.5, -0.6]])
         biases = Tensor([-0.1, 0.2])
 
         analog_tile.set_learning_rate(0.123)
         analog_tile.set_weights(weights, biases)
 
         analog_tile.post_update_step()
 
         tile_weights, tile_biases = analog_tile.get_weights()
 
         self.assertNotAlmostEqualTensor(tile_weights, weights)
-        self.assertNotAlmostEqualTensor(tile_biases, biases)
+        if analog_tile.analog_bias:
+            self.assertNotAlmostEqualTensor(tile_biases, biases)
+        else:
+            self.assertTensorAlmostEqual(tile_biases, biases)
 
-    def test_set_hidden_update_index(self):
+    def test_set_hidden_update_index(self) -> None:
         """Tests hidden update index"""
         rpu_config = self.get_rpu_config()
 
-        if not isinstance(rpu_config, UnitCellRPUConfig) \
-           or not isinstance(rpu_config.device, (VectorUnitCell, ReferenceUnitCell)):
+        if not isinstance(rpu_config, UnitCellRPUConfig) or not isinstance(
+            rpu_config.device, (VectorUnitCell, ReferenceUnitCell)
+        ):
             analog_tile = self.get_tile(2, 3, rpu_config=rpu_config, bias=False)
             index = analog_tile.get_hidden_update_index()
             self.assertEqual(index, 0)
             analog_tile.set_hidden_update_index(1)
             index = analog_tile.get_hidden_update_index()
             self.assertEqual(index, 0)
         else:
@@ -272,79 +310,82 @@
             self.assertEqual(index, 1)
 
             # set weights index 0
             analog_tile.set_hidden_update_index(0)
             weights_0 = Tensor([[0.1, 0.2, 0.3], [0.4, -0.5, -0.6]])
             analog_tile.tile.set_weights(weights_0)
             hidden_par = analog_tile.get_hidden_parameters()
-            self.assertTensorAlmostEqual(hidden_par['hidden_weights_0'],
-                                         weights_0)
-            self.assertTensorAlmostEqual(hidden_par['hidden_weights_1'],
-                                         zeros((2, 3)))
+            self.assertTensorAlmostEqual(hidden_par["hidden_weights_0"], weights_0)
+            self.assertTensorAlmostEqual(hidden_par["hidden_weights_1"], zeros((2, 3)))
 
             # set weights index 1
             analog_tile.set_hidden_update_index(1)
             weights_1 = Tensor([[0.4, 0.1, 0.2], [0.5, -0.2, -0.1]])
             analog_tile.tile.set_weights(weights_1)
 
             hidden_par = analog_tile.get_hidden_parameters()
 
-            self.assertTensorAlmostEqual(hidden_par['hidden_weights_0'],
-                                         weights_0)
-            self.assertTensorAlmostEqual(hidden_par['hidden_weights_1'],
-                                         weights_1)
+            self.assertTensorAlmostEqual(hidden_par["hidden_weights_0"], weights_0)
+            self.assertTensorAlmostEqual(hidden_par["hidden_weights_1"], weights_1)
 
             # update
             analog_tile.set_hidden_update_index(1)
             x = Tensor([[0.1, 0.2, 0.3], [0.4, -0.5, -0.6]])
             d = Tensor([[0.5, 0.1], [0.4, -0.5]])
             if analog_tile.is_cuda:
                 x = x.cuda()
                 d = d.cuda()
             analog_tile.update(x, d)
 
             hidden_par_after = analog_tile.get_hidden_parameters()
 
-            self.assertTensorAlmostEqual(hidden_par['hidden_weights_0'],
-                                         hidden_par_after['hidden_weights_0'])
-            self.assertNotAlmostEqualTensor(hidden_par['hidden_weights_1'],
-                                            hidden_par_after['hidden_weights_1'])
+            self.assertTensorAlmostEqual(
+                hidden_par["hidden_weights_0"], hidden_par_after["hidden_weights_0"]
+            )
+            self.assertNotAlmostEqualTensor(
+                hidden_par["hidden_weights_1"], hidden_par_after["hidden_weights_1"]
+            )
 
-    def test_input_range(self):
+    def test_input_range(self) -> None:
         """Tests whether input range is applied"""
         rpu_config = self.get_rpu_config()
 
         if not isinstance(rpu_config, PrePostProcessingRPU):
-            raise SkipTest('This device does not support input range learning')
+            raise SkipTest("This device does not support input range learning")
 
-        if hasattr(rpu_config, 'forward'):
+        if hasattr(rpu_config, "forward"):
             rpu_config.forward.noise_management = NoiseManagementType.NONE
             rpu_config.forward.bound_management = BoundManagementType.NONE
         rpu_config.pre_post.input_range.enable = True
         rpu_config.pre_post.input_range.init_from_data = 0
         rpu_config.pre_post.input_range.init_value = 3.0
         rpu_config.pre_post.input_range.manage_output_clipping = False
 
         analog_tile = self.get_tile(3, 3, rpu_config=rpu_config, bias=True)
 
         inputs = 5 * ones((3, 3))
+        if self.use_cuda:
+            inputs = inputs.cuda()
         outputs = analog_tile.pre_forward(inputs, 1, False)
         outputs = analog_tile.post_forward(outputs, 1, False)
         self.assertEqual(outputs.max().item(), 3.0)
 
         inputs = -5 * ones((3, 3))
+        if self.use_cuda:
+            inputs = inputs.cuda()
+
         outputs = analog_tile.pre_forward(inputs, 1, False)
         outputs = analog_tile.post_forward(outputs, 1, False)
         self.assertEqual(outputs.max().item(), -3.0)
 
-    def test_cuda_to_cpu(self):
-        """ test the copy to CPU from cuda """
+    def test_cuda_to_cpu(self) -> None:
+        """test the copy to CPU from cuda"""
 
         if not self.use_cuda or SKIP_CUDA_TESTS:
-            raise SkipTest('CUDA tile needed')
+            raise SkipTest("CUDA tile needed")
 
         out_size = 2
         in_size = 3
 
         cuda_analog_tile = self.get_tile(out_size, in_size, bias=True)
         self.assertIsInstance(cuda_analog_tile.tile, self.simulator_tile_class)
 
@@ -363,45 +404,46 @@
         if field:
             # set higher as default otherwise hidden weight might change
             hidden_parameters[field][1][1] = 0.8
         cuda_analog_tile.set_hidden_parameters(hidden_parameters)
 
         self.assertEqual(cuda_analog_tile.is_cuda, True)
         self.assertEqual(cuda_analog_tile.tile.__class__, self.simulator_tile_class)
+
         analog_tile = cuda_analog_tile.cpu()
         self.assertEqual(analog_tile.is_cuda, False)
         self.assertNotEqual(analog_tile.tile.__class__, self.simulator_tile_class)
         # Assert over learning rate.
         self.assertAlmostEqual(analog_tile.get_learning_rate(), learning_rate)
-        self.assertAlmostEqual(analog_tile.get_learning_rate(),
-                               analog_tile.tile.get_learning_rate())
+        self.assertAlmostEqual(
+            analog_tile.get_learning_rate(), analog_tile.tile.get_learning_rate()
+        )
 
         # Assert over weights and biases.
         tile_weights, tile_biases = analog_tile.get_weights()
         self.assertEqual(tile_weights.shape, (out_size, in_size))
         self.assertEqual(tile_biases.shape, (out_size,))
         self.assertTensorAlmostEqual(tile_weights, weights)
         self.assertTensorAlmostEqual(tile_biases, biases)
 
         # Check that the change was propagated to the tile.
         new_hidden_parameters = cuda_analog_tile.get_hidden_parameters()
 
         # Compare old and new hidden parameters tensors.
-        for (_, old), (_, new) in zip(hidden_parameters.items(),
-                                      new_hidden_parameters.items()):
+        for (_, old), (_, new) in zip(hidden_parameters.items(), new_hidden_parameters.items()):
             self.assertTrue(old.allclose(new))
 
         if field:
             self.assertEqual(new_hidden_parameters[field][1][1], 0.8)
 
-    def test_cpu_to_cuda(self):
-        """ test the copy to CUDA from CPU"""
+    def test_cpu_to_cuda(self) -> None:
+        """test the copy to CUDA from CPU"""
 
         if self.use_cuda or SKIP_CUDA_TESTS:
-            raise SkipTest('CUDA tile needed')
+            raise SkipTest("CUDA tile needed")
 
         out_size = 2
         in_size = 3
 
         cpu_analog_tile = self.get_tile(out_size, in_size, bias=True)
         self.assertIsInstance(cpu_analog_tile.tile, self.simulator_tile_class)
 
@@ -426,39 +468,39 @@
         self.assertEqual(cpu_analog_tile.tile.__class__, self.simulator_tile_class)
         analog_tile = cpu_analog_tile.cuda()
         self.assertEqual(analog_tile.is_cuda, True)
         self.assertNotEqual(analog_tile.tile.__class__, self.simulator_tile_class)
 
         # Assert over learning rate.
         self.assertAlmostEqual(analog_tile.get_learning_rate(), learning_rate)
-        self.assertAlmostEqual(analog_tile.get_learning_rate(),
-                               analog_tile.tile.get_learning_rate())
+        self.assertAlmostEqual(
+            analog_tile.get_learning_rate(), analog_tile.tile.get_learning_rate()
+        )
 
         # Assert over weights and biases.
         tile_weights, tile_biases = analog_tile.get_weights()
         self.assertEqual(tile_weights.shape, (out_size, in_size))
         self.assertEqual(tile_biases.shape, (out_size,))
         self.assertTensorAlmostEqual(tile_weights, weights)
         self.assertTensorAlmostEqual(tile_biases, biases)
 
         # Check that the change was propagated to the tile.
         new_hidden_parameters = cpu_analog_tile.get_hidden_parameters()
 
         # Compare old and new hidden parameters tensors.
-        for (_, old), (_, new) in zip(hidden_parameters.items(),
-                                      new_hidden_parameters.items()):
+        for (_, old), (_, new) in zip(hidden_parameters.items(), new_hidden_parameters.items()):
             self.assertTrue(old.allclose(new))
 
         if field:
             self.assertEqual(new_hidden_parameters[field][1][1], 0.8)
 
-    def test_program_weights(self):
+    def test_program_weights(self) -> None:
         """Tests whether weight programming is performed"""
         rpu_config = self.get_rpu_config()
-        if hasattr(rpu_config, 'forward'):
+        if hasattr(rpu_config, "forward"):
             rpu_config.forward.is_perfect = True  # avoid additional reading noise
         analog_tile = self.get_tile(2, 3, rpu_config=rpu_config, bias=True)
 
         weights = Tensor([[0.1, 0.2, 0.3], [0.4, -0.5, -0.6]])
         biases = Tensor([-0.1, 0.2])
         w_amax = 0.6
 
@@ -466,35 +508,107 @@
         analog_tile.set_weights(weights, biases)
 
         tolerance = 0.05
         analog_tile.program_weights()
         tile_weights, tile_biases = analog_tile.get_weights()
 
         self.assertNotAlmostEqualTensor(tile_weights, weights)
-        if analog_tile.bias:
+        if analog_tile.analog_bias:
             self.assertNotAlmostEqualTensor(tile_biases, biases)
 
         # but should be close
-        if analog_tile.bias:
+        if analog_tile.analog_bias:
             deviation = (tile_biases - biases).abs().sum() + (tile_weights - weights).abs().sum()
             deviation /= weights.numel() + biases.numel()
             self.assertTrue(deviation / w_amax < tolerance)
         else:
             self.assertTrue((tile_weights - weights).abs().mean() / w_amax < tolerance)
 
-    def test_read_weights(self):
+    def test_read_weights(self) -> None:
         """Tests whether weight reading is performed"""
         rpu_config = self.get_rpu_config()
         analog_tile = self.get_tile(2, 3, rpu_config=rpu_config, bias=True)
 
         weights = Tensor([[0.1, 0.2, 0.3], [0.4, -0.5, -0.6]])
         biases = Tensor([-0.1, 0.2])
         w_amax = 0.6
 
         analog_tile.set_weights(weights, biases)
 
         tile_weights, tile_biases = analog_tile.read_weights()
 
         tolerance = 0.1
         self.assertTrue((tile_weights - weights).abs().mean() / w_amax < tolerance)
-        if analog_tile.bias:
+        if analog_tile.analog_bias:
             self.assertTrue((tile_biases - biases).abs().mean() / w_amax < tolerance)
+
+    def test_dump_extra(self) -> None:
+        """Tests whether weight reading is performed"""
+
+        rpu_config = self.get_rpu_config()
+        analog_tile = self.get_tile(2, 3, rpu_config=rpu_config, bias=True)
+
+        state = analog_tile.tile.dump_extra()
+
+        self.assertTrue(len(state) > 0)
+        # try loading again (will fail if keys are not found)
+        analog_tile.tile.load_extra(state, True)
+
+        non_empty_keys = [key for key in state.keys() if len(state[key]) > 0]
+        if len(non_empty_keys) == 0:
+            raise SkipTest("No non empty keys")
+        del state[non_empty_keys[-1]]
+        with self.assertRaises(RuntimeError):
+            analog_tile.tile.load_extra(state, True)
+
+
+@parametrize_over_tiles(
+    [ConstantStep, ConstantStepCuda, Inference, InferenceCuda, TorchInference, TorchInferenceCuda]
+)
+class TileForwardBackwardTest(ParametrizedTestCase):
+    """Test some forward aspects."""
+
+    def test_set_forward_out_noise_std(self) -> None:
+        """Test setting forward parameters."""
+        manual_seed(123)
+
+        rpu_config = self.get_rpu_config()
+        rpu_config.forward.is_perfect = False
+        rpu_config.forward.out_noise_std = 1.0
+        rpu_config.forward.out_noise = 0.1
+        rpu_config.forward.w_noise = 0.0
+
+        analog_tile = self.get_tile(2, 2, rpu_config=rpu_config)
+
+        weights = Tensor([[0.1, 0.2], [0.4, -0.5]])
+        biases = Tensor([-0.1, 0.2])
+
+        analog_tile.set_learning_rate(0.123)
+        analog_tile.set_weights(weights, biases)
+
+        forward_parameters = analog_tile.get_forward_parameters()
+
+        field = "out_noise_values"
+        forward_parameters[field][1] = 0.0
+        forward_parameters[field][0] = 0.123
+
+        analog_tile.set_forward_parameters(forward_parameters)
+
+        # Check that the change was propagated to the tile.
+        new_forward_parameters = analog_tile.get_forward_parameters()
+        # Compare old and new hidden parameters tensors.
+        for (_, old), (_, new) in zip(forward_parameters.items(), new_forward_parameters.items()):
+            self.assertTrue(old.allclose(new))
+
+        self.assertEqual(new_forward_parameters[field][0], 0.123)
+
+        inputs = Tensor([[-0.1, 0.4], [-0.5, 0.1]])
+        if analog_tile.is_cuda:
+            inputs = inputs.cuda()
+        y_1 = analog_tile(inputs)
+        y_2 = analog_tile(inputs)
+        self.assertTensorAlmostEqual(y_1[:, 1], y_2[:, 1])
+        self.assertNotAlmostEqualTensor(y_1[:, 0], y_2[:, 0])
+
+        with self.assertRaises(ArgumentError):
+            forward_parameters["_not_existent"] = Tensor([1.0])
+            analog_tile.set_forward_parameters(forward_parameters)
```

### Comparing `aihwkit-0.7.1/tests/test_specific_tiles.py` & `aihwkit-0.8.0/tests/test_specific_tiles.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -12,51 +12,47 @@
 
 """Some more tests for specific tiles."""
 
 from torch import ones, Tensor
 from torch.nn.functional import mse_loss
 
 from aihwkit.simulator.configs.devices import SoftBoundsDevice
-from aihwkit.simulator.configs.compounds import (
-    TransferCompound, ReferenceUnitCell,
-)
+from aihwkit.simulator.configs.compounds import TransferCompound, ReferenceUnitCell
 
 from aihwkit.simulator.configs.configs import UnitCellRPUConfig
 from aihwkit.optim import AnalogSGD
 
 from .helpers.decorators import parametrize_over_layers
 from .helpers.layers import Linear, LinearCuda
 from .helpers.testcases import ParametrizedTestCase
 from .helpers.tiles import FloatingPoint
 
 
 @parametrize_over_layers(
-    layers=[Linear, LinearCuda],
-    tiles=[FloatingPoint],
-    biases=['analog', 'digital', None]
+    layers=[Linear, LinearCuda], tiles=[FloatingPoint], biases=["analog", "digital", None]
 )
 class TransferCompoundTest(ParametrizedTestCase):
     """Tests for transfer compound."""
 
     @staticmethod
     def get_transfer_compound(gamma, **kwargs):
-        """Get a Tiki-taka compound with reference cell """
+        """Get a Tiki-taka compound with reference cell"""
+
         def custom_device(**kwargs):
-            """Custom device """
+            """Custom device"""
             return SoftBoundsDevice(w_max_dtod=0.0, w_min_dtod=0.0, w_max=1.0, w_min=-1.0, **kwargs)
 
         rpu_config = UnitCellRPUConfig(
             device=TransferCompound(
-
                 # Devices that compose the Tiki-taka compound.
                 unit_cell_devices=[
                     # fast "A" matrix
                     ReferenceUnitCell([custom_device(**kwargs), custom_device(**kwargs)]),
                     # slow "C" matrix
-                    ReferenceUnitCell([custom_device(**kwargs), custom_device(**kwargs)])
+                    ReferenceUnitCell([custom_device(**kwargs), custom_device(**kwargs)]),
                 ],
                 gamma=gamma,
             )
         )
         return rpu_config
 
     def test_hidden_parameter_setting(self):
@@ -65,83 +61,82 @@
 
         for gamma in [0.0, 0.1]:
             model = self.get_layer(rpu_config=self.get_transfer_compound(gamma))
 
             weight, bias = model.get_weights()
             model.set_weights(weight * 0.0, bias * 0.0 if bias is not None else None)
 
-            params = model.analog_tile.get_hidden_parameters()
-            shape = params['hidden_weights_0_0'].shape
+            analog_tile = next(model.analog_tiles())
+            params = analog_tile.get_hidden_parameters()
+            shape = params["hidden_weights_0_0"].shape
 
             # just dummy settings
             a, b, c, d = 0.47, 0.21, 0.64, 0.12
-            params['hidden_weights_0_0'] = a*ones(*shape)  # A
-            params['hidden_weights_1_0'] = b*ones(*shape)  # A ref
-            params['hidden_weights_0_1'] = c*ones(*shape)  # C
-            params['hidden_weights_1_1'] = d*ones(*shape)  # C_ref
+            params["hidden_weights_0_0"] = a * ones(*shape)  # A
+            params["hidden_weights_1_0"] = b * ones(*shape)  # A ref
+            params["hidden_weights_0_1"] = c * ones(*shape)  # C
+            params["hidden_weights_1_1"] = d * ones(*shape)  # C_ref
 
-            model.analog_tile.set_hidden_parameters(params)
+            analog_tile.set_hidden_parameters(params)
 
             weight, bias = model.get_weights()
 
             # should be
             if self.digital_bias:
                 self.assertEqual(bias[0], 0.0)
             if self.bias and not self.digital_bias:
-                self.assertEqual(bias[0], gamma*(a - b) + c - d)
+                self.assertEqual(bias[0], gamma * (a - b) + c - d)
 
-            self.assertEqual(weight[0][0], gamma*(a - b) + c - d)
+            self.assertEqual(weight[0][0], gamma * (a - b) + c - d)
 
     def test_decay(self):
         """Test hidden parameter set."""
         # pylint: disable=invalid-name, too-many-locals
 
-        lifetime = 100.  # initial setting (needs to be larger 1)
+        lifetime = 100.0  # initial setting (needs to be larger 1)
         gamma = 0.1
-        reset_bias = 0.1  # decay shift
-        rpu_config = self.get_transfer_compound(gamma=gamma,
-                                                lifetime=lifetime,
-                                                lifetime_dtod=0.0,
-                                                reset=reset_bias)
-
+        reset_bias = 0.3  # decay shift
+        rpu_config = self.get_transfer_compound(
+            gamma=gamma, lifetime=lifetime, lifetime_dtod=0.0, reset=reset_bias, reset_std=0.0
+        )
         model = self.get_layer(in_features=2, out_features=1, rpu_config=rpu_config)
 
         weight, bias = model.get_weights()
         model.set_weights(weight * 0.0, bias * 0.0 if bias is not None else None)
 
-        params = model.analog_tile.get_hidden_parameters()
-        shape = params['hidden_weights_0_0'].shape
+        analog_tile = next(model.analog_tiles())
+        params = analog_tile.get_hidden_parameters()
+        shape = params["hidden_weights_0_0"].shape
 
         # just dummy settings
         a, b, c, d = 0.47, 0.21, 0.64, 0.12
-        params['hidden_weights_0_0'] = a * ones(*shape)  # A
-        params['hidden_weights_1_0'] = b * ones(*shape)  # A ref
-        params['hidden_weights_0_1'] = c * ones(*shape)  # C
-        params['hidden_weights_1_1'] = d * ones(*shape)  # C_ref
+        params["hidden_weights_0_0"] = a * ones(*shape)  # A
+        params["hidden_weights_1_0"] = b * ones(*shape)  # A ref
+        params["hidden_weights_0_1"] = c * ones(*shape)  # C
+        params["hidden_weights_1_1"] = d * ones(*shape)  # C_ref
 
         # explicitly set the decay scales (which is 1-1/lifetime)
-        a_dcy, b_dcy, c_dcy, d_dcy = 0.95, 0.78, 0.93, 0.92
-        params['decay_scales_0_0'] = a_dcy * ones(*shape)  # A
-        params['decay_scales_1_0'] = b_dcy * ones(*shape)  # A ref
-        params['decay_scales_0_1'] = c_dcy * ones(*shape)  # C
-        params['decay_scales_1_1'] = d_dcy * ones(*shape)  # C_ref
-
-        model.analog_tile.set_hidden_parameters(params)
-
-        # LR set to zero. Only lifetime will be applied
-        opt = AnalogSGD(model.parameters(), lr=0.0)
+        a_dcy, b_dcy, c_dcy, d_dcy = 0.95, 0.28, 0.33, 0.12
+        params["decay_scales_0_0"] = a_dcy * ones(*shape)  # A
+        params["decay_scales_1_0"] = b_dcy * ones(*shape)  # A ref
+        params["decay_scales_0_1"] = c_dcy * ones(*shape)  # C
+        params["decay_scales_1_1"] = d_dcy * ones(*shape)  # C_ref
 
+        analog_tile.set_hidden_parameters(params)
+        weight, bias = model.get_weights()
         x_b = Tensor([[0.1, 0.2], [0.2, 0.4]])
         y_b = Tensor([[0.3], [0.6]])
 
         if self.use_cuda:
-            model = model.cuda()
             x_b = x_b.cuda()
             y_b = y_b.cuda()
 
+        # LR set to zero. Only lifetime will be applied
+        opt = AnalogSGD(model.parameters(), lr=0.0)
+
         epochs = 2
         for _ in range(epochs):
             opt.zero_grad()
             pred = model(x_b)
             loss = mse_loss(pred, y_b)
 
             loss.backward()
@@ -154,10 +149,10 @@
         b = (b - reset_bias) * pow(b_dcy, epochs) + reset_bias
         c = (c - reset_bias) * pow(c_dcy, epochs) + reset_bias
         d = (d - reset_bias) * pow(d_dcy, epochs) + reset_bias
 
         if self.digital_bias:
             self.assertAlmostEqual(bias[0].item(), 0.0)
         if self.bias and not self.digital_bias:
-            self.assertAlmostEqual(bias[0].item(), gamma*(a - b) + c - d, 5)
+            self.assertAlmostEqual(bias[0].item(), gamma * (a - b) + c - d, 5)
 
-        self.assertAlmostEqual(weight[0][0].item(), gamma*(a - b) + c - d, 5)
+        self.assertAlmostEqual(weight[0][0].item(), gamma * (a - b) + c - d, 5)
```

### Comparing `aihwkit-0.7.1/tests/test_utils.py` & `aihwkit-0.8.0/tests/test_utils.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 
-# (C) Copyright 2020, 2021, 2022 IBM. All Rights Reserved.
+# (C) Copyright 2020, 2021, 2022, 2023 IBM. All Rights Reserved.
 #
 # This code is licensed under the Apache License, Version 2.0. You may
 # obtain a copy of this license in the LICENSE.txt file in the root directory
 # of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.
 #
 # Any modifications or derivative works of this code must retain this
 # copyright notice, and modified files need to carry a notice indicating
@@ -23,38 +23,67 @@
 from torch import Tensor, load, save, device, manual_seed
 from torch import abs as torch_abs
 from torch.nn import Module, Sequential
 from torch.nn import Linear as torch_linear
 from torch.nn.functional import mse_loss
 from torch.optim import SGD
 
-from aihwkit.nn import (
-    AnalogConv2d, AnalogConv2dMapped, AnalogSequential, AnalogLinearMapped
-)
+from aihwkit.nn import AnalogConv2d, AnalogConv2dMapped, AnalogSequential, AnalogLinearMapped
 from aihwkit.optim import AnalogSGD
 from aihwkit.simulator.configs import SingleRPUConfig, InferenceRPUConfig, FloatingPointRPUConfig
 from aihwkit.simulator.configs.devices import ConstantStepDevice, LinearStepDevice
-from aihwkit.simulator.configs.utils import IOParameters, UpdateParameters, MappingParameter
-from aihwkit.exceptions import TileError, ModuleError
+from aihwkit.simulator.parameters.utils import IOParameters, UpdateParameters, MappingParameter
+from aihwkit.simulator.tiles.base import AnalogTileStateNames
+from aihwkit.exceptions import TileError, TileModuleError
 from aihwkit.nn.conversion import convert_to_analog
 
 from .helpers.decorators import parametrize_over_layers
 from .helpers.layers import (
-    Conv2d, Conv2dCuda, Linear, LinearCuda, LinearMapped,
-    LinearMappedCuda, Conv2dMapped, Conv2dMappedCuda
+    Conv2d,
+    Conv2dCuda,
+    Linear,
+    LinearCuda,
+    LinearMapped,
+    LinearMappedCuda,
+    Conv2dMapped,
+    Conv2dMappedCuda,
 )
 from .helpers.testcases import ParametrizedTestCase, SKIP_CUDA_TESTS
-from .helpers.tiles import FloatingPoint, ConstantStep, Inference, InferenceLearnOutScaling
+from .helpers.tiles import (
+    FloatingPoint,
+    ConstantStep,
+    Inference,
+    InferenceLearnOutScaling,
+    TorchInference,
+    Custom,
+)
+
+SKIP_META_PARAM_TILES = [TorchInference, Custom, FloatingPoint]
 
 
 @parametrize_over_layers(
-    layers=[Linear, Conv2d, LinearMapped, LinearCuda, LinearMappedCuda,
-            Conv2dCuda, Conv2dMapped, Conv2dMappedCuda],
-    tiles=[FloatingPoint, ConstantStep, Inference, InferenceLearnOutScaling],
-    biases=['analog', 'digital', None],
+    layers=[
+        Linear,
+        Conv2d,
+        LinearMapped,
+        LinearCuda,
+        LinearMappedCuda,
+        Conv2dCuda,
+        Conv2dMapped,
+        Conv2dMappedCuda,
+    ],
+    tiles=[
+        FloatingPoint,
+        ConstantStep,
+        Inference,
+        InferenceLearnOutScaling,
+        TorchInference,
+        Custom,
+    ],
+    biases=["digital", None],
 )
 class SerializationTest(ParametrizedTestCase):
     """Tests for serialization."""
 
     @staticmethod
     def train_model(model, loss_func, x_b, y_b):
         """Train the model."""
@@ -79,54 +108,65 @@
             weight, bias = model.get_weights()
             return weight, bias, weight, bias, True
 
         if isinstance(model, AnalogConv2dMapped):
             weight, bias = model.get_weights()
             return weight, bias, weight, bias, True
 
-        weight = model.weight.data.detach().cpu().numpy()
-        if model.use_bias:
+        if model.weight is not None:
+            weight = model.weight.data.detach().cpu().numpy()
+        else:
+            # we do not sync anymore
+            weight, bias = model.get_weights()
+            return weight, bias, weight, bias, True
+
+        if model.bias is not None:
             bias = model.bias.data.detach().cpu().numpy()
         else:
             bias = None
 
         analog_weight, analog_bias = model.get_weights()
         analog_weight = analog_weight.detach().cpu().numpy()
         if analog_bias is not None:
             analog_bias = analog_bias.detach().cpu().numpy()
 
         return weight, bias, analog_weight.reshape(weight.shape), analog_bias, True
 
     @staticmethod
     def get_analog_tile(model):
-        """ Return a (python) analog tile of the model"""
+        """Return a (python) analog tile of the model"""
         return list(model.analog_tiles())[0]
 
     def test_save_load_state_dict_train(self):
         """Test saving and loading using a state dict after training."""
         model = self.get_layer()
 
         # Perform an update in order to modify tile weights and biases.
         loss_func = mse_loss
         if isinstance(model, (AnalogConv2d, AnalogConv2dMapped)):
-            input_x = Tensor(rand(2, 2, 3, 3))*0.2
-            input_y = Tensor(rand(2, 3, 5, 5))*0.2
+            input_x = Tensor(rand(2, 2, 3, 3)) * 0.2
+            input_y = Tensor(rand(2, 3, 5, 5)) * 0.2
         else:
-            input_x = Tensor(rand(2, model.in_features))*0.2
-            input_y = Tensor(rand(2, model.out_features))*0.2
+            input_x = Tensor(rand(2, model.in_features)) * 0.2
+            input_y = Tensor(rand(2, model.out_features)) * 0.2
 
         if self.use_cuda:
             input_x = input_x.cuda()
             input_y = input_y.cuda()
 
         self.train_model(model, loss_func, input_x, input_y)
 
         # Keep track of the current weights and biases for comparing.
-        (model_weights, model_biases,
-         tile_weights, tile_biases, sync) = self.get_layer_and_tile_weights(model)
+        (
+            model_weights,
+            model_biases,
+            tile_weights,
+            tile_biases,
+            sync,
+        ) = self.get_layer_and_tile_weights(model)
 
         # now the tile weights should be out of sync
         if not sync:
             assert_raises(AssertionError, assert_array_almost_equal, model_weights, tile_weights)
             if self.bias:
                 assert_raises(AssertionError, assert_array_almost_equal, model_biases, tile_biases)
 
@@ -135,35 +175,40 @@
             save(model.state_dict(), file)
             # Create a new model and load its state dict.
             file.seek(0)
             new_model = self.get_layer()
             new_model.load_state_dict(load(file))
 
         # Compare the new model weights and biases. they should now be in sync
-        (new_model_weights, new_model_biases,
-         new_tile_weights, new_tile_biases, _) = self.get_layer_and_tile_weights(new_model)
+        (
+            new_model_weights,
+            new_model_biases,
+            new_tile_weights,
+            new_tile_biases,
+            _,
+        ) = self.get_layer_and_tile_weights(new_model)
 
         assert_array_almost_equal(tile_weights, new_model_weights)
         assert_array_almost_equal(tile_weights, new_tile_weights)
         if self.bias:
             assert_array_almost_equal(tile_biases, new_model_biases)
             assert_array_almost_equal(tile_biases, new_tile_biases)
 
     def test_save_load_state_dict_train_after(self):
         """Test saving and loading using a state dict and training after load."""
         model = self.get_layer()
 
         # Perform an update in order to modify tile weights and biases.
         loss_func = mse_loss
         if isinstance(model, (AnalogConv2d, AnalogConv2dMapped)):
-            input_x = Tensor(rand(2, 2, 3, 3))*0.2
-            input_y = Tensor(rand(2, 3, 5, 5))*0.2
+            input_x = Tensor(rand(2, 2, 3, 3)) * 0.2
+            input_y = Tensor(rand(2, 3, 5, 5)) * 0.2
         else:
-            input_x = Tensor(rand(2, model.in_features))*0.2
-            input_y = Tensor(rand(2, model.out_features))*0.2
+            input_x = Tensor(rand(2, model.in_features)) * 0.2
+            input_y = Tensor(rand(2, model.out_features)) * 0.2
 
         if self.use_cuda:
             input_x = input_x.cuda()
             input_y = input_y.cuda()
 
         self.train_model(model, loss_func, input_x, input_y)
 
@@ -172,19 +217,25 @@
 
         # Save the model to a file.
         with TemporaryFile() as file:
             save(model.state_dict(), file)
             # Create a new model and load its state dict.
             file.seek(0)
             new_model = self.get_layer()
-            new_model.load_state_dict(load(file))
+            loaded = load(file)
+            new_model.load_state_dict(loaded)
 
         # Compare the new model weights and biases. they should now be in sync
-        (new_model_weights, new_model_biases,
-         new_tile_weights, new_tile_biases, _) = self.get_layer_and_tile_weights(new_model)
+        (
+            new_model_weights,
+            new_model_biases,
+            new_tile_weights,
+            new_tile_biases,
+            _,
+        ) = self.get_layer_and_tile_weights(new_model)
 
         assert_array_almost_equal(tile_weights, new_model_weights)
         assert_array_almost_equal(tile_weights, new_tile_weights)
         if self.bias:
             assert_array_almost_equal(tile_biases, new_model_biases)
             assert_array_almost_equal(tile_biases, new_tile_biases)
 
@@ -197,19 +248,19 @@
     def test_save_load_state_dict_train_after_old_model(self):
         """Test saving and loading using a state dict and training after load with old model."""
         model = self.get_layer()
 
         # Perform an update in order to modify tile weights and biases.
         loss_func = mse_loss
         if isinstance(model, (AnalogConv2d, AnalogConv2dMapped)):
-            input_x = Tensor(rand(2, 2, 3, 3))*0.2
-            input_y = Tensor(rand(2, 3, 5, 5))*0.2
+            input_x = Tensor(rand(2, 2, 3, 3)) * 0.2
+            input_y = Tensor(rand(2, 3, 5, 5)) * 0.2
         else:
-            input_x = Tensor(rand(2, model.in_features))*0.2
-            input_y = Tensor(rand(2, model.out_features))*0.2
+            input_x = Tensor(rand(2, model.in_features)) * 0.2
+            input_y = Tensor(rand(2, model.out_features)) * 0.2
 
         if self.use_cuda:
             input_x = input_x.cuda()
             input_y = input_y.cuda()
 
         self.train_model(model, loss_func, input_x, input_y)
 
@@ -220,16 +271,21 @@
         with TemporaryFile() as file:
             save(model.state_dict(), file)
             # Create a new model and load its state dict.
             file.seek(0)
             model.load_state_dict(load(file))
 
         # Compare the new model weights and biases. they should now be in sync
-        (new_model_weights, new_model_biases,
-         new_tile_weights, new_tile_biases, _) = self.get_layer_and_tile_weights(model)
+        (
+            new_model_weights,
+            new_model_biases,
+            new_tile_weights,
+            new_tile_biases,
+            _,
+        ) = self.get_layer_and_tile_weights(model)
 
         assert_array_almost_equal(tile_weights, new_model_weights)
         assert_array_almost_equal(tile_weights, new_tile_weights)
         if self.bias:
             assert_array_almost_equal(tile_biases, new_model_biases)
             assert_array_almost_equal(tile_biases, new_tile_biases)
 
@@ -238,19 +294,19 @@
     def test_save_load_train_after(self):
         """Test saving and loading using a state dict and training after load."""
         model = self.get_layer()
 
         # Perform an update in order to modify tile weights and biases.
         loss_func = mse_loss
         if isinstance(model, (AnalogConv2d, AnalogConv2dMapped)):
-            input_x = Tensor(rand(2, 2, 3, 3))*0.2
-            input_y = Tensor(rand(2, 3, 5, 5))*0.2
+            input_x = Tensor(rand(2, 2, 3, 3)) * 0.2
+            input_y = Tensor(rand(2, 3, 5, 5)) * 0.2
         else:
-            input_x = Tensor(rand(2, model.in_features))*0.2
-            input_y = Tensor(rand(2, model.out_features))*0.2
+            input_x = Tensor(rand(2, model.in_features)) * 0.2
+            input_y = Tensor(rand(2, model.out_features)) * 0.2
 
         if self.use_cuda:
             input_x = input_x.cuda()
             input_y = input_y.cuda()
 
         # Keep track of the current weights and biases for comparing.
         _, _, tile_weights, tile_biases, _ = self.get_layer_and_tile_weights(model)
@@ -259,16 +315,21 @@
         with TemporaryFile() as file:
             save(model, file)
             # Create a new model and load its state dict.
             file.seek(0)
             new_model = load(file)
 
         # Compare the new model weights and biases. they should now be in sync
-        (new_model_weights, new_model_biases,
-         new_tile_weights, new_tile_biases, _) = self.get_layer_and_tile_weights(new_model)
+        (
+            new_model_weights,
+            new_model_biases,
+            new_tile_weights,
+            new_tile_biases,
+            _,
+        ) = self.get_layer_and_tile_weights(new_model)
 
         assert_array_almost_equal(tile_weights, new_model_weights)
         assert_array_almost_equal(tile_weights, new_tile_weights)
         if self.bias:
             assert_array_almost_equal(tile_biases, new_model_biases)
             assert_array_almost_equal(tile_biases, new_tile_biases)
 
@@ -278,102 +339,125 @@
             self.assertTensorAlmostEqual(loss, new_loss)
 
     def test_save_load_model(self):
         """Test saving and loading a model directly."""
         model = self.get_layer()
 
         # Keep track of the current weights and biases for comparing.
-        (model_weights, model_biases,
-         tile_weights, tile_biases, _) = self.get_layer_and_tile_weights(model)
+        (
+            model_weights,
+            model_biases,
+            tile_weights,
+            tile_biases,
+            _,
+        ) = self.get_layer_and_tile_weights(model)
         assert_array_almost_equal(model_weights, tile_weights)
         if self.bias:
             assert_array_almost_equal(model_biases, tile_biases)
 
         # Save the model to a file.
         with TemporaryFile() as file:
             save(model, file)
             # Load the model.
             file.seek(0)
             new_model = load(file)
 
         # Compare the new model weights and biases.
-        (new_model_weights, new_model_biases,
-         new_tile_weights, new_tile_biases, _) = self.get_layer_and_tile_weights(new_model)
+        (
+            new_model_weights,
+            new_model_biases,
+            new_tile_weights,
+            new_tile_biases,
+            _,
+        ) = self.get_layer_and_tile_weights(new_model)
 
         assert_array_almost_equal(model_weights, new_model_weights)
         assert_array_almost_equal(tile_weights, new_tile_weights)
         if self.bias:
             assert_array_almost_equal(model_biases, new_model_biases)
             assert_array_almost_equal(tile_biases, new_tile_biases)
 
         # Asserts over the AnalogContext of the new model.
         new_analog_tile = self.get_analog_tile(new_model)
         analog_tile = self.get_analog_tile(model)
-        self.assertTrue(hasattr(new_analog_tile.analog_ctx, 'analog_tile'))
-        self.assertIsInstance(new_analog_tile.analog_ctx.analog_tile,
-                              analog_tile.__class__)
+
+        self.assertTrue(hasattr(new_analog_tile.analog_ctx, "analog_tile"))
+        self.assertIsInstance(new_analog_tile.analog_ctx.analog_tile, analog_tile.__class__)
         self.assertTrue(new_analog_tile.is_cuda == analog_tile.is_cuda)
 
     def test_save_load_model_cross_device(self):
         """Test saving and loading a model directly."""
 
         if SKIP_CUDA_TESTS:
-            raise SkipTest('CUDA not available.')
+            raise SkipTest("CUDA not available.")
 
         model = self.get_layer()
 
-        map_location = 'cuda'
+        map_location = "cuda"
         if self.use_cuda:
-            map_location = 'cpu'
+            map_location = "cpu"
 
         # Keep track of the current weights and biases for comparing.
-        (model_weights, model_biases,
-         tile_weights, tile_biases, _) = self.get_layer_and_tile_weights(model)
+        (
+            model_weights,
+            model_biases,
+            tile_weights,
+            tile_biases,
+            _,
+        ) = self.get_layer_and_tile_weights(model)
         assert_array_almost_equal(model_weights, tile_weights)
         if self.bias:
             assert_array_almost_equal(model_biases, tile_biases)
 
         # Save the model to a file.
         with TemporaryFile() as file:
             save(model, file)
             # Load the model.
             file.seek(0)
             new_model = load(file, map_location=device(map_location))
 
         # Compare the new model weights and biases.
-        (new_model_weights, new_model_biases,
-         new_tile_weights, new_tile_biases, _) = self.get_layer_and_tile_weights(new_model)
+        (
+            new_model_weights,
+            new_model_biases,
+            new_tile_weights,
+            new_tile_biases,
+            _,
+        ) = self.get_layer_and_tile_weights(new_model)
 
         assert_array_almost_equal(model_weights, new_model_weights)
         assert_array_almost_equal(tile_weights, new_tile_weights)
         if self.bias:
             assert_array_almost_equal(model_biases, new_model_biases)
             assert_array_almost_equal(tile_biases, new_tile_biases)
 
         # Asserts over the AnalogContext of the new model.
         new_analog_tile = self.get_analog_tile(new_model)
         analog_tile = self.get_analog_tile(model)
 
-        self.assertTrue(hasattr(new_analog_tile.analog_ctx, 'analog_tile'))
-        self.assertIsInstance(new_analog_tile.analog_ctx.analog_tile,
-                              analog_tile.__class__)
+        self.assertTrue(hasattr(new_analog_tile.analog_ctx, "analog_tile"))
+        self.assertIsInstance(new_analog_tile.analog_ctx.analog_tile, analog_tile.__class__)
 
         self.assertTrue(new_analog_tile.is_cuda != analog_tile.is_cuda)
 
         if analog_tile.shared_weights is not None:
             self.assertTrue(new_analog_tile.shared_weights.device.type == map_location)
 
     def test_save_load_meta_parameter(self):
         """Test saving and loading a device with custom parameters."""
+
+        if self.tile_class in SKIP_META_PARAM_TILES:
+            raise SkipTest("Not available")
+
         # Create the device and the array.
         rpu_config = SingleRPUConfig(
             forward=IOParameters(inp_noise=0.321),
             backward=IOParameters(inp_noise=0.456),
             update=UpdateParameters(desired_bl=78),
-            device=ConstantStepDevice(w_max=0.987)
+            device=ConstantStepDevice(w_max=0.987),
         )
 
         model = self.get_layer(rpu_config=rpu_config)
 
         # Save the model to a file.
         with TemporaryFile() as file:
             save(model, file)
@@ -381,15 +465,15 @@
             file.seek(0)
             new_model = load(file)
 
         # Assert over the new model tile parameters.
         new_analog_tile = self.get_analog_tile(new_model)
         analog_tile = self.get_analog_tile(model)
 
-        parameters = new_analog_tile.tile.get_parameters()
+        parameters = new_analog_tile.tile.get_meta_parameters()
         self.assertAlmostEqual(parameters.forward_io.inp_noise, 0.321)
         self.assertAlmostEqual(parameters.backward_io.inp_noise, 0.456)
         self.assertAlmostEqual(parameters.update.desired_bl, 78)
         self.assertTrue(new_analog_tile.is_cuda == analog_tile.is_cuda)
 
     def test_save_load_hidden_parameters(self):
         """Test saving and loading a device with hidden parameters."""
@@ -412,15 +496,18 @@
 
     def test_save_load_out_scaling_alpha(self):
         """Test saving and loading a device with out_scaling_alpha."""
         # Create the device and the array.
         model = self.get_layer()
         alpha = 2.0
         analog_tile = self.get_analog_tile(model)
-        analog_tile.out_scaling_alpha = Tensor([alpha])
+        if analog_tile.out_scaling_alpha is None:
+            analog_tile.out_scaling_alpha = Tensor([alpha])
+        else:
+            analog_tile.out_scaling_alpha.data = Tensor([alpha])
 
         # Save the model to a file.
         with TemporaryFile() as file:
             save(model, file)
             # Load the model.
             file.seek(0)
             new_model = load(file)
@@ -430,20 +517,19 @@
         alpha_new = new_analog_tile.get_scales().detach().cpu()
         assert_array_almost_equal(array(alpha), array(alpha_new))
 
     def test_save_load_shared_weights(self):
         """Test saving and loading a device with shared_weights."""
 
         if isinstance(self.get_rpu_config(), FloatingPointRPUConfig):
-            raise SkipTest('Not available for FP')
+            raise SkipTest("Not available for FP")
 
         # Create the device and the array.
         model = self.get_layer()
         analog_tile = self.get_analog_tile(model)
-
         shared_weights = None
         if analog_tile.shared_weights is not None:
             shared_weights = analog_tile.shared_weights.detach().cpu().numpy()
 
         # Save the model to a file.
         with TemporaryFile() as file:
             save(model, file)
@@ -477,37 +563,38 @@
         new_analog_tile = self.get_analog_tile(new_model)
         alpha_new = new_analog_tile.get_scales().detach().cpu()
         assert_array_almost_equal(array(alpha), array(alpha_new))
 
     def test_remapping(self):
         """Test remapping of the weights."""
         # Create the device and the array.
-        rpu_config = InferenceRPUConfig(mapping=MappingParameter(
-            weight_scaling_omega=0.4,
-            weight_scaling_columnwise=True))
+        rpu_config = InferenceRPUConfig(
+            mapping=MappingParameter(weight_scaling_omega=0.4, weight_scaling_columnwise=True)
+        )
 
         model = self.get_layer(rpu_config=rpu_config)
         analog_tile = self.get_analog_tile(model)
+        in_features = model.in_features
+        out_features = model.out_features
 
-        user_weights = Tensor(model.out_features, model.in_features).uniform_(-0.1, 0.1)
+        user_weights = Tensor(out_features, in_features).uniform_(-0.1, 0.1)
         if self.bias:
-            user_biases = Tensor(model.out_features).uniform_(-0.1, 0.1)
+            user_biases = Tensor(out_features).uniform_(-0.1, 0.1)
         else:
             user_biases = None
-        model.set_weights(user_weights, user_biases, apply_weight_scaling=True,
-                          force_exact=True)
+        model.set_weights(user_weights, user_biases, apply_weight_scaling=True)
 
         alpha_initial = analog_tile.get_scales().detach().cpu()
         self.assertNotEqual(alpha_initial.max(), 1.0)
 
         # remap module
-        model.remap_weights(weight_scaling_omega=1.0)
+        model.remap_analog_weights(weight_scaling_omega=1.0)
 
-        weights, _ = model.get_weights(apply_weight_scaling=True, force_exact=True)
-        analog_weights, _ = model.get_weights(apply_weight_scaling=False, force_exact=True)
+        weights, _ = model.get_weights(apply_weight_scaling=True)
+        analog_weights, _ = model.get_weights(apply_weight_scaling=False)
 
         assert_array_almost_equal(array(user_weights), array(weights))
 
         alpha = analog_tile.get_scales().detach().cpu()
         assert_raises(AssertionError, assert_array_almost_equal, array(alpha), array(alpha_initial))
 
         self.assertEqual(torch_abs(analog_weights).max(), 1.0)
@@ -534,87 +621,118 @@
 
     def test_state_dict_children_layers_sequential(self):
         """Test using the state_dict with children analog layers via Sequential."""
         children_layer = self.get_layer()
         model = Sequential(children_layer)
 
         # Keep track of the current weights and biases for comparing.
-        (model_weights, model_biases,
-         tile_weights, tile_biases, _) = self.get_layer_and_tile_weights(children_layer)
+        (
+            model_weights,
+            model_biases,
+            tile_weights,
+            tile_biases,
+            _,
+        ) = self.get_layer_and_tile_weights(children_layer)
 
         state_dict = model.state_dict()
-        lst = [key for key in state_dict.keys() if key.startswith('0.')
-               and children_layer.ANALOG_STATE_PREFIX in key]
+        lst = [
+            key
+            for key in state_dict.keys()
+            if key.startswith("0.") and AnalogTileStateNames.ANALOG_STATE_NAME in key
+        ]
         self.assertTrue(len(lst) > 0)
 
         # Update the state_dict of a new model.
         new_children_layer = self.get_layer()
         new_model = Sequential(new_children_layer)
         new_model.load_state_dict(model.state_dict())
 
         # Compare the new model weights and biases.
-        (new_model_weights, new_model_biases, new_tile_weights, new_tile_biases, _) = \
-            self.get_layer_and_tile_weights(new_children_layer)
+        (
+            new_model_weights,
+            new_model_biases,
+            new_tile_weights,
+            new_tile_biases,
+            _,
+        ) = self.get_layer_and_tile_weights(new_children_layer)
 
         assert_array_almost_equal(model_weights, new_model_weights)
         assert_array_almost_equal(tile_weights, new_tile_weights)
         if self.bias:
             assert_array_almost_equal(model_biases, new_model_biases)
             assert_array_almost_equal(tile_biases, new_tile_biases)
 
     def test_state_dict_children_layers_subclassing(self):
         """Test using the state_dict with children analog layers via subclassing."""
 
         class CustomModule(Module):
             """Module that defines its children layers via custom attributes."""
+
             # pylint: disable=abstract-method
             def __init__(self, layer: Module):
                 super().__init__()
                 self.custom_child = layer
 
         children_layer = self.get_layer()
         model = CustomModule(children_layer)
 
         # Keep track of the current weights and biases for comparing.
-        (model_weights, model_biases, tile_weights, tile_biases, _) = \
-            self.get_layer_and_tile_weights(children_layer)
+        (
+            model_weights,
+            model_biases,
+            tile_weights,
+            tile_biases,
+            _,
+        ) = self.get_layer_and_tile_weights(children_layer)
 
         state_dict = model.state_dict()
-        lst = [key for key in state_dict.keys() if key.startswith('custom_child.')
-               and children_layer.ANALOG_STATE_PREFIX in key]
+        lst = [
+            key
+            for key in state_dict.keys()
+            if key.startswith("custom_child.") and AnalogTileStateNames.ANALOG_STATE_NAME in key
+        ]
         self.assertTrue(len(lst) > 0)
 
         # Update the state_dict of a new model.
         new_children_layer = self.get_layer()
         new_model = CustomModule(new_children_layer)
         new_model.load_state_dict(model.state_dict())
 
         # Compare the new model weights and biases.
-        (new_model_weights, new_model_biases, new_tile_weights, new_tile_biases, _) = \
-            self.get_layer_and_tile_weights(new_children_layer)
+        (
+            new_model_weights,
+            new_model_biases,
+            new_tile_weights,
+            new_tile_biases,
+            _,
+        ) = self.get_layer_and_tile_weights(new_children_layer)
 
         assert_array_almost_equal(model_weights, new_model_weights)
         assert_array_almost_equal(tile_weights, new_tile_weights)
         if self.bias:
             assert_array_almost_equal(model_biases, new_model_biases)
             assert_array_almost_equal(tile_biases, new_tile_biases)
 
     def test_state_dict_analog_strict(self):
         """Test the `strict` flag for analog layers."""
         model = self.get_layer()
         state_dict = model.state_dict()
 
         # Remove the analog key from the state dict.
-        lst = [key for key in state_dict.keys() if key.startswith(model.ANALOG_STATE_PREFIX)]
+        lst = [
+            key
+            for key in state_dict.keys()
+            if next(model.analog_tiles()).get_analog_state_name("") in key
+        ]
         del state_dict[lst[0]]
 
         # Check that it fails when using `strict`.
         with self.assertRaises(RuntimeError) as context:
             model.load_state_dict(state_dict, strict=True)
-        self.assertIn('Missing key', str(context.exception))
+        self.assertIn("Missing key", str(context.exception))
 
         # Check that it passes when not using `strict`.
         model.load_state_dict(state_dict, strict=False)
 
     def test_state_dict(self):
         """Test creating a new model using a state dict, without saving to disk."""
         model = self.get_layer()
@@ -622,44 +740,42 @@
         analog_tile = self.get_analog_tile(model)
 
         new_model = self.get_layer()
         new_model.load_state_dict(state_dict)
         new_analog_tile = self.get_analog_tile(new_model)
 
         # Asserts over the AnalogContext of the new model.
-        self.assertTrue(hasattr(new_analog_tile.analog_ctx, 'analog_tile'))
-        self.assertIsInstance(new_analog_tile.analog_ctx.analog_tile,
-                              analog_tile.__class__)
+        self.assertTrue(hasattr(new_analog_tile.analog_ctx, "analog_tile"))
+        self.assertIsInstance(new_analog_tile.analog_ctx.analog_tile, analog_tile.__class__)
 
     def test_hidden_parameter_mismatch(self):
         """Test for error if tile structure mismatches."""
         model = self.get_layer()
         state_dict = model.state_dict()
         analog_tile = self.get_analog_tile(model)
 
         # Create the device and the array.
-        rpu_config = SingleRPUConfig(
-            device=LinearStepDevice()  # different hidden structure
-        )
+        rpu_config = SingleRPUConfig(device=LinearStepDevice())  # different hidden structure
 
         new_model = self.get_layer(rpu_config=rpu_config)
         new_analog_tile = self.get_analog_tile(new_model)
+
         if new_analog_tile.__class__.__name__ != analog_tile.__class__.__name__:
             with self.assertRaises(TileError):
                 new_model.load_state_dict(state_dict)
 
     def test_load_state_load_rpu_config(self):
         """Test creating a new model using a state dict, while using a different RPU config."""
 
         # Create the device and the array.
         rpu_config_org = self.get_rpu_config()
 
-        # Skipped for FP
-        if isinstance(rpu_config_org, FloatingPointRPUConfig):
-            raise SkipTest('Not available for FP')
+        # Skipped for some
+        if self.tile_class in SKIP_META_PARAM_TILES:
+            raise SkipTest("Not available")
 
         rpu_config_org.forward.is_perfect = False
         old_value = 0.11
         rpu_config_org.forward.inp_noise = old_value
 
         model = self.get_layer(rpu_config=rpu_config_org)
         state_dict = model.state_dict()
@@ -669,34 +785,34 @@
         rpu_config.forward.inp_noise = new_value
 
         # Test restore_rpu_config=False
         new_model = self.get_layer(rpu_config=rpu_config)
         new_model.load_state_dict(state_dict, load_rpu_config=False)
         new_analog_tile = self.get_analog_tile(new_model)
 
-        parameters = new_analog_tile.tile.get_parameters()
+        parameters = new_analog_tile.tile.get_meta_parameters()
         self.assertAlmostEqual(parameters.forward_io.inp_noise, new_value)
 
         # Test restore_rpu_config=True
         new_model = self.get_layer(rpu_config=rpu_config)
         new_model.load_state_dict(state_dict, load_rpu_config=True)
         new_analog_tile = self.get_analog_tile(new_model)
 
-        parameters = new_analog_tile.tile.get_parameters()
+        parameters = new_analog_tile.tile.get_meta_parameters()
         self.assertAlmostEqual(parameters.forward_io.inp_noise, old_value)
 
     def test_load_state_load_rpu_config_sequential(self):
         """Test creating a new model using a state dict, while using a different RPU config."""
 
         # Create the device and the array.
         rpu_config_org = self.get_rpu_config()
 
-        # Skipped for FP
-        if isinstance(rpu_config_org, FloatingPointRPUConfig):
-            raise SkipTest('Not available for FP')
+        # Skipped for some
+        if self.tile_class in SKIP_META_PARAM_TILES:
+            raise SkipTest("Not available")
 
         rpu_config_org.forward.is_perfect = False
         old_value = 0.11
         rpu_config_org.forward.inp_noise = old_value
 
         model = AnalogSequential(self.get_layer(rpu_config=rpu_config_org))
         state_dict = model.state_dict()
@@ -706,46 +822,46 @@
         rpu_config.forward.inp_noise = new_value
 
         # Test restore_rpu_config=False
         new_model = AnalogSequential(self.get_layer(rpu_config=rpu_config))
         new_model.load_state_dict(state_dict, load_rpu_config=False)
         new_analog_tile = self.get_analog_tile(new_model[0])
 
-        parameters = new_analog_tile.tile.get_parameters()
+        parameters = new_analog_tile.tile.get_meta_parameters()
         self.assertAlmostEqual(parameters.forward_io.inp_noise, new_value)
 
         # Test restore_rpu_config=True
         new_model = AnalogSequential(self.get_layer(rpu_config=rpu_config))
         new_model.load_state_dict(state_dict, load_rpu_config=True)
         new_analog_tile = self.get_analog_tile(new_model[0])
 
-        parameters = new_analog_tile.tile.get_parameters()
+        parameters = new_analog_tile.tile.get_meta_parameters()
         self.assertAlmostEqual(parameters.forward_io.inp_noise, old_value)
 
     def test_load_state_load_rpu_config_wrong(self):
         """Test creating a new model using a state dict, while using a different RPU config."""
 
         # Skipped for FP
         if isinstance(self.get_rpu_config(), FloatingPointRPUConfig):
-            raise SkipTest('Not available for FP')
+            raise SkipTest("Not available for FP")
 
         # Create the device and the array.
         model = self.get_layer()
         state_dict = model.state_dict()
 
         rpu_config = FloatingPointRPUConfig()
 
         new_model = self.get_layer(rpu_config=rpu_config)
-        assert_raises(ModuleError, new_model.load_state_dict, state_dict, load_rpu_config=False)
+        assert_raises(TileModuleError, new_model.load_state_dict, state_dict, load_rpu_config=False)
 
 
 @parametrize_over_layers(
     layers=[Linear, LinearCuda],
-    tiles=[FloatingPoint],
-    biases=[None],
+    tiles=[FloatingPoint, Inference, TorchInference],
+    biases=["digital"],
 )
 class SerializationTestExtended(ParametrizedTestCase):
     """Tests for serialization."""
 
     @staticmethod
     def train_model_torch(model, loss_func, x_b, y_b):
         """Train the model with torch SGD."""
@@ -757,23 +873,20 @@
             loss = loss_func(pred, y_b)
 
             loss.backward()
             opt.step()
 
     @staticmethod
     def get_torch_model(use_cuda: bool):
-        """ Returns a torch model."""
+        """Returns a torch model."""
         manual_seed(4321)
         torch_model = Sequential(
             torch_linear(4, 3),
             torch_linear(3, 3),
-            Sequential(
-                torch_linear(3, 1),
-                torch_linear(1, 1)
-            )
+            Sequential(torch_linear(3, 1), torch_linear(1, 1)),
         )
         if use_cuda:
             torch_model.cuda()
         return torch_model
 
     def test_load_state_dict_conversion(self):
         """Test loading and setting conversion with alpha."""
@@ -800,20 +913,18 @@
             model = self.get_torch_model(self.use_cuda)
             new_analog_model = convert_to_analog(model, self.get_rpu_config())
             state_dict = load(file)
             new_analog_model.load_state_dict(state_dict, load_rpu_config=True)
 
         new_state_dict = new_analog_model.state_dict()
         for key in new_state_dict.keys():
-
-            if not key.endswith('analog_tile_state'):
+            if not key.endswith("analog_tile_state"):
                 continue
 
             state1 = new_state_dict[key]
             state2 = state_dict[key]
-            assert_array_almost_equal(state1['analog_tile_weights'],
-                                      state2['analog_tile_weights'])
-            assert_array_almost_equal(state1['analog_alpha_scale'],
-                                      state2['analog_alpha_scale'])
+            assert_array_almost_equal(state1["analog_tile_weights"], state2["analog_tile_weights"])
+            # assert_array_almost_equal(state1['analog_alpha_scale'],
+            #                           state2['analog_alpha_scale'])
 
         new_analog_loss = mse_loss(new_analog_model(x_b), y_b)
         self.assertTensorAlmostEqual(new_analog_loss, analog_loss)
```

