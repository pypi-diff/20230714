# Comparing `tmp/snowflake_ml_python-1.0.2-py3-none-any.whl.zip` & `tmp/snowflake_ml_python-1.0.3-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,248 +1,261 @@
-Zip file size: 1855114 bytes, number of entries: 246
--rw-r--r--  2.0 unx      161 b- defN 23-Jun-23 01:55 snowflake/ml/_internal/env.py
--rw-r--r--  2.0 unx    14126 b- defN 23-Jun-23 01:55 snowflake/ml/_internal/env_utils.py
--rw-r--r--  2.0 unx     6353 b- defN 23-Jun-23 01:55 snowflake/ml/_internal/file_utils.py
--rw-r--r--  2.0 unx     2696 b- defN 23-Jun-23 01:55 snowflake/ml/_internal/init_utils.py
--rw-r--r--  2.0 unx    20238 b- defN 23-Jun-23 01:55 snowflake/ml/_internal/telemetry.py
--rw-r--r--  2.0 unx     2168 b- defN 23-Jun-23 01:55 snowflake/ml/_internal/type_utils.py
--rw-r--r--  2.0 unx     3678 b- defN 23-Jun-23 01:55 snowflake/ml/_internal/utils/formatting.py
--rw-r--r--  2.0 unx     7703 b- defN 23-Jun-23 01:55 snowflake/ml/_internal/utils/identifier.py
--rw-r--r--  2.0 unx     2068 b- defN 23-Jun-23 01:55 snowflake/ml/_internal/utils/import_utils.py
--rw-r--r--  2.0 unx     4550 b- defN 23-Jun-23 01:55 snowflake/ml/_internal/utils/parallelize.py
--rw-r--r--  2.0 unx     3722 b- defN 23-Jun-23 01:55 snowflake/ml/_internal/utils/pkg_version_utils.py
--rw-r--r--  2.0 unx    12205 b- defN 23-Jun-23 01:55 snowflake/ml/_internal/utils/query_result_checker.py
--rw-r--r--  2.0 unx     1400 b- defN 23-Jun-23 01:55 snowflake/ml/_internal/utils/temp_file_utils.py
--rw-r--r--  2.0 unx     1841 b- defN 23-Jun-23 01:55 snowflake/ml/_internal/utils/uri.py
--rw-r--r--  2.0 unx    26746 b- defN 23-Jun-23 01:55 snowflake/ml/fileset/fileset.py
--rw-r--r--  2.0 unx     1040 b- defN 23-Jun-23 01:55 snowflake/ml/fileset/fileset_errors.py
--rw-r--r--  2.0 unx     5915 b- defN 23-Jun-23 01:55 snowflake/ml/fileset/parquet_parser.py
--rw-r--r--  2.0 unx    11536 b- defN 23-Jun-23 01:55 snowflake/ml/fileset/sfcfs.py
--rw-r--r--  2.0 unx    14859 b- defN 23-Jun-23 01:55 snowflake/ml/fileset/stage_fs.py
--rw-r--r--  2.0 unx     3462 b- defN 23-Jun-23 01:55 snowflake/ml/fileset/tf_dataset.py
--rw-r--r--  2.0 unx     2386 b- defN 23-Jun-23 01:55 snowflake/ml/fileset/torch_datapipe.py
--r-xr-xr-x  2.0 unx      197 b- defN 23-Jun-23 01:58 snowflake/ml/model/_core_requirements.py
--rw-r--r--  2.0 unx     7962 b- defN 23-Jun-23 01:55 snowflake/ml/model/_deploy_client/warehouse/deploy.py
--rw-r--r--  2.0 unx     2369 b- defN 23-Jun-23 01:55 snowflake/ml/model/_deploy_client/warehouse/infer_template.py
--rw-r--r--  2.0 unx     9548 b- defN 23-Jun-23 01:55 snowflake/ml/model/_deployer.py
--rw-r--r--  2.0 unx     4488 b- defN 23-Jun-23 01:55 snowflake/ml/model/_env.py
--rw-r--r--  2.0 unx     2299 b- defN 23-Jun-23 01:55 snowflake/ml/model/_handlers/_base.py
--rw-r--r--  2.0 unx     6084 b- defN 23-Jun-23 01:55 snowflake/ml/model/_handlers/custom.py
--rw-r--r--  2.0 unx     7558 b- defN 23-Jun-23 01:55 snowflake/ml/model/_handlers/sklearn.py
--rw-r--r--  2.0 unx     7711 b- defN 23-Jun-23 01:55 snowflake/ml/model/_handlers/snowmlmodel.py
--rw-r--r--  2.0 unx     7204 b- defN 23-Jun-23 01:55 snowflake/ml/model/_handlers/xgboost.py
--rw-r--r--  2.0 unx    26448 b- defN 23-Jun-23 01:55 snowflake/ml/model/_model.py
--rw-r--r--  2.0 unx     2101 b- defN 23-Jun-23 01:55 snowflake/ml/model/_model_handler.py
--rw-r--r--  2.0 unx    17684 b- defN 23-Jun-23 01:55 snowflake/ml/model/_model_meta.py
--rw-r--r--  2.0 unx     8016 b- defN 23-Jun-23 01:55 snowflake/ml/model/custom_model.py
--rw-r--r--  2.0 unx    43624 b- defN 23-Jun-23 01:55 snowflake/ml/model/model_signature.py
--rw-r--r--  2.0 unx     4405 b- defN 23-Jun-23 01:55 snowflake/ml/model/type_hints.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/calibration/__init__.py
--r-xr-xr-x  2.0 unx    54140 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/calibration/calibrated_classifier_cv.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/cluster/__init__.py
--r-xr-xr-x  2.0 unx    52067 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/cluster/affinity_propagation.py
--r-xr-xr-x  2.0 unx    54080 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/cluster/agglomerative_clustering.py
--r-xr-xr-x  2.0 unx    51905 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/cluster/birch.py
--r-xr-xr-x  2.0 unx    54287 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/cluster/bisecting_k_means.py
--r-xr-xr-x  2.0 unx    52246 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/cluster/dbscan.py
--r-xr-xr-x  2.0 unx    54620 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/cluster/feature_agglomeration.py
--r-xr-xr-x  2.0 unx    53874 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/cluster/k_means.py
--r-xr-xr-x  2.0 unx    52448 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/cluster/mean_shift.py
--r-xr-xr-x  2.0 unx    55149 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/cluster/mini_batch_k_means.py
--r-xr-xr-x  2.0 unx    55580 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/cluster/optics.py
--r-xr-xr-x  2.0 unx    52638 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/cluster/spectral_biclustering.py
--r-xr-xr-x  2.0 unx    55576 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/cluster/spectral_clustering.py
--r-xr-xr-x  2.0 unx    51768 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/cluster/spectral_coclustering.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/compose/__init__.py
--r-xr-xr-x  2.0 unx    54351 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/compose/column_transformer.py
--r-xr-xr-x  2.0 unx    51936 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/compose/transformed_target_regressor.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/covariance/__init__.py
--r-xr-xr-x  2.0 unx    51908 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/covariance/elliptic_envelope.py
--r-xr-xr-x  2.0 unx    50184 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/covariance/empirical_covariance.py
--r-xr-xr-x  2.0 unx    51458 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/covariance/graphical_lasso.py
--r-xr-xr-x  2.0 unx    52922 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/covariance/graphical_lasso_cv.py
--r-xr-xr-x  2.0 unx    50386 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/covariance/ledoit_wolf.py
--r-xr-xr-x  2.0 unx    51149 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/covariance/min_cov_det.py
--r-xr-xr-x  2.0 unx    50075 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/covariance/oas.py
--r-xr-xr-x  2.0 unx    50361 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/covariance/shrunk_covariance.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/decomposition/__init__.py
--r-xr-xr-x  2.0 unx    55176 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/decomposition/dictionary_learning.py
--r-xr-xr-x  2.0 unx    52548 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/decomposition/factor_analysis.py
--r-xr-xr-x  2.0 unx    53010 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/decomposition/fast_ica.py
--r-xr-xr-x  2.0 unx    51345 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/decomposition/incremental_pca.py
--r-xr-xr-x  2.0 unx    55376 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/decomposition/kernel_pca.py
--r-xr-xr-x  2.0 unx    56359 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/decomposition/mini_batch_dictionary_learning.py
--r-xr-xr-x  2.0 unx    53676 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/decomposition/mini_batch_sparse_pca.py
--r-xr-xr-x  2.0 unx    54220 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/decomposition/pca.py
--r-xr-xr-x  2.0 unx    52541 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/decomposition/sparse_pca.py
--r-xr-xr-x  2.0 unx    52113 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/decomposition/truncated_svd.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/discriminant_analysis/__init__.py
--r-xr-xr-x  2.0 unx    54361 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/discriminant_analysis/linear_discriminant_analysis.py
--r-xr-xr-x  2.0 unx    52426 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/discriminant_analysis/quadratic_discriminant_analysis.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/ensemble/__init__.py
--r-xr-xr-x  2.0 unx    53379 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/ensemble/ada_boost_classifier.py
--r-xr-xr-x  2.0 unx    52278 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/ensemble/ada_boost_regressor.py
--r-xr-xr-x  2.0 unx    54303 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/ensemble/bagging_classifier.py
--r-xr-xr-x  2.0 unx    53547 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/ensemble/bagging_regressor.py
--r-xr-xr-x  2.0 unx    59089 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/ensemble/extra_trees_classifier.py
--r-xr-xr-x  2.0 unx    57700 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/ensemble/extra_trees_regressor.py
--r-xr-xr-x  2.0 unx    60698 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/ensemble/gradient_boosting_classifier.py
--r-xr-xr-x  2.0 unx    60282 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/ensemble/gradient_boosting_regressor.py
--r-xr-xr-x  2.0 unx    60348 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/ensemble/hist_gradient_boosting_classifier.py
--r-xr-xr-x  2.0 unx    58670 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/ensemble/hist_gradient_boosting_regressor.py
--r-xr-xr-x  2.0 unx    53324 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/ensemble/isolation_forest.py
--r-xr-xr-x  2.0 unx    59044 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/ensemble/random_forest_classifier.py
--r-xr-xr-x  2.0 unx    57643 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/ensemble/random_forest_regressor.py
--r-xr-xr-x  2.0 unx    53231 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/ensemble/stacking_regressor.py
--r-xr-xr-x  2.0 unx    52806 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/ensemble/voting_classifier.py
--r-xr-xr-x  2.0 unx    51341 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/ensemble/voting_regressor.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/feature_selection/__init__.py
--r-xr-xr-x  2.0 unx    50854 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/feature_selection/generic_univariate_select.py
--r-xr-xr-x  2.0 unx    50552 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/feature_selection/select_fdr.py
--r-xr-xr-x  2.0 unx    50546 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/feature_selection/select_fpr.py
--r-xr-xr-x  2.0 unx    50554 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/feature_selection/select_fwe.py
--r-xr-xr-x  2.0 unx    50631 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/feature_selection/select_k_best.py
--r-xr-xr-x  2.0 unx    50651 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/feature_selection/select_percentile.py
--r-xr-xr-x  2.0 unx    53304 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/feature_selection/sequential_feature_selector.py
--r-xr-xr-x  2.0 unx    50283 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/feature_selection/variance_threshold.py
--rw-r--r--  2.0 unx     9110 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/framework/_utils.py
--rw-r--r--  2.0 unx    21900 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/framework/base.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/gaussian_process/__init__.py
--r-xr-xr-x  2.0 unx    55849 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/gaussian_process/gaussian_process_classifier.py
--r-xr-xr-x  2.0 unx    54541 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/gaussian_process/gaussian_process_regressor.py
--rw-r--r--  2.0 unx      298 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/impute/__init__.py
--r-xr-xr-x  2.0 unx    56404 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/impute/iterative_imputer.py
--r-xr-xr-x  2.0 unx    52626 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/impute/knn_imputer.py
--r-xr-xr-x  2.0 unx    51423 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/impute/missing_indicator.py
--rw-r--r--  2.0 unx    18118 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/impute/simple_imputer.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/kernel_approximation/__init__.py
--r-xr-xr-x  2.0 unx    50367 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/kernel_approximation/additive_chi2_sampler.py
--r-xr-xr-x  2.0 unx    52240 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/kernel_approximation/nystroem.py
--r-xr-xr-x  2.0 unx    51394 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/kernel_approximation/polynomial_count_sketch.py
--r-xr-xr-x  2.0 unx    50823 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/kernel_approximation/rbf_sampler.py
--r-xr-xr-x  2.0 unx    50822 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/kernel_approximation/skewed_chi2_sampler.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/kernel_ridge/__init__.py
--r-xr-xr-x  2.0 unx    52340 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/kernel_ridge/kernel_ridge.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/lightgbm/__init__.py
--r-xr-xr-x  2.0 unx    51862 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/lightgbm/lgbm_classifier.py
--r-xr-xr-x  2.0 unx    51373 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/lightgbm/lgbm_regressor.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/linear_model/__init__.py
--r-xr-xr-x  2.0 unx    52088 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/linear_model/ard_regression.py
--r-xr-xr-x  2.0 unx    52401 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/linear_model/bayesian_ridge.py
--r-xr-xr-x  2.0 unx    53285 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/linear_model/elastic_net.py
--r-xr-xr-x  2.0 unx    54543 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/linear_model/elastic_net_cv.py
--r-xr-xr-x  2.0 unx    52341 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/linear_model/gamma_regressor.py
--r-xr-xr-x  2.0 unx    51529 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/linear_model/huber_regressor.py
--r-xr-xr-x  2.0 unx    52826 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/linear_model/lars.py
--r-xr-xr-x  2.0 unx    53033 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/linear_model/lars_cv.py
--r-xr-xr-x  2.0 unx    52925 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/linear_model/lasso.py
--r-xr-xr-x  2.0 unx    53700 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/linear_model/lasso_cv.py
--r-xr-xr-x  2.0 unx    53929 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/linear_model/lasso_lars.py
--r-xr-xr-x  2.0 unx    53875 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/linear_model/lasso_lars_cv.py
--r-xr-xr-x  2.0 unx    53220 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/linear_model/lasso_lars_ic.py
--r-xr-xr-x  2.0 unx    51055 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/linear_model/linear_regression.py
--r-xr-xr-x  2.0 unx    57306 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/linear_model/logistic_regression.py
--r-xr-xr-x  2.0 unx    58326 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/linear_model/logistic_regression_cv.py
--r-xr-xr-x  2.0 unx    52511 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/linear_model/multi_task_elastic_net.py
--r-xr-xr-x  2.0 unx    54139 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/linear_model/multi_task_elastic_net_cv.py
--r-xr-xr-x  2.0 unx    52093 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/linear_model/multi_task_lasso.py
--r-xr-xr-x  2.0 unx    53345 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/linear_model/multi_task_lasso_cv.py
--r-xr-xr-x  2.0 unx    51620 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/linear_model/orthogonal_matching_pursuit.py
--r-xr-xr-x  2.0 unx    54971 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/linear_model/passive_aggressive_classifier.py
--r-xr-xr-x  2.0 unx    54046 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/linear_model/passive_aggressive_regressor.py
--r-xr-xr-x  2.0 unx    54476 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/linear_model/perceptron.py
--r-xr-xr-x  2.0 unx    52372 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/linear_model/poisson_regressor.py
--r-xr-xr-x  2.0 unx    55846 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/linear_model/ransac_regressor.py
--r-xr-xr-x  2.0 unx    53906 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/linear_model/ridge.py
--r-xr-xr-x  2.0 unx    54224 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/linear_model/ridge_classifier.py
--r-xr-xr-x  2.0 unx    52763 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/linear_model/ridge_classifier_cv.py
--r-xr-xr-x  2.0 unx    53539 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/linear_model/ridge_cv.py
--r-xr-xr-x  2.0 unx    59892 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/linear_model/sgd_classifier.py
--r-xr-xr-x  2.0 unx    54506 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/linear_model/sgd_one_class_svm.py
--r-xr-xr-x  2.0 unx    57361 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/linear_model/sgd_regressor.py
--r-xr-xr-x  2.0 unx    52794 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/linear_model/theil_sen_regressor.py
--r-xr-xr-x  2.0 unx    53765 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/linear_model/tweedie_regressor.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/manifold/__init__.py
--r-xr-xr-x  2.0 unx    53160 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/manifold/isomap.py
--r-xr-xr-x  2.0 unx    52378 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/manifold/mds.py
--r-xr-xr-x  2.0 unx    53149 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/manifold/spectral_embedding.py
--r-xr-xr-x  2.0 unx    56421 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/manifold/tsne.py
--rw-r--r--  2.0 unx      304 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/metrics/__init__.py
--rw-r--r--  2.0 unx    40077 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/metrics/classification.py
--rw-r--r--  2.0 unx     4921 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/metrics/correlation.py
--rw-r--r--  2.0 unx     4757 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/metrics/covariance.py
--rw-r--r--  2.0 unx    12037 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/metrics/metrics_utils.py
--rw-r--r--  2.0 unx    15397 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/metrics/ranking.py
--rw-r--r--  2.0 unx    23144 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/metrics/regression.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/mixture/__init__.py
--r-xr-xr-x  2.0 unx    57065 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/mixture/bayesian_gaussian_mixture.py
--r-xr-xr-x  2.0 unx    55067 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/mixture/gaussian_mixture.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/model_selection/__init__.py
--r-xr-xr-x  2.0 unx    57616 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/model_selection/grid_search_cv.py
--r-xr-xr-x  2.0 unx    58460 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/model_selection/randomized_search_cv.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/multiclass/__init__.py
--r-xr-xr-x  2.0 unx    51048 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/multiclass/one_vs_one_classifier.py
--r-xr-xr-x  2.0 unx    51976 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/multiclass/one_vs_rest_classifier.py
--r-xr-xr-x  2.0 unx    51306 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/multiclass/output_code_classifier.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/naive_bayes/__init__.py
--r-xr-xr-x  2.0 unx    51633 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/naive_bayes/bernoulli_nb.py
--r-xr-xr-x  2.0 unx    51954 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/naive_bayes/categorical_nb.py
--r-xr-xr-x  2.0 unx    51641 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/naive_bayes/complement_nb.py
--r-xr-xr-x  2.0 unx    50781 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/naive_bayes/gaussian_nb.py
--r-xr-xr-x  2.0 unx    51398 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/naive_bayes/multinomial_nb.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/neighbors/__init__.py
--r-xr-xr-x  2.0 unx    54185 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/neighbors/k_neighbors_classifier.py
--r-xr-xr-x  2.0 unx    53667 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/neighbors/k_neighbors_regressor.py
--r-xr-xr-x  2.0 unx    52144 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/neighbors/kernel_density.py
--r-xr-xr-x  2.0 unx    54425 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/neighbors/local_outlier_factor.py
--r-xr-xr-x  2.0 unx    50948 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/neighbors/nearest_centroid.py
--r-xr-xr-x  2.0 unx    52857 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/neighbors/nearest_neighbors.py
--r-xr-xr-x  2.0 unx    54333 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/neighbors/neighborhood_components_analysis.py
--r-xr-xr-x  2.0 unx    54814 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/neighbors/radius_neighbors_classifier.py
--r-xr-xr-x  2.0 unx    53700 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/neighbors/radius_neighbors_regressor.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/neural_network/__init__.py
--r-xr-xr-x  2.0 unx    51349 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/neural_network/bernoulli_rbm.py
--r-xr-xr-x  2.0 unx    58847 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/neural_network/mlp_classifier.py
--r-xr-xr-x  2.0 unx    58124 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/neural_network/mlp_regressor.py
--rw-r--r--  2.0 unx      298 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/pipeline/__init__.py
--rw-r--r--  2.0 unx    23381 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/pipeline/pipeline.py
--rw-r--r--  2.0 unx      298 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/preprocessing/__init__.py
--rw-r--r--  2.0 unx     6092 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/preprocessing/binarizer.py
--rw-r--r--  2.0 unx    20422 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/preprocessing/k_bins_discretizer.py
--rw-r--r--  2.0 unx     6285 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/preprocessing/label_encoder.py
--rw-r--r--  2.0 unx     8491 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/preprocessing/max_abs_scaler.py
--rw-r--r--  2.0 unx    10716 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/preprocessing/min_max_scaler.py
--rw-r--r--  2.0 unx     5951 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/preprocessing/normalizer.py
--rw-r--r--  2.0 unx    66998 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/preprocessing/one_hot_encoder.py
--rw-r--r--  2.0 unx    27848 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/preprocessing/ordinal_encoder.py
--r-xr-xr-x  2.0 unx    51483 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/preprocessing/polynomial_features.py
--rw-r--r--  2.0 unx    11981 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/preprocessing/robust_scaler.py
--rw-r--r--  2.0 unx    10672 b- defN 23-Jun-23 01:55 snowflake/ml/modeling/preprocessing/standard_scaler.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/semi_supervised/__init__.py
--r-xr-xr-x  2.0 unx    51820 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/semi_supervised/label_propagation.py
--r-xr-xr-x  2.0 unx    52184 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/semi_supervised/label_spreading.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/svm/__init__.py
--r-xr-xr-x  2.0 unx    54362 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/svm/linear_svc.py
--r-xr-xr-x  2.0 unx    52777 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/svm/linear_svr.py
--r-xr-xr-x  2.0 unx    55076 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/svm/nu_svc.py
--r-xr-xr-x  2.0 unx    52152 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/svm/nu_svr.py
--r-xr-xr-x  2.0 unx    55239 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/svm/svc.py
--r-xr-xr-x  2.0 unx    52355 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/svm/svr.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/tree/__init__.py
--r-xr-xr-x  2.0 unx    57438 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/tree/decision_tree_classifier.py
--r-xr-xr-x  2.0 unx    56134 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/tree/decision_tree_regressor.py
--r-xr-xr-x  2.0 unx    56801 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/tree/extra_tree_classifier.py
--r-xr-xr-x  2.0 unx    55506 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/tree/extra_tree_regressor.py
--r-xr-xr-x  2.0 unx      297 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/xgboost/__init__.py
--r-xr-xr-x  2.0 unx    61208 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/xgboost/xgb_classifier.py
--r-xr-xr-x  2.0 unx    60714 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/xgboost/xgb_regressor.py
--r-xr-xr-x  2.0 unx    61372 b- defN 23-Jun-23 01:58 snowflake/ml/modeling/xgboost/xgbrf_classifier.py
--r-xr-xr-x  2.0 unx    60905 b- defN 23-Jun-23 01:59 snowflake/ml/modeling/xgboost/xgbrf_regressor.py
--rw-r--r--  2.0 unx     1381 b- defN 23-Jun-23 01:55 snowflake/ml/registry/_schema.py
--rw-r--r--  2.0 unx    84697 b- defN 23-Jun-23 01:55 snowflake/ml/registry/model_registry.py
--rw-r--r--  2.0 unx     6138 b- defN 23-Jun-23 01:55 snowflake/ml/utils/connection_params.py
--rw-r--r--  2.0 unx     3893 b- defN 23-Jun-23 01:55 snowflake/ml/utils/sparse.py
--r-xr-xr-x  2.0 unx       16 b- defN 23-Jun-23 01:58 snowflake/ml/version.py
-?rw-------  2.0 unx       91 b- defN 23-Jun-23 01:59 snowflake_ml_python-1.0.2.dist-info/WHEEL
-?rw-------  2.0 unx    11756 b- defN 23-Jun-23 01:59 snowflake_ml_python-1.0.2.dist-info/METADATA
-?rw-------  2.0 unx    26137 b- defN 23-Jun-23 01:59 snowflake_ml_python-1.0.2.dist-info/RECORD
-246 files, 9029188 bytes uncompressed, 1812344 bytes compressed:  79.9%
+Zip file size: 1957662 bytes, number of entries: 259
+-rw-r--r--  2.0 unx      161 b- defN 23-Jul-14 18:30 snowflake/ml/_internal/env.py
+-rw-r--r--  2.0 unx    14175 b- defN 23-Jul-14 18:30 snowflake/ml/_internal/env_utils.py
+-rw-r--r--  2.0 unx     7502 b- defN 23-Jul-14 18:30 snowflake/ml/_internal/file_utils.py
+-rw-r--r--  2.0 unx     2696 b- defN 23-Jul-14 18:30 snowflake/ml/_internal/init_utils.py
+-rw-r--r--  2.0 unx    20145 b- defN 23-Jul-14 18:30 snowflake/ml/_internal/telemetry.py
+-rw-r--r--  2.0 unx     2168 b- defN 23-Jul-14 18:30 snowflake/ml/_internal/type_utils.py
+-rw-r--r--  2.0 unx     3678 b- defN 23-Jul-14 18:30 snowflake/ml/_internal/utils/formatting.py
+-rw-r--r--  2.0 unx     7703 b- defN 23-Jul-14 18:30 snowflake/ml/_internal/utils/identifier.py
+-rw-r--r--  2.0 unx     2068 b- defN 23-Jul-14 18:30 snowflake/ml/_internal/utils/import_utils.py
+-rw-r--r--  2.0 unx     4550 b- defN 23-Jul-14 18:30 snowflake/ml/_internal/utils/parallelize.py
+-rw-r--r--  2.0 unx     3722 b- defN 23-Jul-14 18:30 snowflake/ml/_internal/utils/pkg_version_utils.py
+-rw-r--r--  2.0 unx    12205 b- defN 23-Jul-14 18:30 snowflake/ml/_internal/utils/query_result_checker.py
+-rw-r--r--  2.0 unx     1400 b- defN 23-Jul-14 18:30 snowflake/ml/_internal/utils/temp_file_utils.py
+-rw-r--r--  2.0 unx     2117 b- defN 23-Jul-14 18:30 snowflake/ml/_internal/utils/uri.py
+-rw-r--r--  2.0 unx    26746 b- defN 23-Jul-14 18:30 snowflake/ml/fileset/fileset.py
+-rw-r--r--  2.0 unx     1040 b- defN 23-Jul-14 18:30 snowflake/ml/fileset/fileset_errors.py
+-rw-r--r--  2.0 unx     5915 b- defN 23-Jul-14 18:30 snowflake/ml/fileset/parquet_parser.py
+-rw-r--r--  2.0 unx    11536 b- defN 23-Jul-14 18:30 snowflake/ml/fileset/sfcfs.py
+-rw-r--r--  2.0 unx    14859 b- defN 23-Jul-14 18:30 snowflake/ml/fileset/stage_fs.py
+-rw-r--r--  2.0 unx     3462 b- defN 23-Jul-14 18:30 snowflake/ml/fileset/tf_dataset.py
+-rw-r--r--  2.0 unx     2386 b- defN 23-Jul-14 18:30 snowflake/ml/fileset/torch_datapipe.py
+-r-xr-xr-x  2.0 unx      197 b- defN 23-Jul-14 18:32 snowflake/ml/model/_core_requirements.py
+-rw-r--r--  2.0 unx      345 b- defN 23-Jul-14 18:30 snowflake/ml/model/_deploy_client/image_builds/base_image_builder.py
+-rw-r--r--  2.0 unx    11239 b- defN 23-Jul-14 18:30 snowflake/ml/model/_deploy_client/image_builds/client_image_builder.py
+-rw-r--r--  2.0 unx     3998 b- defN 23-Jul-14 18:30 snowflake/ml/model/_deploy_client/image_builds/docker_context.py
+-rw-r--r--  2.0 unx      887 b- defN 23-Jul-14 18:30 snowflake/ml/model/_deploy_client/image_builds/gunicorn_run.sh
+-rw-r--r--  2.0 unx     4894 b- defN 23-Jul-14 18:30 snowflake/ml/model/_deploy_client/image_builds/inference_server/main.py
+-rw-r--r--  2.0 unx     1264 b- defN 23-Jul-14 18:30 snowflake/ml/model/_deploy_client/image_builds/templates/dockerfile_template
+-rw-r--r--  2.0 unx     8602 b- defN 23-Jul-14 18:30 snowflake/ml/model/_deploy_client/snowservice/deploy.py
+-rw-r--r--  2.0 unx     3801 b- defN 23-Jul-14 18:30 snowflake/ml/model/_deploy_client/snowservice/deploy_options.py
+-rw-r--r--  2.0 unx      579 b- defN 23-Jul-14 18:30 snowflake/ml/model/_deploy_client/snowservice/templates/service_spec_template
+-rw-r--r--  2.0 unx     1470 b- defN 23-Jul-14 18:30 snowflake/ml/model/_deploy_client/utils/constants.py
+-rw-r--r--  2.0 unx     7494 b- defN 23-Jul-14 18:30 snowflake/ml/model/_deploy_client/utils/snowservice_client.py
+-rw-r--r--  2.0 unx     9160 b- defN 23-Jul-14 18:30 snowflake/ml/model/_deploy_client/warehouse/deploy.py
+-rw-r--r--  2.0 unx     2489 b- defN 23-Jul-14 18:30 snowflake/ml/model/_deploy_client/warehouse/infer_template.py
+-rw-r--r--  2.0 unx     8855 b- defN 23-Jul-14 18:30 snowflake/ml/model/_deployer.py
+-rw-r--r--  2.0 unx     4560 b- defN 23-Jul-14 18:30 snowflake/ml/model/_env.py
+-rw-r--r--  2.0 unx     2299 b- defN 23-Jul-14 18:30 snowflake/ml/model/_handlers/_base.py
+-rw-r--r--  2.0 unx     6423 b- defN 23-Jul-14 18:30 snowflake/ml/model/_handlers/custom.py
+-rw-r--r--  2.0 unx     7166 b- defN 23-Jul-14 18:30 snowflake/ml/model/_handlers/pytorch.py
+-rw-r--r--  2.0 unx     7843 b- defN 23-Jul-14 18:30 snowflake/ml/model/_handlers/sklearn.py
+-rw-r--r--  2.0 unx     7993 b- defN 23-Jul-14 18:30 snowflake/ml/model/_handlers/snowmlmodel.py
+-rw-r--r--  2.0 unx     7287 b- defN 23-Jul-14 18:30 snowflake/ml/model/_handlers/torchscript.py
+-rw-r--r--  2.0 unx     7603 b- defN 23-Jul-14 18:30 snowflake/ml/model/_handlers/xgboost.py
+-rw-r--r--  2.0 unx    26469 b- defN 23-Jul-14 18:30 snowflake/ml/model/_model.py
+-rw-r--r--  2.0 unx     2101 b- defN 23-Jul-14 18:30 snowflake/ml/model/_model_handler.py
+-rw-r--r--  2.0 unx    17945 b- defN 23-Jul-14 18:30 snowflake/ml/model/_model_meta.py
+-rw-r--r--  2.0 unx     8016 b- defN 23-Jul-14 18:30 snowflake/ml/model/custom_model.py
+-rw-r--r--  2.0 unx    60533 b- defN 23-Jul-14 18:30 snowflake/ml/model/model_signature.py
+-rw-r--r--  2.0 unx     5187 b- defN 23-Jul-14 18:30 snowflake/ml/model/type_hints.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/calibration/__init__.py
+-r-xr-xr-x  2.0 unx    55506 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/calibration/calibrated_classifier_cv.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/cluster/__init__.py
+-r-xr-xr-x  2.0 unx    53433 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/cluster/affinity_propagation.py
+-r-xr-xr-x  2.0 unx    55446 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/cluster/agglomerative_clustering.py
+-r-xr-xr-x  2.0 unx    53271 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/cluster/birch.py
+-r-xr-xr-x  2.0 unx    55653 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/cluster/bisecting_k_means.py
+-r-xr-xr-x  2.0 unx    53612 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/cluster/dbscan.py
+-r-xr-xr-x  2.0 unx    55986 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/cluster/feature_agglomeration.py
+-r-xr-xr-x  2.0 unx    55240 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/cluster/k_means.py
+-r-xr-xr-x  2.0 unx    53814 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/cluster/mean_shift.py
+-r-xr-xr-x  2.0 unx    56515 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/cluster/mini_batch_k_means.py
+-r-xr-xr-x  2.0 unx    56946 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/cluster/optics.py
+-r-xr-xr-x  2.0 unx    54004 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/cluster/spectral_biclustering.py
+-r-xr-xr-x  2.0 unx    56942 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/cluster/spectral_clustering.py
+-r-xr-xr-x  2.0 unx    53134 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/cluster/spectral_coclustering.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/compose/__init__.py
+-r-xr-xr-x  2.0 unx    55717 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/compose/column_transformer.py
+-r-xr-xr-x  2.0 unx    53302 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/compose/transformed_target_regressor.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/covariance/__init__.py
+-r-xr-xr-x  2.0 unx    53274 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/covariance/elliptic_envelope.py
+-r-xr-xr-x  2.0 unx    51550 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/covariance/empirical_covariance.py
+-r-xr-xr-x  2.0 unx    52824 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/covariance/graphical_lasso.py
+-r-xr-xr-x  2.0 unx    54288 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/covariance/graphical_lasso_cv.py
+-r-xr-xr-x  2.0 unx    51752 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/covariance/ledoit_wolf.py
+-r-xr-xr-x  2.0 unx    52515 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/covariance/min_cov_det.py
+-r-xr-xr-x  2.0 unx    51441 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/covariance/oas.py
+-r-xr-xr-x  2.0 unx    51727 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/covariance/shrunk_covariance.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/decomposition/__init__.py
+-r-xr-xr-x  2.0 unx    56542 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/decomposition/dictionary_learning.py
+-r-xr-xr-x  2.0 unx    53914 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/decomposition/factor_analysis.py
+-r-xr-xr-x  2.0 unx    54376 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/decomposition/fast_ica.py
+-r-xr-xr-x  2.0 unx    52711 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/decomposition/incremental_pca.py
+-r-xr-xr-x  2.0 unx    56742 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/decomposition/kernel_pca.py
+-r-xr-xr-x  2.0 unx    57725 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/decomposition/mini_batch_dictionary_learning.py
+-r-xr-xr-x  2.0 unx    55042 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/decomposition/mini_batch_sparse_pca.py
+-r-xr-xr-x  2.0 unx    55586 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/decomposition/pca.py
+-r-xr-xr-x  2.0 unx    53907 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/decomposition/sparse_pca.py
+-r-xr-xr-x  2.0 unx    53479 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/decomposition/truncated_svd.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/discriminant_analysis/__init__.py
+-r-xr-xr-x  2.0 unx    55727 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/discriminant_analysis/linear_discriminant_analysis.py
+-r-xr-xr-x  2.0 unx    53792 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/discriminant_analysis/quadratic_discriminant_analysis.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/ensemble/__init__.py
+-r-xr-xr-x  2.0 unx    54745 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/ensemble/ada_boost_classifier.py
+-r-xr-xr-x  2.0 unx    53644 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/ensemble/ada_boost_regressor.py
+-r-xr-xr-x  2.0 unx    55669 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/ensemble/bagging_classifier.py
+-r-xr-xr-x  2.0 unx    54913 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/ensemble/bagging_regressor.py
+-r-xr-xr-x  2.0 unx    60455 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/ensemble/extra_trees_classifier.py
+-r-xr-xr-x  2.0 unx    59066 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/ensemble/extra_trees_regressor.py
+-r-xr-xr-x  2.0 unx    62064 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/ensemble/gradient_boosting_classifier.py
+-r-xr-xr-x  2.0 unx    61648 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/ensemble/gradient_boosting_regressor.py
+-r-xr-xr-x  2.0 unx    61714 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/ensemble/hist_gradient_boosting_classifier.py
+-r-xr-xr-x  2.0 unx    60036 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/ensemble/hist_gradient_boosting_regressor.py
+-r-xr-xr-x  2.0 unx    54690 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/ensemble/isolation_forest.py
+-r-xr-xr-x  2.0 unx    60410 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/ensemble/random_forest_classifier.py
+-r-xr-xr-x  2.0 unx    59009 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/ensemble/random_forest_regressor.py
+-r-xr-xr-x  2.0 unx    54597 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/ensemble/stacking_regressor.py
+-r-xr-xr-x  2.0 unx    54172 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/ensemble/voting_classifier.py
+-r-xr-xr-x  2.0 unx    52707 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/ensemble/voting_regressor.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/feature_selection/__init__.py
+-r-xr-xr-x  2.0 unx    52220 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/feature_selection/generic_univariate_select.py
+-r-xr-xr-x  2.0 unx    51918 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/feature_selection/select_fdr.py
+-r-xr-xr-x  2.0 unx    51912 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/feature_selection/select_fpr.py
+-r-xr-xr-x  2.0 unx    51920 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/feature_selection/select_fwe.py
+-r-xr-xr-x  2.0 unx    51997 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/feature_selection/select_k_best.py
+-r-xr-xr-x  2.0 unx    52017 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/feature_selection/select_percentile.py
+-r-xr-xr-x  2.0 unx    54670 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/feature_selection/sequential_feature_selector.py
+-r-xr-xr-x  2.0 unx    51649 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/feature_selection/variance_threshold.py
+-rw-r--r--  2.0 unx     9110 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/framework/_utils.py
+-rw-r--r--  2.0 unx    21900 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/framework/base.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/gaussian_process/__init__.py
+-r-xr-xr-x  2.0 unx    57215 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/gaussian_process/gaussian_process_classifier.py
+-r-xr-xr-x  2.0 unx    55907 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/gaussian_process/gaussian_process_regressor.py
+-rw-r--r--  2.0 unx      298 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/impute/__init__.py
+-r-xr-xr-x  2.0 unx    57770 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/impute/iterative_imputer.py
+-r-xr-xr-x  2.0 unx    53992 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/impute/knn_imputer.py
+-r-xr-xr-x  2.0 unx    52789 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/impute/missing_indicator.py
+-rw-r--r--  2.0 unx    18118 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/impute/simple_imputer.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/kernel_approximation/__init__.py
+-r-xr-xr-x  2.0 unx    51733 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/kernel_approximation/additive_chi2_sampler.py
+-r-xr-xr-x  2.0 unx    53606 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/kernel_approximation/nystroem.py
+-r-xr-xr-x  2.0 unx    52760 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/kernel_approximation/polynomial_count_sketch.py
+-r-xr-xr-x  2.0 unx    52189 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/kernel_approximation/rbf_sampler.py
+-r-xr-xr-x  2.0 unx    52188 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/kernel_approximation/skewed_chi2_sampler.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/kernel_ridge/__init__.py
+-r-xr-xr-x  2.0 unx    53706 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/kernel_ridge/kernel_ridge.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/lightgbm/__init__.py
+-r-xr-xr-x  2.0 unx    53228 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/lightgbm/lgbm_classifier.py
+-r-xr-xr-x  2.0 unx    52739 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/lightgbm/lgbm_regressor.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/__init__.py
+-r-xr-xr-x  2.0 unx    53454 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/ard_regression.py
+-r-xr-xr-x  2.0 unx    53767 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/bayesian_ridge.py
+-r-xr-xr-x  2.0 unx    54651 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/elastic_net.py
+-r-xr-xr-x  2.0 unx    55909 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/elastic_net_cv.py
+-r-xr-xr-x  2.0 unx    53707 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/gamma_regressor.py
+-r-xr-xr-x  2.0 unx    52895 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/huber_regressor.py
+-r-xr-xr-x  2.0 unx    54192 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/lars.py
+-r-xr-xr-x  2.0 unx    54399 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/lars_cv.py
+-r-xr-xr-x  2.0 unx    54291 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/lasso.py
+-r-xr-xr-x  2.0 unx    55066 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/lasso_cv.py
+-r-xr-xr-x  2.0 unx    55295 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/lasso_lars.py
+-r-xr-xr-x  2.0 unx    55241 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/lasso_lars_cv.py
+-r-xr-xr-x  2.0 unx    54586 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/lasso_lars_ic.py
+-r-xr-xr-x  2.0 unx    52421 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/linear_regression.py
+-r-xr-xr-x  2.0 unx    58672 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/logistic_regression.py
+-r-xr-xr-x  2.0 unx    59692 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/logistic_regression_cv.py
+-r-xr-xr-x  2.0 unx    53877 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/multi_task_elastic_net.py
+-r-xr-xr-x  2.0 unx    55505 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/multi_task_elastic_net_cv.py
+-r-xr-xr-x  2.0 unx    53459 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/multi_task_lasso.py
+-r-xr-xr-x  2.0 unx    54711 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/multi_task_lasso_cv.py
+-r-xr-xr-x  2.0 unx    52986 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/orthogonal_matching_pursuit.py
+-r-xr-xr-x  2.0 unx    56337 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/passive_aggressive_classifier.py
+-r-xr-xr-x  2.0 unx    55412 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/passive_aggressive_regressor.py
+-r-xr-xr-x  2.0 unx    55842 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/perceptron.py
+-r-xr-xr-x  2.0 unx    53738 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/poisson_regressor.py
+-r-xr-xr-x  2.0 unx    57212 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/ransac_regressor.py
+-r-xr-xr-x  2.0 unx    55272 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/ridge.py
+-r-xr-xr-x  2.0 unx    55590 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/ridge_classifier.py
+-r-xr-xr-x  2.0 unx    54129 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/ridge_classifier_cv.py
+-r-xr-xr-x  2.0 unx    54905 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/ridge_cv.py
+-r-xr-xr-x  2.0 unx    61258 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/sgd_classifier.py
+-r-xr-xr-x  2.0 unx    55872 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/sgd_one_class_svm.py
+-r-xr-xr-x  2.0 unx    58727 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/sgd_regressor.py
+-r-xr-xr-x  2.0 unx    54160 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/theil_sen_regressor.py
+-r-xr-xr-x  2.0 unx    55131 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/linear_model/tweedie_regressor.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/manifold/__init__.py
+-r-xr-xr-x  2.0 unx    54526 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/manifold/isomap.py
+-r-xr-xr-x  2.0 unx    53744 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/manifold/mds.py
+-r-xr-xr-x  2.0 unx    54515 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/manifold/spectral_embedding.py
+-r-xr-xr-x  2.0 unx    57787 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/manifold/tsne.py
+-rw-r--r--  2.0 unx      304 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/metrics/__init__.py
+-rw-r--r--  2.0 unx    40077 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/metrics/classification.py
+-rw-r--r--  2.0 unx     4921 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/metrics/correlation.py
+-rw-r--r--  2.0 unx     4757 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/metrics/covariance.py
+-rw-r--r--  2.0 unx    12037 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/metrics/metrics_utils.py
+-rw-r--r--  2.0 unx    15397 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/metrics/ranking.py
+-rw-r--r--  2.0 unx    23144 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/metrics/regression.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/mixture/__init__.py
+-r-xr-xr-x  2.0 unx    58431 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/mixture/bayesian_gaussian_mixture.py
+-r-xr-xr-x  2.0 unx    56433 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/mixture/gaussian_mixture.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/model_selection/__init__.py
+-r-xr-xr-x  2.0 unx    58982 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/model_selection/grid_search_cv.py
+-r-xr-xr-x  2.0 unx    59826 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/model_selection/randomized_search_cv.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/multiclass/__init__.py
+-r-xr-xr-x  2.0 unx    52414 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/multiclass/one_vs_one_classifier.py
+-r-xr-xr-x  2.0 unx    53342 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/multiclass/one_vs_rest_classifier.py
+-r-xr-xr-x  2.0 unx    52672 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/multiclass/output_code_classifier.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/naive_bayes/__init__.py
+-r-xr-xr-x  2.0 unx    52999 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/naive_bayes/bernoulli_nb.py
+-r-xr-xr-x  2.0 unx    53320 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/naive_bayes/categorical_nb.py
+-r-xr-xr-x  2.0 unx    53007 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/naive_bayes/complement_nb.py
+-r-xr-xr-x  2.0 unx    52147 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/naive_bayes/gaussian_nb.py
+-r-xr-xr-x  2.0 unx    52764 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/naive_bayes/multinomial_nb.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/neighbors/__init__.py
+-r-xr-xr-x  2.0 unx    55551 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/neighbors/k_neighbors_classifier.py
+-r-xr-xr-x  2.0 unx    55033 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/neighbors/k_neighbors_regressor.py
+-r-xr-xr-x  2.0 unx    53510 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/neighbors/kernel_density.py
+-r-xr-xr-x  2.0 unx    55791 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/neighbors/local_outlier_factor.py
+-r-xr-xr-x  2.0 unx    52314 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/neighbors/nearest_centroid.py
+-r-xr-xr-x  2.0 unx    54223 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/neighbors/nearest_neighbors.py
+-r-xr-xr-x  2.0 unx    55699 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/neighbors/neighborhood_components_analysis.py
+-r-xr-xr-x  2.0 unx    56180 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/neighbors/radius_neighbors_classifier.py
+-r-xr-xr-x  2.0 unx    55066 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/neighbors/radius_neighbors_regressor.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/neural_network/__init__.py
+-r-xr-xr-x  2.0 unx    52715 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/neural_network/bernoulli_rbm.py
+-r-xr-xr-x  2.0 unx    60213 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/neural_network/mlp_classifier.py
+-r-xr-xr-x  2.0 unx    59490 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/neural_network/mlp_regressor.py
+-rw-r--r--  2.0 unx      298 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/pipeline/__init__.py
+-rw-r--r--  2.0 unx    23381 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/pipeline/pipeline.py
+-rw-r--r--  2.0 unx      298 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/preprocessing/__init__.py
+-rw-r--r--  2.0 unx     6092 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/preprocessing/binarizer.py
+-rw-r--r--  2.0 unx    20422 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/preprocessing/k_bins_discretizer.py
+-rw-r--r--  2.0 unx     6285 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/preprocessing/label_encoder.py
+-rw-r--r--  2.0 unx     8491 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/preprocessing/max_abs_scaler.py
+-rw-r--r--  2.0 unx    10716 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/preprocessing/min_max_scaler.py
+-rw-r--r--  2.0 unx     5951 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/preprocessing/normalizer.py
+-rw-r--r--  2.0 unx    66998 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/preprocessing/one_hot_encoder.py
+-rw-r--r--  2.0 unx    27956 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/preprocessing/ordinal_encoder.py
+-r-xr-xr-x  2.0 unx    52849 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/preprocessing/polynomial_features.py
+-rw-r--r--  2.0 unx    11981 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/preprocessing/robust_scaler.py
+-rw-r--r--  2.0 unx    10672 b- defN 23-Jul-14 18:30 snowflake/ml/modeling/preprocessing/standard_scaler.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/semi_supervised/__init__.py
+-r-xr-xr-x  2.0 unx    53186 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/semi_supervised/label_propagation.py
+-r-xr-xr-x  2.0 unx    53550 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/semi_supervised/label_spreading.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/svm/__init__.py
+-r-xr-xr-x  2.0 unx    55728 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/svm/linear_svc.py
+-r-xr-xr-x  2.0 unx    54143 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/svm/linear_svr.py
+-r-xr-xr-x  2.0 unx    56442 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/svm/nu_svc.py
+-r-xr-xr-x  2.0 unx    53518 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/svm/nu_svr.py
+-r-xr-xr-x  2.0 unx    56605 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/svm/svc.py
+-r-xr-xr-x  2.0 unx    53721 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/svm/svr.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/tree/__init__.py
+-r-xr-xr-x  2.0 unx    58804 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/tree/decision_tree_classifier.py
+-r-xr-xr-x  2.0 unx    57500 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/tree/decision_tree_regressor.py
+-r-xr-xr-x  2.0 unx    58167 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/tree/extra_tree_classifier.py
+-r-xr-xr-x  2.0 unx    56872 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/tree/extra_tree_regressor.py
+-r-xr-xr-x  2.0 unx      297 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/xgboost/__init__.py
+-r-xr-xr-x  2.0 unx    62574 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/xgboost/xgb_classifier.py
+-r-xr-xr-x  2.0 unx    62080 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/xgboost/xgb_regressor.py
+-r-xr-xr-x  2.0 unx    62738 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/xgboost/xgbrf_classifier.py
+-r-xr-xr-x  2.0 unx    62271 b- defN 23-Jul-14 18:31 snowflake/ml/modeling/xgboost/xgbrf_regressor.py
+-rw-r--r--  2.0 unx     1381 b- defN 23-Jul-14 18:30 snowflake/ml/registry/_schema.py
+-rw-r--r--  2.0 unx    85709 b- defN 23-Jul-14 18:30 snowflake/ml/registry/model_registry.py
+-rw-r--r--  2.0 unx     6138 b- defN 23-Jul-14 18:30 snowflake/ml/utils/connection_params.py
+-rw-r--r--  2.0 unx     3893 b- defN 23-Jul-14 18:30 snowflake/ml/utils/sparse.py
+-r-xr-xr-x  2.0 unx       16 b- defN 23-Jul-14 18:31 snowflake/ml/version.py
+?rw-------  2.0 unx       91 b- defN 23-Jul-14 18:32 snowflake_ml_python-1.0.3.dist-info/WHEEL
+?rw-------  2.0 unx    13340 b- defN 23-Jul-14 18:32 snowflake_ml_python-1.0.3.dist-info/METADATA
+?rw-------  2.0 unx    27677 b- defN 23-Jul-14 18:32 snowflake_ml_python-1.0.3.dist-info/RECORD
+259 files, 9322812 bytes uncompressed, 1912302 bytes compressed:  79.5%
```

## zipnote {}

```diff
@@ -60,14 +60,47 @@
 
 Filename: snowflake/ml/fileset/torch_datapipe.py
 Comment: 
 
 Filename: snowflake/ml/model/_core_requirements.py
 Comment: 
 
+Filename: snowflake/ml/model/_deploy_client/image_builds/base_image_builder.py
+Comment: 
+
+Filename: snowflake/ml/model/_deploy_client/image_builds/client_image_builder.py
+Comment: 
+
+Filename: snowflake/ml/model/_deploy_client/image_builds/docker_context.py
+Comment: 
+
+Filename: snowflake/ml/model/_deploy_client/image_builds/gunicorn_run.sh
+Comment: 
+
+Filename: snowflake/ml/model/_deploy_client/image_builds/inference_server/main.py
+Comment: 
+
+Filename: snowflake/ml/model/_deploy_client/image_builds/templates/dockerfile_template
+Comment: 
+
+Filename: snowflake/ml/model/_deploy_client/snowservice/deploy.py
+Comment: 
+
+Filename: snowflake/ml/model/_deploy_client/snowservice/deploy_options.py
+Comment: 
+
+Filename: snowflake/ml/model/_deploy_client/snowservice/templates/service_spec_template
+Comment: 
+
+Filename: snowflake/ml/model/_deploy_client/utils/constants.py
+Comment: 
+
+Filename: snowflake/ml/model/_deploy_client/utils/snowservice_client.py
+Comment: 
+
 Filename: snowflake/ml/model/_deploy_client/warehouse/deploy.py
 Comment: 
 
 Filename: snowflake/ml/model/_deploy_client/warehouse/infer_template.py
 Comment: 
 
 Filename: snowflake/ml/model/_deployer.py
@@ -78,20 +111,26 @@
 
 Filename: snowflake/ml/model/_handlers/_base.py
 Comment: 
 
 Filename: snowflake/ml/model/_handlers/custom.py
 Comment: 
 
+Filename: snowflake/ml/model/_handlers/pytorch.py
+Comment: 
+
 Filename: snowflake/ml/model/_handlers/sklearn.py
 Comment: 
 
 Filename: snowflake/ml/model/_handlers/snowmlmodel.py
 Comment: 
 
+Filename: snowflake/ml/model/_handlers/torchscript.py
+Comment: 
+
 Filename: snowflake/ml/model/_handlers/xgboost.py
 Comment: 
 
 Filename: snowflake/ml/model/_model.py
 Comment: 
 
 Filename: snowflake/ml/model/_model_handler.py
@@ -723,17 +762,17 @@
 
 Filename: snowflake/ml/utils/sparse.py
 Comment: 
 
 Filename: snowflake/ml/version.py
 Comment: 
 
-Filename: snowflake_ml_python-1.0.2.dist-info/WHEEL
+Filename: snowflake_ml_python-1.0.3.dist-info/WHEEL
 Comment: 
 
-Filename: snowflake_ml_python-1.0.2.dist-info/METADATA
+Filename: snowflake_ml_python-1.0.3.dist-info/METADATA
 Comment: 
 
-Filename: snowflake_ml_python-1.0.2.dist-info/RECORD
+Filename: snowflake_ml_python-1.0.3.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## snowflake/ml/_internal/env_utils.py

```diff
@@ -306,15 +306,16 @@
             parsed_python_version = version.Version(python_version)
             sql = textwrap.dedent(
                 f"""
                 SELECT PACKAGE_NAME, VERSION
                 FROM information_schema.packages
                 WHERE ({pkg_names_str})
                 AND language = 'python'
-                AND runtime_version = '{parsed_python_version.major}.{parsed_python_version.minor}';
+                AND (runtime_version = '{parsed_python_version.major}.{parsed_python_version.minor}'
+                    OR runtime_version is null);
                 """
             )
         else:
             sql = textwrap.dedent(
                 f"""
                 SELECT PACKAGE_NAME, VERSION
                 FROM information_schema.packages
```

## snowflake/ml/_internal/file_utils.py

```diff
@@ -55,48 +55,62 @@
             will be "dir2/" and "dir2/test.py". Defaults to None.
         ignore_generated_py_file: Whether to ignore some generated python files
             in the directory. Defaults to True.
 
     Raises:
         FileNotFoundError: Raised when the given path does not exist.
         ValueError: Raised when the leading path is not a actual leading path of path
+        ValueError: Raised when the arcname cannot be encoded using ASCII.
 
     Yields:
         A bytes IO stream containing the zip file.
     """
+    # TODO(SNOW-862576): Should remove check on ASCII encoding after SNOW-862576 fixed.
     if not os.path.exists(path):
         raise FileNotFoundError(f"{path} is not found")
     if leading_path and not path.startswith(leading_path):
         raise ValueError(f"{leading_path} doesn't lead to {path}")
     # if leading_path is not provided, just use the parent path,
     # and the compression will start from the parent directory
     start_path = leading_path if leading_path else os.path.join(path, "..")
 
     with io.BytesIO() as input_stream:
         with zipfile.ZipFile(input_stream, mode="w", compression=zipfile.ZIP_DEFLATED) as zf:
             if os.path.realpath(path) != os.path.realpath(start_path):
                 cur_path = os.path.dirname(path)
                 while os.path.realpath(cur_path) != os.path.realpath(start_path):
-                    zf.writestr(f"{os.path.relpath(cur_path, start_path)}/", "")
+                    arcname = os.path.relpath(cur_path, start_path)
+                    if not _able_ascii_encode(arcname):
+                        raise ValueError(f"File name {arcname} cannot be encoded using ASCII. Please rename.")
+                    zf.write(cur_path, arcname)
                     cur_path = os.path.dirname(cur_path)
 
             if os.path.isdir(path):
-                for dirname, _, files in os.walk(path):
+                for dirpath, _, files in os.walk(path):
                     # ignore __pycache__
-                    if ignore_generated_py_file and "__pycache__" in dirname:
+                    if ignore_generated_py_file and "__pycache__" in dirpath:
                         continue
-                    zf.write(dirname, os.path.relpath(dirname, start_path))
+                    arcname = os.path.relpath(dirpath, start_path)
+                    if not _able_ascii_encode(arcname):
+                        raise ValueError(f"File name {arcname} cannot be encoded using ASCII. Please rename.")
+                    zf.write(dirpath, arcname)
                     for file in files:
                         # ignore generated python files
                         if ignore_generated_py_file and file.endswith(GENERATED_PY_FILE_EXT):
                             continue
-                        filename = os.path.join(dirname, file)
-                        zf.write(filename, os.path.relpath(filename, start_path))
+                        file_path = os.path.join(dirpath, file)
+                        arcname = os.path.relpath(file_path, start_path)
+                        if not _able_ascii_encode(arcname):
+                            raise ValueError(f"File name {arcname} cannot be encoded using ASCII. Please rename.")
+                        zf.write(file_path, arcname)
             else:
-                zf.write(path, os.path.relpath(path, start_path))
+                arcname = os.path.relpath(path, start_path)
+                if not _able_ascii_encode(arcname):
+                    raise ValueError(f"File name {arcname} cannot be encoded using ASCII. Please rename.")
+                zf.write(path, arcname)
 
         yield input_stream
 
 
 @contextlib.contextmanager
 def unzip_stream_in_temp_dir(stream: IO[bytes], temp_root: Optional[str] = None) -> Generator[str, None, None]:
     """Unzip an IO stream into a temporary directory.
@@ -141,7 +155,15 @@
 
 def get_all_modules(dirname: str, prefix: str = "") -> List[pkgutil.ModuleInfo]:
     subdirs = [f.path for f in os.scandir(dirname) if f.is_dir()]
     modules = list(pkgutil.iter_modules(subdirs, prefix=prefix))
     for dirname in subdirs:
         modules.extend(get_all_modules(dirname, prefix=f"{prefix}.{dirname}" if prefix else dirname))
     return modules
+
+
+def _able_ascii_encode(s: str) -> bool:
+    try:
+        s.encode("ascii", errors="strict")
+        return True
+    except UnicodeEncodeError:
+        return False
```

## snowflake/ml/_internal/telemetry.py

```diff
@@ -2,15 +2,14 @@
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import enum
 import functools
 import inspect
 import operator
-import threading
 import types
 from typing import (
     Any,
     Callable,
     Dict,
     Iterable,
     List,
@@ -25,15 +24,14 @@
 
 from snowflake import connector
 from snowflake.connector import telemetry as connector_telemetry, time_util
 from snowflake.ml._internal import env
 from snowflake.snowpark import dataframe, exceptions, session
 from snowflake.snowpark._internal import utils
 
-_rlock = threading.RLock()
 _log_counter = 0
 _FLUSH_SIZE = 10
 
 _Args = ParamSpec("_Args")
 _ReturnValue = TypeVar("_ReturnValue")
 
 
@@ -304,20 +302,19 @@
                 error = repr(e)
                 telemetry_args["error"] = error
                 raise
             else:
                 return res
             finally:
                 telemetry.send_function_usage_telemetry(**telemetry_args)
-                with _rlock:
-                    global _log_counter
-                    _log_counter += 1
-                    if _log_counter >= _FLUSH_SIZE or "error" in telemetry_args:
-                        telemetry.send_batch()
-                        _log_counter = 0
+                global _log_counter
+                _log_counter += 1
+                if _log_counter >= _FLUSH_SIZE or "error" in telemetry_args:
+                    telemetry.send_batch()
+                    _log_counter = 0
 
         return cast(Callable[_Args, _ReturnValue], wrap)
 
     return decorator
 
 
 def add_stmt_params_to_df(
```

## snowflake/ml/_internal/utils/uri.py

```diff
@@ -1,8 +1,8 @@
-import os.path
+import posixpath
 from typing import Optional
 from urllib.parse import ParseResult, urlparse, urlunparse
 
 _LOCAL_URI_SCHEMES = ["", "file"]
 _HTTP_URI_SCHEMES = ["http", "https"]
 _SNOWFLAKE_STAGE_URI_SCHEMES = ["sfc", "sfstage"]
 
@@ -31,15 +31,20 @@
     Returns:
         The Snowflake stage location encoded by the given URI. Returns None if the URI is not pointing to a Snowflake
             stage.
     """
     if not is_snowflake_stage_uri(uri):
         return None
     uri_components = urlparse(uri)
-    return os.path.join(uri_components.netloc.strip("/"), uri_components.path.strip("/")).rstrip("/")
+    # posixpath.join will drop other components if any of arguments is absolute path.
+    # The path we get is actually absolute (starting with '/'), however, since we concat them to stage location,
+    # it should not.
+    return posixpath.normpath(
+        posixpath.join(posixpath.normpath(uri_components.netloc), posixpath.normpath(uri_components.path.lstrip("/")))
+    )
 
 
 def get_uri_scheme(uri: str) -> str:
     """Returns the scheme for the given URI."""
     return urlparse(uri).scheme
```

## snowflake/ml/model/_deploy_client/warehouse/deploy.py

```diff
@@ -1,16 +1,18 @@
 import os
+import posixpath
 import tempfile
 import warnings
 from types import ModuleType
 from typing import IO, List, Optional, Tuple, TypedDict, Union
 
 from typing_extensions import Unpack
 
-from snowflake.ml._internal import env_utils
+from snowflake.ml._internal import env_utils, file_utils
+from snowflake.ml._internal.utils import identifier
 from snowflake.ml.model import (
     _env as model_env,
     _model,
     _model_meta,
     type_hints as model_types,
 )
 from snowflake.ml.model._deploy_client.warehouse import infer_template
@@ -33,48 +35,59 @@
         model_dir_path: Path to model directory. Exclusive with model_stage_file_path.
         model_stage_file_path: Path to the stored model zip file in the stage. Exclusive with model_dir_path.
         udf_name: Name of the UDF.
         target_method: The name of the target method to be deployed.
         **kwargs: Options that control some features in generated udf code.
 
     Raises:
+        ValueError: Raised when model file name is unable to encoded using ASCII.
         ValueError: Raised when incompatible model.
         ValueError: Raised when target method does not exist in model.
         ValueError: Raised when confronting invalid stage location.
 
     Returns:
         The metadata of the model deployed.
     """
+    # TODO(SNOW-862576): Should remove check on ASCII encoding after SNOW-862576 fixed.
     if model_dir_path:
         model_dir_path = os.path.normpath(model_dir_path)
         model_dir_name = os.path.basename(model_dir_path)
+        if not file_utils._able_ascii_encode(model_dir_name):
+            raise ValueError(f"Model file name {model_dir_name} cannot be encoded using ASCII. Please rename.")
         extract_model_code = infer_template._EXTRACT_LOCAL_MODEL_CODE.format(model_dir_name=model_dir_name)
         meta = _model.load_model(model_dir_path=model_dir_path, meta_only=True)
     else:
         assert model_stage_file_path is not None, "Unreachable assertion error."
-        model_stage_file_name = os.path.basename(model_stage_file_path)
+        model_stage_file_name = posixpath.basename(model_stage_file_path)
+        if not file_utils._able_ascii_encode(model_stage_file_name):
+            raise ValueError(f"Model file name {model_stage_file_name} cannot be encoded using ASCII. Please rename.")
+
         extract_model_code = infer_template._EXTRACT_STAGE_MODEL_CODE.format(
             model_stage_file_name=model_stage_file_name
         )
         meta = _model.load_model(session=session, model_stage_file_path=model_stage_file_path, meta_only=True)
 
     relax_version = kwargs.get("relax_version", False)
 
+    disable_local_conda_resolver = kwargs.get("disable_local_conda_resolver", False)
+
     if target_method not in meta.signatures.keys():
         raise ValueError(f"Target method {target_method} does not exist in model.")
 
-    final_packages = _get_model_final_packages(meta, session, relax_version=relax_version)
+    final_packages = _get_model_final_packages(
+        meta, session, relax_version=relax_version, disable_local_conda_resolver=disable_local_conda_resolver
+    )
 
     stage_location = kwargs.get("permanent_udf_stage_location", None)
     if stage_location:
-        stage_location = stage_location.strip().rstrip("/")
+        stage_location = posixpath.normpath(stage_location.strip())
         if not stage_location.startswith("@"):
             raise ValueError(f"Invalid stage location {stage_location}.")
 
-    with tempfile.NamedTemporaryFile(mode="w", suffix=".py", delete=False) as f:
+    with tempfile.NamedTemporaryFile(mode="w", suffix=".py", delete=False, encoding="utf-8") as f:
         _write_UDF_py_file(f.file, extract_model_code, target_method, **kwargs)
         print(f"Generated UDF file is persisted at: {f.name}")
         imports = ([model_dir_path] if model_dir_path else []) + (
             [model_stage_file_path] if model_stage_file_path else []
         )
 
         class _UDFParams(TypedDict):
@@ -85,15 +98,15 @@
             return_type: st.DataType
             imports: List[Union[str, Tuple[str, str]]]
             packages: List[Union[str, ModuleType]]
 
         params = _UDFParams(
             file_path=f.name,
             func_name="infer",
-            name=f"{udf_name}",
+            name=identifier.get_inferred_name(udf_name),
             return_type=st.PandasSeriesType(st.MapType(st.StringType(), st.VariantType())),
             input_types=[st.PandasDataFrameType([st.MapType()])],
             imports=list(imports),
             packages=list(final_packages),
         )
         if stage_location is None:  # Temporary UDF
             session.udf.register_from_file(**params, replace=True)
@@ -135,22 +148,25 @@
     f.flush()
 
 
 def _get_model_final_packages(
     meta: _model_meta.ModelMetadata,
     session: snowpark_session.Session,
     relax_version: Optional[bool] = False,
+    disable_local_conda_resolver: Optional[bool] = False,
 ) -> List[str]:
     """Generate final packages list of dependency of a model to be deployed to warehouse.
 
     Args:
         meta: Model metadata to get dependency information.
         session: Snowpark connection session.
         relax_version: Whether or not relax the version restriction when fail to resolve dependencies.
             Defaults to False.
+        disable_local_conda_resolver: Set to disable use local conda resolver to do pre-check on environment and rely on
+            the information schema only. Defaults to False.
 
     Raises:
         RuntimeError: Raised when PIP requirements and dependencies from non-Snowflake anaconda channel found.
         RuntimeError: Raised when not all packages are available in snowflake conda channel.
 
     Returns:
         List of final packages string that is accepted by Snowpark register UDF call.
@@ -161,14 +177,16 @@
         or meta.pip_requirements
     ):
         raise RuntimeError("PIP requirements and dependencies from non-Snowflake anaconda channel is not supported.")
 
     deps = meta._conda_dependencies[""]
 
     try:
+        if disable_local_conda_resolver:
+            raise ImportError("Raise to disable local conda resolver. Should be captured.")
         final_packages = env_utils.resolve_conda_environment(
             deps, [model_env._SNOWFLAKE_CONDA_CHANNEL_URL], python_version=meta.python_version
         )
         if final_packages is None and relax_version:
             final_packages = env_utils.resolve_conda_environment(
                 list(map(env_utils.relax_requirement_version, deps)),
                 [model_env._SNOWFLAKE_CONDA_CHANNEL_URL],
```

## snowflake/ml/model/_deploy_client/warehouse/infer_template.py

```diff
@@ -50,20 +50,23 @@
 
 {extract_model_code}
 
 sys.path.insert(0, os.path.join(extracted_model_dir_path, "{code_dir_name}"))
 from snowflake.ml.model import _model
 model, meta = _model._load_model_for_deploy(extracted_model_dir_path)
 
+features = meta.signatures["{target_method}"].inputs
+input_cols = [feature.name for feature in features]
+dtype_map = {{feature.name: feature.as_dtype() for feature in features}}
+
 # TODO(halu): Wire `max_batch_size`.
 # TODO(halu): Avoid per batch async detection branching.
 @vectorized(input=pd.DataFrame, max_batch_size=10)
 def infer(df):
-    input_cols = [spec.name for spec in meta.signatures["{target_method}"].inputs]
-    input_df = pd.io.json.json_normalize(df[0])
+    input_df = pd.io.json.json_normalize(df[0]).astype(dtype=dtype_map)
     if inspect.iscoroutinefunction(model.{target_method}):
         predictions_df = anyio.run(model.{target_method}, input_df[input_cols])
     else:
         predictions_df = model.{target_method}(input_df[input_cols])
 
     {keep_order_code}
```

## snowflake/ml/model/_deployer.py

```diff
@@ -1,13 +1,11 @@
-import json
 import traceback
 from enum import Enum
 from typing import Optional, TypedDict, Union, overload
 
-import numpy as np
 import pandas as pd
 from typing_extensions import Required
 
 from snowflake.ml._internal.utils import identifier
 from snowflake.ml.model import model_signature, type_hints as model_types
 from snowflake.ml.model._deploy_client.warehouse import (
     deploy as warehouse_deploy,
@@ -180,54 +178,55 @@
     Args:
         session: Snowpark Connection Session.
         deployment: The deployment info to use for predict.
         X: The input dataframe.
 
     Raises:
         ValueError: Raised when the input is too large to use keep_order option.
-        NotImplementedError: FeatureGroupSpec is not supported.
 
     Returns:
         The output dataframe.
     """
 
     # Get options
     INTERMEDIATE_OBJ_NAME = "tmp_result"
     sig = deployment["signature"]
     keep_order = deployment["options"].get("keep_order", True)
     output_with_input_features = deployment["options"].get("output_with_input_features", False)
 
     # Validate and prepare input
     if not isinstance(X, SnowparkDataFrame):
         df = model_signature._convert_and_validate_local_data(X, sig.inputs)
-        s_df = session.create_dataframe(df)
+        s_df = model_signature._SnowparkDataFrameHandler.convert_from_df(session, df, keep_order=keep_order)
     else:
         model_signature._validate_snowpark_data(X, sig.inputs)
         s_df = X
 
-    if keep_order:
-        # ID is UINT64 type, this we should limit.
-        if s_df.count() > 2**64:
-            raise ValueError("Unable to keep order of a DataFrame with more than 2 ** 64 rows.")
-        s_df = s_df.with_column(
-            infer_template._KEEP_ORDER_COL_NAME,
-            F.monotonically_increasing_id(),
-        )
+        if keep_order:
+            # ID is UINT64 type, this we should limit.
+            if s_df.count() > 2**64:
+                raise ValueError("Unable to keep order of a DataFrame with more than 2 ** 64 rows.")
+            s_df = s_df.with_column(
+                infer_template._KEEP_ORDER_COL_NAME,
+                F.monotonically_increasing_id(),
+            )
 
     # Infer and get intermediate result
     input_cols = []
     for col_name in s_df.columns:
         literal_col_name = identifier.get_unescaped_names(col_name)
         input_cols.extend(
             [
                 F.lit(literal_col_name),  # type:ignore[arg-type]
                 F.col(col_name),
             ]
         )
-    output_obj = F.call_udf(deployment["name"], F.object_construct(*input_cols))  # type:ignore[arg-type]
+    output_obj = F.call_udf(
+        identifier.get_inferred_name(deployment["name"]), F.object_construct(*input_cols)  # type:ignore[arg-type]
+    )
     if output_with_input_features:
         df_res = s_df.with_column(INTERMEDIATE_OBJ_NAME, output_obj)
     else:
         df_res = s_df.select(output_obj.alias(INTERMEDIATE_OBJ_NAME))
 
     if keep_order:
         df_res = df_res.order_by(
@@ -239,28 +238,16 @@
 
     # Prepare the output
     output_cols = []
     for output_feature in sig.outputs:
         output_cols.append(F.col(INTERMEDIATE_OBJ_NAME)[output_feature.name].astype(output_feature.as_snowpark_type()))
 
     df_res = df_res.with_columns(
-        [identifier.quote_name_without_upper_casing(output_feature.name) for output_feature in sig.outputs],
+        [identifier.get_inferred_name(output_feature.name) for output_feature in sig.outputs],
         output_cols,
     ).drop(INTERMEDIATE_OBJ_NAME)
 
     # Get final result
     if not isinstance(X, SnowparkDataFrame):
-        dtype_map = {}
-        for feature in sig.outputs:
-            if isinstance(feature, model_signature.FeatureGroupSpec):
-                raise NotImplementedError("FeatureGroupSpec is not supported.")
-            assert isinstance(feature, model_signature.FeatureSpec), "Invalid feature kind."
-            dtype_map[feature.name] = feature.as_dtype()
-        df_local = df_res.to_pandas()
-        # This is because Array and object will generate variant type and requires an additional loads to
-        # get correct data otherwise it would be string.
-        for col_name in [col_name for col_name, col_dtype in dtype_map.items() if col_dtype == np.object0]:
-            df_local[col_name] = df_local[col_name].map(json.loads)
-        df_local = df_local.astype(dtype=dtype_map)
-        return pd.DataFrame(df_local)
+        return model_signature._SnowparkDataFrameHandler.convert_to_df(df_res, features=sig.outputs)
     else:
         return df_res
```

## snowflake/ml/model/_env.py

```diff
@@ -32,15 +32,15 @@
     env: Dict[str, Any] = dict()
     env["name"] = "snow-env"
     env["channels"] = [_SNOWFLAKE_CONDA_CHANNEL_URL, "nodefaults"]
     env["dependencies"] = [f"python=={python_version}"]
     for chan, reqs in deps.items():
         env["dependencies"].extend([f"{chan}::{str(req)}" if chan else str(req) for req in reqs])
 
-    with open(path, "w") as f:
+    with open(path, "w", encoding="utf-8") as f:
         yaml.safe_dump(env, stream=f, default_flow_style=False)
 
     return path
 
 
 def save_requirements_file(dir_path: str, pip_deps: List[requirements.Requirement]) -> str:
     """Generate Python requirements.txt file in the given directory path.
@@ -50,30 +50,30 @@
         pip_deps: List of dependencies string after validated.
 
     Returns:
         The path to pip requirements file.
     """
     requirements = "\n".join(map(str, pip_deps))
     path = os.path.join(dir_path, _REQUIREMENTS_FILE_NAME)
-    with open(path, "w") as out:
+    with open(path, "w", encoding="utf-8") as out:
         out.write(requirements)
 
     return path
 
 
 def load_conda_env_file(path: str) -> Tuple[DefaultDict[str, List[requirements.Requirement]], Optional[str]]:
     """Read conda.yaml file to get n a dict of dependencies after validation.
 
     Args:
         path: Path to conda.yaml.
 
     Returns:
         A tuple of Dict of conda dependencies after validated and a string 'major.minor.patchlevel' of python version.
     """
-    with open(path) as f:
+    with open(path, encoding="utf-8") as f:
         env = yaml.safe_load(stream=f)
 
     assert isinstance(env, dict)
 
     deps = []
 
     python_version = None
@@ -95,15 +95,15 @@
 
     Args:
         path: Path to the requirements.txt file.
 
     Returns:
         List of dependencies string after validated.
     """
-    with open(path) as f:
+    with open(path, encoding="utf-8") as f:
         reqs = f.readlines()
 
     return env_utils.validate_pip_requirement_string_list(reqs)
 
 
 def validate_py_runtime_version(provided_py_version_str: str) -> None:
     if provided_py_version_str != snowml_env.PYTHON_VERSION:
```

## snowflake/ml/model/_handlers/custom.py

```diff
@@ -1,20 +1,23 @@
 import inspect
 import os
+import pathlib
 import sys
 from typing import TYPE_CHECKING, Dict, Optional
 
 import anyio
 import cloudpickle
+import pandas as pd
 from typing_extensions import TypeGuard, Unpack
 
 from snowflake.ml._internal import file_utils, type_utils
 from snowflake.ml.model import (
     _model_handler,
     _model_meta as model_meta_api,
+    model_signature,
     type_hints as model_types,
 )
 from snowflake.ml.model._handlers import _base
 
 if TYPE_CHECKING:
     from snowflake.ml.model import custom_model
 
@@ -51,14 +54,18 @@
 
         def get_prediction(
             target_method_name: str, sample_input: model_types.SupportedLocalDataType
         ) -> model_types.SupportedLocalDataType:
             target_method = getattr(model, target_method_name, None)
             assert callable(target_method) and inspect.ismethod(target_method)
             target_method = target_method.__func__
+
+            if not isinstance(sample_input, pd.DataFrame):
+                sample_input = model_signature._convert_local_data_to_df(sample_input)
+
             if inspect.iscoroutinefunction(target_method):
                 with anyio.start_blocking_portal() as portal:
                     predictions_df = portal.call(target_method, model, sample_input)
             else:
                 predictions_df = target_method(model, sample_input)
             return predictions_df
 
@@ -98,15 +105,17 @@
         with open(os.path.join(model_blob_path, _CustomModelHandler.MODEL_BLOB_FILE), "wb") as f:
             cloudpickle.dump(model, f)
         model_meta.models[name] = model_meta_api._ModelBlobMetadata(
             name=name,
             model_type=_CustomModelHandler.handler_type,
             path=_CustomModelHandler.MODEL_BLOB_FILE,
             artifacts={
-                name: os.path.join(_CustomModelHandler.MODEL_ARTIFACTS_DIR, os.path.basename(os.path.normpath(uri)))
+                name: pathlib.Path(
+                    os.path.join(_CustomModelHandler.MODEL_ARTIFACTS_DIR, os.path.basename(os.path.normpath(path=uri)))
+                ).as_posix()
                 for name, uri in model.context.artifacts.items()
             },
         )
 
     @staticmethod
     def _load_model(
         name: str, model_meta: model_meta_api.ModelMetadata, model_blobs_dir_path: str
@@ -125,15 +134,18 @@
         with open(os.path.join(model_blob_path, model_blob_filename), "rb") as f:
             m = cloudpickle.load(f)
         ModelClass = type(m)
 
         assert issubclass(ModelClass, custom_model.CustomModel)
 
         artifacts_meta = model_blob_metadata.artifacts
-        artifacts = {name: os.path.join(model_blob_path, rel_path) for name, rel_path in artifacts_meta.items()}
+        artifacts = {
+            name: str(pathlib.PurePath(model_blob_path) / pathlib.PurePosixPath(rel_path))
+            for name, rel_path in artifacts_meta.items()
+        }
         models: Dict[str, model_types.SupportedModelType] = dict()
         for sub_model_name, _ref in m.context.model_refs.items():
             model_type = model_meta.models[sub_model_name].model_type
             handler = _model_handler._load_handler(model_type)
             assert handler
             sub_model = handler._load_model(
                 name=sub_model_name,
```

## snowflake/ml/model/_handlers/sklearn.py

```diff
@@ -1,19 +1,20 @@
 import os
-from typing import TYPE_CHECKING, Callable, Optional, Sequence, Type, Union, cast
+from typing import TYPE_CHECKING, Callable, Optional, Type, Union, cast
 
 import cloudpickle
 import numpy as np
 import pandas as pd
 from typing_extensions import TypeGuard, Unpack
 
 from snowflake.ml._internal import type_utils
 from snowflake.ml.model import (
     _model_meta as model_meta_api,
     custom_model,
+    model_signature,
     type_hints as model_types,
 )
 from snowflake.ml.model._handlers import _base
 
 if TYPE_CHECKING:
     import sklearn.base
     import sklearn.pipeline
@@ -76,14 +77,17 @@
                 target_methods=kwargs.pop("target_methods", None),
                 default_target_methods=_SKLModelHandler.DEFAULT_TARGET_METHODS,
             )
 
             def get_prediction(
                 target_method_name: str, sample_input: model_types.SupportedLocalDataType
             ) -> model_types.SupportedLocalDataType:
+                if not isinstance(sample_input, (pd.DataFrame, np.ndarray)):
+                    sample_input = model_signature._convert_local_data_to_df(sample_input)
+
                 target_method = getattr(model, target_method_name, None)
                 assert callable(target_method)
                 predictions_df = target_method(sample_input)
                 return predictions_df
 
             model_meta = model_meta_api._validate_signature(
                 model=model,
@@ -97,15 +101,15 @@
         os.makedirs(model_blob_path, exist_ok=True)
         with open(os.path.join(model_blob_path, _SKLModelHandler.MODEL_BLOB_FILE), "wb") as f:
             cloudpickle.dump(model, f)
         base_meta = model_meta_api._ModelBlobMetadata(
             name=name, model_type=_SKLModelHandler.handler_type, path=_SKLModelHandler.MODEL_BLOB_FILE
         )
         model_meta.models[name] = base_meta
-        model_meta._include_if_absent([("scikit-learn", "scikit-learn")])
+        model_meta._include_if_absent([model_meta_api.Dependency(conda_name="scikit-learn", pip_name="scikit-learn")])
 
     @staticmethod
     def _load_model(
         name: str, model_meta: model_meta_api.ModelMetadata, model_blobs_dir_path: str
     ) -> Union["sklearn.base.BaseEstimator", "sklearn.pipeline.Pipeline"]:
         model_blob_path = os.path.join(model_blobs_dir_path, name)
         if not hasattr(model_meta, "models"):
@@ -143,34 +147,35 @@
 
         def _create_custom_model(
             raw_model: Union["sklearn.base.BaseEstimator", "sklearn.pipeline.Pipeline"],
             model_meta: model_meta_api.ModelMetadata,
         ) -> Type[custom_model.CustomModel]:
             def fn_factory(
                 raw_model: Union["sklearn.base.BaseEstimator", "sklearn.pipeline.Pipeline"],
-                output_col_names: Sequence[str],
+                signature: model_signature.ModelSignature,
                 target_method: str,
             ) -> Callable[[custom_model.CustomModel, pd.DataFrame], pd.DataFrame]:
                 @custom_model.inference_api
                 def fn(self: custom_model.CustomModel, X: pd.DataFrame) -> pd.DataFrame:
                     res = getattr(raw_model, target_method)(X)
 
                     if isinstance(res, list) and len(res) > 0 and isinstance(res[0], np.ndarray):
                         # In case of multi-output estimators, predict_proba(), decision_function(), etc., functions
-                        # return a list of ndarrays. We need to concatenate them.
-                        res = np.concatenate(res, axis=1)
-                    return pd.DataFrame(res, columns=output_col_names)
+                        # return a list of ndarrays. We need to deal them seperately
+                        df = model_signature._SeqOfNumpyArrayHandler.convert_to_df(res)
+                    else:
+                        df = pd.DataFrame(res)
+
+                    return model_signature._rename_pandas_df(df, signature.outputs)
 
                 return fn
 
             type_method_dict = {}
             for target_method_name, sig in model_meta.signatures.items():
-                type_method_dict[target_method_name] = fn_factory(
-                    raw_model, [spec.name for spec in sig.outputs], target_method_name
-                )
+                type_method_dict[target_method_name] = fn_factory(raw_model, sig, target_method_name)
 
             _SKLModel = type(
                 "_SKLModel",
                 (custom_model.CustomModel,),
                 type_method_dict,
             )
```

## snowflake/ml/model/_handlers/snowmlmodel.py

```diff
@@ -1,19 +1,20 @@
 import os
-from typing import TYPE_CHECKING, Callable, Optional, Sequence, Type, cast
+from typing import TYPE_CHECKING, Callable, Optional, Type, cast
 
 import cloudpickle
 import numpy as np
 import pandas as pd
 from typing_extensions import TypeGuard, Unpack
 
 from snowflake.ml._internal import type_utils
 from snowflake.ml.model import (
     _model_meta as model_meta_api,
     custom_model,
+    model_signature,
     type_hints as model_types,
 )
 from snowflake.ml.model._handlers import _base
 
 if TYPE_CHECKING:
     from snowflake.ml.modeling.framework.base import BaseEstimator
 
@@ -77,14 +78,17 @@
                     target_methods=kwargs.pop("target_methods", None),
                     default_target_methods=_SnowMLModelHandler.DEFAULT_TARGET_METHODS,
                 )
 
                 def get_prediction(
                     target_method_name: str, sample_input: model_types.SupportedLocalDataType
                 ) -> model_types.SupportedLocalDataType:
+                    if not isinstance(sample_input, (pd.DataFrame,)):
+                        sample_input = model_signature._convert_local_data_to_df(sample_input)
+
                     target_method = getattr(model, target_method_name, None)
                     assert callable(target_method)
                     predictions_df = target_method(sample_input)
                     return predictions_df
 
                 model_meta = model_meta_api._validate_signature(
                     model=model,
@@ -102,15 +106,15 @@
             name=name, model_type=_SnowMLModelHandler.handler_type, path=_SnowMLModelHandler.MODEL_BLOB_FILE
         )
         model_meta.models[name] = base_meta
         _include_if_absent_pkgs = []
         model_dependencies = model._get_dependencies()
         for dep in model_dependencies:
             pkg_name = dep.split("==")[0]
-            _include_if_absent_pkgs.append((pkg_name, pkg_name))
+            _include_if_absent_pkgs.append(model_meta_api.Dependency(conda_name=pkg_name, pip_name=pkg_name))
         model_meta._include_if_absent(_include_if_absent_pkgs)
 
     @staticmethod
     def _load_model(name: str, model_meta: model_meta_api.ModelMetadata, model_blobs_dir_path: str) -> "BaseEstimator":
         model_blob_path = os.path.join(model_blobs_dir_path, name)
         if not hasattr(model_meta, "models"):
             raise ValueError("Ill model metadata found.")
@@ -146,34 +150,35 @@
 
         def _create_custom_model(
             raw_model: "BaseEstimator",
             model_meta: model_meta_api.ModelMetadata,
         ) -> Type[custom_model.CustomModel]:
             def fn_factory(
                 raw_model: "BaseEstimator",
-                output_col_names: Sequence[str],
+                signature: model_signature.ModelSignature,
                 target_method: str,
             ) -> Callable[[custom_model.CustomModel, pd.DataFrame], pd.DataFrame]:
                 @custom_model.inference_api
                 def fn(self: custom_model.CustomModel, X: pd.DataFrame) -> pd.DataFrame:
                     res = getattr(raw_model, target_method)(X)
 
                     if isinstance(res, list) and len(res) > 0 and isinstance(res[0], np.ndarray):
                         # In case of multi-output estimators, predict_proba(), decision_function(), etc., functions
-                        # return a list of ndarrays. We need to concatenate them.
-                        res = np.concatenate(res, axis=1)
-                    return pd.DataFrame(res, columns=output_col_names)
+                        # return a list of ndarrays. We need to deal them seperately
+                        df = model_signature._SeqOfNumpyArrayHandler.convert_to_df(res)
+                    else:
+                        df = pd.DataFrame(res)
+
+                    return model_signature._rename_pandas_df(df, signature.outputs)
 
                 return fn
 
             type_method_dict = {}
             for target_method_name, sig in model_meta.signatures.items():
-                type_method_dict[target_method_name] = fn_factory(
-                    raw_model, [spec.name for spec in sig.outputs], target_method_name
-                )
+                type_method_dict[target_method_name] = fn_factory(raw_model, sig, target_method_name)
 
             _SnowMLModel = type(
                 "_SnowMLModel",
                 (custom_model.CustomModel,),
                 type_method_dict,
             )
```

## snowflake/ml/model/_handlers/xgboost.py

```diff
@@ -1,19 +1,20 @@
 # mypy: disable-error-code="import"
 import os
-from typing import TYPE_CHECKING, Callable, Optional, Sequence, Type, Union
+from typing import TYPE_CHECKING, Callable, Optional, Type, Union
 
 import numpy as np
 import pandas as pd
 from typing_extensions import TypeGuard, Unpack
 
 from snowflake.ml._internal import type_utils
 from snowflake.ml.model import (
     _model_meta as model_meta_api,
     custom_model,
+    model_signature,
     type_hints as model_types,
 )
 from snowflake.ml.model._handlers import _base
 
 if TYPE_CHECKING:
     import xgboost
 
@@ -68,14 +69,17 @@
                 target_methods=kwargs.pop("target_methods", None),
                 default_target_methods=_XGBModelHandler.DEFAULT_TARGET_METHODS,
             )
 
             def get_prediction(
                 target_method_name: str, sample_input: model_types.SupportedLocalDataType
             ) -> model_types.SupportedLocalDataType:
+                if not isinstance(sample_input, (pd.DataFrame, np.ndarray)):
+                    sample_input = model_signature._convert_local_data_to_df(sample_input)
+
                 target_method = getattr(model, target_method_name, None)
                 assert callable(target_method)
                 predictions_df = target_method(sample_input)
                 return predictions_df
 
             model_meta = model_meta_api._validate_signature(
                 model=model,
@@ -91,15 +95,20 @@
         base_meta = model_meta_api._ModelBlobMetadata(
             name=name,
             model_type=_XGBModelHandler.handler_type,
             path=_XGBModelHandler.MODEL_BLOB_FILE,
             options={"xgb_estimator_type": model.__class__.__name__},
         )
         model_meta.models[name] = base_meta
-        model_meta._include_if_absent([("scikit-learn", "scikit-learn"), ("xgboost", "xgboost")])
+        model_meta._include_if_absent(
+            [
+                model_meta_api.Dependency(conda_name="scikit-learn", pip_name="scikit-learn"),
+                model_meta_api.Dependency(conda_name="xgboost", pip_name="xgboost"),
+            ]
+        )
 
     @staticmethod
     def _load_model(
         name: str, model_meta: model_meta_api.ModelMetadata, model_blobs_dir_path: str
     ) -> Union["xgboost.Booster", "xgboost.XGBModel"]:
         import xgboost
 
@@ -139,34 +148,35 @@
 
         def _create_custom_model(
             raw_model: Union["xgboost.Booster", "xgboost.XGBModel"],
             model_meta: model_meta_api.ModelMetadata,
         ) -> Type[custom_model.CustomModel]:
             def fn_factory(
                 raw_model: Union["xgboost.Booster", "xgboost.XGBModel"],
-                output_col_names: Sequence[str],
+                signature: model_signature.ModelSignature,
                 target_method: str,
             ) -> Callable[[custom_model.CustomModel, pd.DataFrame], pd.DataFrame]:
                 @custom_model.inference_api
                 def fn(self: custom_model.CustomModel, X: pd.DataFrame) -> pd.DataFrame:
                     res = getattr(raw_model, target_method)(X)
 
                     if isinstance(res, list) and len(res) > 0 and isinstance(res[0], np.ndarray):
                         # In case of multi-output estimators, predict_proba(), decision_function(), etc., functions
-                        # return a list of ndarrays. We need to concatenate them.
-                        res = np.concatenate(res, axis=1)
-                    return pd.DataFrame(res, columns=output_col_names)
+                        # return a list of ndarrays. We need to deal them seperately
+                        df = model_signature._SeqOfNumpyArrayHandler.convert_to_df(res)
+                    else:
+                        df = pd.DataFrame(res)
+
+                    return model_signature._rename_pandas_df(df, signature.outputs)
 
                 return fn
 
             type_method_dict = {}
             for target_method_name, sig in model_meta.signatures.items():
-                type_method_dict[target_method_name] = fn_factory(
-                    raw_model, [spec.name for spec in sig.outputs], target_method_name
-                )
+                type_method_dict[target_method_name] = fn_factory(raw_model, sig, target_method_name)
 
             _XGBModel = type(
                 "_XGBModel",
                 (custom_model.CustomModel,),
                 type_method_dict,
             )
```

## snowflake/ml/model/_model.py

```diff
@@ -1,8 +1,9 @@
 import os
+import posixpath
 import tempfile
 import warnings
 from types import ModuleType
 from typing import TYPE_CHECKING, Dict, List, Literal, Optional, Tuple, Union, overload
 
 from snowflake.ml._internal import file_utils, type_utils
 from snowflake.ml.model import (
@@ -360,15 +361,15 @@
             python_version=python_version,
             ext_modules=ext_modules,
             code_paths=code_paths,
             options=options,
         )
 
     assert session and model_stage_file_path
-    if os.path.splitext(model_stage_file_path)[1] != ".zip":
+    if posixpath.splitext(model_stage_file_path)[1] != ".zip":
         raise ValueError(f"Provided model path in the stage {model_stage_file_path} must be a path to a zip file.")
 
     with tempfile.TemporaryDirectory() as temp_local_model_dir_path:
         meta = _save(
             name=name,
             model=model,
             local_dir_path=temp_local_model_dir_path,
@@ -539,15 +540,15 @@
             raise ValueError(f"Provided model directory {model_dir_path} does not exist.")
         if not os.path.isdir(model_dir_path):
             raise ValueError(f"Provided model directory {model_dir_path} is not a directory.")
 
         return _load(local_dir_path=model_dir_path, meta_only=meta_only)
 
     assert session and model_stage_file_path
-    if os.path.splitext(model_stage_file_path)[1] != ".zip":
+    if posixpath.splitext(model_stage_file_path)[1] != ".zip":
         raise ValueError(f"Provided model path in the stage {model_stage_file_path} must be a path to a zip file.")
 
     fo = FileOperation(session=session)
     zf = fo.get_stream(model_stage_file_path)
     with file_utils.unzip_stream_in_temp_dir(stream=zf) as temp_local_model_dir_path:
         return _load(local_dir_path=temp_local_model_dir_path, meta_only=meta_only)
```

## snowflake/ml/model/_model_meta.py

```diff
@@ -1,16 +1,17 @@
 import dataclasses
 import importlib
 import os
 import sys
 import warnings
+from collections import namedtuple
 from contextlib import contextmanager
 from datetime import datetime
 from types import ModuleType
-from typing import Any, Callable, Dict, Generator, List, Optional, Sequence, Tuple, cast
+from typing import Any, Callable, Dict, Generator, List, Optional, Sequence, cast
 
 import cloudpickle
 import yaml
 from packaging import version
 
 from snowflake.ml._internal import env as snowml_env, env_utils, file_utils
 from snowflake.ml.model import (
@@ -20,14 +21,16 @@
     type_hints as model_types,
 )
 from snowflake.snowpark import DataFrame as SnowparkDataFrame
 
 MODEL_METADATA_VERSION = 1
 _BASIC_DEPENDENCIES = _core_requirements.REQUIREMENTS
 
+Dependency = namedtuple("Dependency", ["conda_name", "pip_name"])
+
 
 @dataclasses.dataclass
 class _ModelBlobMetadata:
     """Dataclass to store metadata of an individual model blob (sub-model) in the packed model.
 
     Attributes:
         name: The name to refer the sub-model.
@@ -210,17 +213,19 @@
         self._conda_dependencies = env_utils.validate_conda_dependency_string_list(
             conda_dependencies if conda_dependencies else []
         )
         self._pip_requirements = env_utils.validate_pip_requirement_string_list(
             pip_requirements if pip_requirements else []
         )
         if "local_ml_library_version" in kwargs:
-            self._include_if_absent([(dep, dep) for dep in _BASIC_DEPENDENCIES])
+            self._include_if_absent([Dependency(conda_name=dep, pip_name=dep) for dep in _BASIC_DEPENDENCIES])
         else:
-            self._include_if_absent([(dep, dep) for dep in _BASIC_DEPENDENCIES + [env_utils._SNOWML_PKG_NAME]])
+            self._include_if_absent(
+                [Dependency(conda_name=dep, pip_name=dep) for dep in _BASIC_DEPENDENCIES + [env_utils._SNOWML_PKG_NAME]]
+            )
 
         self.__dict__.update(kwargs)
 
     @property
     def pip_requirements(self) -> List[str]:
         """List of pip Python packages requirements for running the model."""
         return list(sorted(map(str, self._pip_requirements)))
@@ -230,15 +235,15 @@
         """List of conda channel and dependencies from that to run the model"""
         return sorted(
             f"{chan}::{str(req)}" if chan else str(req)
             for chan, reqs in self._conda_dependencies.items()
             for req in reqs
         )
 
-    def _include_if_absent(self, pkgs: List[Tuple[str, str]]) -> None:
+    def _include_if_absent(self, pkgs: List[Dependency]) -> None:
         conda_reqs_str, pip_reqs_str = tuple(zip(*pkgs))
         pip_reqs = env_utils.validate_pip_requirement_string_list(list(pip_reqs_str))
         conda_reqs = env_utils.validate_conda_dependency_string_list(list(conda_reqs_str))
 
         for conda_req, pip_req in zip(conda_reqs[""], pip_reqs):
             req_to_add = env_utils.get_local_installed_version_of_pip_package(pip_req)
             req_to_add.name = conda_req.name
@@ -323,15 +328,15 @@
     def save_model_metadata(self, path: str) -> None:
         """Save the model metadata as a yaml file in the model directory.
 
         Args:
             path: The path of the directory to write a yaml file in it.
         """
         model_yaml_path = os.path.join(path, ModelMetadata.MODEL_METADATA_FILE)
-        with open(model_yaml_path, "w") as out:
+        with open(model_yaml_path, "w", encoding="utf-8") as out:
             yaml.safe_dump({**self.to_dict(), "version": MODEL_METADATA_VERSION}, stream=out, default_flow_style=False)
 
         env_dir_path = os.path.join(path, ModelMetadata.ENV_DIR)
         os.makedirs(env_dir_path, exist_ok=True)
 
         _env.save_conda_env_file(env_dir_path, self._conda_dependencies, self.python_version)
         _env.save_requirements_file(env_dir_path, self._pip_requirements)
@@ -346,15 +351,15 @@
         Raises:
             NotImplementedError: raised when version is not found or unsupported in metadata file.
 
         Returns:
             Loaded model metadata object.
         """
         model_yaml_path = os.path.join(path, ModelMetadata.MODEL_METADATA_FILE)
-        with open(model_yaml_path) as f:
+        with open(model_yaml_path, encoding="utf-8") as f:
             loaded_mata = yaml.safe_load(f.read())
 
         loaded_mata_version = loaded_mata.pop("version", None)
         if not loaded_mata_version or loaded_mata_version != MODEL_METADATA_VERSION:
             raise NotImplementedError("Unknown or unsupported model metadata file found.")
 
         meta = ModelMetadata.from_dict(loaded_mata)
@@ -388,15 +393,15 @@
         sample_input is not None
     ), "Model signature and sample input are None at the same time. This should not happen with local model."
     model_meta._signatures = {}
     trunc_sample_input = model_signature._truncate_data(sample_input)
     if isinstance(sample_input, SnowparkDataFrame):
         # Added because of Any from missing stubs.
         trunc_sample_input = cast(SnowparkDataFrame, trunc_sample_input)
-        local_sample_input = trunc_sample_input.to_pandas()
+        local_sample_input = model_signature._SnowparkDataFrameHandler.convert_to_df(trunc_sample_input)
     else:
         local_sample_input = trunc_sample_input
     for target_method in target_methods:
         predictions_df = get_prediction_fn(target_method, local_sample_input)
         sig = model_signature.infer_signature(sample_input, predictions_df)
         model_meta._signatures[target_method] = sig
     return model_meta
```

## snowflake/ml/model/model_signature.py

```diff
@@ -1,12 +1,14 @@
+import json
 import textwrap
 import warnings
 from abc import ABC, abstractmethod
 from enum import Enum
 from typing import (
+    TYPE_CHECKING,
     Any,
     Callable,
     Dict,
     Final,
     Generic,
     List,
     Literal,
@@ -22,40 +24,46 @@
 import numpy as np
 import numpy.typing as npt
 import pandas as pd
 from typing_extensions import TypeGuard
 
 import snowflake.snowpark
 import snowflake.snowpark.types as spt
+from snowflake.ml._internal import type_utils
 from snowflake.ml._internal.utils import formatting, identifier
 from snowflake.ml.model import type_hints as model_types
+from snowflake.ml.model._deploy_client.warehouse import infer_template
+
+if TYPE_CHECKING:
+    import tensorflow
+    import torch
 
 
 class DataType(Enum):
     def __init__(self, value: str, snowpark_type: Type[spt.DataType], numpy_type: npt.DTypeLike) -> None:
         self._value = value
         self._snowpark_type = snowpark_type
         self._numpy_type = numpy_type
 
-    INT8 = ("int8", spt.IntegerType, np.int8)
-    INT16 = ("int16", spt.IntegerType, np.int16)
+    INT8 = ("int8", spt.ByteType, np.int8)
+    INT16 = ("int16", spt.ShortType, np.int16)
     INT32 = ("int32", spt.IntegerType, np.int32)
-    INT64 = ("int64", spt.IntegerType, np.int64)
+    INT64 = ("int64", spt.LongType, np.int64)
 
     FLOAT = ("float", spt.FloatType, np.float32)
     DOUBLE = ("double", spt.DoubleType, np.float64)
 
-    UINT8 = ("uint8", spt.IntegerType, np.uint8)
-    UINT16 = ("uint16", spt.IntegerType, np.uint16)
+    UINT8 = ("uint8", spt.ByteType, np.uint8)
+    UINT16 = ("uint16", spt.ShortType, np.uint16)
     UINT32 = ("uint32", spt.IntegerType, np.uint32)
-    UINT64 = ("uint64", spt.IntegerType, np.uint64)
+    UINT64 = ("uint64", spt.LongType, np.uint64)
 
-    BOOL = ("bool", spt.BooleanType, np.bool8)
-    STRING = ("string", spt.StringType, np.str0)
-    BYTES = ("bytes", spt.BinaryType, np.bytes0)
+    BOOL = ("bool", spt.BooleanType, np.bool_)
+    STRING = ("string", spt.StringType, np.str_)
+    BYTES = ("bytes", spt.BinaryType, np.bytes_)
 
     def as_snowpark_type(self) -> spt.DataType:
         """Convert to corresponding Snowpark Type.
 
         Returns:
             A Snowpark type.
         """
@@ -81,50 +89,89 @@
         for potential_type in np_to_snowml_type_mapping.keys():
             if np.can_cast(np_type, potential_type, casting="no"):
                 # This is used since the same dtype might represented in different ways.
                 return np_to_snowml_type_mapping[potential_type]
         raise NotImplementedError(f"Type {np_type} is not supported as a DataType.")
 
     @classmethod
+    def from_torch_type(cls, torch_type: "torch.dtype") -> "DataType":
+        import torch
+
+        """Translate torch dtype to DataType for signature definition.
+
+        Args:
+            torch_type: The torch dtype.
+
+        Returns:
+            Corresponding DataType.
+        """
+        torch_dtype_to_numpy_dtype_mapping = {
+            torch.uint8: np.uint8,
+            torch.int8: np.int8,
+            torch.int16: np.int16,
+            torch.int32: np.int32,
+            torch.int64: np.int64,
+            torch.float32: np.float32,
+            torch.float64: np.float64,
+            torch.bool: np.bool_,
+        }
+        return cls.from_numpy_type(torch_dtype_to_numpy_dtype_mapping[torch_type])
+
+    @classmethod
     def from_snowpark_type(cls, snowpark_type: spt.DataType) -> "DataType":
         """Translate snowpark type to DataType for signature definition.
 
         Args:
             snowpark_type: The snowpark type.
 
         Raises:
             NotImplementedError: Raised when the given numpy type is not supported.
 
         Returns:
             Corresponding DataType.
         """
+        if isinstance(snowpark_type, spt.ArrayType):
+            actual_sp_type = snowpark_type.element_type
+        else:
+            actual_sp_type = snowpark_type
+
         snowpark_to_snowml_type_mapping: Dict[Type[spt.DataType], DataType] = {
-            spt._IntegralType: DataType.INT64,
-            **{i._snowpark_type: i for i in DataType if i._snowpark_type != spt.IntegerType},
+            i._snowpark_type: i
+            for i in DataType
+            # We by default infer as signed integer.
+            if i not in [DataType.UINT8, DataType.UINT16, DataType.UINT32, DataType.UINT64]
         }
         for potential_type in snowpark_to_snowml_type_mapping.keys():
-            if isinstance(snowpark_type, potential_type):
+            if isinstance(actual_sp_type, potential_type):
                 return snowpark_to_snowml_type_mapping[potential_type]
+        # Fallback for decimal type.
+        if isinstance(snowpark_type, spt.DecimalType):
+            if snowpark_type.scale == 0:
+                return DataType.INT64
         raise NotImplementedError(f"Type {snowpark_type} is not supported as a DataType.")
 
     def is_same_snowpark_type(self, incoming_snowpark_type: spt.DataType) -> bool:
         """Check if provided snowpark type is the same as Data Type.
-            Since for Snowflake all integer types are same, thus when datatype is a integer type, the incoming snowpark
-            type can be any type inherit from _IntegralType.
 
         Args:
             incoming_snowpark_type: The snowpark type.
 
+        Raises:
+            NotImplementedError: Raised when the given numpy type is not supported.
+
         Returns:
             If the provided snowpark type is the same as the DataType.
         """
-        if self._snowpark_type == spt.IntegerType:
-            return isinstance(incoming_snowpark_type, spt._IntegralType)
-        else:
-            return isinstance(incoming_snowpark_type, self._snowpark_type)
+        # Special handle for Decimal Type.
+        if isinstance(incoming_snowpark_type, spt.DecimalType):
+            if incoming_snowpark_type.scale == 0:
+                return self == DataType.INT64 or self == DataType.UINT64
+            raise NotImplementedError(f"Type {incoming_snowpark_type} is not supported as a DataType.")
+
+        return isinstance(incoming_snowpark_type, self._snowpark_type)
 
 
 class BaseFeatureSpec(ABC):
     """Abstract Class for specification of a feature."""
 
     def __init__(self, name: str) -> None:
         self._name = name
@@ -170,32 +217,42 @@
                 -1 is used to represent variable length.Defaults to None.
 
                 E.g.
                 None: scalar
                 (2,): 1d list with fixed len of 2.
                 (-1,): 1d list with variable length. Used for ragged tensor representation.
                 (d1, d2, d3): 3d tensor.
+
+        Raises:
+            TypeError: Raised when the dtype input type is incorrect.
+            TypeError: Raised when the shape input type is incorrect.
         """
         super().__init__(name=name)
+
+        if not isinstance(dtype, DataType):
+            raise TypeError("dtype should be a model signature datatype.")
         self._dtype = dtype
+
+        if shape and not isinstance(shape, tuple):
+            raise TypeError("Shape should be a tuple if presented.")
         self._shape = shape
 
     def as_snowpark_type(self) -> spt.DataType:
         result_type = self._dtype.as_snowpark_type()
         if not self._shape:
             return result_type
         for _ in range(len(self._shape)):
             result_type = spt.ArrayType(result_type)
         return result_type
 
     def as_dtype(self) -> npt.DTypeLike:
         """Convert to corresponding local Type."""
         if not self._shape:
             return self._dtype._numpy_type
-        return np.object0
+        return np.object_
 
     def __eq__(self, other: object) -> bool:
         if isinstance(other, FeatureSpec):
             return self._name == other._name and self._dtype == other._dtype and self._shape == other._shape
         else:
             return False
 
@@ -225,14 +282,16 @@
             input_dict: The dict containing information of the feature specification.
 
         Returns:
             A feature specification instance deserialized and created from the dict.
         """
         name = input_dict["name"]
         shape = input_dict.get("shape", None)
+        if shape:
+            shape = tuple(shape)
         type = DataType[input_dict["type"]]
         return FeatureSpec(name=name, dtype=type, shape=shape)
 
 
 class FeatureGroupSpec(BaseFeatureSpec):
     """Specification of a group of features in Snowflake native model packaging."""
 
@@ -417,15 +476,15 @@
     @staticmethod
     @abstractmethod
     def infer_signature(data: model_types._DataType, role: Literal["input", "output"]) -> Sequence[BaseFeatureSpec]:
         ...
 
     @staticmethod
     @abstractmethod
-    def convert_to_df(data: model_types._DataType) -> Union[pd.DataFrame, snowflake.snowpark.DataFrame]:
+    def convert_to_df(data: model_types._DataType, ensure_serializable: bool = True) -> pd.DataFrame:
         ...
 
 
 class _PandasDataFrameHandler(_BaseDataHandler[pd.DataFrame]):
     @staticmethod
     def can_handle(data: model_types.SupportedDataType) -> TypeGuard[pd.DataFrame]:
         return isinstance(data, pd.DataFrame)
@@ -450,15 +509,15 @@
         if len(df_cols) == 0:
             raise ValueError("Data Validation Error: Empty data is found.")
 
         if df_cols.dtype not in [
             np.int64,
             np.uint64,
             np.float64,
-            np.object0,
+            np.object_,
         ]:  # To keep compatibility with Pandas 2.x and 1.x
             raise ValueError("Data Validation Error: Unsupported column index type is found.")
 
         df_col_dtypes = [data[col].dtype for col in data.columns]
         for df_col, df_col_dtype in zip(df_cols, df_col_dtypes):
             if df_col_dtype == np.dtype("O"):
                 # Check if all objects have the same type
@@ -534,15 +593,25 @@
                 elif isinstance(data[df_col][0], bytes):
                     specs.append(FeatureSpec(dtype=DataType.BYTES, name=ft_name))
             else:
                 specs.append(FeatureSpec(dtype=DataType.from_numpy_type(df_col_dtype), name=ft_name))
         return specs
 
     @staticmethod
-    def convert_to_df(data: pd.DataFrame) -> pd.DataFrame:
+    def convert_to_df(data: pd.DataFrame, ensure_serializable: bool = True) -> pd.DataFrame:
+        if not ensure_serializable:
+            return data
+        # This convert is necessary since numpy dataframe cannot be correctly handled when provided as an element of
+        # a list when creating Snowpark Dataframe.
+        df_cols = data.columns
+        df_col_dtypes = [data[col].dtype for col in data.columns]
+        for df_col, df_col_dtype in zip(df_cols, df_col_dtypes):
+            if df_col_dtype == np.dtype("O"):
+                if isinstance(data[df_col][0], np.ndarray):
+                    data[df_col] = data[df_col].map(np.ndarray.tolist)
         return data
 
 
 class _NumpyArrayHandler(_BaseDataHandler[model_types._SupportedNumpyArray]):
     @staticmethod
     def can_handle(data: model_types.SupportedDataType) -> TypeGuard[model_types._SupportedNumpyArray]:
         return isinstance(data, np.ndarray)
@@ -565,15 +634,15 @@
             # scalar
             raise ValueError("Data Validation Error: Scalar data is found.")
 
     @staticmethod
     def infer_signature(
         data: model_types._SupportedNumpyArray, role: Literal["input", "output"]
     ) -> Sequence[BaseFeatureSpec]:
-        feature_prefix = f"{_PandasDataFrameHandler.FEATURE_PREFIX}_"
+        feature_prefix = f"{_NumpyArrayHandler.FEATURE_PREFIX}_"
         dtype = DataType.from_numpy_type(data.dtype)
         role_prefix = (_NumpyArrayHandler.INPUT_PREFIX if role == "input" else _NumpyArrayHandler.OUTPUT_PREFIX) + "_"
         if len(data.shape) == 1:
             return [FeatureSpec(dtype=dtype, name=f"{role_prefix}{feature_prefix}0")]
         else:
             # For high-dimension array, 0-axis is for batch, 1-axis is for column, further more is details of columns.
             features = []
@@ -584,76 +653,277 @@
                     ft_shape = np.shape(col_data)
                     features.append(FeatureSpec(dtype=dtype, name=ft_name, shape=ft_shape))
                 else:
                     features.append(FeatureSpec(dtype=dtype, name=ft_name))
             return features
 
     @staticmethod
-    def convert_to_df(data: model_types._SupportedNumpyArray) -> pd.DataFrame:
+    def convert_to_df(data: model_types._SupportedNumpyArray, ensure_serializable: bool = True) -> pd.DataFrame:
         if len(data.shape) == 1:
             data = np.expand_dims(data, axis=1)
         n_cols = data.shape[1]
         if len(data.shape) == 2:
-            return pd.DataFrame(data={i: data[:, i] for i in range(n_cols)})
+            return pd.DataFrame(data)
         else:
             n_rows = data.shape[0]
-            return pd.DataFrame(data={i: [np.array(data[k, i]) for k in range(n_rows)] for i in range(n_cols)})
+            if ensure_serializable:
+                return pd.DataFrame(data={i: [data[k, i].tolist() for k in range(n_rows)] for i in range(n_cols)})
+            return pd.DataFrame(data={i: [list(data[k, i]) for k in range(n_rows)] for i in range(n_cols)})
 
 
-class _ListOfNumpyArrayHandler(_BaseDataHandler[List[model_types._SupportedNumpyArray]]):
+class _SeqOfNumpyArrayHandler(_BaseDataHandler[Sequence[model_types._SupportedNumpyArray]]):
     @staticmethod
-    def can_handle(data: model_types.SupportedDataType) -> TypeGuard[List[model_types._SupportedNumpyArray]]:
-        return (
-            isinstance(data, list)
-            and len(data) > 0
-            and all(_NumpyArrayHandler.can_handle(data_col) for data_col in data)
-        )
+    def can_handle(data: model_types.SupportedDataType) -> TypeGuard[Sequence[model_types._SupportedNumpyArray]]:
+        if not isinstance(data, list):
+            return False
+        if len(data) == 0:
+            return False
+        if isinstance(data[0], np.ndarray):
+            return all(isinstance(data_col, np.ndarray) for data_col in data)
+        return False
 
     @staticmethod
-    def count(data: List[model_types._SupportedNumpyArray]) -> int:
+    def count(data: Sequence[model_types._SupportedNumpyArray]) -> int:
         return min(_NumpyArrayHandler.count(data_col) for data_col in data)
 
     @staticmethod
-    def truncate(data: List[model_types._SupportedNumpyArray]) -> List[model_types._SupportedNumpyArray]:
+    def truncate(data: Sequence[model_types._SupportedNumpyArray]) -> Sequence[model_types._SupportedNumpyArray]:
         return [
-            data_col[: min(_ListOfNumpyArrayHandler.count(data), _ListOfNumpyArrayHandler.SIG_INFER_ROWS_COUNT_LIMIT)]
+            data_col[: min(_SeqOfNumpyArrayHandler.count(data), _SeqOfNumpyArrayHandler.SIG_INFER_ROWS_COUNT_LIMIT)]
             for data_col in data
         ]
 
     @staticmethod
-    def validate(data: List[model_types._SupportedNumpyArray]) -> None:
+    def validate(data: Sequence[model_types._SupportedNumpyArray]) -> None:
         for data_col in data:
             _NumpyArrayHandler.validate(data_col)
 
     @staticmethod
     def infer_signature(
-        data: List[model_types._SupportedNumpyArray], role: Literal["input", "output"]
+        data: Sequence[model_types._SupportedNumpyArray], role: Literal["input", "output"]
     ) -> Sequence[BaseFeatureSpec]:
+        feature_prefix = f"{_SeqOfNumpyArrayHandler.FEATURE_PREFIX}_"
         features: List[BaseFeatureSpec] = []
         role_prefix = (
-            _ListOfNumpyArrayHandler.INPUT_PREFIX if role == "input" else _ListOfNumpyArrayHandler.OUTPUT_PREFIX
+            _SeqOfNumpyArrayHandler.INPUT_PREFIX if role == "input" else _SeqOfNumpyArrayHandler.OUTPUT_PREFIX
         ) + "_"
 
         for i, data_col in enumerate(data):
-            inferred_res = _NumpyArrayHandler.infer_signature(data_col, role)
-            for ft in inferred_res:
-                ft._name = f"{role_prefix}{i}_{ft._name[len(role_prefix):]}"
-            features.extend(inferred_res)
+            dtype = DataType.from_numpy_type(data_col.dtype)
+            ft_name = f"{role_prefix}{feature_prefix}{i}"
+            if len(data_col.shape) == 1:
+                features.append(FeatureSpec(dtype=dtype, name=ft_name))
+            else:
+                ft_shape = tuple(data_col.shape[1:])
+                features.append(FeatureSpec(dtype=dtype, name=ft_name, shape=ft_shape))
         return features
 
     @staticmethod
-    def convert_to_df(data: List[model_types._SupportedNumpyArray]) -> pd.DataFrame:
-        l_data = []
+    def convert_to_df(
+        data: Sequence[model_types._SupportedNumpyArray], ensure_serializable: bool = True
+    ) -> pd.DataFrame:
+        if ensure_serializable:
+            return pd.DataFrame(data={i: data_col.tolist() for i, data_col in enumerate(data)})
+        return pd.DataFrame(data={i: list(data_col) for i, data_col in enumerate(data)})
+
+
+class _SeqOfPyTorchTensorHandler(_BaseDataHandler[Sequence["torch.Tensor"]]):
+    @staticmethod
+    def can_handle(data: model_types.SupportedDataType) -> TypeGuard[Sequence["torch.Tensor"]]:
+        if not isinstance(data, list):
+            return False
+        if len(data) == 0:
+            return False
+        if type_utils.LazyType("torch.Tensor").isinstance(data[0]):
+            return all(type_utils.LazyType("torch.Tensor").isinstance(data_col) for data_col in data)
+        return False
+
+    @staticmethod
+    def count(data: Sequence["torch.Tensor"]) -> int:
+        return min(data_col.shape[0] for data_col in data)
+
+    @staticmethod
+    def truncate(data: Sequence["torch.Tensor"]) -> Sequence["torch.Tensor"]:
+        return [
+            data_col[
+                : min(_SeqOfPyTorchTensorHandler.count(data), _SeqOfPyTorchTensorHandler.SIG_INFER_ROWS_COUNT_LIMIT)
+            ]
+            for data_col in data
+        ]
+
+    @staticmethod
+    def validate(data: Sequence["torch.Tensor"]) -> None:
+        import torch
+
+        for data_col in data:
+            if data_col.shape == torch.Size([0]):
+                # Empty array
+                raise ValueError("Data Validation Error: Empty data is found.")
+
+            if data_col.shape == torch.Size([1]):
+                # scalar
+                raise ValueError("Data Validation Error: Scalar data is found.")
+
+    @staticmethod
+    def infer_signature(data: Sequence["torch.Tensor"], role: Literal["input", "output"]) -> Sequence[BaseFeatureSpec]:
+        feature_prefix = f"{_SeqOfPyTorchTensorHandler.FEATURE_PREFIX}_"
+        features: List[BaseFeatureSpec] = []
+        role_prefix = (
+            _SeqOfPyTorchTensorHandler.INPUT_PREFIX if role == "input" else _SeqOfPyTorchTensorHandler.OUTPUT_PREFIX
+        ) + "_"
+
+        for i, data_col in enumerate(data):
+            dtype = DataType.from_torch_type(data_col.dtype)
+            ft_name = f"{role_prefix}{feature_prefix}{i}"
+            if len(data_col.shape) == 1:
+                features.append(FeatureSpec(dtype=dtype, name=ft_name))
+            else:
+                ft_shape = tuple(data_col.shape[1:])
+                features.append(FeatureSpec(dtype=dtype, name=ft_name, shape=ft_shape))
+        return features
+
+    @staticmethod
+    def convert_to_df(data: Sequence["torch.Tensor"], ensure_serializable: bool = True) -> pd.DataFrame:
+        # Use list(...) instead of .tolist() to ensure that
+        # the content is still numpy array so that the type could be preserved.
+        # But that would not serializable and cannot use as UDF input and output.
+        if ensure_serializable:
+            return pd.DataFrame({i: data_col.detach().to("cpu").numpy().tolist() for i, data_col in enumerate(data)})
+        return pd.DataFrame({i: list(data_col.detach().to("cpu").numpy()) for i, data_col in enumerate(data)})
+
+    @staticmethod
+    def convert_from_df(
+        df: pd.DataFrame, features: Optional[Sequence[BaseFeatureSpec]] = None
+    ) -> Sequence["torch.Tensor"]:
+        import torch
+
+        res = []
+        if features:
+            for feature in features:
+                if isinstance(feature, FeatureGroupSpec):
+                    raise NotImplementedError("FeatureGroupSpec is not supported.")
+                assert isinstance(feature, FeatureSpec), "Invalid feature kind."
+                res.append(torch.from_numpy(np.stack(df[feature.name].to_numpy()).astype(feature._dtype._numpy_type)))
+            return res
+        return [torch.from_numpy(np.stack(df[col].to_numpy())) for col in df]
+
+
+class _SeqOfTensorflowTensorHandler(_BaseDataHandler[Sequence[Union["tensorflow.Tensor", "tensorflow.Variable"]]]):
+    @staticmethod
+    def can_handle(
+        data: model_types.SupportedDataType,
+    ) -> TypeGuard[Sequence[Union["tensorflow.Tensor", "tensorflow.Variable"]]]:
+        if not isinstance(data, list):
+            return False
+        if len(data) == 0:
+            return False
+        if type_utils.LazyType("tensorflow.Tensor").isinstance(data[0]) or type_utils.LazyType(
+            "tensorflow.Variable"
+        ).isinstance(data[0]):
+            return all(
+                type_utils.LazyType("tensorflow.Tensor").isinstance(data_col)
+                or type_utils.LazyType("tensorflow.Variable").isinstance(data_col)
+                for data_col in data
+            )
+        return False
+
+    @staticmethod
+    def count(data: Sequence[Union["tensorflow.Tensor", "tensorflow.Variable"]]) -> int:
+        import tensorflow as tf
+
+        rows = []
         for data_col in data:
+            shapes = data_col.shape.as_list()
+            if data_col.shape == tf.TensorShape(None) or (not shapes) or (shapes[0] is None):
+                # Unknown shape array
+                raise ValueError("Data Validation Error: Unknown shape data is found.")
+            # Make mypy happy
+            assert isinstance(shapes[0], int)
+
+            rows.append(shapes[0])
+
+        return min(rows)
+
+    @staticmethod
+    def truncate(
+        data: Sequence[Union["tensorflow.Tensor", "tensorflow.Variable"]]
+    ) -> Sequence[Union["tensorflow.Tensor", "tensorflow.Variable"]]:
+        return [
+            data_col[
+                : min(
+                    _SeqOfTensorflowTensorHandler.count(data), _SeqOfTensorflowTensorHandler.SIG_INFER_ROWS_COUNT_LIMIT
+                )
+            ]
+            for data_col in data
+        ]
+
+    @staticmethod
+    def validate(data: Sequence[Union["tensorflow.Tensor", "tensorflow.Variable"]]) -> None:
+        import tensorflow as tf
+
+        for data_col in data:
+            if data_col.shape == tf.TensorShape(None) or any(dim is None for dim in data_col.shape.as_list()):
+                # Unknown shape array
+                raise ValueError("Data Validation Error: Unknown shape data is found.")
+
+            if data_col.shape == tf.TensorShape([0]):
+                # Empty array
+                raise ValueError("Data Validation Error: Empty data is found.")
+
+            if data_col.shape == tf.TensorShape([1]) or data_col.shape == tf.TensorShape([]):
+                # scalar
+                raise ValueError("Data Validation Error: Scalar data is found.")
+
+    @staticmethod
+    def infer_signature(
+        data: Sequence[Union["tensorflow.Tensor", "tensorflow.Variable"]], role: Literal["input", "output"]
+    ) -> Sequence[BaseFeatureSpec]:
+        feature_prefix = f"{_SeqOfTensorflowTensorHandler.FEATURE_PREFIX}_"
+        features: List[BaseFeatureSpec] = []
+        role_prefix = (
+            _SeqOfTensorflowTensorHandler.INPUT_PREFIX
+            if role == "input"
+            else _SeqOfTensorflowTensorHandler.OUTPUT_PREFIX
+        ) + "_"
+
+        for i, data_col in enumerate(data):
+            dtype = DataType.from_numpy_type(data_col.dtype.as_numpy_dtype)
+            ft_name = f"{role_prefix}{feature_prefix}{i}"
             if len(data_col.shape) == 1:
-                l_data.append(np.expand_dims(data_col, axis=1))
+                features.append(FeatureSpec(dtype=dtype, name=ft_name))
             else:
-                l_data.append(data_col)
-        arr = np.concatenate(l_data, axis=1)
-        return _NumpyArrayHandler.convert_to_df(arr)
+                ft_shape = tuple(data_col.shape[1:])
+                features.append(FeatureSpec(dtype=dtype, name=ft_name, shape=ft_shape))
+        return features
+
+    @staticmethod
+    def convert_to_df(
+        data: Sequence[Union["tensorflow.Tensor", "tensorflow.Variable"]], ensure_serializable: bool = True
+    ) -> pd.DataFrame:
+        if ensure_serializable:
+            return pd.DataFrame({i: data_col.numpy().tolist() for i, data_col in enumerate(iterable=data)})
+        return pd.DataFrame({i: list(data_col.numpy()) for i, data_col in enumerate(iterable=data)})
+
+    @staticmethod
+    def convert_from_df(
+        df: pd.DataFrame, features: Optional[Sequence[BaseFeatureSpec]] = None
+    ) -> Sequence[Union["tensorflow.Tensor", "tensorflow.Variable"]]:
+        import tensorflow as tf
+
+        res = []
+        if features:
+            for feature in features:
+                if isinstance(feature, FeatureGroupSpec):
+                    raise NotImplementedError("FeatureGroupSpec is not supported.")
+                assert isinstance(feature, FeatureSpec), "Invalid feature kind."
+                res.append(
+                    tf.convert_to_tensor(np.stack(df[feature.name].to_numpy()).astype(feature._dtype._numpy_type))
+                )
+            return res
+        return [tf.convert_to_tensor(np.stack(df[col].to_numpy())) for col in df]
 
 
 class _ListOfBuiltinHandler(_BaseDataHandler[model_types._SupportedBuiltinsList]):
     @staticmethod
     def can_handle(data: model_types.SupportedDataType) -> TypeGuard[model_types._SupportedBuiltinsList]:
         return (
             isinstance(data, list)
@@ -680,15 +950,18 @@
     @staticmethod
     def infer_signature(
         data: model_types._SupportedBuiltinsList, role: Literal["input", "output"]
     ) -> Sequence[BaseFeatureSpec]:
         return _PandasDataFrameHandler.infer_signature(pd.DataFrame(data), role)
 
     @staticmethod
-    def convert_to_df(data: model_types._SupportedBuiltinsList) -> pd.DataFrame:
+    def convert_to_df(
+        data: model_types._SupportedBuiltinsList,
+        ensure_serializable: bool = True,
+    ) -> pd.DataFrame:
         return pd.DataFrame(data)
 
 
 class _SnowparkDataFrameHandler(_BaseDataHandler[snowflake.snowpark.DataFrame]):
     @staticmethod
     def can_handle(data: model_types.SupportedDataType) -> TypeGuard[snowflake.snowpark.DataFrame]:
         return isinstance(data, snowflake.snowpark.DataFrame)
@@ -701,40 +974,117 @@
     def truncate(data: snowflake.snowpark.DataFrame) -> snowflake.snowpark.DataFrame:
         return cast(snowflake.snowpark.DataFrame, data.limit(_SnowparkDataFrameHandler.SIG_INFER_ROWS_COUNT_LIMIT))
 
     @staticmethod
     def validate(data: snowflake.snowpark.DataFrame) -> None:
         schema = data.schema
         for field in schema.fields:
-            if not any(type.is_same_snowpark_type(field.datatype) for type in DataType):
+            data_type = field.datatype
+            if isinstance(data_type, spt.ArrayType):
+                actual_data_type = data_type.element_type
+            else:
+                actual_data_type = data_type
+            if not any(type.is_same_snowpark_type(actual_data_type) for type in DataType):
                 raise ValueError(
                     f"Data Validation Error: Unsupported data type {field.datatype} in column {field.name}."
                 )
 
     @staticmethod
     def infer_signature(
         data: snowflake.snowpark.DataFrame, role: Literal["input", "output"]
     ) -> Sequence[BaseFeatureSpec]:
         features: List[BaseFeatureSpec] = []
         schema = data.schema
         for field in schema.fields:
             name = identifier.get_unescaped_names(field.name)
-            features.append(FeatureSpec(name=name, dtype=DataType.from_snowpark_type(field.datatype)))
+            if isinstance(field.datatype, spt.ArrayType):
+                raise NotImplementedError("Cannot infer model signature from Snowpark DataFrame with Array Type.")
+            else:
+                features.append(FeatureSpec(name=name, dtype=DataType.from_snowpark_type(field.datatype)))
         return features
 
     @staticmethod
-    def convert_to_df(data: snowflake.snowpark.DataFrame) -> snowflake.snowpark.DataFrame:
-        return data
+    def convert_to_df(
+        data: snowflake.snowpark.DataFrame,
+        ensure_serializable: bool = True,
+        features: Optional[Sequence[BaseFeatureSpec]] = None,
+    ) -> pd.DataFrame:
+        # This method do things on top of to_pandas, to make sure the local dataframe got is in correct shape.
+        dtype_map = {}
+        if features:
+            for feature in features:
+                if isinstance(feature, FeatureGroupSpec):
+                    raise NotImplementedError("FeatureGroupSpec is not supported.")
+                assert isinstance(feature, FeatureSpec), "Invalid feature kind."
+                dtype_map[feature.name] = feature.as_dtype()
+        df_local = data.to_pandas()
+        # This is because Array will become string (Even though the correct schema is set)
+        # and object will become variant type and requires an additional loads
+        # to get correct data otherwise it would be string.
+        for field in data.schema.fields:
+            if isinstance(field.datatype, spt.ArrayType):
+                df_local[identifier.get_unescaped_names(field.name)] = df_local[
+                    identifier.get_unescaped_names(field.name)
+                ].map(json.loads)
+        # Only when the feature is not from inference, we are confident to do the type casting.
+        # Otherwise, dtype_map will be empty
+        df_local = df_local.astype(dtype=dtype_map)
+        return df_local
+
+    @staticmethod
+    def convert_from_df(
+        session: snowflake.snowpark.Session, df: pd.DataFrame, keep_order: bool = True
+    ) -> snowflake.snowpark.DataFrame:
+        # This method is necessary to create the Snowpark Dataframe in correct schema.
+        # Snowpark ignore the schema argument when providing a pandas DataFrame.
+        # However, in this case, if a cell of the original Dataframe is some array type,
+        # they will be inferred as VARIANT.
+        # To make sure Snowpark get the correct schema, we have to provide in a list of records.
+        # However, in this case, the order could not be preserved. Thus, a _ID column has to be added,
+        # if keep_order is True.
+        # Although in this case, the column with array type can get correct ARRAY type, however, the element
+        # type is not preserved, and will become string type. This affect the implementation of convert_from_df.
+        df = _PandasDataFrameHandler.convert_to_df(df)
+        df_cols = df.columns
+        if df_cols.dtype != np.object_:
+            raise ValueError("Cannot convert a Pandas DataFrame whose column index is not a string")
+        features = _PandasDataFrameHandler.infer_signature(df, role="input")
+        # Role will be no effect on the column index. That is to say, the feature name is the actual column name.
+        schema_list = []
+        for feature in features:
+            if isinstance(feature, FeatureGroupSpec):
+                raise NotImplementedError("FeatureGroupSpec is not supported.")
+            assert isinstance(feature, FeatureSpec), "Invalid feature kind."
+            schema_list.append(
+                spt.StructField(
+                    identifier.get_inferred_name(feature.name),
+                    feature.as_snowpark_type(),
+                    nullable=df[feature.name].isnull().any(),
+                )
+            )
+
+        data = df.rename(columns=identifier.get_inferred_name).to_dict("records")
+        if keep_order:
+            for idx, data_item in enumerate(data):
+                data_item[infer_template._KEEP_ORDER_COL_NAME] = idx
+            schema_list.append(spt.StructField(infer_template._KEEP_ORDER_COL_NAME, spt.LongType(), nullable=False))
+        sp_df = session.create_dataframe(
+            data,  # To make sure the schema can be used, otherwise, array will become variant.
+            spt.StructType(schema_list),
+        )
+        return sp_df
 
 
 _LOCAL_DATA_HANDLERS: List[Type[_BaseDataHandler[Any]]] = [
     _PandasDataFrameHandler,
     _NumpyArrayHandler,
-    _ListOfNumpyArrayHandler,
     _ListOfBuiltinHandler,
+    _SeqOfNumpyArrayHandler,
+    _SeqOfPyTorchTensorHandler,
+    _SeqOfTensorflowTensorHandler,
 ]
 _ALL_DATA_HANDLERS = _LOCAL_DATA_HANDLERS + [_SnowparkDataFrameHandler]
 
 
 def _truncate_data(data: model_types.SupportedDataType) -> model_types.SupportedDataType:
     for handler in _ALL_DATA_HANDLERS:
         if handler.can_handle(data):
@@ -1003,49 +1353,79 @@
                         + " inference might fail if there is null value.",
                         category=RuntimeWarning,
                     )
                 if isinstance(feature, FeatureGroupSpec):
                     raise NotImplementedError("FeatureGroupSpec is not supported.")
                 assert isinstance(feature, FeatureSpec), "Invalid feature kind."
                 ft_type = feature._dtype
-                if not ft_type.is_same_snowpark_type(field.datatype):
-                    raise ValueError(
-                        f"Data Validation Error in feature {ft_name}: "
-                        + f"Feature type {ft_type} is not met by column {field.name}."
+                field_data_type = field.datatype
+                if isinstance(field_data_type, spt.ArrayType):
+                    if feature._shape is None:
+                        raise ValueError(
+                            f"Data Validation Error in feature {ft_name}: "
+                            + f"Feature is a array feature, while {field.name} is not."
+                        )
+                    warnings.warn(
+                        f"Warn in feature {ft_name}: Feature is a array feature," + " type validation cannot happen.",
+                        category=RuntimeWarning,
                     )
+                else:
+                    if feature._shape:
+                        raise ValueError(
+                            f"Data Validation Error in feature {ft_name}: "
+                            + f"Feature is a scalar feature, while {field.name} is not."
+                        )
+                    if not ft_type.is_same_snowpark_type(field_data_type):
+                        raise ValueError(
+                            f"Data Validation Error in feature {ft_name}: "
+                            + f"Feature type {ft_type} is not met by column {field.name}."
+                        )
         if not found:
             raise ValueError(f"Data Validation Error: feature {ft_name} does not exist in data.")
 
 
-def _convert_and_validate_local_data(
-    data: model_types.SupportedDataType, features: Sequence[BaseFeatureSpec]
-) -> pd.DataFrame:
-    """Validate the data with features in model signature and convert to DataFrame
+def _convert_local_data_to_df(data: model_types.SupportedLocalDataType) -> pd.DataFrame:
+    """Convert local data to pandas DataFrame or Snowpark DataFrame
 
     Args:
-        features: A list of feature specs that the data should follow.
         data: The provided data.
 
     Raises:
         ValueError: Raised when data cannot be handled by any data handler.
 
     Returns:
         The converted dataframe with renamed column index.
     """
     df = None
     for handler in _LOCAL_DATA_HANDLERS:
         if handler.can_handle(data):
             handler.validate(data)
-            df = handler.convert_to_df(data)
+            df = handler.convert_to_df(data, ensure_serializable=False)
             break
     if df is None:
         raise ValueError(f"Data Validation Error: Un-supported type {type(data)} provided.")
-    assert isinstance(df, pd.DataFrame)
+    return df
+
+
+def _convert_and_validate_local_data(
+    data: model_types.SupportedLocalDataType, features: Sequence[BaseFeatureSpec]
+) -> pd.DataFrame:
+    """Validate the data with features in model signature and convert to DataFrame
+
+    Args:
+        features: A list of feature specs that the data should follow.
+        data: The provided data.
+
+    Returns:
+        The converted dataframe with renamed column index.
+    """
+    df = _convert_local_data_to_df(data)
     df = _rename_pandas_df(df, features)
     _validate_pandas_df(df, features)
+    df = _PandasDataFrameHandler.convert_to_df(df, ensure_serializable=True)
 
     return df
 
 
 def infer_signature(
     input_data: model_types.SupportedLocalDataType,
     output_data: model_types.SupportedLocalDataType,
```

## snowflake/ml/model/type_hints.py

```diff
@@ -5,14 +5,16 @@
 from typing_extensions import NotRequired, TypeAlias
 
 if TYPE_CHECKING:
     import numpy as np
     import pandas as pd
     import sklearn.base
     import sklearn.pipeline
+    import tensorflow
+    import torch
     import xgboost
 
     import snowflake.ml.model.custom_model
     import snowflake.snowpark
     from snowflake.ml.modeling.framework import base  # noqa: F401
 
 
@@ -30,31 +32,34 @@
     "np.uint64",
     "np.bool8",
     "np.str0",
     "np.bytes0",
 ]
 _SupportedNumpyArray = npt.NDArray[_SupportedNumpyDtype]
 _SupportedBuiltinsList = Sequence[_SupportedBuiltins]
+_SupportedArrayLike = Union[_SupportedNumpyArray, "torch.Tensor", "tensorflow.Tensor", "tensorflow.Variable"]
 
 SupportedLocalDataType = Union[
-    "pd.DataFrame", _SupportedNumpyArray, Sequence[_SupportedNumpyArray], _SupportedBuiltinsList
+    "pd.DataFrame", _SupportedNumpyArray, Sequence[_SupportedArrayLike], _SupportedBuiltinsList
 ]
 
 SupportedDataType = Union[SupportedLocalDataType, "snowflake.snowpark.DataFrame"]
 
 _DataType = TypeVar("_DataType", bound=SupportedDataType)
 
 CustomModelType = TypeVar("CustomModelType", bound="snowflake.ml.model.custom_model.CustomModel")
 
 SupportedLocalModelType = Union[
     "snowflake.ml.model.custom_model.CustomModel",
     "sklearn.base.BaseEstimator",
     "sklearn.pipeline.Pipeline",
     "xgboost.XGBModel",
     "xgboost.Booster",
+    "torch.nn.Module",
+    "torch.jit.ScriptModule",  # type:ignore[name-defined]
 ]
 
 SupportedSnowMLModelType: TypeAlias = "base.BaseEstimator"
 
 SupportedModelType = Union[
     SupportedLocalModelType,
     SupportedSnowMLModelType,
@@ -66,36 +71,42 @@
 |---------------------------------|--------------|---------------------|
 | snowflake.ml.model.custom_model.CustomModel | custom.py    | _CustomModelHandler |
 | sklearn.base.BaseEstimator      | sklearn.py   | _SKLModelHandler    |
 | sklearn.pipeline.Pipeline       | sklearn.py   | _SKLModelHandler    |
 | xgboost.XGBModel       | xgboost.py   | _XGBModelHandler    |
 | xgboost.Booster        | xgboost.py   | _XGBModelHandler    |
 | snowflake.ml.framework.base.BaseEstimator      | snowmlmodel.py   | _SnowMLModelHandler    |
+| torch.nn.Module      | pytroch.py   | _PyTorchHandler    |
+| torch.jit.ScriptModule      | torchscript.py   | _TorchScripthHandler    |
 """
 
 
 _ModelType = TypeVar("_ModelType", bound=SupportedModelType)
 
 
 class DeployOptions(TypedDict):
     """Common Options for deploying to Snowflake.
 
-    output_with_input_features: Whether or not preserve the input columns in the output when predicting.
-        Defaults to False.
+    disable_local_conda_resolver: Set to disable use local conda resolver to do pre-check on environment and rely on
+        the information schema only. Defaults to False.
     keep_order: Whether or not preserve the row order when predicting. Only available for dataframe has fewer than 2**64
         rows. Defaults to True.
+    output_with_input_features: Whether or not preserve the input columns in the output when predicting.
+        Defaults to False.
     """
 
-    output_with_input_features: NotRequired[bool]
+    disable_local_conda_resolver: NotRequired[bool]
     keep_order: NotRequired[bool]
+    output_with_input_features: NotRequired[bool]
 
 
 class WarehouseDeployOptions(DeployOptions):
     """Options for deploying to the Snowflake Warehouse.
 
+
     permanent_udf_stage_location: A Snowflake stage option where the UDF should be persisted. If specified, the model
         will be deployed as a permanent UDF, otherwise temporary.
     relax_version: Whether or not relax the version constraints of the dependencies if unresolvable. Defaults to False.
     replace_udf: Flag to indicate when deploying model as permanent UDF, whether overwriting existed UDF is allowed.
         Default to False.
     """
 
@@ -126,7 +137,15 @@
 
 class XGBModelSaveOptions(ModelSaveOption):
     target_methods: NotRequired[Sequence[str]]
 
 
 class SNOWModelSaveOptions(ModelSaveOption):
     target_methods: NotRequired[Sequence[str]]
+
+
+class PyTorchSaveOptions(ModelSaveOption):
+    target_methods: NotRequired[Sequence[str]]
+
+
+class TorchScriptSaveOptions(ModelSaveOption):
+    target_methods: NotRequired[Sequence[str]]
```

## snowflake/ml/modeling/calibration/calibrated_classifier_cv.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -243,15 +245,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         deps = deps | _gather_dependencies(estimator)
         deps = deps | _gather_dependencies(base_estimator)
         self._deps = list(deps)
         estimator = _transform_snowml_obj_to_sklearn_obj(estimator)
         base_estimator = _transform_snowml_obj_to_sklearn_obj(base_estimator)
         init_args = {'estimator':(estimator, None, False),
@@ -270,14 +271,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -348,32 +358,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -391,23 +402,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -466,33 +479,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -530,15 +543,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -622,15 +635,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -789,19 +802,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -864,18 +884,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1096,30 +1116,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1137,22 +1158,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1194,22 +1217,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1219,26 +1242,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/cluster/affinity_propagation.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -220,15 +222,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'damping':(damping, 0.5, False),
             'max_iter':(max_iter, 200, False),
             'convergence_iter':(convergence_iter, 15, False),
@@ -247,14 +248,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -325,32 +335,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -368,23 +379,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -443,33 +456,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -507,15 +520,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -599,15 +612,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -766,19 +779,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -841,18 +861,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1069,30 +1089,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1110,22 +1131,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1167,22 +1190,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1192,26 +1215,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/cluster/agglomerative_clustering.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -252,15 +254,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'n_clusters':(n_clusters, 2, False),
             'affinity':(affinity, "deprecated", False),
             'metric':(metric, None, False),
@@ -280,14 +281,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -358,32 +368,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -401,23 +412,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -476,33 +489,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -540,15 +553,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -632,15 +645,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -797,19 +810,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -872,18 +892,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1100,30 +1120,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1141,22 +1162,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1198,22 +1221,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1223,26 +1246,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/cluster/birch.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -214,15 +216,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'threshold':(threshold, 0.5, False),
             'branching_factor':(branching_factor, 50, False),
             'n_clusters':(n_clusters, 3, False),
@@ -238,14 +239,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -316,32 +326,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -359,23 +370,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -434,33 +447,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -498,15 +511,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -590,15 +603,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -757,19 +770,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -834,18 +854,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1062,30 +1082,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1103,22 +1124,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1160,22 +1183,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1185,26 +1208,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/cluster/bisecting_k_means.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -258,15 +260,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'n_clusters':(n_clusters, 8, False),
             'init':(init, "random", False),
             'n_init':(n_init, 1, False),
@@ -287,14 +288,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -365,32 +375,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -408,23 +419,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -483,33 +496,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -547,15 +560,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -639,15 +652,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -806,19 +819,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -883,18 +903,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1111,30 +1131,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1152,22 +1173,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1209,22 +1232,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1234,26 +1257,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/cluster/dbscan.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -228,15 +230,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'eps':(eps, 0.5, False),
             'min_samples':(min_samples, 5, False),
             'metric':(metric, "euclidean", False),
@@ -255,14 +256,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -333,32 +343,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -376,23 +387,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -451,33 +464,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -515,15 +528,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -607,15 +620,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -772,19 +785,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -847,18 +867,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1075,30 +1095,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1116,22 +1137,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1173,22 +1196,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1198,26 +1221,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/cluster/feature_agglomeration.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -258,15 +260,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'n_clusters':(n_clusters, 2, False),
             'affinity':(affinity, "deprecated", False),
             'metric':(metric, None, False),
@@ -287,14 +288,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -365,32 +375,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -408,23 +419,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -483,33 +496,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -547,15 +560,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -639,15 +652,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -804,19 +817,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -881,18 +901,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1109,30 +1129,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1150,22 +1171,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1207,22 +1230,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1232,26 +1255,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/cluster/k_means.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -254,15 +256,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'n_clusters':(n_clusters, 8, False),
             'init':(init, "k-means++", False),
             'n_init':(n_init, "warn", False),
@@ -282,14 +283,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -360,32 +370,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -403,23 +414,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -478,33 +491,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -542,15 +555,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -634,15 +647,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -801,19 +814,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -878,18 +898,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1106,30 +1126,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1147,22 +1168,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1204,22 +1227,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1229,26 +1252,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/cluster/mean_shift.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -232,15 +234,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'bandwidth':(bandwidth, None, False),
             'seeds':(seeds, None, False),
             'bin_seeding':(bin_seeding, False, False),
@@ -258,14 +259,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -336,32 +346,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -379,23 +390,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -454,33 +467,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -518,15 +531,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -610,15 +623,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -777,19 +790,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -852,18 +872,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1080,30 +1100,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1121,22 +1142,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1178,22 +1201,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1203,26 +1226,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/cluster/mini_batch_k_means.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -277,15 +279,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'n_clusters':(n_clusters, 8, False),
             'init':(init, "k-means++", False),
             'max_iter':(max_iter, 100, False),
@@ -308,14 +309,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -386,32 +396,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -429,23 +440,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -504,33 +517,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -568,15 +581,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -660,15 +673,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -827,19 +840,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -904,18 +924,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1132,30 +1152,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1173,22 +1194,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1230,22 +1253,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1255,26 +1278,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/cluster/optics.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -295,15 +297,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'min_samples':(min_samples, 5, False),
             'max_eps':(max_eps, np.inf, False),
             'metric':(metric, "minkowski", False),
@@ -328,14 +329,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -406,32 +416,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -449,23 +460,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -524,33 +537,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -588,15 +601,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -680,15 +693,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -845,19 +858,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -920,18 +940,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1148,30 +1168,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1189,22 +1210,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1246,22 +1269,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1271,26 +1294,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/cluster/spectral_biclustering.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -237,15 +239,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'n_clusters':(n_clusters, 3, False),
             'method':(method, "bistochastic", False),
             'n_components':(n_components, 6, False),
@@ -266,14 +267,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -344,32 +354,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -387,23 +398,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -462,33 +475,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -526,15 +539,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -618,15 +631,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -783,19 +796,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -858,18 +878,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1086,30 +1106,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1127,22 +1148,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1184,22 +1207,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1209,26 +1232,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/cluster/spectral_clustering.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -290,15 +292,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'n_clusters':(n_clusters, 8, False),
             'eigen_solver':(eigen_solver, None, False),
             'n_components':(n_components, None, False),
@@ -324,14 +325,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -402,32 +412,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -445,23 +456,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -520,33 +533,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -584,15 +597,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -676,15 +689,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -841,19 +854,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -916,18 +936,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1144,30 +1164,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1185,22 +1206,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1242,22 +1265,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1267,26 +1290,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/cluster/spectral_coclustering.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -219,15 +221,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'n_clusters':(n_clusters, 3, False),
             'svd_method':(svd_method, "randomized", False),
             'n_svd_vecs':(n_svd_vecs, None, False),
@@ -245,14 +246,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -323,32 +333,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -366,23 +377,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -441,33 +454,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -505,15 +518,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -597,15 +610,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -762,19 +775,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -837,18 +857,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1065,30 +1085,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1106,22 +1127,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1163,22 +1186,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1188,26 +1211,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/compose/column_transformer.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -249,15 +251,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         deps = deps | _gather_dependencies(transformers)
         self._deps = list(deps)
         transformers = _transform_snowml_obj_to_sklearn_obj(transformers)
         init_args = {'transformers':(transformers, None, True),
             'remainder':(remainder, "drop", False),
             'sparse_threshold':(sparse_threshold, 0.3, False),
@@ -275,14 +276,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -353,32 +363,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -396,23 +407,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -471,33 +484,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -535,15 +548,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -627,15 +640,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -792,19 +805,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -869,18 +889,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1097,30 +1117,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1138,22 +1159,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1195,22 +1218,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1220,26 +1243,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/compose/transformed_target_regressor.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -210,15 +212,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'regressor':(regressor, None, False),
             'transformer':(transformer, None, False),
             'func':(func, None, False),
@@ -234,14 +235,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -312,32 +322,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -355,23 +366,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -430,33 +443,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -494,15 +507,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -586,15 +599,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -753,19 +766,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -828,18 +848,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1056,30 +1076,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1097,22 +1118,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1154,22 +1177,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1179,26 +1202,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/covariance/elliptic_envelope.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -207,15 +209,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'store_precision':(store_precision, True, False),
             'assume_centered':(assume_centered, False, False),
             'support_fraction':(support_fraction, None, False),
@@ -231,14 +232,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -309,32 +319,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -352,23 +363,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -427,33 +440,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -491,15 +504,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -583,15 +596,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -750,19 +763,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -825,18 +845,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1055,30 +1075,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1096,22 +1117,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1153,22 +1176,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1178,26 +1201,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/covariance/empirical_covariance.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -186,15 +188,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'store_precision':(store_precision, True, False),
             'assume_centered':(assume_centered, False, False),}
         cleaned_up_init_args = _validate_sklearn_args(
@@ -207,14 +208,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -285,32 +295,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -328,23 +339,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -403,33 +416,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -467,15 +480,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -559,15 +572,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -724,19 +737,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -799,18 +819,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1027,30 +1047,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1068,22 +1089,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1125,22 +1148,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1150,26 +1173,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/covariance/graphical_lasso.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -215,15 +217,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'alpha':(alpha, 0.01, False),
             'mode':(mode, "cd", False),
             'tol':(tol, 0.0001, False),
@@ -241,14 +242,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -319,32 +329,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -362,23 +373,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -437,33 +450,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -501,15 +514,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -593,15 +606,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -758,19 +771,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -833,18 +853,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1061,30 +1081,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1102,22 +1123,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1159,22 +1182,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1184,26 +1207,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/covariance/graphical_lasso_cv.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -245,15 +247,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'alphas':(alphas, 4, False),
             'n_refinements':(n_refinements, 4, False),
             'cv':(cv, None, False),
@@ -274,14 +275,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -352,32 +362,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -395,23 +406,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -470,33 +483,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -534,15 +547,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -626,15 +639,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -791,19 +804,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -866,18 +886,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1094,30 +1114,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1135,22 +1156,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1192,22 +1215,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1217,26 +1240,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/covariance/ledoit_wolf.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -192,15 +194,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'store_precision':(store_precision, True, False),
             'assume_centered':(assume_centered, False, False),
             'block_size':(block_size, 1000, False),}
@@ -214,14 +215,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -292,32 +302,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -335,23 +346,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -410,33 +423,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -474,15 +487,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -566,15 +579,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -731,19 +744,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -806,18 +826,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1034,30 +1054,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1075,22 +1096,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1132,22 +1155,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1157,26 +1180,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/covariance/min_cov_det.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -203,15 +205,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'store_precision':(store_precision, True, False),
             'assume_centered':(assume_centered, False, False),
             'support_fraction':(support_fraction, None, False),
@@ -226,14 +227,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -304,32 +314,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -347,23 +358,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -422,33 +435,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -486,15 +499,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -578,15 +591,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -743,19 +756,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -818,18 +838,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1046,30 +1066,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1087,22 +1108,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1144,22 +1167,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1169,26 +1192,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/covariance/oas.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -186,15 +188,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'store_precision':(store_precision, True, False),
             'assume_centered':(assume_centered, False, False),}
         cleaned_up_init_args = _validate_sklearn_args(
@@ -207,14 +208,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -285,32 +295,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -328,23 +339,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -403,33 +416,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -467,15 +480,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -559,15 +572,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -724,19 +737,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -799,18 +819,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1027,30 +1047,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1068,22 +1089,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1125,22 +1148,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1150,26 +1173,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/covariance/shrunk_covariance.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -191,15 +193,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'store_precision':(store_precision, True, False),
             'assume_centered':(assume_centered, False, False),
             'shrinkage':(shrinkage, 0.1, False),}
@@ -213,14 +214,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -291,32 +301,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -334,23 +345,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -409,33 +422,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -473,15 +486,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -565,15 +578,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -730,19 +743,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -805,18 +825,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1033,30 +1053,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1074,22 +1095,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1131,22 +1154,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1156,26 +1179,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/decomposition/dictionary_learning.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -278,15 +280,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'n_components':(n_components, None, False),
             'alpha':(alpha, 1, False),
             'max_iter':(max_iter, 1000, False),
@@ -314,14 +315,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -392,32 +402,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -435,23 +446,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -510,33 +523,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -574,15 +587,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -666,15 +679,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -831,19 +844,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -908,18 +928,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1136,30 +1156,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1177,22 +1198,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1234,22 +1257,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1259,26 +1282,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/decomposition/factor_analysis.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -228,15 +230,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'n_components':(n_components, None, False),
             'tol':(tol, 0.01, False),
             'copy':(copy, True, False),
@@ -256,14 +257,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -334,32 +344,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -377,23 +388,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -452,33 +465,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -516,15 +529,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -608,15 +621,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -773,19 +786,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -850,18 +870,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1078,30 +1098,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1119,22 +1140,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1176,22 +1199,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1201,26 +1224,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/decomposition/fast_ica.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -245,15 +247,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'n_components':(n_components, None, False),
             'algorithm':(algorithm, "parallel", False),
             'whiten':(whiten, "warn", False),
@@ -274,14 +275,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -352,32 +362,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -395,23 +406,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -470,33 +483,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -534,15 +547,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -626,15 +639,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -791,19 +804,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -868,18 +888,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1096,30 +1116,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1137,22 +1158,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1194,22 +1217,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1219,26 +1242,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/decomposition/incremental_pca.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -203,15 +205,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'n_components':(n_components, None, False),
             'whiten':(whiten, False, False),
             'copy':(copy, True, False),
@@ -226,14 +227,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -304,32 +314,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -347,23 +358,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -422,33 +435,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -486,15 +499,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -578,15 +591,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -743,19 +756,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -820,18 +840,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1048,30 +1068,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1089,22 +1110,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1146,22 +1169,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1171,26 +1194,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/decomposition/kernel_pca.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -287,15 +289,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'n_components':(n_components, None, False),
             'kernel':(kernel, "linear", False),
             'gamma':(gamma, None, False),
@@ -322,14 +323,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -400,32 +410,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -443,23 +454,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -518,33 +531,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -582,15 +595,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -674,15 +687,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -839,19 +852,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -916,18 +936,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1144,30 +1164,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1185,22 +1206,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1242,22 +1265,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1267,26 +1290,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/decomposition/mini_batch_dictionary_learning.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -304,15 +306,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'n_components':(n_components, None, False),
             'alpha':(alpha, 1, False),
             'n_iter':(n_iter, "deprecated", False),
@@ -344,14 +345,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -422,32 +432,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -465,23 +476,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -540,33 +553,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -604,15 +617,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -696,15 +709,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -861,19 +874,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -938,18 +958,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1166,30 +1186,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1207,22 +1228,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1264,22 +1287,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1289,26 +1312,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/decomposition/mini_batch_sparse_pca.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -256,15 +258,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'n_components':(n_components, None, False),
             'alpha':(alpha, 1, False),
             'ridge_alpha':(ridge_alpha, 0.01, False),
@@ -289,14 +290,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -367,32 +377,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -410,23 +421,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -485,33 +498,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -549,15 +562,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -641,15 +654,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -806,19 +819,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -883,18 +903,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1111,30 +1131,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1152,22 +1173,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1209,22 +1232,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1234,26 +1257,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/decomposition/pca.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -263,15 +265,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'n_components':(n_components, None, False),
             'copy':(copy, True, False),
             'whiten':(whiten, False, False),
@@ -291,14 +292,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -369,32 +379,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -412,23 +423,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -487,33 +500,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -551,15 +564,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -643,15 +656,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -808,19 +821,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -885,18 +905,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1113,30 +1133,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1154,22 +1175,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1211,22 +1234,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1236,26 +1259,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/decomposition/sparse_pca.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -234,15 +236,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'n_components':(n_components, None, False),
             'alpha':(alpha, 1, False),
             'ridge_alpha':(ridge_alpha, 0.01, False),
@@ -264,14 +265,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -342,32 +352,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -385,23 +396,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -460,33 +473,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -524,15 +537,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -616,15 +629,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -781,19 +794,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -858,18 +878,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1086,30 +1106,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1127,22 +1148,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1184,22 +1207,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1209,26 +1232,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/decomposition/truncated_svd.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -219,15 +221,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'n_components':(n_components, 2, False),
             'algorithm':(algorithm, "randomized", False),
             'n_iter':(n_iter, 5, False),
@@ -245,14 +246,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -323,32 +333,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -366,23 +377,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -441,33 +454,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -505,15 +518,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -597,15 +610,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -762,19 +775,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -839,18 +859,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1067,30 +1087,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1108,22 +1129,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1165,22 +1188,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1190,26 +1213,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/discriminant_analysis/linear_discriminant_analysis.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -234,15 +236,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'solver':(solver, "svd", False),
             'shrinkage':(shrinkage, None, False),
             'priors':(priors, None, False),
@@ -260,14 +261,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -338,32 +348,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -381,23 +392,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -456,33 +469,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -520,15 +533,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -612,15 +625,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -779,19 +792,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -856,18 +876,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1090,30 +1110,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1131,22 +1152,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1188,22 +1211,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1213,26 +1236,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/discriminant_analysis/quadratic_discriminant_analysis.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -199,15 +201,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'priors':(priors, None, False),
             'reg_param':(reg_param, 0.0, False),
             'store_covariance':(store_covariance, False, False),
@@ -222,14 +223,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -300,32 +310,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -343,23 +354,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -418,33 +431,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -482,15 +495,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -574,15 +587,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -741,19 +754,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -816,18 +836,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1050,30 +1070,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1091,22 +1112,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1148,22 +1171,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1173,26 +1196,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/ensemble/ada_boost_classifier.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -220,15 +222,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         deps = deps | _gather_dependencies(estimator)
         deps = deps | _gather_dependencies(base_estimator)
         self._deps = list(deps)
         estimator = _transform_snowml_obj_to_sklearn_obj(estimator)
         base_estimator = _transform_snowml_obj_to_sklearn_obj(base_estimator)
         init_args = {'estimator':(estimator, None, False),
@@ -247,14 +248,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -325,32 +335,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -368,23 +379,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -443,33 +456,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -507,15 +520,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -599,15 +612,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -766,19 +779,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -841,18 +861,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1075,30 +1095,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1116,22 +1137,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1173,22 +1196,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1198,26 +1221,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/ensemble/ada_boost_regressor.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -217,15 +219,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         deps = deps | _gather_dependencies(estimator)
         deps = deps | _gather_dependencies(base_estimator)
         self._deps = list(deps)
         estimator = _transform_snowml_obj_to_sklearn_obj(estimator)
         base_estimator = _transform_snowml_obj_to_sklearn_obj(base_estimator)
         init_args = {'estimator':(estimator, None, False),
@@ -244,14 +245,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -322,32 +332,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -365,23 +376,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -440,33 +453,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -504,15 +517,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -596,15 +609,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -763,19 +776,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -838,18 +858,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1066,30 +1086,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1107,22 +1128,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1164,22 +1187,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1189,26 +1212,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/ensemble/bagging_classifier.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -246,15 +248,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         deps = deps | _gather_dependencies(estimator)
         deps = deps | _gather_dependencies(base_estimator)
         self._deps = list(deps)
         estimator = _transform_snowml_obj_to_sklearn_obj(estimator)
         base_estimator = _transform_snowml_obj_to_sklearn_obj(base_estimator)
         init_args = {'estimator':(estimator, None, False),
@@ -279,14 +280,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -357,32 +367,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -400,23 +411,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -475,33 +488,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -539,15 +552,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -631,15 +644,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -798,19 +811,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -873,18 +893,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1107,30 +1127,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1148,22 +1169,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1205,22 +1228,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1230,26 +1253,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/ensemble/bagging_regressor.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -246,15 +248,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         deps = deps | _gather_dependencies(estimator)
         deps = deps | _gather_dependencies(base_estimator)
         self._deps = list(deps)
         estimator = _transform_snowml_obj_to_sklearn_obj(estimator)
         base_estimator = _transform_snowml_obj_to_sklearn_obj(base_estimator)
         init_args = {'estimator':(estimator, None, False),
@@ -279,14 +280,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -357,32 +367,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -400,23 +411,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -475,33 +488,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -539,15 +552,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -631,15 +644,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -798,19 +811,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -873,18 +893,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1101,30 +1121,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1142,22 +1163,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1199,22 +1222,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1224,26 +1247,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/ensemble/extra_trees_classifier.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -344,15 +346,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'n_estimators':(n_estimators, 100, False),
             'criterion':(criterion, "gini", False),
             'max_depth':(max_depth, None, False),
@@ -381,14 +382,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -459,32 +469,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -502,23 +513,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -577,33 +590,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -641,15 +654,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -733,15 +746,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -900,19 +913,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -975,18 +995,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1207,30 +1227,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1248,22 +1269,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1305,22 +1328,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1330,26 +1353,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/ensemble/extra_trees_regressor.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -324,15 +326,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'n_estimators':(n_estimators, 100, False),
             'criterion':(criterion, "squared_error", False),
             'max_depth':(max_depth, None, False),
@@ -360,14 +361,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -438,32 +448,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -481,23 +492,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -556,33 +569,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -620,15 +633,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -712,15 +725,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -879,19 +892,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -954,18 +974,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1182,30 +1202,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1223,22 +1244,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1280,22 +1303,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1305,26 +1328,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/ensemble/gradient_boosting_classifier.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -356,15 +358,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'loss':(loss, "log_loss", False),
             'learning_rate':(learning_rate, 0.1, False),
             'n_estimators':(n_estimators, 100, False),
@@ -395,14 +396,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -473,32 +483,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -516,23 +527,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -591,33 +604,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -655,15 +668,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -747,15 +760,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -914,19 +927,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -989,18 +1009,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1223,30 +1243,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1264,22 +1285,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1321,22 +1344,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1346,26 +1369,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/ensemble/gradient_boosting_regressor.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -364,15 +366,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'loss':(loss, "squared_error", False),
             'learning_rate':(learning_rate, 0.1, False),
             'n_estimators':(n_estimators, 100, False),
@@ -404,14 +405,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -482,32 +492,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -525,23 +536,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -600,33 +613,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -664,15 +677,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -756,15 +769,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -923,19 +936,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -998,18 +1018,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1226,30 +1246,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1267,22 +1288,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1324,22 +1347,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1349,26 +1372,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/ensemble/hist_gradient_boosting_classifier.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -334,15 +336,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'loss':(loss, "log_loss", False),
             'learning_rate':(learning_rate, 0.1, False),
             'max_iter':(max_iter, 100, False),
@@ -373,14 +374,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -451,32 +461,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -494,23 +505,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -569,33 +582,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -633,15 +646,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -725,15 +738,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -892,19 +905,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -967,18 +987,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1201,30 +1221,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1242,22 +1263,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1299,22 +1322,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1324,26 +1347,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/ensemble/hist_gradient_boosting_regressor.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -324,15 +326,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'loss':(loss, "squared_error", False),
             'quantile':(quantile, None, False),
             'learning_rate':(learning_rate, 0.1, False),
@@ -363,14 +364,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -441,32 +451,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -484,23 +495,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -559,33 +572,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -623,15 +636,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -715,15 +728,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -882,19 +895,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -957,18 +977,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1185,30 +1205,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1226,22 +1247,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1283,22 +1306,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1308,26 +1331,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/ensemble/isolation_forest.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -240,15 +242,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'n_estimators':(n_estimators, 100, False),
             'max_samples':(max_samples, "auto", False),
             'contamination':(contamination, "auto", False),
@@ -268,14 +269,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -346,32 +356,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -389,23 +400,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -464,33 +477,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -528,15 +541,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -620,15 +633,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -787,19 +800,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -862,18 +882,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1092,30 +1112,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1133,22 +1154,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1190,22 +1213,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1215,26 +1238,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/ensemble/random_forest_classifier.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -340,15 +342,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'n_estimators':(n_estimators, 100, False),
             'criterion':(criterion, "gini", False),
             'max_depth':(max_depth, None, False),
@@ -377,14 +378,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -455,32 +465,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -498,23 +509,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -573,33 +586,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -637,15 +650,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -729,15 +742,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -896,19 +909,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -971,18 +991,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1203,30 +1223,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1244,22 +1265,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1301,22 +1324,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1326,26 +1349,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/ensemble/random_forest_regressor.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -320,15 +322,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'n_estimators':(n_estimators, 100, False),
             'criterion':(criterion, "squared_error", False),
             'max_depth':(max_depth, None, False),
@@ -356,14 +357,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -434,32 +444,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -477,23 +488,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -552,33 +565,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -616,15 +629,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -708,15 +721,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -875,19 +888,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -950,18 +970,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1178,30 +1198,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1219,22 +1240,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1276,22 +1299,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1301,26 +1324,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/ensemble/stacking_regressor.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -231,15 +233,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         deps = deps | _gather_dependencies(estimators)
         deps = deps | _gather_dependencies(final_estimator)
         self._deps = list(deps)
         estimators = _transform_snowml_obj_to_sklearn_obj(estimators)
         final_estimator = _transform_snowml_obj_to_sklearn_obj(final_estimator)
         init_args = {'estimators':(estimators, None, True),
@@ -258,14 +259,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -336,32 +346,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -379,23 +390,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -454,33 +467,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -518,15 +531,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -610,15 +623,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -777,19 +790,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -854,18 +874,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1082,30 +1102,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1123,22 +1144,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1180,22 +1203,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1205,26 +1228,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/ensemble/voting_classifier.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -215,15 +217,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         deps = deps | _gather_dependencies(estimators)
         self._deps = list(deps)
         estimators = _transform_snowml_obj_to_sklearn_obj(estimators)
         init_args = {'estimators':(estimators, None, True),
             'voting':(voting, "hard", False),
             'weights':(weights, None, False),
@@ -240,14 +241,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -318,32 +328,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -361,23 +372,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -436,33 +449,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -500,15 +513,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -592,15 +605,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -759,19 +772,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -836,18 +856,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1068,30 +1088,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1109,22 +1130,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1166,22 +1189,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1191,26 +1214,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/ensemble/voting_regressor.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -199,15 +201,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         deps = deps | _gather_dependencies(estimators)
         self._deps = list(deps)
         estimators = _transform_snowml_obj_to_sklearn_obj(estimators)
         init_args = {'estimators':(estimators, None, True),
             'weights':(weights, None, False),
             'n_jobs':(n_jobs, None, False),
@@ -222,14 +223,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -300,32 +310,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -343,23 +354,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -418,33 +431,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -482,15 +495,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -574,15 +587,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -741,19 +754,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -818,18 +838,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1046,30 +1066,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1087,22 +1108,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1144,22 +1167,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1169,26 +1192,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/feature_selection/generic_univariate_select.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -24,14 +25,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -190,15 +192,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'score_func':(score_func, sklearn.feature_selection._univariate_selection.f_classif, False),
             'mode':(mode, "percentile", False),
             'param':(param, 1e-05, False),}
@@ -212,14 +213,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -290,32 +300,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -333,23 +344,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -408,33 +421,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -472,15 +485,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -564,15 +577,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -729,19 +742,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -806,18 +826,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1034,30 +1054,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1075,22 +1096,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1132,22 +1155,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1157,26 +1180,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/feature_selection/select_fdr.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -24,14 +25,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -187,15 +189,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'score_func':(score_func, sklearn.feature_selection._univariate_selection.f_classif, False),
             'alpha':(alpha, 0.05, False),}
         cleaned_up_init_args = _validate_sklearn_args(
@@ -208,14 +209,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -286,32 +296,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -329,23 +340,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -404,33 +417,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -468,15 +481,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -560,15 +573,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -725,19 +738,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -802,18 +822,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1030,30 +1050,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1071,22 +1092,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1128,22 +1151,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1153,26 +1176,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/feature_selection/select_fpr.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -24,14 +25,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -187,15 +189,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'score_func':(score_func, sklearn.feature_selection._univariate_selection.f_classif, False),
             'alpha':(alpha, 0.05, False),}
         cleaned_up_init_args = _validate_sklearn_args(
@@ -208,14 +209,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -286,32 +296,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -329,23 +340,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -404,33 +417,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -468,15 +481,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -560,15 +573,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -725,19 +738,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -802,18 +822,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1030,30 +1050,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1071,22 +1092,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1128,22 +1151,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1153,26 +1176,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/feature_selection/select_fwe.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -24,14 +25,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -187,15 +189,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'score_func':(score_func, sklearn.feature_selection._univariate_selection.f_classif, False),
             'alpha':(alpha, 0.05, False),}
         cleaned_up_init_args = _validate_sklearn_args(
@@ -208,14 +209,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -286,32 +296,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -329,23 +340,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -404,33 +417,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -468,15 +481,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -560,15 +573,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -725,19 +738,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -802,18 +822,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1030,30 +1050,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1071,22 +1092,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1128,22 +1151,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1153,26 +1176,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/feature_selection/select_k_best.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -24,14 +25,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -188,15 +190,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'score_func':(score_func, sklearn.feature_selection._univariate_selection.f_classif, False),
             'k':(k, 10, False),}
         cleaned_up_init_args = _validate_sklearn_args(
@@ -209,14 +210,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -287,32 +297,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -330,23 +341,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -405,33 +418,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -469,15 +482,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -561,15 +574,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -726,19 +739,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -803,18 +823,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1031,30 +1051,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1072,22 +1093,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1129,22 +1152,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1154,26 +1177,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/feature_selection/select_percentile.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -24,14 +25,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -187,15 +189,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'score_func':(score_func, sklearn.feature_selection._univariate_selection.f_classif, False),
             'percentile':(percentile, 10, False),}
         cleaned_up_init_args = _validate_sklearn_args(
@@ -208,14 +209,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -286,32 +296,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -329,23 +340,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -404,33 +417,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -468,15 +481,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -560,15 +573,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -725,19 +738,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -802,18 +822,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1030,30 +1050,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1071,22 +1092,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1128,22 +1151,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1153,26 +1176,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/feature_selection/sequential_feature_selector.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -242,15 +244,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         deps = deps | _gather_dependencies(estimator)
         self._deps = list(deps)
         estimator = _transform_snowml_obj_to_sklearn_obj(estimator)
         init_args = {'estimator':(estimator, None, True),
             'n_features_to_select':(n_features_to_select, "warn", False),
             'tol':(tol, None, False),
@@ -268,14 +269,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -346,32 +356,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -389,23 +400,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -464,33 +477,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -528,15 +541,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -620,15 +633,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -785,19 +798,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -862,18 +882,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1090,30 +1110,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1131,22 +1152,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1188,22 +1211,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1213,26 +1236,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/feature_selection/variance_threshold.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -181,15 +183,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'threshold':(threshold, 0.0, False),}
         cleaned_up_init_args = _validate_sklearn_args(
             args=init_args,
@@ -201,14 +202,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -279,32 +289,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -322,23 +333,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -397,33 +410,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -461,15 +474,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -553,15 +566,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -718,19 +731,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -795,18 +815,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1023,30 +1043,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1064,22 +1085,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1121,22 +1144,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1146,26 +1169,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/gaussian_process/gaussian_process_classifier.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -266,15 +268,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'kernel':(kernel, None, False),
             'optimizer':(optimizer, "fmin_l_bfgs_b", False),
             'n_restarts_optimizer':(n_restarts_optimizer, 0, False),
@@ -294,14 +295,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -372,32 +382,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -415,23 +426,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -490,33 +503,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -554,15 +567,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -646,15 +659,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -813,19 +826,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -888,18 +908,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1120,30 +1140,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1161,22 +1182,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1218,22 +1241,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1243,26 +1266,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/gaussian_process/gaussian_process_regressor.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -251,15 +253,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'kernel':(kernel, None, False),
             'alpha':(alpha, 1e-10, False),
             'optimizer':(optimizer, "fmin_l_bfgs_b", False),
@@ -277,14 +278,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -355,32 +365,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -398,23 +409,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -473,33 +486,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -537,15 +550,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -629,15 +642,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -796,19 +809,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -871,18 +891,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1099,30 +1119,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1140,22 +1161,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1197,22 +1220,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1222,26 +1245,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/impute/iterative_imputer.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -24,14 +25,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -286,15 +288,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         deps = deps | _gather_dependencies(estimator)
         self._deps = list(deps)
         estimator = _transform_snowml_obj_to_sklearn_obj(estimator)
         init_args = {'estimator':(estimator, None, False),
             'missing_values':(missing_values, np.nan, False),
             'sample_posterior':(sample_posterior, False, False),
@@ -320,14 +321,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -398,32 +408,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -441,23 +452,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -516,33 +529,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -580,15 +593,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -672,15 +685,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -837,19 +850,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -914,18 +934,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1142,30 +1162,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1183,22 +1204,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1240,22 +1263,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1265,26 +1288,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/impute/knn_imputer.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -229,15 +231,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'missing_values':(missing_values, np.nan, False),
             'n_neighbors':(n_neighbors, 5, False),
             'weights':(weights, "uniform", False),
@@ -255,14 +256,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -333,32 +343,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -376,23 +387,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -451,33 +464,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -515,15 +528,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -607,15 +620,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -772,19 +785,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -849,18 +869,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1077,30 +1097,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1118,22 +1139,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1175,22 +1198,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1200,26 +1223,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/impute/missing_indicator.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -206,15 +208,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'missing_values':(missing_values, np.nan, False),
             'features':(features, "missing-only", False),
             'sparse':(sparse, "auto", False),
@@ -229,14 +230,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -307,32 +317,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -350,23 +361,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -425,33 +438,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -489,15 +502,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -581,15 +594,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -746,19 +759,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -823,18 +843,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1051,30 +1071,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1092,22 +1113,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1149,22 +1172,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1174,26 +1197,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/kernel_approximation/additive_chi2_sampler.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -183,15 +185,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'sample_steps':(sample_steps, 2, False),
             'sample_interval':(sample_interval, None, False),}
         cleaned_up_init_args = _validate_sklearn_args(
@@ -204,14 +205,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -282,32 +292,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -325,23 +336,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -400,33 +413,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -464,15 +477,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -556,15 +569,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -721,19 +734,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -798,18 +818,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1026,30 +1046,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1067,22 +1088,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1124,22 +1147,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1149,26 +1172,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/kernel_approximation/nystroem.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -225,15 +227,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'kernel':(kernel, "rbf", False),
             'gamma':(gamma, None, False),
             'coef0':(coef0, None, False),
@@ -252,14 +253,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -330,32 +340,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -373,23 +384,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -448,33 +461,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -512,15 +525,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -604,15 +617,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -769,19 +782,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -846,18 +866,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1074,30 +1094,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1115,22 +1136,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1172,22 +1195,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1197,26 +1220,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/kernel_approximation/polynomial_count_sketch.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -204,15 +206,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'gamma':(gamma, 1.0, False),
             'degree':(degree, 2, False),
             'coef0':(coef0, 0, False),
@@ -228,14 +229,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -306,32 +316,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -349,23 +360,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -424,33 +437,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -488,15 +501,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -580,15 +593,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -745,19 +758,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -822,18 +842,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1050,30 +1070,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1091,22 +1112,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1148,22 +1171,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1173,26 +1196,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/kernel_approximation/rbf_sampler.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -193,15 +195,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'gamma':(gamma, 1.0, False),
             'n_components':(n_components, 100, False),
             'random_state':(random_state, None, False),}
@@ -215,14 +216,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -293,32 +303,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -336,23 +347,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -411,33 +424,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -475,15 +488,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -567,15 +580,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -732,19 +745,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -809,18 +829,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1037,30 +1057,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1078,22 +1099,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1135,22 +1158,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1160,26 +1183,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/kernel_approximation/skewed_chi2_sampler.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -191,15 +193,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'skewedness':(skewedness, 1.0, False),
             'n_components':(n_components, 100, False),
             'random_state':(random_state, None, False),}
@@ -213,14 +214,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -291,32 +301,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -334,23 +345,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -409,33 +422,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -473,15 +486,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -565,15 +578,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -730,19 +743,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -807,18 +827,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1035,30 +1055,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1076,22 +1097,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1133,22 +1156,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1158,26 +1181,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/kernel_ridge/kernel_ridge.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -222,15 +224,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'alpha':(alpha, 1, False),
             'kernel':(kernel, "linear", False),
             'gamma':(gamma, None, False),
@@ -247,14 +248,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -325,32 +335,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -368,23 +379,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -443,33 +456,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -507,15 +520,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -599,15 +612,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -766,19 +779,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -841,18 +861,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1069,30 +1089,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1110,22 +1131,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1167,22 +1190,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1192,26 +1215,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/lightgbm/lgbm_classifier.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -22,14 +23,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -196,15 +198,14 @@
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
         **kwargs,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'lightgbm=={lightgbm.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'boosting_type':(boosting_type, "gbdt", False),
             'num_leaves':(num_leaves, 31, False),
             'max_depth':(max_depth, -1, False),
@@ -236,14 +237,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -314,32 +324,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -357,23 +368,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -432,33 +445,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -496,15 +509,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -588,15 +601,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -755,19 +768,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -830,18 +850,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1062,30 +1082,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1103,22 +1124,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1160,22 +1183,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1185,26 +1208,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/lightgbm/lgbm_regressor.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -22,14 +23,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -196,15 +198,14 @@
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
         **kwargs,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'lightgbm=={lightgbm.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'boosting_type':(boosting_type, "gbdt", False),
             'num_leaves':(num_leaves, 31, False),
             'max_depth':(max_depth, -1, False),
@@ -236,14 +237,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -314,32 +324,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -357,23 +368,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -432,33 +445,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -496,15 +509,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -588,15 +601,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -755,19 +768,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -830,18 +850,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1058,30 +1078,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1099,22 +1120,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1156,22 +1179,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1181,26 +1204,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/linear_model/ard_regression.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -226,15 +228,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'n_iter':(n_iter, 300, False),
             'tol':(tol, 0.001, False),
             'alpha_1':(alpha_1, 1e-06, False),
@@ -256,14 +257,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -334,32 +344,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -377,23 +388,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -452,33 +465,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -516,15 +529,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -608,15 +621,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -775,19 +788,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -850,18 +870,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1078,30 +1098,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1119,22 +1140,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1176,22 +1199,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1201,26 +1224,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/linear_model/bayesian_ridge.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -234,15 +236,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'n_iter':(n_iter, 300, False),
             'tol':(tol, 0.001, False),
             'alpha_1':(alpha_1, 1e-06, False),
@@ -265,14 +266,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -343,32 +353,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -386,23 +397,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -461,33 +474,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -525,15 +538,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -617,15 +630,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -784,19 +797,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -859,18 +879,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1087,30 +1107,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1128,22 +1149,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1185,22 +1208,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1210,26 +1233,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/linear_model/elastic_net.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -241,15 +243,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'alpha':(alpha, 1.0, False),
             'l1_ratio':(l1_ratio, 0.5, False),
             'fit_intercept':(fit_intercept, True, False),
@@ -271,14 +272,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -349,32 +359,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -392,23 +403,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -467,33 +480,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -531,15 +544,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -623,15 +636,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -790,19 +803,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -865,18 +885,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1093,30 +1113,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1134,22 +1155,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1191,22 +1214,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1216,26 +1239,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/linear_model/elastic_net_cv.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -273,15 +275,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'l1_ratio':(l1_ratio, 0.5, False),
             'eps':(eps, 0.001, False),
             'n_alphas':(n_alphas, 100, False),
@@ -307,14 +308,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -385,32 +395,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -428,23 +439,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -503,33 +516,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -567,15 +580,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -659,15 +672,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -826,19 +839,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -901,18 +921,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1129,30 +1149,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1170,22 +1191,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1227,22 +1250,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1252,26 +1275,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/linear_model/gamma_regressor.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -226,15 +228,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'alpha':(alpha, 1.0, False),
             'fit_intercept':(fit_intercept, True, False),
             'solver':(solver, "lbfgs", False),
@@ -252,14 +253,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -330,32 +340,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -373,23 +384,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -448,33 +461,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -512,15 +525,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -604,15 +617,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -771,19 +784,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -846,18 +866,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1074,30 +1094,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1115,22 +1136,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1172,22 +1195,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1197,26 +1220,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/linear_model/huber_regressor.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -210,15 +212,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'epsilon':(epsilon, 1.35, False),
             'max_iter':(max_iter, 100, False),
             'alpha':(alpha, 0.0001, False),
@@ -235,14 +236,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -313,32 +323,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -356,23 +367,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -431,33 +444,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -495,15 +508,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -587,15 +600,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -754,19 +767,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -829,18 +849,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1057,30 +1077,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1098,22 +1119,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1155,22 +1178,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1180,26 +1203,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/linear_model/lars.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -235,15 +237,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'fit_intercept':(fit_intercept, True, False),
             'verbose':(verbose, False, False),
             'normalize':(normalize, "deprecated", False),
@@ -264,14 +265,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -342,32 +352,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -385,23 +396,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -460,33 +473,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -524,15 +537,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -616,15 +629,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -783,19 +796,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -858,18 +878,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1086,30 +1106,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1127,22 +1148,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1184,22 +1207,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1209,26 +1232,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/linear_model/lars_cv.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -243,15 +245,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'fit_intercept':(fit_intercept, True, False),
             'verbose':(verbose, False, False),
             'max_iter':(max_iter, 500, False),
@@ -272,14 +273,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -350,32 +360,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -393,23 +404,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -468,33 +481,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -532,15 +545,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -624,15 +637,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -791,19 +804,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -866,18 +886,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1094,30 +1114,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1135,22 +1156,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1192,22 +1215,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1217,26 +1240,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/linear_model/lasso.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -236,15 +238,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'alpha':(alpha, 1.0, False),
             'fit_intercept':(fit_intercept, True, False),
             'precompute':(precompute, False, False),
@@ -265,14 +266,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -343,32 +353,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -386,23 +397,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -461,33 +474,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -525,15 +538,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -617,15 +630,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -784,19 +797,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -859,18 +879,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1087,30 +1107,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1128,22 +1149,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1185,22 +1208,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1210,26 +1233,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/linear_model/lasso_cv.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -260,15 +262,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'eps':(eps, 0.001, False),
             'n_alphas':(n_alphas, 100, False),
             'alphas':(alphas, None, False),
@@ -293,14 +294,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -371,32 +381,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -414,23 +425,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -489,33 +502,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -553,15 +566,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -645,15 +658,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -812,19 +825,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -887,18 +907,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1115,30 +1135,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1156,22 +1177,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1213,22 +1236,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1238,26 +1261,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/linear_model/lasso_lars.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -254,15 +256,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'alpha':(alpha, 1.0, False),
             'fit_intercept':(fit_intercept, True, False),
             'verbose':(verbose, False, False),
@@ -285,14 +286,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -363,32 +373,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -406,23 +417,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -481,33 +494,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -545,15 +558,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -637,15 +650,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -804,19 +817,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -879,18 +899,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1107,30 +1127,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1148,22 +1169,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1205,22 +1228,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1230,26 +1253,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/linear_model/lasso_lars_cv.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -256,15 +258,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'fit_intercept':(fit_intercept, True, False),
             'verbose':(verbose, False, False),
             'max_iter':(max_iter, 500, False),
@@ -286,14 +287,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -364,32 +374,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -407,23 +418,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -482,33 +495,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -546,15 +559,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -638,15 +651,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -805,19 +818,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -880,18 +900,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1108,30 +1128,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1149,22 +1170,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1206,22 +1229,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1231,26 +1254,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/linear_model/lasso_lars_ic.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -240,15 +242,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'criterion':(criterion, "aic", False),
             'fit_intercept':(fit_intercept, True, False),
             'verbose':(verbose, False, False),
@@ -269,14 +270,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -347,32 +357,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -390,23 +401,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -465,33 +478,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -529,15 +542,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -621,15 +634,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -788,19 +801,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -863,18 +883,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1091,30 +1111,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1132,22 +1153,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1189,22 +1212,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1214,26 +1237,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/linear_model/linear_regression.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -199,15 +201,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'fit_intercept':(fit_intercept, True, False),
             'copy_X':(copy_X, True, False),
             'n_jobs':(n_jobs, None, False),
@@ -222,14 +223,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -300,32 +310,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -343,23 +354,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -418,33 +431,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -482,15 +495,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -574,15 +587,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -741,19 +754,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -816,18 +836,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1044,30 +1064,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1085,22 +1106,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1142,22 +1165,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1167,26 +1190,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/linear_model/logistic_regression.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -302,15 +304,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'penalty':(penalty, "l2", False),
             'dual':(dual, False, False),
             'tol':(tol, 0.0001, False),
@@ -336,14 +337,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -414,32 +424,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -457,23 +468,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -532,33 +545,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -596,15 +609,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -688,15 +701,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -855,19 +868,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -930,18 +950,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1164,30 +1184,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1205,22 +1226,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1262,22 +1285,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1287,26 +1310,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/linear_model/logistic_regression_cv.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -321,15 +323,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'Cs':(Cs, 10, False),
             'fit_intercept':(fit_intercept, True, False),
             'cv':(cv, None, False),
@@ -357,14 +358,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -435,32 +445,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -478,23 +489,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -553,33 +566,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -617,15 +630,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -709,15 +722,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -876,19 +889,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -951,18 +971,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1185,30 +1205,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1226,22 +1247,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1283,22 +1306,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1308,26 +1331,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/linear_model/multi_task_elastic_net.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -227,15 +229,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'alpha':(alpha, 1.0, False),
             'l1_ratio':(l1_ratio, 0.5, False),
             'fit_intercept':(fit_intercept, True, False),
@@ -255,14 +256,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -333,32 +343,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -376,23 +387,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -451,33 +464,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -515,15 +528,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -607,15 +620,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -774,19 +787,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -849,18 +869,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1077,30 +1097,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1118,22 +1139,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1175,22 +1198,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1200,26 +1223,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/linear_model/multi_task_elastic_net_cv.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -264,15 +266,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'l1_ratio':(l1_ratio, 0.5, False),
             'eps':(eps, 0.001, False),
             'n_alphas':(n_alphas, 100, False),
@@ -296,14 +297,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -374,32 +384,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -417,23 +428,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -492,33 +505,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -556,15 +569,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -648,15 +661,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -815,19 +828,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -890,18 +910,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1118,30 +1138,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1159,22 +1180,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1216,22 +1239,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1241,26 +1264,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/linear_model/multi_task_lasso.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -220,15 +222,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'alpha':(alpha, 1.0, False),
             'fit_intercept':(fit_intercept, True, False),
             'copy_X':(copy_X, True, False),
@@ -247,14 +248,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -325,32 +335,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -368,23 +379,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -443,33 +456,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -507,15 +520,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -599,15 +612,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -766,19 +779,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -841,18 +861,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1069,30 +1089,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1110,22 +1131,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1167,22 +1190,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1192,26 +1215,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/linear_model/multi_task_lasso_cv.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -251,15 +253,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'eps':(eps, 0.001, False),
             'n_alphas':(n_alphas, 100, False),
             'alphas':(alphas, None, False),
@@ -282,14 +283,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -360,32 +370,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -403,23 +414,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -478,33 +491,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -542,15 +555,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -634,15 +647,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -801,19 +814,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -876,18 +896,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1104,30 +1124,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1145,22 +1166,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1202,22 +1225,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1227,26 +1250,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/linear_model/orthogonal_matching_pursuit.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -206,15 +208,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'n_nonzero_coefs':(n_nonzero_coefs, None, False),
             'tol':(tol, None, False),
             'fit_intercept':(fit_intercept, True, False),
@@ -230,14 +231,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -308,32 +318,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -351,23 +362,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -426,33 +439,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -490,15 +503,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -582,15 +595,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -749,19 +762,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -824,18 +844,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1052,30 +1072,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1093,22 +1114,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1150,22 +1173,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1175,26 +1198,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/linear_model/passive_aggressive_classifier.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -270,15 +272,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'C':(C, 1.0, False),
             'fit_intercept':(fit_intercept, True, False),
             'max_iter':(max_iter, 1000, False),
@@ -304,14 +305,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -382,32 +392,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -425,23 +436,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -500,33 +513,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -564,15 +577,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -656,15 +669,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -823,19 +836,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -898,18 +918,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1128,30 +1148,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1169,22 +1190,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1226,22 +1249,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1251,26 +1274,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/linear_model/passive_aggressive_regressor.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -258,15 +260,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'C':(C, 1.0, False),
             'fit_intercept':(fit_intercept, True, False),
             'max_iter':(max_iter, 1000, False),
@@ -291,14 +292,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -369,32 +379,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -412,23 +423,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -487,33 +500,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -551,15 +564,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -643,15 +656,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -810,19 +823,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -885,18 +905,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1113,30 +1133,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1154,22 +1175,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1211,22 +1234,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1236,26 +1259,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/linear_model/perceptron.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -269,15 +271,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'penalty':(penalty, None, False),
             'alpha':(alpha, 0.0001, False),
             'l1_ratio':(l1_ratio, 0.15, False),
@@ -304,14 +305,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -382,32 +392,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -425,23 +436,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -500,33 +513,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -564,15 +577,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -656,15 +669,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -823,19 +836,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -898,18 +918,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1128,30 +1148,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1169,22 +1190,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1226,22 +1249,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1251,26 +1274,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/linear_model/poisson_regressor.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -226,15 +228,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'alpha':(alpha, 1.0, False),
             'fit_intercept':(fit_intercept, True, False),
             'solver':(solver, "lbfgs", False),
@@ -252,14 +253,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -330,32 +340,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -373,23 +384,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -448,33 +461,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -512,15 +525,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -604,15 +617,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -771,19 +784,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -846,18 +866,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1074,30 +1094,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1115,22 +1136,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1172,22 +1195,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1197,26 +1220,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/linear_model/ransac_regressor.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -281,15 +283,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         deps = deps | _gather_dependencies(estimator)
         deps = deps | _gather_dependencies(base_estimator)
         self._deps = list(deps)
         estimator = _transform_snowml_obj_to_sklearn_obj(estimator)
         base_estimator = _transform_snowml_obj_to_sklearn_obj(base_estimator)
         init_args = {'estimator':(estimator, None, False),
@@ -315,14 +316,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -393,32 +403,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -436,23 +447,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -511,33 +524,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -575,15 +588,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -667,15 +680,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -834,19 +847,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -909,18 +929,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1137,30 +1157,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1178,22 +1199,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1235,22 +1258,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1260,26 +1283,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/linear_model/ridge.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -258,15 +260,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'alpha':(alpha, 1.0, False),
             'fit_intercept':(fit_intercept, True, False),
             'copy_X':(copy_X, True, False),
@@ -285,14 +286,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -363,32 +373,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -406,23 +417,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -481,33 +494,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -545,15 +558,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -637,15 +650,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -804,19 +817,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -879,18 +899,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1107,30 +1127,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1148,22 +1169,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1205,22 +1228,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1230,26 +1253,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/linear_model/ridge_classifier.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -257,15 +259,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'alpha':(alpha, 1.0, False),
             'fit_intercept':(fit_intercept, True, False),
             'copy_X':(copy_X, True, False),
@@ -285,14 +286,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -363,32 +373,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -406,23 +417,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -481,33 +494,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -545,15 +558,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -637,15 +650,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -804,19 +817,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -879,18 +899,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1109,30 +1129,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1150,22 +1171,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1207,22 +1230,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1232,26 +1255,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/linear_model/ridge_classifier_cv.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -226,15 +228,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'alphas':(alphas, (0.1, 1.0, 10.0), False),
             'fit_intercept':(fit_intercept, True, False),
             'scoring':(scoring, None, False),
@@ -251,14 +252,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -329,32 +339,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -372,23 +383,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -447,33 +460,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -511,15 +524,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -603,15 +616,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -770,19 +783,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -845,18 +865,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1075,30 +1095,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1116,22 +1137,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1173,22 +1196,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1198,26 +1221,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/linear_model/ridge_cv.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -246,15 +248,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'alphas':(alphas, (0.1, 1.0, 10.0), False),
             'fit_intercept':(fit_intercept, True, False),
             'scoring':(scoring, None, False),
@@ -272,14 +273,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -350,32 +360,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -393,23 +404,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -468,33 +481,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -532,15 +545,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -624,15 +637,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -791,19 +804,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -866,18 +886,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1094,30 +1114,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1135,22 +1156,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1192,22 +1215,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1217,26 +1240,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/linear_model/sgd_classifier.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -351,15 +353,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'loss':(loss, "hinge", False),
             'penalty':(penalty, "l2", False),
             'alpha':(alpha, 0.0001, False),
@@ -391,14 +392,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -469,32 +479,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -512,23 +523,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -587,33 +600,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -651,15 +664,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -743,15 +756,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -910,19 +923,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -985,18 +1005,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1219,30 +1239,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1260,22 +1281,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1317,22 +1340,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1342,26 +1365,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/linear_model/sgd_one_class_svm.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -260,15 +262,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'nu':(nu, 0.5, False),
             'fit_intercept':(fit_intercept, True, False),
             'max_iter':(max_iter, 1000, False),
@@ -291,14 +292,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -369,32 +379,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -412,23 +423,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -487,33 +500,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -551,15 +564,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -643,15 +656,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -810,19 +823,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -885,18 +905,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1115,30 +1135,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1156,22 +1177,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1213,22 +1236,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1238,26 +1261,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/linear_model/sgd_regressor.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -319,15 +321,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'loss':(loss, "squared_error", False),
             'penalty':(penalty, "l2", False),
             'alpha':(alpha, 0.0001, False),
@@ -357,14 +358,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -435,32 +445,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -478,23 +489,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -553,33 +566,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -617,15 +630,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -709,15 +722,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -876,19 +889,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -951,18 +971,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1179,30 +1199,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1220,22 +1241,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1277,22 +1300,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1302,26 +1325,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/linear_model/theil_sen_regressor.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -231,15 +233,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'fit_intercept':(fit_intercept, True, False),
             'copy_X':(copy_X, True, False),
             'max_subpopulation':(max_subpopulation, 10000.0, False),
@@ -259,14 +260,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -337,32 +347,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -380,23 +391,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -455,33 +468,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -519,15 +532,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -611,15 +624,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -778,19 +791,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -853,18 +873,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1081,30 +1101,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1122,22 +1143,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1179,22 +1202,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1204,26 +1227,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/linear_model/tweedie_regressor.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -257,15 +259,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'power':(power, 0.0, False),
             'alpha':(alpha, 1.0, False),
             'fit_intercept':(fit_intercept, True, False),
@@ -285,14 +286,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -363,32 +373,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -406,23 +417,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -481,33 +494,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -545,15 +558,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -637,15 +650,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -804,19 +817,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -879,18 +899,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1107,30 +1127,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1148,22 +1169,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1205,22 +1228,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1230,26 +1253,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/manifold/isomap.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -252,15 +254,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'n_neighbors':(n_neighbors, 5, False),
             'radius':(radius, None, False),
             'n_components':(n_components, 2, False),
@@ -283,14 +284,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -361,32 +371,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -404,23 +415,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -479,33 +492,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -543,15 +556,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -635,15 +648,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -800,19 +813,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -877,18 +897,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1105,30 +1125,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1146,22 +1167,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1203,22 +1226,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1228,26 +1251,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/manifold/mds.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -237,15 +239,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'n_components':(n_components, 2, False),
             'metric':(metric, True, False),
             'n_init':(n_init, 4, False),
@@ -266,14 +267,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -344,32 +354,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -387,23 +398,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -462,33 +475,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -526,15 +539,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -618,15 +631,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -783,19 +796,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -858,18 +878,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1086,30 +1106,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1127,22 +1148,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1184,22 +1207,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1209,26 +1232,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/manifold/spectral_embedding.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -241,15 +243,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'n_components':(n_components, 2, False),
             'affinity':(affinity, "nearest_neighbors", False),
             'gamma':(gamma, None, False),
@@ -268,14 +269,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -346,32 +356,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -389,23 +400,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -464,33 +477,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -528,15 +541,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -620,15 +633,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -785,19 +798,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -860,18 +880,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1088,30 +1108,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1129,22 +1150,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1186,22 +1209,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1211,26 +1234,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/manifold/tsne.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -298,15 +300,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'n_components':(n_components, 2, False),
             'perplexity':(perplexity, 30.0, False),
             'early_exaggeration':(early_exaggeration, 12.0, False),
@@ -333,14 +334,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -411,32 +421,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -454,23 +465,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -529,33 +542,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -593,15 +606,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -685,15 +698,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -850,19 +863,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -925,18 +945,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1153,30 +1173,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1194,22 +1215,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1251,22 +1274,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1276,26 +1299,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/mixture/bayesian_gaussian_mixture.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -294,15 +296,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'n_components':(n_components, 1, False),
             'covariance_type':(covariance_type, "full", False),
             'tol':(tol, 0.001, False),
@@ -330,14 +331,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -408,32 +418,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -451,23 +462,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -526,33 +539,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -590,15 +603,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -682,15 +695,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -849,19 +862,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -924,18 +944,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1156,30 +1176,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1197,22 +1218,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1254,22 +1277,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1279,26 +1302,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/mixture/gaussian_mixture.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -270,15 +272,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'n_components':(n_components, 1, False),
             'covariance_type':(covariance_type, "full", False),
             'tol':(tol, 0.001, False),
@@ -303,14 +304,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -381,32 +391,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -424,23 +435,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -499,33 +512,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -563,15 +576,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -655,15 +668,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -822,19 +835,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -897,18 +917,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1129,30 +1149,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1170,22 +1191,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1227,22 +1250,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1252,26 +1275,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/model_selection/grid_search_cv.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -310,15 +312,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         deps = deps | _gather_dependencies(estimator)
         self._deps = list(deps)
         estimator = _transform_snowml_obj_to_sklearn_obj(estimator)
         init_args = {'estimator':(estimator, None, True),
             'param_grid':(param_grid, None, True),
             'scoring':(scoring, None, False),
@@ -339,14 +340,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -417,32 +427,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -460,23 +471,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -535,33 +548,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -599,15 +612,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -691,15 +704,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -858,19 +871,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -935,18 +955,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1169,30 +1189,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1210,22 +1231,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1267,22 +1290,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1292,26 +1315,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/model_selection/randomized_search_cv.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -323,15 +325,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         deps = deps | _gather_dependencies(estimator)
         self._deps = list(deps)
         estimator = _transform_snowml_obj_to_sklearn_obj(estimator)
         init_args = {'estimator':(estimator, None, True),
             'param_distributions':(param_distributions, None, True),
             'n_iter':(n_iter, 10, False),
@@ -354,14 +355,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -432,32 +442,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -475,23 +486,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -550,33 +563,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -614,15 +627,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -706,15 +719,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -873,19 +886,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -950,18 +970,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1184,30 +1204,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1225,22 +1246,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1282,22 +1305,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1307,26 +1330,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/multiclass/one_vs_one_classifier.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -192,15 +194,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         deps = deps | _gather_dependencies(estimator)
         self._deps = list(deps)
         estimator = _transform_snowml_obj_to_sklearn_obj(estimator)
         init_args = {'estimator':(estimator, None, True),
             'n_jobs':(n_jobs, None, False),}
         cleaned_up_init_args = _validate_sklearn_args(
@@ -213,14 +214,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -291,32 +301,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -334,23 +345,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -409,33 +422,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -473,15 +486,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -565,15 +578,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -732,19 +745,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -807,18 +827,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1037,30 +1057,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1078,22 +1099,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1135,22 +1158,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1160,26 +1183,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/multiclass/one_vs_rest_classifier.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -200,15 +202,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         deps = deps | _gather_dependencies(estimator)
         self._deps = list(deps)
         estimator = _transform_snowml_obj_to_sklearn_obj(estimator)
         init_args = {'estimator':(estimator, None, True),
             'n_jobs':(n_jobs, None, False),
             'verbose':(verbose, 0, False),}
@@ -222,14 +223,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -300,32 +310,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -343,23 +354,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -418,33 +431,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -482,15 +495,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -574,15 +587,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -741,19 +754,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -816,18 +836,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1050,30 +1070,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1091,22 +1112,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1148,22 +1171,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1173,26 +1196,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/multiclass/output_code_classifier.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -202,15 +204,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         deps = deps | _gather_dependencies(estimator)
         self._deps = list(deps)
         estimator = _transform_snowml_obj_to_sklearn_obj(estimator)
         init_args = {'estimator':(estimator, None, True),
             'code_size':(code_size, 1.5, False),
             'random_state':(random_state, None, False),
@@ -225,14 +226,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -303,32 +313,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -346,23 +357,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -421,33 +434,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -485,15 +498,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -577,15 +590,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -744,19 +757,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -819,18 +839,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1047,30 +1067,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1088,22 +1109,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1145,22 +1168,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1170,26 +1193,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/naive_bayes/bernoulli_nb.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -201,15 +203,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'alpha':(alpha, 1.0, False),
             'force_alpha':(force_alpha, "warn", False),
             'binarize':(binarize, 0.0, False),
@@ -225,14 +226,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -303,32 +313,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -346,23 +357,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -421,33 +434,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -485,15 +498,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -577,15 +590,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -744,19 +757,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -819,18 +839,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1051,30 +1071,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1092,22 +1113,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1149,22 +1172,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1174,26 +1197,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/naive_bayes/categorical_nb.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -207,15 +209,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'alpha':(alpha, 1.0, False),
             'force_alpha':(force_alpha, "warn", False),
             'fit_prior':(fit_prior, True, False),
@@ -231,14 +232,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -309,32 +319,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -352,23 +363,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -427,33 +440,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -491,15 +504,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -583,15 +596,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -750,19 +763,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -825,18 +845,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1057,30 +1077,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1098,22 +1119,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1155,22 +1178,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1180,26 +1203,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/naive_bayes/complement_nb.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -201,15 +203,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'alpha':(alpha, 1.0, False),
             'force_alpha':(force_alpha, "warn", False),
             'fit_prior':(fit_prior, True, False),
@@ -225,14 +226,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -303,32 +313,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -346,23 +357,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -421,33 +434,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -485,15 +498,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -577,15 +590,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -744,19 +757,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -819,18 +839,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1051,30 +1071,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1092,22 +1113,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1149,22 +1172,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1174,26 +1197,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/naive_bayes/gaussian_nb.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -185,15 +187,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'priors':(priors, None, False),
             'var_smoothing':(var_smoothing, 1e-09, False),}
         cleaned_up_init_args = _validate_sklearn_args(
@@ -206,14 +207,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -284,32 +294,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -327,23 +338,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -402,33 +415,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -466,15 +479,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -558,15 +571,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -725,19 +738,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -800,18 +820,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1032,30 +1052,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1073,22 +1094,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1130,22 +1153,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1155,26 +1178,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/naive_bayes/multinomial_nb.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -196,15 +198,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'alpha':(alpha, 1.0, False),
             'force_alpha':(force_alpha, "warn", False),
             'fit_prior':(fit_prior, True, False),
@@ -219,14 +220,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -297,32 +307,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -340,23 +351,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -415,33 +428,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -479,15 +492,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -571,15 +584,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -738,19 +751,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -813,18 +833,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1045,30 +1065,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1086,22 +1107,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1143,22 +1166,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1168,26 +1191,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/neighbors/k_neighbors_classifier.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -249,15 +251,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'n_neighbors':(n_neighbors, 5, False),
             'weights':(weights, "uniform", False),
             'algorithm':(algorithm, "auto", False),
@@ -276,14 +277,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -354,32 +364,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -397,23 +408,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -472,33 +485,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -536,15 +549,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -628,15 +641,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -795,19 +808,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -870,18 +890,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1102,30 +1122,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1143,22 +1164,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1200,22 +1223,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1225,26 +1248,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/neighbors/k_neighbors_regressor.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -251,15 +253,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'n_neighbors':(n_neighbors, 5, False),
             'weights':(weights, "uniform", False),
             'algorithm':(algorithm, "auto", False),
@@ -278,14 +279,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -356,32 +366,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -399,23 +410,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -474,33 +487,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -538,15 +551,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -630,15 +643,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -797,19 +810,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -872,18 +892,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1100,30 +1120,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1141,22 +1162,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1198,22 +1221,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1223,26 +1246,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/neighbors/kernel_density.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -229,15 +231,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'bandwidth':(bandwidth, 1.0, False),
             'algorithm':(algorithm, "auto", False),
             'kernel':(kernel, "gaussian", False),
@@ -257,14 +258,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -335,32 +345,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -378,23 +389,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -453,33 +466,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -517,15 +530,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -609,15 +622,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -774,19 +787,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -849,18 +869,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1077,30 +1097,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1118,22 +1139,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1175,22 +1198,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1200,26 +1223,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/neighbors/local_outlier_factor.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -257,15 +259,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'n_neighbors':(n_neighbors, 20, False),
             'algorithm':(algorithm, "auto", False),
             'leaf_size':(leaf_size, 30, False),
@@ -285,14 +286,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -363,32 +373,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -406,23 +417,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -481,33 +494,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -545,15 +558,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -637,15 +650,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -804,19 +817,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -879,18 +899,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1109,30 +1129,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1150,22 +1171,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1207,22 +1230,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1232,26 +1255,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/neighbors/nearest_centroid.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -195,15 +197,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'metric':(metric, "euclidean", False),
             'shrink_threshold':(shrink_threshold, None, False),}
         cleaned_up_init_args = _validate_sklearn_args(
@@ -216,14 +217,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -294,32 +304,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -337,23 +348,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -412,33 +425,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -476,15 +489,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -568,15 +581,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -735,19 +748,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -810,18 +830,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1038,30 +1058,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1079,22 +1100,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1136,22 +1159,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1161,26 +1184,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/neighbors/nearest_neighbors.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -241,15 +243,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'n_neighbors':(n_neighbors, 5, False),
             'radius':(radius, 1.0, False),
             'algorithm':(algorithm, "auto", False),
@@ -268,14 +269,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -346,32 +356,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -389,23 +400,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -464,33 +477,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -528,15 +541,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -620,15 +633,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -785,19 +798,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -860,18 +880,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1088,30 +1108,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1129,22 +1150,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1186,22 +1209,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1211,26 +1234,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/neighbors/neighborhood_components_analysis.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -260,15 +262,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'n_components':(n_components, None, False),
             'init':(init, "auto", False),
             'warm_start':(warm_start, False, False),
@@ -287,14 +288,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -365,32 +375,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -408,23 +419,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -483,33 +496,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -547,15 +560,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -639,15 +652,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -804,19 +817,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -881,18 +901,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1109,30 +1129,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1150,22 +1171,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1207,22 +1230,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1232,26 +1255,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/neighbors/radius_neighbors_classifier.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -260,15 +262,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'radius':(radius, 1.0, False),
             'weights':(weights, "uniform", False),
             'algorithm':(algorithm, "auto", False),
@@ -288,14 +289,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -366,32 +376,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -409,23 +420,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -484,33 +497,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -548,15 +561,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -640,15 +653,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -807,19 +820,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -882,18 +902,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1114,30 +1134,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1155,22 +1176,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1212,22 +1235,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1237,26 +1260,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/neighbors/radius_neighbors_regressor.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -251,15 +253,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'radius':(radius, 1.0, False),
             'weights':(weights, "uniform", False),
             'algorithm':(algorithm, "auto", False),
@@ -278,14 +279,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -356,32 +366,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -399,23 +410,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -474,33 +487,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -538,15 +551,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -630,15 +643,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -797,19 +810,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -872,18 +892,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1100,30 +1120,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1141,22 +1162,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1198,22 +1221,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1223,26 +1246,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/neural_network/bernoulli_rbm.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -212,15 +214,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'n_components':(n_components, 256, False),
             'learning_rate':(learning_rate, 0.1, False),
             'batch_size':(batch_size, 10, False),
@@ -237,14 +238,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -315,32 +325,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -358,23 +369,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -433,33 +446,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -497,15 +510,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -589,15 +602,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -754,19 +767,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -831,18 +851,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1059,30 +1079,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1100,22 +1121,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1157,22 +1180,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1182,26 +1205,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/neural_network/mlp_classifier.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -348,15 +350,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'hidden_layer_sizes':(hidden_layer_sizes, (100,), False),
             'activation':(activation, "relu", False),
             'solver':(solver, "adam", False),
@@ -390,14 +391,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -468,32 +478,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -511,23 +522,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -586,33 +599,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -650,15 +663,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -742,15 +755,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -909,19 +922,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -984,18 +1004,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1216,30 +1236,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1257,22 +1278,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1314,22 +1337,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1339,26 +1362,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/neural_network/mlp_regressor.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -344,15 +346,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'hidden_layer_sizes':(hidden_layer_sizes, (100,), False),
             'activation':(activation, "relu", False),
             'solver':(solver, "adam", False),
@@ -386,14 +387,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -464,32 +474,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -507,23 +518,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -582,33 +595,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -646,15 +659,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -738,15 +751,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -905,19 +918,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -980,18 +1000,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1208,30 +1228,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1249,22 +1270,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1306,22 +1329,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1331,26 +1354,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/preprocessing/ordinal_encoder.py

```diff
@@ -117,14 +117,15 @@
         self.handle_unknown = handle_unknown
         self.unknown_value = unknown_value
         self.encoded_missing_value = encoded_missing_value
 
         self.categories_: Dict[str, type_utils.LiteralNDArrayType] = {}
         self._categories_list: List[type_utils.LiteralNDArrayType] = []
         self._missing_indices: Dict[int, int] = {}
+        self._infrequent_enabled = False
         self._vocab_table_name = "snowml_preprocessing_ordinal_encoder_temp_table_" + uuid.uuid4().hex
 
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
 
     def _reset(self) -> None:
         """
@@ -543,14 +544,15 @@
         Returns:
             Sklearn OrdinalEncoder.
         """
         encoder = self._create_unfitted_sklearn_object()
         if self._is_fitted:
             encoder.categories_ = self._categories_list
             encoder._missing_indices = self._missing_indices
+            encoder._infrequent_enabled = self._infrequent_enabled
         return encoder
 
     def _validate_keywords(self) -> None:
         if isinstance(self.categories, str) and self.categories != "auto":
             raise ValueError(f"Unsupported value {self.categories} for parameter `categories`.")
         elif isinstance(self.categories, dict):
             if len(self.categories) != len(self.input_cols):
```

## snowflake/ml/modeling/preprocessing/polynomial_features.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -204,15 +206,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'degree':(degree, 2, False),
             'interaction_only':(interaction_only, False, False),
             'include_bias':(include_bias, True, False),
@@ -227,14 +228,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -305,32 +315,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -348,23 +359,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -423,33 +436,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -487,15 +500,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -579,15 +592,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -744,19 +757,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -821,18 +841,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1049,30 +1069,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1090,22 +1111,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1147,22 +1170,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1172,26 +1195,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/semi_supervised/label_propagation.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -206,15 +208,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'kernel':(kernel, "rbf", False),
             'gamma':(gamma, 20, False),
             'n_neighbors':(n_neighbors, 7, False),
@@ -231,14 +232,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -309,32 +319,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -352,23 +363,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -427,33 +440,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -491,15 +504,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -583,15 +596,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -750,19 +763,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -825,18 +845,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1057,30 +1077,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1098,22 +1119,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1155,22 +1178,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1180,26 +1203,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/semi_supervised/label_spreading.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -214,15 +216,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'kernel':(kernel, "rbf", False),
             'gamma':(gamma, 20, False),
             'n_neighbors':(n_neighbors, 7, False),
@@ -240,14 +241,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -318,32 +328,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -361,23 +372,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -436,33 +449,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -500,15 +513,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -592,15 +605,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -759,19 +772,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -834,18 +854,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1066,30 +1086,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1107,22 +1128,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1164,22 +1187,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1189,26 +1212,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/svm/linear_svc.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -260,15 +262,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'penalty':(penalty, "l2", False),
             'loss':(loss, "squared_hinge", False),
             'dual':(dual, True, False),
@@ -291,14 +292,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -369,32 +379,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -412,23 +423,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -487,33 +500,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -551,15 +564,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -643,15 +656,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -810,19 +823,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -885,18 +905,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1115,30 +1135,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1156,22 +1177,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1213,22 +1236,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1238,26 +1261,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/svm/linear_svr.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -235,15 +237,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'epsilon':(epsilon, 0.0, False),
             'tol':(tol, 0.0001, False),
             'C':(C, 1.0, False),
@@ -264,14 +265,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -342,32 +352,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -385,23 +396,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -460,33 +473,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -524,15 +537,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -616,15 +629,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -783,19 +796,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -858,18 +878,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1086,30 +1106,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1127,22 +1148,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1184,22 +1207,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1209,26 +1232,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/svm/nu_svc.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -268,15 +270,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'nu':(nu, 0.5, False),
             'kernel':(kernel, "rbf", False),
             'degree':(degree, 3, False),
@@ -302,14 +303,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -380,32 +390,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -423,23 +434,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -498,33 +511,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -562,15 +575,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -654,15 +667,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -821,19 +834,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -896,18 +916,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1130,30 +1150,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1171,22 +1192,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1228,22 +1251,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1253,26 +1276,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/svm/nu_svr.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -233,15 +235,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'nu':(nu, 0.5, False),
             'C':(C, 1.0, False),
             'kernel':(kernel, "rbf", False),
@@ -263,14 +264,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -341,32 +351,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -384,23 +395,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -459,33 +472,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -523,15 +536,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -615,15 +628,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -782,19 +795,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -857,18 +877,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1085,30 +1105,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1126,22 +1147,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1183,22 +1206,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1208,26 +1231,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/svm/svc.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -271,15 +273,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'C':(C, 1.0, False),
             'kernel':(kernel, "rbf", False),
             'degree':(degree, 3, False),
@@ -305,14 +306,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -383,32 +393,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -426,23 +437,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -501,33 +514,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -565,15 +578,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -657,15 +670,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -824,19 +837,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -899,18 +919,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1133,30 +1153,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1174,22 +1195,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1231,22 +1254,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1256,26 +1279,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/svm/svr.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -236,15 +238,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'kernel':(kernel, "rbf", False),
             'degree':(degree, 3, False),
             'gamma':(gamma, "scale", False),
@@ -266,14 +267,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -344,32 +354,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -387,23 +398,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -462,33 +475,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -526,15 +539,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -618,15 +631,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -785,19 +798,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -860,18 +880,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1088,30 +1108,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1129,22 +1150,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1186,22 +1209,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1211,26 +1234,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/tree/decision_tree_classifier.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -303,15 +305,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'criterion':(criterion, "gini", False),
             'splitter':(splitter, "best", False),
             'max_depth':(max_depth, None, False),
@@ -334,14 +335,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -412,32 +422,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -455,23 +466,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -530,33 +543,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -594,15 +607,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -686,15 +699,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -853,19 +866,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -928,18 +948,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1160,30 +1180,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1201,22 +1222,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1258,22 +1281,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1283,26 +1306,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/tree/decision_tree_regressor.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -286,15 +288,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'criterion':(criterion, "squared_error", False),
             'splitter':(splitter, "best", False),
             'max_depth':(max_depth, None, False),
@@ -316,14 +317,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -394,32 +404,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -437,23 +448,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -512,33 +525,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -576,15 +589,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -668,15 +681,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -835,19 +848,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -910,18 +930,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1138,30 +1158,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1179,22 +1200,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1236,22 +1259,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1261,26 +1284,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/tree/extra_tree_classifier.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -295,15 +297,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'criterion':(criterion, "gini", False),
             'splitter':(splitter, "random", False),
             'max_depth':(max_depth, None, False),
@@ -326,14 +327,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -404,32 +414,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -447,23 +458,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -522,33 +535,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -586,15 +599,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -678,15 +691,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -845,19 +858,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -920,18 +940,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1152,30 +1172,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1193,22 +1214,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1250,22 +1273,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1275,26 +1298,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/tree/extra_tree_regressor.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -23,14 +24,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -278,15 +280,14 @@
         input_cols: Optional[Union[str, Iterable[str]]] = None,
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'scikit-learn=={sklearn.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'criterion':(criterion, "squared_error", False),
             'splitter':(splitter, "random", False),
             'max_depth':(max_depth, None, False),
@@ -308,14 +309,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -386,32 +396,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -429,23 +440,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -504,33 +517,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -568,15 +581,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -660,15 +673,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -827,19 +840,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -902,18 +922,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1130,30 +1150,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1171,22 +1192,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1228,22 +1251,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1253,26 +1276,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/xgboost/xgb_classifier.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -22,14 +23,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -386,15 +388,14 @@
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
         **kwargs,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'xgboost=={xgboost.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'objective':(objective, "binary:logistic", False),
             'use_label_encoder':(use_label_encoder, None, False),}
         cleaned_up_init_args = _validate_sklearn_args(
@@ -408,14 +409,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -486,32 +496,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -529,23 +540,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -604,33 +617,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -668,15 +681,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -760,15 +773,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -927,19 +940,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -1002,18 +1022,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1234,30 +1254,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1275,22 +1296,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1332,22 +1355,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1357,26 +1380,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/xgboost/xgb_regressor.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -22,14 +23,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -386,15 +388,14 @@
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
         **kwargs,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'xgboost=={xgboost.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'objective':(objective, "reg:squarederror", False),}
         cleaned_up_init_args = _validate_sklearn_args(
             args=init_args,
@@ -407,14 +408,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -485,32 +495,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -528,23 +539,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -603,33 +616,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -667,15 +680,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -759,15 +772,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -926,19 +939,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -1001,18 +1021,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1229,30 +1249,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1270,22 +1291,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1327,22 +1350,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1352,26 +1375,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/xgboost/xgbrf_classifier.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -22,14 +23,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -388,15 +390,14 @@
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
         **kwargs,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'xgboost=={xgboost.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'learning_rate':(learning_rate, 1.0, False),
             'subsample':(subsample, 0.8, False),
             'colsample_bynode':(colsample_bynode, 0.8, False),
@@ -412,14 +413,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -490,32 +500,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -533,23 +544,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -608,33 +621,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -672,15 +685,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -764,15 +777,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -931,19 +944,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = ""
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -1006,18 +1026,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1238,30 +1258,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1279,22 +1300,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1336,22 +1359,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1361,26 +1384,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/modeling/xgboost/xgbrf_regressor.py

```diff
@@ -3,14 +3,15 @@
 # Do not modify the auto-generated code(except automatic reformating by precommit hooks).
 #
 #
 # Copyright (c) 2012-2022 Snowflake Computing Inc. All rights reserved.
 #
 import inspect
 import os
+import posixpath
 from typing import Iterable, Optional, Union, List, Any, Dict, Callable, Set
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 import numpy as np
 import numpy
@@ -22,14 +23,15 @@
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils import pkg_version_utils, identifier
 from snowflake.ml._internal.utils.temp_file_utils import cleanup_temp_files, get_temp_file_path
 from snowflake.snowpark import DataFrame, Session
 from snowflake.snowpark.functions import pandas_udf, sproc
 from snowflake.snowpark.types import PandasSeries
+from snowflake.snowpark._internal.type_utils import convert_sp_to_sf_type
 
 from snowflake.ml.model.model_signature import (
     DataType,
     FeatureSpec,
     ModelSignature,
     _infer_signature,
     _rename_features,
@@ -388,15 +390,14 @@
         output_cols: Optional[Union[str, Iterable[str]]] = None,
         label_cols: Optional[Union[str, Iterable[str]]] = None,
         drop_input_cols: Optional[bool] = False,
         sample_weight_col: Optional[str] = None,
         **kwargs,
     ) -> None:
         super().__init__()
-        self.id = str(uuid4()).replace("-", "_").upper()
         deps: Set[str] = set([f'numpy=={np.__version__}', f'xgboost=={xgboost.__version__}', f'cloudpickle=={cp.__version__}'])
         
         self._deps = list(deps)
         
         init_args = {'learning_rate':(learning_rate, 1.0, False),
             'subsample':(subsample, 0.8, False),
             'colsample_bynode':(colsample_bynode, 0.8, False),
@@ -412,14 +413,23 @@
         self._model_signature_dict = None
         self.set_input_cols(input_cols)
         self.set_output_cols(output_cols)
         self.set_label_cols(label_cols)
         self.set_drop_input_cols(drop_input_cols)
         self.set_sample_weight_col(sample_weight_col)
 
+    def _get_rand_id(self) -> str:
+        """
+        Generate random id to be used in sproc and stage names.
+
+        Returns:
+            Random id string usable in sproc, table, and stage names.
+        """
+        return str(uuid4()).replace("-", "_").upper()
+
     def _infer_input_output_cols(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
         """
         Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
 
         Args:
             dataset: Input dataset.
         """
@@ -490,32 +500,33 @@
 
         # Create a temp file and dump the transform to that file.
         local_transform_file_name = get_temp_file_path()
         with open(local_transform_file_name, mode="w+b") as local_transform_file:
             cp.dump(self._sklearn_object, local_transform_file)
 
         # Create temp stage to run fit.
-        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self.id)
+        transform_stage_name = "SNOWML_TRANSFORM_{safe_id}".format(safe_id=self._get_rand_id())
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {transform_stage_name} successfully created."
         ).validate()
 
-        stage_transform_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(transform_stage_name, os.path.basename(local_transform_file_name))
         local_result_file_name = get_temp_file_path()
-        stage_result_file_name = os.path.join(transform_stage_name, os.path.basename(local_transform_file_name))
 
-        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self.id)
+        fit_sproc_name = "SNOWML_FIT_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -533,23 +544,25 @@
         @sproc(
             is_permanent=False,
             name=fit_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def fit_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> str:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -608,33 +621,33 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        sproc_export_file_name = session.call(
-            fit_sproc_name,
+        sproc_export_file_name = fit_wrapper_sproc(
+            session,
             query,
             stage_transform_file_name,
             stage_result_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         if "|" in sproc_export_file_name:
             fields = sproc_export_file_name.strip().split("|")
             sproc_export_file_name = fields[0]
             if len(fields) > 1:
                 print("\n".join(fields[1:]))
 
         session.file.get(
-            os.path.join(stage_result_file_name, sproc_export_file_name),
+            posixpath.join(stage_result_file_name, sproc_export_file_name),
             local_result_file_name,
             statement_params=statement_params
         )
         with open(os.path.join(local_result_file_name, sproc_export_file_name),mode="r+b") as result_file_obj:
             self._sklearn_object = cp.load(result_file_obj)
 
         cleanup_temp_files([local_transform_file_name, local_result_file_name])
@@ -672,15 +685,15 @@
         session = dataset._session
         # Validate that key package version in user workspace are supported in snowflake conda channel
         pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
             pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
 
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = "SNOWML_BATCH_INFERENCE_{safe_id}_{method}".format(
-                safe_id=self.id, method=inference_method)
+                safe_id=self._get_rand_id(), method=inference_method)
 
         # Need to do this since if we use self._sklearn_object directly in the UDF, Snowpark
         # will try to pickle all of self which fails.
         estimator = self._sklearn_object
 
         # Input columns for UDF are sorted by column names.
         # We need actual order of input cols to reorder dataframe before calling inference methods.
@@ -764,15 +777,15 @@
                 series = pd.Series(transformed_numpy_array.tolist())
                 transformed_pandas_df = pd.DataFrame(series, columns=expected_output_cols_list)
             else:
                 transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=expected_output_cols_list)
             return transformed_pandas_df.to_dict("records")
 
         batch_inference_table_name = "SNOWML_BATCH_INFERENCE_INPUT_TABLE_{safe_id}".format(
-            safe_id=self.id
+            safe_id=self._get_rand_id()
         )
 
         pass_through_columns = self._get_pass_through_columns(dataset)
         # Run Transform
         query_from_df = str(dataset.queries["queries"][0])
 
         outer_select_list = pass_through_columns[:]
@@ -931,19 +944,26 @@
             dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
                 Snowpark or Pandas DataFrame.
 
         Returns:
             Transformed dataset.
         """
         if isinstance(dataset, DataFrame):
+            expected_type_inferred = "float"
+            # when it is classifier, infer the datatype from label columns
+            if expected_type_inferred == "" and 'predict' in self.model_signatures:
+                expected_type_inferred = convert_sp_to_sf_type(
+                    self.model_signatures['predict'].outputs[0].as_snowpark_type()
+                )
+            
             output_df = self._batch_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,
-                expected_output_cols_type="float",
+                expected_output_cols_type=expected_type_inferred,
             )
         elif isinstance(dataset, pd.DataFrame):
             output_df = self._sklearn_inference(
                 dataset=dataset,
                 inference_method="predict",
                 expected_output_cols_list=self.output_cols,)
         else:
@@ -1006,18 +1026,18 @@
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
 
         return output_df
 
     def _get_output_column_names(self, output_cols_prefix: str) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
-        Returns an empty list if current object is not a classifier or not yet fitted.
+        Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         if getattr(self._sklearn_object, "classes_", None) is None:
-            return []
+            return [output_cols_prefix]
 
         classes = self._sklearn_object.classes_
         if isinstance(classes, numpy.ndarray):
             return [f'{output_cols_prefix}{c}' for c in classes.tolist()]
         elif isinstance(classes, list) and len(classes) > 0 and isinstance(classes[0], numpy.ndarray):
             # If the estimator is a multioutput estimator, classes_ will be a list of ndarrays.
             output_cols = []
@@ -1234,30 +1254,31 @@
 
         # Create a temp file and dump the score to that file.
         local_score_file_name = get_temp_file_path()
         with open(local_score_file_name, mode="w+b") as local_score_file:
             cp.dump(self._sklearn_object, local_score_file)
 
         # Create temp stage to run score.
-        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        score_stage_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         session = dataset._session
         stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {score_stage_name};"
         SqlResultValidator(
             session=session,
             query=stage_creation_query
         ).has_dimensions(
             expected_rows=1, expected_cols=1
         ).has_value_match(
             row_idx=0,
             col_idx=0,
             expected_value=f"Stage area {score_stage_name} successfully created."
         ).validate()
 
-        stage_score_file_name = os.path.join(score_stage_name, os.path.basename(local_score_file_name))
-        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self.id)
+        # Use posixpath to construct stage paths
+        stage_score_file_name = posixpath.join(score_stage_name, os.path.basename(local_score_file_name))
+        score_sproc_name = "SNOWML_SCORE_{safe_id}".format(safe_id=self._get_rand_id())
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[sproc],
@@ -1275,22 +1296,24 @@
         @sproc(
             is_permanent=False,
             name=score_sproc_name,
             packages=["snowflake-snowpark-python"] + self._get_dependencies(),
             replace=True,
             session=session,
             statement_params=statement_params,
+            anonymous=True
         )
         def score_wrapper_sproc(
             session: Session,
             sql_query: str,
             stage_score_file_name: str,
             input_cols: List[str],
             label_cols: List[str],
-            sample_weight_col: Optional[str]
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str]
         ) -> float:
             import cloudpickle as cp
             import numpy as np
             import os
             import pandas
             import tempfile
             import inspect
@@ -1332,22 +1355,22 @@
             subproject=_SUBPROJECT,
             function_name=telemetry.get_statement_params_full_func_name(
                 inspect.currentframe(), self.__class__.__name__
             ),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]),
         )
-        score = session.call(
-            score_sproc_name,
+        score = score_wrapper_sproc(
+            session,
             query,
             stage_score_file_name,
             identifier.get_unescaped_names(self.input_cols),
             identifier.get_unescaped_names(self.label_cols),
             identifier.get_unescaped_names(self.sample_weight_col),
-            statement_params=statement_params,
+            statement_params,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
 
     def _get_model_signatures(self, dataset: Union[DataFrame, pd.DataFrame]) -> None:
@@ -1357,26 +1380,28 @@
 
         inputs = _infer_signature(dataset[self.input_cols], "input")
         if hasattr(self, "predict"):
             # For classifier, the type of predict is the same as the type of label
             if self._sklearn_object._estimator_type == 'classifier':
                 outputs = _infer_signature(dataset[self.label_cols], "output")  # label columns is the desired type for output
                 outputs = _rename_features(outputs, self.output_cols)  # rename the output columns
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
             # For regressor, the type of predict is float64
             elif self._sklearn_object._estimator_type == 'regressor':
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in self.output_cols]
-                self._model_signature_dict["predict"] = ModelSignature(inputs, outputs)
-
+                self._model_signature_dict["predict"] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
         for prob_func in PROB_FUNCTIONS:
             if hasattr(self, prob_func):
                 output_cols_prefix: str = f"{prob_func}_"
                 output_column_names = self._get_output_column_names(output_cols_prefix)
                 outputs = [FeatureSpec(dtype=DataType.DOUBLE, name=c) for c in output_column_names]
-                self._model_signature_dict[prob_func] = ModelSignature(inputs, outputs)
+                self._model_signature_dict[prob_func] = ModelSignature(inputs, 
+                                                                       ([] if self._drop_input_cols else inputs) + outputs)
 
     @property
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise RuntimeError("Estimator not fitted before accessing property model_signatures! ")
         return self._model_signature_dict
```

## snowflake/ml/registry/model_registry.py

```diff
@@ -1,10 +1,11 @@
 import inspect
 import json
 import os
+import posixpath
 import sys
 import tempfile
 import types
 import zipfile
 from typing import TYPE_CHECKING, Any, Dict, List, Optional, cast
 from uuid import uuid1
 
@@ -30,15 +31,15 @@
 if TYPE_CHECKING:
     import pandas as pd
 
 _DEFAULT_REGISTRY_NAME: str = "_SYSTEM_MODEL_REGISTRY"
 _DEFAULT_SCHEMA_NAME: str = "_SYSTEM_MODEL_REGISTRY_SCHEMA"
 _MODELS_TABLE_NAME: str = "_SYSTEM_REGISTRY_MODELS"
 _METADATA_TABLE_NAME: str = "_SYSTEM_REGISTRY_METADATA"
-_DEPLOYMENT_TABLE_NAME: str = "_SYSTEM_REGISRTRY_DEPLOYMENTS"
+_DEPLOYMENT_TABLE_NAME: str = "_SYSTEM_REGISTRY_DEPLOYMENTS"
 
 # Metadata operation types.
 _SET_METADATA_OPERATION: str = "SET"
 _ADD_METADATA_OPERATION: str = "ADD"
 _DROP_METADATA_OPERATION: str = "DROP"
 
 # Metadata types.
@@ -79,17 +80,19 @@
 
     Returns:
         True if the creation of the model registry internal data structures was successful,
         False otherwise.
     """
 
     # These might be exposed as parameters in the future.
-    registry_table_name = _MODELS_TABLE_NAME
-    metadata_table_name = _METADATA_TABLE_NAME
-    deployment_table_name = _DEPLOYMENT_TABLE_NAME
+    database_name = identifier.get_inferred_name(database_name)
+    schema_name = identifier.get_inferred_name(schema_name)
+    registry_table_name = identifier.get_inferred_name(_MODELS_TABLE_NAME)
+    metadata_table_name = identifier.get_inferred_name(_METADATA_TABLE_NAME)
+    deployment_table_name = identifier.get_inferred_name(_DEPLOYMENT_TABLE_NAME)
     statement_params = telemetry.get_function_usage_statement_params(
         project=_TELEMETRY_PROJECT,
         subproject=_TELEMETRY_SUBPROJECT,
         function_name=telemetry.get_statement_params_full_func_name(inspect.currentframe(), ""),
     )
 
     _create_registry_database(session, database_name, statement_params)
@@ -125,22 +128,22 @@
     The creation will be skipped if the target database already exists.
 
     Args:
         session: Session object to communicate with Snowflake.
         database_name: Desired name of the model registry database.
         statement_params: Function usage statement parameters used in sql query executions.
     """
-    registry_databases = session.sql(f"SHOW DATABASES LIKE '{database_name}'").collect(
+    registry_databases = session.sql(f"SHOW DATABASES LIKE '{identifier.get_unescaped_names(database_name)}'").collect(
         statement_params=statement_params
     )
     if len(registry_databases) > 0:
         logging.warning(f"The database {database_name} already exists. Skipping creation.")
         return
 
-    session.sql(f'CREATE DATABASE "{database_name}"').collect(statement_params=statement_params)
+    session.sql(f"CREATE DATABASE {database_name}").collect(statement_params=statement_params)
 
 
 def _create_registry_schema(
     session: snowpark.Session, database_name: str, schema_name: str, statement_params: Dict[str, Any]
 ) -> None:
     """Private helper to create the model registry schema.
 
@@ -149,39 +152,39 @@
     Args:
         session: Session object to communicate with Snowflake.
         database_name: Desired name of the model registry database.
         schema_name: Desired name of the schema used by this model registry inside the database.
         statement_params: Function usage statement parameters used in sql query executions.
     """
     # The default PUBLIC schema is created by default so it might already exist even in a new database.
-    registry_schemas = session.sql(f"SHOW SCHEMAS LIKE '{schema_name}' IN DATABASE \"{database_name}\"").collect(
-        statement_params=statement_params
-    )
+    registry_schemas = session.sql(
+        f"SHOW SCHEMAS LIKE '{identifier.get_unescaped_names(schema_name)}' IN DATABASE {database_name}"
+    ).collect(statement_params=statement_params)
 
     if len(registry_schemas) > 0:
-        logging.warning(f'The schmea "{database_name}"."{schema_name}" already exists. Skipping creation.')
+        logging.warning(
+            f"The schema {_get_fully_qualified_schema_name(database_name, schema_name)}already exists. "
+            + "Skipping creation."
+        )
         return
 
-    session.sql(f'CREATE SCHEMA "{database_name}"."{schema_name}"').collect(statement_params=statement_params)
+    session.sql(f"CREATE SCHEMA {_get_fully_qualified_schema_name(database_name, schema_name)}").collect(
+        statement_params=statement_params
+    )
 
 
 def _get_fully_qualified_schema_name(database_name: str, schema_name: str) -> str:
-    return ".".join(
-        [
-            f"{identifier.quote_name_without_upper_casing(database_name)}",
-            f"{identifier.quote_name_without_upper_casing(schema_name)}",
-        ]
-    )
+    return ".".join([database_name, schema_name])
 
 
 def _get_fully_qualified_table_name(database_name: str, schema_name: str, table_name: str) -> str:
     return ".".join(
         [
             _get_fully_qualified_schema_name(database_name, schema_name),
-            f"{identifier.quote_name_without_upper_casing(table_name)}",
+            table_name,
         ]
     )
 
 
 def _create_registry_tables(
     session: snowpark.Session,
     database_name: str,
@@ -287,77 +290,73 @@
 
     # Create a view on active permanent deployments.
     _create_active_permanent_deployment_view(
         session, fully_qualified_schema_name, registry_table_name, deployment_table_name, statement_params
     )
 
     # Create views on most recent metadata items.
-    metadata_view_name_prefix = metadata_table_name + "_LAST_"
+    metadata_view_name_prefix = identifier.concat_names([metadata_table_name, "_LAST_"])
     metadata_view_template = formatting.unwrap(
-        """CREATE OR REPLACE VIEW "{database}"."{schema}"."{attribute_view}" COPY GRANTS AS
-            SELECT DISTINCT MODEL_ID, {select_expression} AS {final_attribute_name} FROM "{metadata_table}"
+        """CREATE OR REPLACE VIEW {database}.{schema}.{attribute_view} COPY GRANTS AS
+            SELECT DISTINCT MODEL_ID, {select_expression} AS {final_attribute_name} FROM {metadata_table}
             WHERE ATTRIBUTE_NAME = '{attribute_name}'"""
     )
 
     # Create a separate view for the most recent item in each metadata column.
     metadata_view_names = []
     metadata_select_fields = []
     for attribute_name in _LIST_METADATA_ATTRIBUTE:
-        view_name = f"{metadata_view_name_prefix}{attribute_name}"
+        view_name = identifier.concat_names([metadata_view_name_prefix, attribute_name])
         select_expression = f"(LAST_VALUE(VALUE) OVER (PARTITION BY MODEL_ID ORDER BY SEQUENCE_ID))['{attribute_name}']"
         sql = metadata_view_template.format(
             database=database_name,
             schema=schema_name,
             select_expression=select_expression,
             attribute_view=view_name,
             attribute_name=attribute_name,
             final_attribute_name=attribute_name,
             metadata_table=metadata_table_name,
         )
         session.sql(sql).collect(statement_params=statement_params)
         metadata_view_names.append(view_name)
-        metadata_select_fields.append(
-            f"{identifier.quote_name_without_upper_casing(view_name)}.{attribute_name} AS {attribute_name}"
-        )
+        metadata_select_fields.append(f"{view_name}.{attribute_name} AS {attribute_name}")
 
     # Create a special view for the registration timestamp.
     attribute_name = _METADATA_ATTRIBUTE_REGISTRATION
-    final_attribute_name = attribute_name + "_TIMESTAMP"
-    view_name = f"{metadata_view_name_prefix}{attribute_name}"
+    final_attribute_name = identifier.concat_names([attribute_name, "_TIMESTAMP"])
+    view_name = identifier.concat_names([metadata_view_name_prefix, attribute_name])
     create_registration_view_sql = metadata_view_template.format(
         database=database_name,
         schema=schema_name,
         select_expression="EVENT_TIMESTAMP",
         attribute_view=view_name,
         attribute_name=attribute_name,
         final_attribute_name=final_attribute_name,
         metadata_table=metadata_table_name,
     )
     session.sql(create_registration_view_sql).collect(statement_params=statement_params)
     metadata_view_names.append(view_name)
-    metadata_select_fields.append(
-        f"{identifier.quote_name_without_upper_casing(view_name)}.{final_attribute_name} AS {final_attribute_name}"
-    )
+    metadata_select_fields.append(f"{view_name}.{final_attribute_name} AS {final_attribute_name}")
 
     metadata_views_join = " ".join(
         [
-            'LEFT JOIN "{view}" ON ("{view}".MODEL_ID = "{registry_table}".ID)'.format(
+            "LEFT JOIN {view} ON ({view}.MODEL_ID = {registry_table}.ID)".format(
                 view=view, registry_table=registry_table_name
             )
             for view in metadata_view_names
         ]
     )
 
     # Create view to combine all attributes.
-    registry_view_name = registry_table_name + "_VIEW"
+    registry_view_name = identifier.concat_names([registry_table_name, "_VIEW"])
     metadata_select_fields_formatted = ",".join(metadata_select_fields)
     session.sql(
-        f"""CREATE OR REPLACE VIEW {fully_qualified_schema_name}."{registry_view_name}" COPY GRANTS AS
-                SELECT "{registry_table_name}".*, {metadata_select_fields_formatted}
-                FROM "{registry_table_name}" {metadata_views_join}"""
+        f"""CREATE OR REPLACE VIEW {fully_qualified_schema_name}.{registry_view_name} COPY GRANTS AS
+                SELECT {registry_table_name}.*, {metadata_select_fields_formatted}
+                FROM {registry_table_name} {metadata_views_join}"""
     ).collect(statement_params=statement_params)
 
 
 def _create_active_permanent_deployment_view(
     session: snowpark.Session,
     fully_qualified_schema_name: str,
     registry_table_name: str,
@@ -372,16 +371,17 @@
         registry_table_name: Name for the main model registry table.
         deployment_table_name: Name of the deployment table.
         statement_params: Function usage statement parameters used in sql query executions.
     """
 
     # Create a view on active permanent deployments
     # Active deployments are those whose last operation is not DROP.
+    active_deployments_view_name = identifier.concat_names([deployment_table_name, "_VIEW"])
     active_deployments_view_expr = f"""
-        CREATE OR REPLACE VIEW {fully_qualified_schema_name}."{deployment_table_name}_VIEW"
+        CREATE OR REPLACE VIEW {fully_qualified_schema_name}.{active_deployments_view_name}
         COPY GRANTS AS
         SELECT
             DEPLOYMENT_NAME,
             MODEL_ID,
             {registry_table_name}.NAME as MODEL_NAME,
             {registry_table_name}.VERSION as MODEL_VERSION,
             {deployment_table_name}.CREATION_TIME as CREATION_TIME,
@@ -412,22 +412,22 @@
         Opens an already-created registry.
 
         Args:
             session: Session object to communicate with Snowflake.
             database_name: Desired name of the model registry database.
             schema_name: Desired name of the schema used by this model registry inside the database.
         """
-        self._name = database_name
-        self._schema = schema_name
-        self._registry_table = _MODELS_TABLE_NAME
-        self._registry_table_view = self._registry_table + "_VIEW"
-        self._metadata_table = _METADATA_TABLE_NAME
-        self._deployment_table = _DEPLOYMENT_TABLE_NAME
-        self._permanent_deployment_view = self._deployment_table + "_VIEW"
-        self._permanent_deployment_stage = self._deployment_table + "_STAGE"
+        self._name = identifier.get_inferred_name(database_name)
+        self._schema = identifier.get_inferred_name(schema_name)
+        self._registry_table = identifier.get_inferred_name(_MODELS_TABLE_NAME)
+        self._registry_table_view = identifier.concat_names([self._registry_table, "_VIEW"])
+        self._metadata_table = identifier.get_inferred_name(_METADATA_TABLE_NAME)
+        self._deployment_table = identifier.get_inferred_name(_DEPLOYMENT_TABLE_NAME)
+        self._permanent_deployment_view = identifier.concat_names([self._deployment_table, "_VIEW"])
+        self._permanent_deployment_stage = identifier.concat_names([self._deployment_table, "_STAGE"])
 
         self._session = session
 
         # A in-memory deployment info cache to store information of temporary deployments
         # TODO(zhe): Use a temporary table to replace the in-memory cache.
         self._temporary_deployments: Dict[str, _deployer.Deployment] = {}
 
@@ -436,31 +436,47 @@
     # Private methods
 
     def _check_access(self) -> None:
         """Check access db/schema/tables."""
         # Check that the required tables exist and are accessible by the current role.
 
         query_result_checker.SqlResultValidator(
-            self._session, query=f"SHOW DATABASES LIKE '{self._name}'"
+            self._session, query=f"SHOW DATABASES LIKE '{identifier.get_unescaped_names(self._name)}'"
         ).has_dimensions(expected_rows=1).validate()
 
         query_result_checker.SqlResultValidator(
-            self._session, query=f"SHOW SCHEMAS LIKE '{self._schema}' IN DATABASE \"{self._name}\""
+            self._session,
+            query=f"SHOW SCHEMAS LIKE '{identifier.get_unescaped_names(self._schema)}' IN DATABASE {self._name}",
         ).has_dimensions(expected_rows=1).validate()
 
         query_result_checker.SqlResultValidator(
-            self._session, query=f"SHOW TABLES LIKE '{self._registry_table}' IN {self._fully_qualified_schema_name()}"
+            self._session,
+            query=formatting.unwrap(
+                f"""
+            SHOW TABLES LIKE '{identifier.get_unescaped_names(self._registry_table)}'
+            IN {self._fully_qualified_schema_name()}"""
+            ),
         ).has_dimensions(expected_rows=1).validate()
 
         query_result_checker.SqlResultValidator(
-            self._session, query=f"SHOW TABLES LIKE '{self._metadata_table}' IN {self._fully_qualified_schema_name()}"
+            self._session,
+            query=formatting.unwrap(
+                f"""
+            SHOW TABLES LIKE '{identifier.get_unescaped_names(self._metadata_table)}'
+            IN {self._fully_qualified_schema_name()}"""
+            ),
         ).has_dimensions(expected_rows=1).validate()
 
         query_result_checker.SqlResultValidator(
-            self._session, query=f"SHOW TABLES LIKE '{self._deployment_table}' IN {self._fully_qualified_schema_name()}"
+            self._session,
+            query=formatting.unwrap(
+                f"""
+            SHOW TABLES LIKE '{identifier.get_unescaped_names(self._deployment_table)}'
+            IN {self._fully_qualified_schema_name()}"""
+            ),
         ).has_dimensions(expected_rows=1).validate()
 
         # TODO(zzhu): Also check validity of views. Consider checking schema as well.
 
     def _get_statement_params(self, frame: Optional[types.FrameType]) -> Dict[str, Any]:
         return telemetry.get_function_usage_statement_params(
             project=_TELEMETRY_PROJECT,
@@ -820,15 +836,15 @@
             model_name: Model Name string. Required if id is None.
             model_version: Model Version string. Required if version is None.
             operation: the operation type of the metadata entry.
             enable_model_presence_check: If True, we will check if the model with the given ID is currently registered
                 before setting the metadata attribute. False by default meaning that by default we will check.
 
         Raises:
-            DataError: Failed to set the meatdata attribute.
+            DataError: Failed to set the metadata attribute.
             KeyError: The target model doesn't exist
         """
         selected_models = self._list_selected_models(id=id, model_name=model_name, model_version=model_version)
         identifier = f"id {id}" if id else f"{model_name}/{model_version}"
         try:
             model_info = self._validate_exact_one_result(selected_models, identifier)
         except KeyError as e:
@@ -950,21 +966,21 @@
         # Copy model from local disk to remote stage.
         # TODO(zhe): Check if we could use the same stage for multiple models.
         fully_qualified_model_stage_name = self._prepare_model_stage(model_id=id)
 
         # Check if directory or file and adapt accordingly.
         # TODO: Unify and explicit about compression for both file and directory.
         if os.path.isfile(path):
-            self._session.file.put(path, f"{fully_qualified_model_stage_name}/data")
+            self._session.file.put(path, posixpath.join(fully_qualified_model_stage_name, "data"))
         elif os.path.isdir(path):
             with file_utils.zip_file_or_directory_to_stream(path, path) as input_stream:
                 self._session._conn.upload_stream(
                     input_stream=input_stream,
                     stage_location=fully_qualified_model_stage_name,
-                    dest_filename=f"{os.path.basename(path)}.zip",
+                    dest_filename=f"{posixpath.basename(path)}.zip",
                     dest_prefix="",
                     source_compression="DEFLATE",
                     compress_data=False,
                     overwrite=True,
                     is_in_udf=True,
                 )
         self._register_model_with_id(
@@ -1062,15 +1078,15 @@
         Returns:
             snowpark.DataFrame with the list of models. Access is read-only through the snowpark.DataFrame.
             The resulting snowpark.dataframe will have an "id" column that uniquely identifies each model and can be
             used to reference the model when performing operations.
         """
         # Explicitly not calling collect.
         return self._session.sql(
-            'SELECT * FROM "{database}"."{schema}"."{view}"'.format(
+            "SELECT * FROM {database}.{schema}.{view}".format(
                 database=self._name, schema=self._schema, view=self._registry_table_view
             )
         )
 
     @telemetry.send_api_usage_telemetry(
         project=_TELEMETRY_PROJECT,
         subproject=_TELEMETRY_SUBPROJECT,
@@ -1119,15 +1135,17 @@
         """
         # This method uses a read-modify-write pattern for updating tags.
 
         model_tags = self.get_tags(model_name=model_name, model_version=model_version)
         try:
             del model_tags[tag_name]
         except KeyError:
-            raise connector.DataError(f"Model id {id} has not tag named {tag_name}. Full list of tags: {model_tags}")
+            raise connector.DataError(
+                f"Model {model_name}/{model_version} has no tag named {tag_name}. Full list of tags: {model_tags}"
+            )
 
         self._set_metadata_attribute(
             _METADATA_ATTRIBUTE_TAGS, model_tags, model_name=model_name, model_version=model_version
         )
 
     @telemetry.send_api_usage_telemetry(
         project=_TELEMETRY_PROJECT,
@@ -1222,15 +1240,15 @@
 
         Returns:
             Descrption of the model or None.
         """
         result = self._get_metadata_attribute(
             _METADATA_ATTRIBUTE_DESCRIPTION, model_name=model_name, model_version=model_version
         )
-        return None if result is None else str(result)
+        return None if result is None else json.loads(result)
 
     @telemetry.send_api_usage_telemetry(
         project=_TELEMETRY_PROJECT,
         subproject=_TELEMETRY_SUBPROJECT,
     )
     @snowpark._internal.utils.private_preview(version="0.2.0")
     def set_model_description(
@@ -1554,15 +1572,15 @@
         Returns:
             Restored model object.
         """
         remote_model_path = self._get_model_path(model_name=model_name, model_version=model_version)
         restored_model = None
         with tempfile.TemporaryDirectory() as local_model_directory:
             self._session.file.get(remote_model_path, local_model_directory)
-            local_path = os.path.join(local_model_directory, os.path.basename(remote_model_path))
+            local_path = os.path.join(local_model_directory, posixpath.basename(remote_model_path))
             if zipfile.is_zipfile(local_path):
                 extracted_dir = os.path.join(local_model_directory, "extracted")
                 with zipfile.ZipFile(local_path, "r") as myzip:
                     if len(myzip.namelist()) > 1:
                         myzip.extractall(extracted_dir)
                         restored_model, _ = model_api.load_model(model_dir_path=extracted_dir)
```

## snowflake/ml/version.py

```diff
@@ -1 +1 @@
-VERSION="1.0.2"
+VERSION="1.0.3"
```

## Comparing `snowflake_ml_python-1.0.2.dist-info/METADATA` & `snowflake_ml_python-1.0.3.dist-info/METADATA`

 * *Files 13% similar despite different names*

```diff
@@ -1,15 +1,19 @@
 Metadata-Version: 2.1
 Name: snowflake-ml-python
-Version: 1.0.2
-Description-Content-Type: text/markdown
 Author: Snowflake, Inc
 Author-email: support@snowflake.com
 Home-page: https://github.com/snowflakedb/snowflake-ml-python
 License: Apache License, Version 2.0
+Description-Content-Type: text/markdown
+Summary: The machine learning client library that is used for interacting with Snowflake to build machine learning solutions.
+Project-URL: Changelog, https://github.com/snowflakedb/snowflake-ml-python/blob/main/CHANGELOG.md
+Project-URL: Documentation, https://docs.snowflake.com/developer-guide/snowpark-ml
+Project-URL: Issues, https://github.com/snowflakedb/snowflake-ml-python/issues
+Project-URL: Source, https://github.com/snowflakedb/snowflake-ml-python
 Classifier: Development Status :: 3 - Alpha
 Classifier: Environment :: Console
 Classifier: Environment :: Other Environment
 Classifier: Intended Audience :: Developers
 Classifier: Intended Audience :: Education
 Classifier: Intended Audience :: Information Technology
 Classifier: Intended Audience :: System Administrators
@@ -21,25 +25,23 @@
 Classifier: Topic :: Database
 Classifier: Topic :: Software Development
 Classifier: Topic :: Software Development :: Libraries
 Classifier: Topic :: Software Development :: Libraries :: Application Frameworks
 Classifier: Topic :: Software Development :: Libraries :: Python Modules
 Classifier: Topic :: Scientific/Engineering :: Information Analysis
 Requires-Python: >=3.8,<4
-Summary: The machine learning client library that is used for interacting with Snowflake to build machine learning solutions.
 Requires-Dist: absl-py>=0.15,<2
 Requires-Dist: anyio>=3.5.0,<4
 Requires-Dist: cloudpickle
-Requires-Dist: cryptography>=3.1.0,<39.0.0
 Requires-Dist: fsspec[http]>=2022.11,<=2023.1
 Requires-Dist: numpy>=1.23,<2
 Requires-Dist: packaging>=20.9,<24
 Requires-Dist: pandas>=1.0.0,<2
 Requires-Dist: pyyaml>=6.0,<7
-Requires-Dist: scikit-learn>=1.2.1,<2
+Requires-Dist: scikit-learn>=1.2.1,<1.3
 Requires-Dist: scipy>=1.9,<2
 Requires-Dist: snowflake-connector-python[pandas]>=3.0.3,<4
 Requires-Dist: snowflake-snowpark-python>=1.4.0,<2
 Requires-Dist: sqlparse>=0.4,<1
 Requires-Dist: typing-extensions>=4.1.0,<5
 Requires-Dist: xgboost>=1.7.3,<2
 Provides-Extra: all
@@ -48,14 +50,15 @@
 Requires-Dist: torchdata>=0.4,<1; extra == 'all'
 Provides-Extra: lightgbm
 Requires-Dist: lightgbm==3.3.5; extra == 'lightgbm'
 Provides-Extra: tensorflow
 Requires-Dist: tensorflow>=2.9,<3; extra == 'tensorflow'
 Provides-Extra: torch
 Requires-Dist: torchdata>=0.4,<1; extra == 'torch'
+Version: 1.0.3
 
 # Snowpark ML
 
 Snowpark ML is a set of tools including SDKs and underlying infrastructure to build and deploy machine learning models. With Snowpark ML, you can pre-process data, train, manage and deploy ML models all within Snowflake, using a single SDK, and benefit from Snowflake’s proven performance, scalability, stability and governance at every stage of the Machine Learning workflow.
 
 ## Key Components of Snowpark ML
 The Snowpark ML Python SDK provides a number of APIs to support each stage of an end-to-end Machine Learning development and deployment process, and includes two key components.
@@ -73,17 +76,15 @@
 Snowpark MLOps complements the Snowpark ML Development API, and provides model management capabilities along with integrated deployment into Snowflake. Currently, the API consists of
 1. FileSet API: FileSet provides a Python fsspec-compliant API for materializing data into a Snowflake internal stage from a query or Snowpark Dataframe along with a number of convenience APIs.
 
 1. Model Registry: A python API for managing models within Snowflake which also supports deployment of ML models into Snowflake Warehouses as vectorized UDFs.
 
 During PrPr, we are iterating on API without backward compatibility guarantees. It is better to recreate your registry everytime you update the package. This means, at this time, you cannot use the registry for production use.
 
-- [Documentation](http://docs.snowflake.com/developer-guide/snowpark/python/snowpark-ml-modeling)
-- [Issues](https://github.com/snowflakedb/snowflake-ml-python/issues)
-- [Source](https://github.com/snowflakedb/snowflake-ml-python/)
+- [Documentation](https://docs.snowflake.com/developer-guide/snowpark-ml)
 
 ## Getting started
 ### Have your Snowflake account ready
 If you don't have a Snowflake account yet, you can [sign up for a 30-day free trial account](https://signup.snowflake.com/).
 
 ### Create a Python virtual environment
 Python 3.8 is required. You can use [miniconda](https://docs.conda.io/en/latest/miniconda.html), [anaconda](https://www.anaconda.com/), or [virtualenv](https://docs.python.org/3/tutorial/venv.html) to create a Python 3.8 virtual environment.
@@ -92,14 +93,32 @@
 
 ### Install the library to the Python virtual environment
 ```
 pip install snowflake-ml-python
 ```
 # Release History
 
+## 1.0.3 (2023-07-14)
+
+### Behavior Changes
+- Model Registry: When predicting a model whose output is a list of NumPy ndarray, the output would not be flattened, instead, every ndarray will act as a feature(column) in the output.
+
+### New Features
+- Model Registry: Added support save/load/deploy PyTorch models (`torch.nn.Module` and `torch.jit.ScriptModule`).
+
+### Bug Fixes
+
+- Model Registry: Fix an issue that when database or schema name provided to `create_model_registry` contains special characters, the model registry cannot be created.
+- Model Registry: Fix an issue that `get_model_description` returns with additional quotes.
+- Model Registry: Fix incorrect error message when attempting to remove a unset tag of a model.
+- Model Registry: Fix a typo in the default deployment table name.
+- Model Registry: Snowpark dataframe for sample input or input for `predict` method that contains a column with Snowflake `NUMBER(precision, scale)` data type where `scale = 0` will not lead to error, and will now correctly recognized as `INT64` data type in model signature.
+- Model Registry: Fix an issue that prevent model logged in the system whose default encoding is not UTF-8 compatible from deploying.
+- Model Registry: Added earlier and better error message when any file name in the model or the file name of model itself contains characters that are unable to be encoded using ASCII. It is currently not supported to deploy such a model.
+
 ## 1.0.2 (2023-06-22)
 
 ### Behavior Changes
 - Model Registry: Prohibit non-snowflake-native models from being logged.
 - Model Registry: `_use_local_snowml` parameter in options of `deploy()` has been removed.
 - Model Registry: A default `False` `embed_local_ml_library` parameter has been added to the options of `log_model()`. With this set to `False` (default), the version of the local snowflake-ml-python library will be recorded and used when deploying the model. With this set to `True`, local snowflake-ml-python library will be embedded into the logged model, and will be used when you load or deploy the model.
```

## Comparing `snowflake_ml_python-1.0.2.dist-info/RECORD` & `snowflake_ml_python-1.0.3.dist-info/RECORD`

 * *Files 10% similar despite different names*

```diff
@@ -1,246 +1,259 @@
 snowflake/ml/_internal/env.py,sha256=kCrJTRnqQ97VGUVI1cWUPD8HuBWeL5vOOtwUR0NB9Mg,161
-snowflake/ml/_internal/env_utils.py,sha256=tL-5IswRvbcuAZHvi1tIgGuuwg6_I0losgJSdAjfZPQ,14126
-snowflake/ml/_internal/file_utils.py,sha256=ue1mqkjz2sxipycEfLAxkYEX34SwHJKbnkEjWgSd4c0,6353
+snowflake/ml/_internal/env_utils.py,sha256=9h9-UMX6l8qjz5tENbrJKcezwicoFMF7z2zhe72kJZg,14175
+snowflake/ml/_internal/file_utils.py,sha256=AWBSgyQjMJJB33GMKRXOVYIgbeYM4zSYTzG_oBSX05I,7502
 snowflake/ml/_internal/init_utils.py,sha256=U-oPOtyVf22hCwDH_CH2uDr9yuN6Mr3kwQ_yRAs1mcM,2696
-snowflake/ml/_internal/telemetry.py,sha256=CPcC6ZBbIVVkX6Ny3f4-EZ8s3A7O9u_S85H-qxJ6X4M,20238
+snowflake/ml/_internal/telemetry.py,sha256=pM1irUwe5-caRFP-fjUiYPTbTyjh0U-RwKyoat7pCU4,20145
 snowflake/ml/_internal/type_utils.py,sha256=0AjimiQoAPHGnpLV_zCR6vlMR5lJ8CkZkKFwiUHYDCo,2168
 snowflake/ml/_internal/utils/formatting.py,sha256=pz3dFq11BzeHVcZugrU5lQOmPeBKmfkggEsTnDm8ggw,3678
 snowflake/ml/_internal/utils/identifier.py,sha256=zA2Eoc_p8u4kphGuVUbaYt1Fl6xSTjIYu6Qu8BrDZ1c,7703
 snowflake/ml/_internal/utils/import_utils.py,sha256=eexwIe7auT17s4aVxAns7se0_K15rcq3O17MkIvDpPI,2068
 snowflake/ml/_internal/utils/parallelize.py,sha256=zYtkYBq2_N7R49AvSzJynmvixNhUw3YBBZQ3uxVtTEA,4550
 snowflake/ml/_internal/utils/pkg_version_utils.py,sha256=AMR97AZCOr26Je2Q4fIePJRMf7cASr910R5-wr7ANpM,3722
 snowflake/ml/_internal/utils/query_result_checker.py,sha256=IrzUJ4fJvxjJ5ma-6mejWHpxoEtwnMKo9XTJ-YsECnk,12205
 snowflake/ml/_internal/utils/temp_file_utils.py,sha256=77k4ZAZJfyJBMw0IOfn4aItW2mUFGIl_3RgCNS_U4f4,1400
-snowflake/ml/_internal/utils/uri.py,sha256=wi5LTs306Prcs8tL1CR19b2nUto8U2FLlOyVQrUQcn0,1841
+snowflake/ml/_internal/utils/uri.py,sha256=sZIf7Ph9TXZZ7lU4IPpVfsc7oHflWEm6atzNPJ7qpsw,2117
 snowflake/ml/fileset/fileset.py,sha256=hwKtNENBiNpEeHKyNra2QM11TYklzjyB_PtIQ8x5r_g,26746
 snowflake/ml/fileset/fileset_errors.py,sha256=ZJfkpeDgRIw3qA876fk9FIzxIrm-yZ8I9RXUbzaeM84,1040
 snowflake/ml/fileset/parquet_parser.py,sha256=yTJdYFTzaTPsgb1rGMj_jv_wDjmuwJZzbVRRmk--yA8,5915
 snowflake/ml/fileset/sfcfs.py,sha256=YWL2D8P-3KcSoGmz6_nvMjQgRNTKzXbwGRhIZYYVZQo,11536
 snowflake/ml/fileset/stage_fs.py,sha256=deFiXBXqab_v2WG6-A0BaepWvNxh4afpDsGbYh0jNWA,14859
 snowflake/ml/fileset/tf_dataset.py,sha256=MrFtGiFu1FX3MSjAjWnZcEa5Ow4fsAHlUXW-BLqFWus,3462
 snowflake/ml/fileset/torch_datapipe.py,sha256=kjfUmAqEQ55Gd1nMUFP-3crp1XG46oJ4E74Euk4HEW8,2386
 snowflake/ml/model/_core_requirements.py,sha256=6HGtzvyZVGSIMYkJQ-J4TSyWwPt69uXnPXj7A4Nm34Q,197
-snowflake/ml/model/_deploy_client/warehouse/deploy.py,sha256=AUv7H3qQVCkaevgEMENugBYW-_eL1r21vnleM7UezbQ,7962
-snowflake/ml/model/_deploy_client/warehouse/infer_template.py,sha256=qaGEbWhJCpdLse0KGw6kIS6gGD8iSA4j4By1wc-Lh2Y,2369
-snowflake/ml/model/_deployer.py,sha256=c08kn3R6krNV0RaPGhFjQJAWxJ1zsM3kFMJ7VQ0O4OI,9548
-snowflake/ml/model/_env.py,sha256=7vJHt77WusrMDDeKSRTyE-X9P1QICg-q68fxSx8scvg,4488
+snowflake/ml/model/_deploy_client/image_builds/base_image_builder.py,sha256=hslB0piUdrw15xUOK0rdvS4dOuwQFRnG6WSxgua_UWA,345
+snowflake/ml/model/_deploy_client/image_builds/client_image_builder.py,sha256=Wmz7I-KMoCqd8lSAXgmbM74vghRBG-2rQ6b6QdDLAYI,11239
+snowflake/ml/model/_deploy_client/image_builds/docker_context.py,sha256=RIKXtfPFYd96JBktJSbYR0bvkj70G4Cc68ISveX1dFk,3998
+snowflake/ml/model/_deploy_client/image_builds/gunicorn_run.sh,sha256=iZWbzTZXpHmjIr9qRLzSCwgWIO6K3-8YgBu-1Q2zrEE,887
+snowflake/ml/model/_deploy_client/image_builds/inference_server/main.py,sha256=MzYtOVUOxJPV239dy3WA2wBcWwMB0zpnBlrVRw4QZ6Q,4894
+snowflake/ml/model/_deploy_client/image_builds/templates/dockerfile_template,sha256=YDOjxmU9-GqBYVn8_pW6wz0v5tEe62qXtv05uQdL3lM,1264
+snowflake/ml/model/_deploy_client/snowservice/deploy.py,sha256=dg5evnL-hx6LG5JFGjqx7HC06LjokVHwfL4BmduvobQ,8602
+snowflake/ml/model/_deploy_client/snowservice/deploy_options.py,sha256=gIzSUpMf1AhlD-s10L6HU1CKrUIq3kHEHlTKLyXy1LQ,3801
+snowflake/ml/model/_deploy_client/snowservice/templates/service_spec_template,sha256=pB-DK7y7BxOS2n-yUepo8Okwnx273nDqbkjq_U1iEAg,579
+snowflake/ml/model/_deploy_client/utils/constants.py,sha256=dzN8F8E_aQv7m1q1zx_0sKYPp9I_0tlE0vnREg1YjfY,1470
+snowflake/ml/model/_deploy_client/utils/snowservice_client.py,sha256=OWT2VQnuVlHblhv34camONpv0dzYLh-VT_R8ygp0KcQ,7494
+snowflake/ml/model/_deploy_client/warehouse/deploy.py,sha256=mr83w-CamAQn6jL1txaWqimoykYhCThSDzwmlhT_Los,9160
+snowflake/ml/model/_deploy_client/warehouse/infer_template.py,sha256=ofWbsLZhyWor8KMmV_r-EpZwzbFxo9IgVEnNynfJq_w,2489
+snowflake/ml/model/_deployer.py,sha256=xi8tss6FYcJIetfl3H6IT4d57gl_IoVtxpR1kGMf-qk,8855
+snowflake/ml/model/_env.py,sha256=VnGo8RnbO0snFcI-1iGhAkln_pY0vHCpmFPmetD3qrs,4560
 snowflake/ml/model/_handlers/_base.py,sha256=JUPnwTCGgMkKzqVns2zeVCF4-MtxnVKDieqNZR1X8sc,2299
-snowflake/ml/model/_handlers/custom.py,sha256=Hjf_bg6LxhQWctkg6h35Knnu7-FHo2HWZLrPHRsEtWM,6084
-snowflake/ml/model/_handlers/sklearn.py,sha256=OrNHd6_k7l8AbqpUCKcVeK1-ypwQUybDjYQr6IYtmBc,7558
-snowflake/ml/model/_handlers/snowmlmodel.py,sha256=P35oabm3ERwGjnrREVi35a1JS1o9wdTzFJLThHt_uT8,7711
-snowflake/ml/model/_handlers/xgboost.py,sha256=8WLW_tKDB7t0AjFCy8DzpCat7ojRK61h0AMFKRF0mlg,7204
-snowflake/ml/model/_model.py,sha256=wBcwYjjmTlGhJcOilndqeZALsqfqR3cU30fF7ciTDm4,26448
+snowflake/ml/model/_handlers/custom.py,sha256=q6IritKreteNZ_mQ6hzortkeGgH6C0LgHSwcGFkvZCs,6423
+snowflake/ml/model/_handlers/pytorch.py,sha256=0ZNi94cpDW1ALU9KUnoTkVEZ4n5KxgyPDrLuI6ATdas,7166
+snowflake/ml/model/_handlers/sklearn.py,sha256=dEJNor34rvKfClFo0ukFB-G9intdoXiGL3ilwgBHCao,7843
+snowflake/ml/model/_handlers/snowmlmodel.py,sha256=3RG6cveksBgwlqsQrOJIrwG-dnS5CgM2EESle3xS1wQ,7993
+snowflake/ml/model/_handlers/torchscript.py,sha256=OqpjJzXASaQjt3tws7C7RKQN_G9VPi7O9g84huK0Mag,7287
+snowflake/ml/model/_handlers/xgboost.py,sha256=FUGjmVtl9V0xXMalxC6FVVFyVX3T8Gds7V9HQXL2o2I,7603
+snowflake/ml/model/_model.py,sha256=Gm8uFI91cjxHdlH7HU_weP3J93cgrkekHjBnkchLNbs,26469
 snowflake/ml/model/_model_handler.py,sha256=a1upCULZlNuxUiFoJbK85nERGkA2VkEsn5-IIZn7pro,2101
-snowflake/ml/model/_model_meta.py,sha256=FRhp90-SxVcE-_FxNZ39M_Bqycyu5h_LiNoMb61Ia_8,17684
+snowflake/ml/model/_model_meta.py,sha256=NWknKLXh1oMBO9ED62PnnX2ltevnMN3wu6H8QiGLph0,17945
 snowflake/ml/model/custom_model.py,sha256=8qEHi8myHcp02jcpFbG9Kqscn9YRv3QnzehCrTSI8ds,8016
-snowflake/ml/model/model_signature.py,sha256=Q_n1mcetW5btVYCS4VWMef29TshctoZSPC8Gk3Xqv2U,43624
-snowflake/ml/model/type_hints.py,sha256=Vlpk52yXo2WcBKVdhoJM0gjnj20Tr6vwb3AOM3n35g4,4405
+snowflake/ml/model/model_signature.py,sha256=SoWjugyzw8yut5n17O1-PxtNCxlR0ZJwtFXeF4Xu9VA,60533
+snowflake/ml/model/type_hints.py,sha256=SrRLRJzhrVms22rO80Ee8JWHrOHV6y_EGtAOxRF54eQ,5187
 snowflake/ml/modeling/calibration/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/calibration/calibrated_classifier_cv.py,sha256=Fh6Yq3jvpDvnQvtN9UPPo6c1p8266OwqQ77aT5ZhQGo,54140
+snowflake/ml/modeling/calibration/calibrated_classifier_cv.py,sha256=tuVQT6v63PTOZhHP3E3APYdiyFEWA0w4sQdWvuhg40Y,55506
 snowflake/ml/modeling/cluster/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/cluster/affinity_propagation.py,sha256=2Yco1Od7Yy0Av_4DW84VFJLs96rPJCy8xz8CMEH_O4A,52067
-snowflake/ml/modeling/cluster/agglomerative_clustering.py,sha256=-U-ZBqQpURTwAT45rTECN-udcTRdq9iWAHyla3ZRgxo,54080
-snowflake/ml/modeling/cluster/birch.py,sha256=_vm0DzphdPTij-tnBNszMhjO2ryIcWTKYWU_NdS0tUE,51905
-snowflake/ml/modeling/cluster/bisecting_k_means.py,sha256=Ae2AOU1qp2J0BszoKH3OC13ua9ut3xc4DdJ_DjFNf9A,54287
-snowflake/ml/modeling/cluster/dbscan.py,sha256=n5lG9ZKq4wEnD-4-HQDFaXj7-_lk9yOAPJPEEHbLawY,52246
-snowflake/ml/modeling/cluster/feature_agglomeration.py,sha256=tK0owQ9esTy_VVSuybejL5gEsGofar70CIVrE5GI5lk,54620
-snowflake/ml/modeling/cluster/k_means.py,sha256=Z0QFBDwU-aHt-ua8Cs91revqgFvMgg5tFMh3--lKpXg,53874
-snowflake/ml/modeling/cluster/mean_shift.py,sha256=VIe24mRUvAk35fNISI0JBYpRjIcmsv6XjRr4uvzq8Ic,52448
-snowflake/ml/modeling/cluster/mini_batch_k_means.py,sha256=-FNLzyqaYZmQ6Hh5dtL7_CW_9G9MH-0ZHjrwsPIphVc,55149
-snowflake/ml/modeling/cluster/optics.py,sha256=c97yzv0KWqoOJyqk4RLggsdTqBfDl_M6WLbAMd7Wuak,55580
-snowflake/ml/modeling/cluster/spectral_biclustering.py,sha256=9-9qPdHANEnO7bwIm50M-Y_veYjmWXhrP63k9417Nao,52638
-snowflake/ml/modeling/cluster/spectral_clustering.py,sha256=moI4NNxtXhSFze2pAmlwPVGxMbylrFJkL9w2BChFbpw,55576
-snowflake/ml/modeling/cluster/spectral_coclustering.py,sha256=vtJmPSB5RmPhBQgXWf2FqviSWsrVT9X06FMeFLbN1IY,51768
+snowflake/ml/modeling/cluster/affinity_propagation.py,sha256=pFn8z4prI5O-thwhNE39g4jDPTlZjGWz__AhYw2_MYk,53433
+snowflake/ml/modeling/cluster/agglomerative_clustering.py,sha256=5PbmUs1wxwyUPY-B-iWDfZ6qlMA_NjiWzPBxDHRx5xE,55446
+snowflake/ml/modeling/cluster/birch.py,sha256=bNVfRZesHwYsL1Maep_FIzmWrbHtO75ZK-XtqSEVqXE,53271
+snowflake/ml/modeling/cluster/bisecting_k_means.py,sha256=D8NmOxC_0UvgctdtECdfCS1AJoymJCRJvTczxXbwmMw,55653
+snowflake/ml/modeling/cluster/dbscan.py,sha256=hG8AUIFlUD55klTHvZd9JA0WF9LLHVNGJk0jHgi_L8Q,53612
+snowflake/ml/modeling/cluster/feature_agglomeration.py,sha256=IHDwX6hUef6NyGGyApPEdNX6FQWTbaOdr98j_Ypo6l0,55986
+snowflake/ml/modeling/cluster/k_means.py,sha256=qOx-mWWDrdxVwj9Uffh-zw-HtzrQxpCfZCyNfwFjDJM,55240
+snowflake/ml/modeling/cluster/mean_shift.py,sha256=egzx5sFedAGPEI-wE7SbMIYlFp4ASZRhyOlW0AfRg1U,53814
+snowflake/ml/modeling/cluster/mini_batch_k_means.py,sha256=7NNevIzp8EXjYfbfZqjS9TQmvY2LmP1Kr-xvweK6CyY,56515
+snowflake/ml/modeling/cluster/optics.py,sha256=ogkbb0M-dJw9yclFxB8EEi2kyLoxbhW6cDAgihlVadY,56946
+snowflake/ml/modeling/cluster/spectral_biclustering.py,sha256=nLDe1dF0GaCWJxM9wY-jd8rTiQFAZXuGeORzDvJeojs,54004
+snowflake/ml/modeling/cluster/spectral_clustering.py,sha256=IrIqiSPbjYzDo1iqhEkhzT3N_2SIfh3jJylSFFbtQYA,56942
+snowflake/ml/modeling/cluster/spectral_coclustering.py,sha256=ecJkDo5sKtYYOCndJcUsnyUe517lmpkGvZWN7ZeUONQ,53134
 snowflake/ml/modeling/compose/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/compose/column_transformer.py,sha256=6LdwUi3XcdikyqmZhQo8Q7NIAIzGg_pu2AXe77DAGh0,54351
-snowflake/ml/modeling/compose/transformed_target_regressor.py,sha256=fq48CubtxA_SbKtlxvF1ksdTIJAta3ArzaYLPWPkmok,51936
+snowflake/ml/modeling/compose/column_transformer.py,sha256=bHQ9xxA6TlTJiTDyRvJA3MN_RkH8Vff8OCz2Eitka5w,55717
+snowflake/ml/modeling/compose/transformed_target_regressor.py,sha256=zXsli2w6yTH1x0YIFRa7mlonxS4Ui7jRZmbHqlwEsgM,53302
 snowflake/ml/modeling/covariance/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/covariance/elliptic_envelope.py,sha256=kbJ0rgwMD-FBtGox7MG3ZaGkVzqOU9GGa7h1otgsgJg,51908
-snowflake/ml/modeling/covariance/empirical_covariance.py,sha256=RMAb0G8lhSfHDkXP3lfzWFyLMJbGT9RXs99hINq2n8Y,50184
-snowflake/ml/modeling/covariance/graphical_lasso.py,sha256=ruc4U-p-0vyC3m_2DGoXwbm8RJnnysVucv7TiLY1wAU,51458
-snowflake/ml/modeling/covariance/graphical_lasso_cv.py,sha256=Z8os4_OgmL-7SEad1JVbU_l_uQTPN2etjFqTTF6o-Ko,52922
-snowflake/ml/modeling/covariance/ledoit_wolf.py,sha256=Lvld0ax1YowkWBiDQs8Jmn6vfwevg4ciM_r-DaAFPEA,50386
-snowflake/ml/modeling/covariance/min_cov_det.py,sha256=iHBPAUBZOJMrFwuJMbpG-D6aBfZknWKz2IqJZ0xBJNU,51149
-snowflake/ml/modeling/covariance/oas.py,sha256=7-KZ1KSN4Y4Gt_FAg03XE9vH6LC3gBnK2yt75PfKLRE,50075
-snowflake/ml/modeling/covariance/shrunk_covariance.py,sha256=l9afcrkEqgM2zbZuv8-Wdu_LwK6eHCxiteOyzfzFfys,50361
+snowflake/ml/modeling/covariance/elliptic_envelope.py,sha256=W3EOdTi2JcPeFVR3L03Bg6kfwg2N-jwGZ8nC7wUATtM,53274
+snowflake/ml/modeling/covariance/empirical_covariance.py,sha256=L5zVU9NPs3qQ5MCFfTyFcxdEKQ4aUbrJVCgquR0xLeg,51550
+snowflake/ml/modeling/covariance/graphical_lasso.py,sha256=IbnfUfNkifEQ9bXLPTBicX6y8M0ibWjpCb-71UBwtRw,52824
+snowflake/ml/modeling/covariance/graphical_lasso_cv.py,sha256=fqxHKQNuz_CE6GKfcmvzLylWCiDNuhLB-rVp_Tt9z94,54288
+snowflake/ml/modeling/covariance/ledoit_wolf.py,sha256=CoL4LoAE87rdyCZiSKfp7bHeMvEUU909KLtCBPgkAgY,51752
+snowflake/ml/modeling/covariance/min_cov_det.py,sha256=cPa3cjq-2APJvGvwOcCqSPAoDuMPaTif6cD1X56yAww,52515
+snowflake/ml/modeling/covariance/oas.py,sha256=500og54f9Ubt_smfasP2ITXJAen0Fb000D5H53cbl1E,51441
+snowflake/ml/modeling/covariance/shrunk_covariance.py,sha256=zHH2jFGQ24dLlpMkI2Pt6cZycYzAQxaTS9KhJNsmnxc,51727
 snowflake/ml/modeling/decomposition/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/decomposition/dictionary_learning.py,sha256=TgyT361-kmtHHJ015svz_d_NsOShWtLT_Hqa2oSgsuE,55176
-snowflake/ml/modeling/decomposition/factor_analysis.py,sha256=TTlDQkaAUo2bWwZM-5UmH8zWltGPVj8ZWHAb3LRlLks,52548
-snowflake/ml/modeling/decomposition/fast_ica.py,sha256=T9_bqJwvIe-HWebGkDwHhBAKip13QlZS8zp_H3wTJ_Y,53010
-snowflake/ml/modeling/decomposition/incremental_pca.py,sha256=m3qmci-Q8tu6BDBybHt5gTGc91oa4tzKYieuTUH5VqA,51345
-snowflake/ml/modeling/decomposition/kernel_pca.py,sha256=7R008lOnVyiOOE_vGBSZfSkYPnHBPeJ_IEZy2yqo8_8,55376
-snowflake/ml/modeling/decomposition/mini_batch_dictionary_learning.py,sha256=sdGzUim1jbwQoMbqgmK4Yt2ujnrSNovPcmwMRr5kUuY,56359
-snowflake/ml/modeling/decomposition/mini_batch_sparse_pca.py,sha256=S0ypJAaODsN7QIpoxaITjLt8PujLR7duy6JRw2b-kq4,53676
-snowflake/ml/modeling/decomposition/pca.py,sha256=zDw-12Xl0RVPlv1DRNej1wVZHVg4UbULL1-59XJI2JM,54220
-snowflake/ml/modeling/decomposition/sparse_pca.py,sha256=yt9_-fHwmJly0bmrP4zF99nAFXNdOghC1tvlqe0uQ-s,52541
-snowflake/ml/modeling/decomposition/truncated_svd.py,sha256=gyPfuP13xKVqxWxVRmfSPBsByDVip-TUmYX3JsZA82U,52113
+snowflake/ml/modeling/decomposition/dictionary_learning.py,sha256=GKRNAMxdCcJw90-FxDqZ9gNOtEy5JZz71qZeHJSLaG8,56542
+snowflake/ml/modeling/decomposition/factor_analysis.py,sha256=R8H16Suj-JmAI1MafJj4EZp-JKHzP00Z_V2NZ--Hyvw,53914
+snowflake/ml/modeling/decomposition/fast_ica.py,sha256=UBgMFe78BHGpPnAG8foDB41zRmldpQH8QSLEMpkspw0,54376
+snowflake/ml/modeling/decomposition/incremental_pca.py,sha256=5MUCSQwpK1n-M8742wUB3ApjoQ_H44y_N6fLw8RKdWc,52711
+snowflake/ml/modeling/decomposition/kernel_pca.py,sha256=BFsn3p36-U_9qQmztHoCnXN9QLXCcgns55c9eMnuXDw,56742
+snowflake/ml/modeling/decomposition/mini_batch_dictionary_learning.py,sha256=MUoRNIf2WuqbQ0NF-8rAq-jkxCjNy3u7xB-M4QrUYoc,57725
+snowflake/ml/modeling/decomposition/mini_batch_sparse_pca.py,sha256=J9_p5A1UMxG-6UC27n_Gq8xOsZ1Q6i0uINg7YOIwoic,55042
+snowflake/ml/modeling/decomposition/pca.py,sha256=3ntwAL2DU7mRSuXMQSuw_smDgJGU6z_0GbCGKfITB48,55586
+snowflake/ml/modeling/decomposition/sparse_pca.py,sha256=ixsbIaqssO64T8BeD3cA-WzebnlxfdSPzgGPUp-BI1w,53907
+snowflake/ml/modeling/decomposition/truncated_svd.py,sha256=Ds7wa1DIk4rPQWF3Y_6A6SK0NmM77LfSQjvNpXMtHPU,53479
 snowflake/ml/modeling/discriminant_analysis/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/discriminant_analysis/linear_discriminant_analysis.py,sha256=pFO9WhFbvZxtNLT-qwMu25gFOwHUlqSzXk2SUWiYFbQ,54361
-snowflake/ml/modeling/discriminant_analysis/quadratic_discriminant_analysis.py,sha256=dYkvp3HMhJA5BuhkLMbNCm-QqPspZwQAUfZdP1appaU,52426
+snowflake/ml/modeling/discriminant_analysis/linear_discriminant_analysis.py,sha256=Phy_u7CJ6xqhoUNWXI7BZdrCJsGemEsz8ZIG5NEZWDg,55727
+snowflake/ml/modeling/discriminant_analysis/quadratic_discriminant_analysis.py,sha256=syBNrxFoIYtaLtijz9JW66yCT83o3MQkJHKylEylOu8,53792
 snowflake/ml/modeling/ensemble/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/ensemble/ada_boost_classifier.py,sha256=wI-ZlS9RxmrbdrlVhnOysyNi_HI6F3lJ0l4CXCnnBTc,53379
-snowflake/ml/modeling/ensemble/ada_boost_regressor.py,sha256=T2hy8Vvh8q8S2FMdwlr6gUHpasrY5hts1zqzR1ylJZU,52278
-snowflake/ml/modeling/ensemble/bagging_classifier.py,sha256=uKoQc6Wgt4vLDPHGngumU0mdOnuT9B2b7oKuR3M4Ews,54303
-snowflake/ml/modeling/ensemble/bagging_regressor.py,sha256=brIXqQWVAZOk71nLIV21GYoW9SnnmosGJRG477wa1RQ,53547
-snowflake/ml/modeling/ensemble/extra_trees_classifier.py,sha256=aBemY9GMjkvYlAahwx2eXaxy59WcHywUNFDvfkVN3z0,59089
-snowflake/ml/modeling/ensemble/extra_trees_regressor.py,sha256=6CbTrwPsrZ4Ln6H7PGd7_JFepir6lWshjKjTpw-GhV8,57700
-snowflake/ml/modeling/ensemble/gradient_boosting_classifier.py,sha256=SC00Sy4eHVXziKjoxgmK6yTNHYdZae1oPI1JIbPZd8k,60698
-snowflake/ml/modeling/ensemble/gradient_boosting_regressor.py,sha256=y9KHokK5HsfmQwoRYsNUcPQAcpGeAVYdLa9ydwmuy1w,60282
-snowflake/ml/modeling/ensemble/hist_gradient_boosting_classifier.py,sha256=yjlgQA8MTp8lxwY_BnnLOZfGWK2zvZKUcz6nuGSdBUs,60348
-snowflake/ml/modeling/ensemble/hist_gradient_boosting_regressor.py,sha256=lAFA9_pmShlfPaa-Pw8gKTUnrO2hcydhElUSprIxHgc,58670
-snowflake/ml/modeling/ensemble/isolation_forest.py,sha256=RkFtwkAmIq5NAv-peoS4TuszU2MCW4wcTfyjaENyJn8,53324
-snowflake/ml/modeling/ensemble/random_forest_classifier.py,sha256=831nscGhY0toqmPYdq5aSvdVdOqJ1DVcxcq7Fr4Ht8k,59044
-snowflake/ml/modeling/ensemble/random_forest_regressor.py,sha256=KTZVh4R1zLdILlKQD3WPo4hFP2DINNgvuPQrgZGnoI8,57643
-snowflake/ml/modeling/ensemble/stacking_regressor.py,sha256=xjeZPZIN0AOcXSg14lQOrB60Ah8E24ODQJpcOF2aS5U,53231
-snowflake/ml/modeling/ensemble/voting_classifier.py,sha256=W-8z2EwwmgqHNm72aZCO_qeiB4PTGt9e5ok5T_hGTFc,52806
-snowflake/ml/modeling/ensemble/voting_regressor.py,sha256=bnwzchGtAvU4VzgwvePCd0luAQrUYqAC2DRVbQyNZ7c,51341
+snowflake/ml/modeling/ensemble/ada_boost_classifier.py,sha256=YJNdZYkT-K4tPUc9tBLWb9MfnsyCpeDjlY___-3Jxno,54745
+snowflake/ml/modeling/ensemble/ada_boost_regressor.py,sha256=oB4umnfZlORTqtjzM03s5w7cj-g3lG5h4fqT5qTvOgI,53644
+snowflake/ml/modeling/ensemble/bagging_classifier.py,sha256=5KYF_zT6WhTBbgzLqHuFH_zP_4e9WSa-uRuyYxAy2PM,55669
+snowflake/ml/modeling/ensemble/bagging_regressor.py,sha256=LRBh0I8U6lL_RrqFDhSQ3c3WbK09wYdxkRbN4HTf-bs,54913
+snowflake/ml/modeling/ensemble/extra_trees_classifier.py,sha256=FSlAb0RbE9yKulnjcGfu_cW1huKT3u-_F6GvxbYE6OY,60455
+snowflake/ml/modeling/ensemble/extra_trees_regressor.py,sha256=83jHLpSFuBr4lsApeQ3BvzF00Gbr9WfnJCY_awSb-j0,59066
+snowflake/ml/modeling/ensemble/gradient_boosting_classifier.py,sha256=gtdgUDzv8Wf5Hnl291dnR7ziyqP7FENeKtzbExvX9Lc,62064
+snowflake/ml/modeling/ensemble/gradient_boosting_regressor.py,sha256=J1NJVLPYRQDvNXwGhlbp1rdFBH0PgCT8Km0Va6cqdnY,61648
+snowflake/ml/modeling/ensemble/hist_gradient_boosting_classifier.py,sha256=7DpIW3IDiRBssTwU9Kxd4LDo5iuG3cu9qZNn74tLz_U,61714
+snowflake/ml/modeling/ensemble/hist_gradient_boosting_regressor.py,sha256=IAwdSzFzS7_f9bBPzjTXaYgyGkFgkq_mWIjCbA1zpbk,60036
+snowflake/ml/modeling/ensemble/isolation_forest.py,sha256=AXuvTAKDIDbC19c5g8HB6hLPwnteQTRaydcg_ZAwkqA,54690
+snowflake/ml/modeling/ensemble/random_forest_classifier.py,sha256=KOPrrJpaZplfeWrVc9SwLw9Augjwf_MGt_fQKg_BvWE,60410
+snowflake/ml/modeling/ensemble/random_forest_regressor.py,sha256=43TqrL5F2kNdO5xbQeVlzTkbVpNxBu9IECmxSRxZTFA,59009
+snowflake/ml/modeling/ensemble/stacking_regressor.py,sha256=dnycH8T_TMUvOXAHbxvJlHQCaEQ6cqbBlwZJBvdI7wE,54597
+snowflake/ml/modeling/ensemble/voting_classifier.py,sha256=z1kWVglksUkHFDMGcexqUwpQnz5tVfWUaquKIQ4m74s,54172
+snowflake/ml/modeling/ensemble/voting_regressor.py,sha256=MjPdM34PuhSnR4SxFAlsNPhC5hEbLQ6zi4no3RydzIY,52707
 snowflake/ml/modeling/feature_selection/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/feature_selection/generic_univariate_select.py,sha256=2Zpf6cTF5Q1Qxmp4VhDU-Bw1NhcApNbr2zUvXu1cfLI,50854
-snowflake/ml/modeling/feature_selection/select_fdr.py,sha256=zit_0nZp5bPXaac18IWC-6_tI8791ayR-ENTH-fdHZA,50552
-snowflake/ml/modeling/feature_selection/select_fpr.py,sha256=tzP0ae_1EFjaKG5AV4RzzfSJkKiiK7EA0fyro9GAb8o,50546
-snowflake/ml/modeling/feature_selection/select_fwe.py,sha256=MZpzDF57EWhQYNUIHB6q22RbUNcKhhhDIs6SfNXcl64,50554
-snowflake/ml/modeling/feature_selection/select_k_best.py,sha256=b4jGwVHFlYG1MBh-TKqVAvLv3Gjnb7mHeP2rAC7NvKI,50631
-snowflake/ml/modeling/feature_selection/select_percentile.py,sha256=5CbzG9cHWj1IeGQGPWsomLXAY3IfO9q-IYJnLycVPs8,50651
-snowflake/ml/modeling/feature_selection/sequential_feature_selector.py,sha256=YEzSgY5_WX96DBXNQAu_u4F_PjEysmw9HScHvkqlZaI,53304
-snowflake/ml/modeling/feature_selection/variance_threshold.py,sha256=4YDgM5XGpigBA0wOIdmkp1Qpd-i6gyQsjZs2tepIM14,50283
+snowflake/ml/modeling/feature_selection/generic_univariate_select.py,sha256=QOygJRRKHfxGBK1CFh2byjoVcnXsnz8VtIVebn2-ZDg,52220
+snowflake/ml/modeling/feature_selection/select_fdr.py,sha256=B_GPPVLwmIb9ScHCViCEwt-LGtvjlKK5rEtSkLoKTsQ,51918
+snowflake/ml/modeling/feature_selection/select_fpr.py,sha256=1wJJRFSC-qZWSz-hFcoZjYA7d8oVWfY1OlIgtCyhfNY,51912
+snowflake/ml/modeling/feature_selection/select_fwe.py,sha256=HL1MlwdFiHepBVDe_H-hjsQB9zb1rCaxLniyXs4Js2M,51920
+snowflake/ml/modeling/feature_selection/select_k_best.py,sha256=V7HoRkdtsVAFAE81U1FQQKkWUnBt9ZiAfunhxwod6lw,51997
+snowflake/ml/modeling/feature_selection/select_percentile.py,sha256=SalYYsSYikQ73TzkcuXe22mwkPKMvzEv-FfRNFT1oPk,52017
+snowflake/ml/modeling/feature_selection/sequential_feature_selector.py,sha256=qVOjicEqGVHujZuU14G-jXfViVkDyV0CCt8JhQtfCSk,54670
+snowflake/ml/modeling/feature_selection/variance_threshold.py,sha256=ucnTz_bLAyRApkcdt7J9pAFvijuJFYV-uearYMEinqc,51649
 snowflake/ml/modeling/framework/_utils.py,sha256=So72kQZXXP0U9D47rXx0U5mxbkkRujKwmCh-f2OVf3E,9110
 snowflake/ml/modeling/framework/base.py,sha256=hxRwBOKQtQFVZtHopgj_bgBUIU5TK9vJj4p4ZDynFWc,21900
 snowflake/ml/modeling/gaussian_process/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/gaussian_process/gaussian_process_classifier.py,sha256=1SCwt_cR_NSOl8K38GDHgbZZ9urFfOcKeW4Iq5qe5Go,55849
-snowflake/ml/modeling/gaussian_process/gaussian_process_regressor.py,sha256=bPuQg6EA5zfEylQgC7jnMsWKozVzDiR5PlyeM1zywBI,54541
+snowflake/ml/modeling/gaussian_process/gaussian_process_classifier.py,sha256=Po-HwoK-vELk0R36D86KH1iafgcL8Txs9EuJkRD9wXg,57215
+snowflake/ml/modeling/gaussian_process/gaussian_process_regressor.py,sha256=582hyNE0W-mZ58Vn2yCYROkydamXVsfF5UZxKHBDax4,55907
 snowflake/ml/modeling/impute/__init__.py,sha256=dYtqk_GD_hAAZjGfH1maWlZQ30h4hu_KGaf-_y9_AD8,298
-snowflake/ml/modeling/impute/iterative_imputer.py,sha256=lEPVoxZaWd1NXp2WeiaWJduLeIVmzVK7WF3LdHGMpXI,56404
-snowflake/ml/modeling/impute/knn_imputer.py,sha256=QR_-MNV6v2XC7BMgNxICUMjgOVwLTq6kZdFIPeIFRnk,52626
-snowflake/ml/modeling/impute/missing_indicator.py,sha256=VwmisPgtu_uUA_5_5i1fbDVlXzhUxUcJSfJrygG9q0g,51423
+snowflake/ml/modeling/impute/iterative_imputer.py,sha256=kz4gc_dubKSu14DU77KV_XMeRfeRDZPBK-dbmToxJW8,57770
+snowflake/ml/modeling/impute/knn_imputer.py,sha256=ay7KVv2XuPuGcazIJPlO6lDrX5-0Kkl7kNkTxrhcDVk,53992
+snowflake/ml/modeling/impute/missing_indicator.py,sha256=Mvh9deXKmobw1d2Ow0fzBwZV7eMlzesLxQtVdu405Ko,52789
 snowflake/ml/modeling/impute/simple_imputer.py,sha256=AuqGFxRvVEuIdhTNhmk6T0Uz5K-k1RCKCTnQFCNQxWA,18118
 snowflake/ml/modeling/kernel_approximation/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/kernel_approximation/additive_chi2_sampler.py,sha256=VqNhJGP4xuqMmHOCLwAMivpcvGs_MBSavRGi84mXuYc,50367
-snowflake/ml/modeling/kernel_approximation/nystroem.py,sha256=wE8KrYZOBHDqFdZMVdxLj_B_PVEjrivZDxHrJr_Ow7Q,52240
-snowflake/ml/modeling/kernel_approximation/polynomial_count_sketch.py,sha256=iGlxvqAjZDgJ5_C8LGcip7Tp_9Ug1WAEr4a5KfjN-Po,51394
-snowflake/ml/modeling/kernel_approximation/rbf_sampler.py,sha256=_5PE-_HRtbdi4Nd_cOu5vJIkZXVx5o4dMQDCXIoDpOo,50823
-snowflake/ml/modeling/kernel_approximation/skewed_chi2_sampler.py,sha256=uZvRARvLG1NI_VPFMS25o_IboKxA8UtgMXaNZ8niSC8,50822
+snowflake/ml/modeling/kernel_approximation/additive_chi2_sampler.py,sha256=Wi4eHHhqZqa2zWTOvNjZ654WfqJE45IiE5FURqg35fg,51733
+snowflake/ml/modeling/kernel_approximation/nystroem.py,sha256=1KbkTFSpLfgiHsttIKC0y6WybxEJmIJnWaucMSEPW2k,53606
+snowflake/ml/modeling/kernel_approximation/polynomial_count_sketch.py,sha256=YmBuv1wvTmozyu1bWEUAEAjdR4YJAaUXyT1i2G62hXQ,52760
+snowflake/ml/modeling/kernel_approximation/rbf_sampler.py,sha256=CXkTlAbqUTXmIndPOaZJBzVJON7cptYpoX-Bl7VQTQY,52189
+snowflake/ml/modeling/kernel_approximation/skewed_chi2_sampler.py,sha256=H4VXdL5P_J4YLZXR4wHPfXg1gLQqE_f6xfZPbBvGnMw,52188
 snowflake/ml/modeling/kernel_ridge/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/kernel_ridge/kernel_ridge.py,sha256=K5rwm_PEU_U8nFGfEeUIWx0q4z0sGC8P_D-3PZkXZZo,52340
+snowflake/ml/modeling/kernel_ridge/kernel_ridge.py,sha256=QlEvYhnWYMEf5Vz-usvmLrP1nlYrNMZTEYF4FGdviAM,53706
 snowflake/ml/modeling/lightgbm/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/lightgbm/lgbm_classifier.py,sha256=p8WVRLf-_VnwpqLxW6dnVrj-2wxPm43TwyYnpvsaS-o,51862
-snowflake/ml/modeling/lightgbm/lgbm_regressor.py,sha256=HgV0YR1OoahM9qdBIslU8JTV3c3811npxITla-3WVog,51373
+snowflake/ml/modeling/lightgbm/lgbm_classifier.py,sha256=UGz_qfrr3rehtQjRr-dozP_IgTZBHeak1OwkYXtf4WI,53228
+snowflake/ml/modeling/lightgbm/lgbm_regressor.py,sha256=pX_ewzyd1KhJLCZAlLbzNpYgk4cQfWoUnSrXVEu01n8,52739
 snowflake/ml/modeling/linear_model/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/linear_model/ard_regression.py,sha256=dggAjhWDP60kbKiOAJojAQXtg3Wo9Dm0bYVtYrggnXk,52088
-snowflake/ml/modeling/linear_model/bayesian_ridge.py,sha256=ibrYuablj-vfo82qLxX_CMye93HcI3N1CGxJXscLI2Y,52401
-snowflake/ml/modeling/linear_model/elastic_net.py,sha256=DYLFUV5om3RAGM9ksDQXUKsilxZtjm03a5fyW55VQqg,53285
-snowflake/ml/modeling/linear_model/elastic_net_cv.py,sha256=lyioafMUyXvkYg7irIKNvNpM9O4EDnZaDZ-Ke7TNkDU,54543
-snowflake/ml/modeling/linear_model/gamma_regressor.py,sha256=GSfFfwvgD9_FnEm4kvSCeNqRz0opwS90cO8vFPkKz_A,52341
-snowflake/ml/modeling/linear_model/huber_regressor.py,sha256=bcFYgj993IaUgcJLFhglTK2yI7z5b9fMu2d4H2Gd4ag,51529
-snowflake/ml/modeling/linear_model/lars.py,sha256=mtwe3OdWTqhHOfRrjfNABKz4UMUuDOJQ-dSlzF34q7Y,52826
-snowflake/ml/modeling/linear_model/lars_cv.py,sha256=SRwQj4xDW0iOzxeitBEcvN80LIhnpVfcCmb5DbMNLH0,53033
-snowflake/ml/modeling/linear_model/lasso.py,sha256=3iLGhBze1F7t-BfN5-Vylr9hkVDAaLac-_vuscKd2Ys,52925
-snowflake/ml/modeling/linear_model/lasso_cv.py,sha256=bx1ujpMUGVpqKiXQCmdA9Zn8jOExB55yArAI6TCq82Y,53700
-snowflake/ml/modeling/linear_model/lasso_lars.py,sha256=lVZrQFB1Aglg2knrP85nPsohPPy-KFNMoU9gf7ttnMw,53929
-snowflake/ml/modeling/linear_model/lasso_lars_cv.py,sha256=FRRH0rb-MeC2SCuzvbh8vNdVP9_LDxzv9ROW2D6Xu3U,53875
-snowflake/ml/modeling/linear_model/lasso_lars_ic.py,sha256=bpzkYt94SMLgdlQXc92ecGF-j2y2LjlyKcZYUMqsNX8,53220
-snowflake/ml/modeling/linear_model/linear_regression.py,sha256=fPJuWbciS2kVXduXxGcuEdyiV2_2QZaHpdp7EYijtg8,51055
-snowflake/ml/modeling/linear_model/logistic_regression.py,sha256=zxGgjDlyEzMoffE4L-55CZXa7BB3wnRKg9thAdg2snQ,57306
-snowflake/ml/modeling/linear_model/logistic_regression_cv.py,sha256=nDyKdYdCRUQ_9hgVTvSPmy3ABUEaOgcuv4jbNhj-GNw,58326
-snowflake/ml/modeling/linear_model/multi_task_elastic_net.py,sha256=CGOn8vgTrLABtCCyGbndbCNUVzub-tlup6OZ_Il22qc,52511
-snowflake/ml/modeling/linear_model/multi_task_elastic_net_cv.py,sha256=yow9Tmcfgl1uVBcDbJ44nbu2RKP0hWIrgW3fmYvvgF0,54139
-snowflake/ml/modeling/linear_model/multi_task_lasso.py,sha256=onLqMztO07DHdhweQ0Lr3huLRLDblFzMjYH1h_679tI,52093
-snowflake/ml/modeling/linear_model/multi_task_lasso_cv.py,sha256=B-kNPTuK9OcNVsg39taYRJiHchzE4rzkJLYAQiCVfEQ,53345
-snowflake/ml/modeling/linear_model/orthogonal_matching_pursuit.py,sha256=iDBwlSFe3A3rTnYCoMhQtPDt1luK9oZ2kXQjeE8ZrXY,51620
-snowflake/ml/modeling/linear_model/passive_aggressive_classifier.py,sha256=ZVRUYCkTqnyxVh5I28Lxir-wbillRSZErEfuk0zCl38,54971
-snowflake/ml/modeling/linear_model/passive_aggressive_regressor.py,sha256=77JuwJf8YLQXhbMOkBozT1cZgpjklzxQ7LG11P3esA4,54046
-snowflake/ml/modeling/linear_model/perceptron.py,sha256=npzkWzJhgzBvl6YDCu3ouxRhcH8NM9mY3D6WmvTfS8E,54476
-snowflake/ml/modeling/linear_model/poisson_regressor.py,sha256=G_9YM6RWmGptMNvOsz1yaU_MoyjYLiPLyuWDmFOHnn0,52372
-snowflake/ml/modeling/linear_model/ransac_regressor.py,sha256=Eooggs-DyzKWikkF_-oJrkTJVqBGbcTnEGuIGQHMhPc,55846
-snowflake/ml/modeling/linear_model/ridge.py,sha256=JeTgVVpjBJdoaAFIoSMCc1dp1t0ylPCdKUZuwt21AIA,53906
-snowflake/ml/modeling/linear_model/ridge_classifier.py,sha256=RVvzPvAiuM0-4neplIU9O6zP-ORDF3NfMGilwbFprGc,54224
-snowflake/ml/modeling/linear_model/ridge_classifier_cv.py,sha256=H_kE-ZRnaqhTVbIYdqSX2MB-K4tGStNjll-FuruM9Sc,52763
-snowflake/ml/modeling/linear_model/ridge_cv.py,sha256=IgH-7EtxjFfTI7Wih80NIk9EdZTug5nWvTQUBxe4XCY,53539
-snowflake/ml/modeling/linear_model/sgd_classifier.py,sha256=Fx68BRRQ7nXhEi7UCd1osFvtebWs0ZsY90wI9G2jKI0,59892
-snowflake/ml/modeling/linear_model/sgd_one_class_svm.py,sha256=URNcQGuY6xEazEbL5SxOLE-msOVHR1HwAHvu82ylJQk,54506
-snowflake/ml/modeling/linear_model/sgd_regressor.py,sha256=DJqvXt6gYgvlWn4i2ilrlT7N4GPdEiWbprOhOH6T3fo,57361
-snowflake/ml/modeling/linear_model/theil_sen_regressor.py,sha256=jSoeUvjm-Iih83byRAP6Non9_d7HiiOgSi-fjuD8lRI,52794
-snowflake/ml/modeling/linear_model/tweedie_regressor.py,sha256=FH1yc3D1_dZ-NeVXtoDhQZDL9fC47nz9Qt9eDzqT4ok,53765
+snowflake/ml/modeling/linear_model/ard_regression.py,sha256=qBLYHo6dWB-czD-VA3TRFviFtwEhWKrK7HNXpAVORb4,53454
+snowflake/ml/modeling/linear_model/bayesian_ridge.py,sha256=UYruGveqQzh2l4xQ4xprSmrj4WVzhj-7wVUxe0VfXAk,53767
+snowflake/ml/modeling/linear_model/elastic_net.py,sha256=qZuAJ19QTxCL1k69Wq8-tE99XLDp_0iaC-x--0oHkUg,54651
+snowflake/ml/modeling/linear_model/elastic_net_cv.py,sha256=gg1Mdg2C7zT7Fwu9slZUfk-fRQuuPw5S2N084GrQswE,55909
+snowflake/ml/modeling/linear_model/gamma_regressor.py,sha256=-PsCJ8RskWxaW3Ep4SGGCMOiH9QxIFosSLzNA5p_SGE,53707
+snowflake/ml/modeling/linear_model/huber_regressor.py,sha256=yA-5V0xJxQL4DNZk_UZY_I-YbU8_B-U00L-TnkdxxQA,52895
+snowflake/ml/modeling/linear_model/lars.py,sha256=XCqIF1K5fW4Z5e4lbFpyVdZ16DKP5ctI0iabtrJmqcQ,54192
+snowflake/ml/modeling/linear_model/lars_cv.py,sha256=mGY3qwPKXTrgsaOokKZTxLFH4JDHC9uqbXhose8tvWw,54399
+snowflake/ml/modeling/linear_model/lasso.py,sha256=DhbxwaaEjLHVB27WO4Do39sH8XJo8tjqo9JCITj2-rg,54291
+snowflake/ml/modeling/linear_model/lasso_cv.py,sha256=-9OTnRXuF7tR7pmGiBkbNYXtdpdBKQZdOHQoxGLzFsc,55066
+snowflake/ml/modeling/linear_model/lasso_lars.py,sha256=lUX3Waet6aBpZ2rM4mxFtbbHZLROOdhpJ7c4loWxIJE,55295
+snowflake/ml/modeling/linear_model/lasso_lars_cv.py,sha256=Yv4-ubZ0VFunMkFz5Ha1yD-6Q3qEWY6p7MkHskHW8x4,55241
+snowflake/ml/modeling/linear_model/lasso_lars_ic.py,sha256=0mPTAy4_AtbQ9CRZdhaHL9Ie6I0BCmUYb79kO6i4rSU,54586
+snowflake/ml/modeling/linear_model/linear_regression.py,sha256=e6O-N3DqbR64l2D4suZEp2YePe5bhtjqwg-7EY8IGdM,52421
+snowflake/ml/modeling/linear_model/logistic_regression.py,sha256=DTIK8OneFS4HJEMUzPvwuYTY_VUR7FQ_Z5VrURiwBKI,58672
+snowflake/ml/modeling/linear_model/logistic_regression_cv.py,sha256=9s8x5-7EBl2DgD_6ShPsSN3jAGn8Qg3BEWakZ3ekYuk,59692
+snowflake/ml/modeling/linear_model/multi_task_elastic_net.py,sha256=jgjH31l0Iq7d30Rig0KvaOt44vFlxZM_15eAv2vtkVw,53877
+snowflake/ml/modeling/linear_model/multi_task_elastic_net_cv.py,sha256=h0L934Dc-ucQKCfI0Hg-7sLwYjPs1MzuKwqnZ6UtQ-s,55505
+snowflake/ml/modeling/linear_model/multi_task_lasso.py,sha256=Lq4eLjl0y75uTsYxu7zyse_uYqIoe2rJPlqJsgbaEbE,53459
+snowflake/ml/modeling/linear_model/multi_task_lasso_cv.py,sha256=JrRbi_-a6UEJGPWTbcgWcttTdRZ2d2qddYm1fifL4is,54711
+snowflake/ml/modeling/linear_model/orthogonal_matching_pursuit.py,sha256=E1e9IaO9GMPZbbcjv_jlyjlu8JyulPusX-2R-hgTIag,52986
+snowflake/ml/modeling/linear_model/passive_aggressive_classifier.py,sha256=RCfEXkTltDsy55UWXWdmpqTNdTnw4vJ42Ti0dLQUS8g,56337
+snowflake/ml/modeling/linear_model/passive_aggressive_regressor.py,sha256=3JXfzZbYSJlhAGN6vHQRUq7Uy1RlnHnYXVWSSe4xv3w,55412
+snowflake/ml/modeling/linear_model/perceptron.py,sha256=PFsyxUd6SVOA40rZQ5aVjmb8OD4Y1fYLzn5hRDj7dEc,55842
+snowflake/ml/modeling/linear_model/poisson_regressor.py,sha256=rpbURsXkCkh4WDLJXcg3YtV1PqtaBD5usOwS_-1NCSo,53738
+snowflake/ml/modeling/linear_model/ransac_regressor.py,sha256=JiWyvAg-LKL6qTUF0M9NChND49yQypt3lpCfaHn5etw,57212
+snowflake/ml/modeling/linear_model/ridge.py,sha256=EUTdq5QniCZDJVNqU7o9Ujxl4Mj_lz_9qmzxBkdN0t4,55272
+snowflake/ml/modeling/linear_model/ridge_classifier.py,sha256=4PwcLr-Rt1vW2zu9dwkJLHc_tZUjQMf7Hfnh6sKb6aw,55590
+snowflake/ml/modeling/linear_model/ridge_classifier_cv.py,sha256=a6Xh90A3ZOD47Ls6tQheaegrUaaq3wrbc1pSW4GHq8w,54129
+snowflake/ml/modeling/linear_model/ridge_cv.py,sha256=0nN19Drn4j62cpHikVxuFvrFYUhhhisUAiwdLFEDGWw,54905
+snowflake/ml/modeling/linear_model/sgd_classifier.py,sha256=aivpS5qfKnUX974mRaCHhckk_hWO0ROMypiBzEK3Muw,61258
+snowflake/ml/modeling/linear_model/sgd_one_class_svm.py,sha256=XSfwAJf7AlNFsyLn2iYHF--gnlYaGgrYQz2WDc1hxxs,55872
+snowflake/ml/modeling/linear_model/sgd_regressor.py,sha256=2_5EwFzgQvn8sri8k5s7TlmhQ2GzoGwvCWQRd5573GY,58727
+snowflake/ml/modeling/linear_model/theil_sen_regressor.py,sha256=TBDTE2nPajqqYnnX7HOnB84Fgnz3qjP1yUx0gAgAXKM,54160
+snowflake/ml/modeling/linear_model/tweedie_regressor.py,sha256=mX9qdhARD3RKBNF54lu3F8wPzmQkqH5xooUhUYELx94,55131
 snowflake/ml/modeling/manifold/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/manifold/isomap.py,sha256=Y9I0ZBAtgF2wdChAday86N4FqSnACAlVBZ8zUuy55XM,53160
-snowflake/ml/modeling/manifold/mds.py,sha256=8PxHji0zRLZEzKm6crf7zhPe0lHy0PPx6b4a4wVsBFU,52378
-snowflake/ml/modeling/manifold/spectral_embedding.py,sha256=aJSl5NC_ygakrOhsT_-dlKxd2ELuOLASJ2suwaf0kdU,53149
-snowflake/ml/modeling/manifold/tsne.py,sha256=gwEXvNWqzPxBLX5ID9BVHJylNE35nMmRlM8a1Q4iToE,56421
+snowflake/ml/modeling/manifold/isomap.py,sha256=9fiVKwZ8G09fvKJ2tgs_oU9wgzGRdHR1lGO6nDMlO68,54526
+snowflake/ml/modeling/manifold/mds.py,sha256=tyaIhilgwzt5JE9s5X-PdUjlouNvHhjrvkkWAtSQJ-w,53744
+snowflake/ml/modeling/manifold/spectral_embedding.py,sha256=PBed_JavNtd1a14jIxjTfbOa1QgF6s7UfAL8_PgoDrk,54515
+snowflake/ml/modeling/manifold/tsne.py,sha256=m_JKk_XYmDJJrlnypDyCWDjjTqf7hOcXjx__XAmjJ5s,57787
 snowflake/ml/modeling/metrics/__init__.py,sha256=wp2LehkoLtyt4u_HBhglrKrV6E-dKt5vr-0N3MkJFaY,304
 snowflake/ml/modeling/metrics/classification.py,sha256=ZtTQ3ziMMglimNW1hG7oGDhAW5a6HBXOfQq8g3iptC8,40077
 snowflake/ml/modeling/metrics/correlation.py,sha256=4cjKDl07C3PGcx_VPwOqSFYjuBEA266btKuw9wd5D7w,4921
 snowflake/ml/modeling/metrics/covariance.py,sha256=hS_yILgo3OUjBVrPCL-NXR7cSyPjXOFftXlZJ1xaLus,4757
 snowflake/ml/modeling/metrics/metrics_utils.py,sha256=jvjOabIwGi02I1aEiSo_3NfgXLAIU7ggShQXDAAjCFs,12037
 snowflake/ml/modeling/metrics/ranking.py,sha256=KzRbI1bZf3G1U3wlSnvpX1GMTkddfGwy9y2gopxoW6E,15397
 snowflake/ml/modeling/metrics/regression.py,sha256=yqTiBnbFc1GtBR4LJfUiEGE8Pv3uNT2ZuFiaEyzxyhM,23144
 snowflake/ml/modeling/mixture/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/mixture/bayesian_gaussian_mixture.py,sha256=Q9-C2JHJtOFuI70qqMe4jXpL3xqZakD90d9gsdTMDzE,57065
-snowflake/ml/modeling/mixture/gaussian_mixture.py,sha256=LkqMA3cxhIoHnbJeYbcECNUIbuxTLZnfkFQ_08IaQoY,55067
+snowflake/ml/modeling/mixture/bayesian_gaussian_mixture.py,sha256=-n1RTh-9K8irI5Z2ROti8EpyeMfYqHufr2XTcEaoJqk,58431
+snowflake/ml/modeling/mixture/gaussian_mixture.py,sha256=kMzOcefnjIYh0FX-HLpbfAuax6IroF50bAMhpk5h964,56433
 snowflake/ml/modeling/model_selection/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/model_selection/grid_search_cv.py,sha256=1LuLtTFpd0Qeuv91xsMvY3uO2Dj8ePEYH5NjWc4id-M,57616
-snowflake/ml/modeling/model_selection/randomized_search_cv.py,sha256=iLacSg4kt-NyJE4pRFX4F5nW8iZlRieXQhsgcrmUAjs,58460
+snowflake/ml/modeling/model_selection/grid_search_cv.py,sha256=KohZGv0rroFdiEqg5WLFQJo5C-fprC9G1K49V0sYYEo,58982
+snowflake/ml/modeling/model_selection/randomized_search_cv.py,sha256=FZS-Fc7HWNKFZvgv4H3pNiezeAAk1Kgh8mbiMZ9u0K4,59826
 snowflake/ml/modeling/multiclass/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/multiclass/one_vs_one_classifier.py,sha256=m49bTRLKzJHPj3_4hmPEdWPeHk_JADgnmdxTJz2t4Gs,51048
-snowflake/ml/modeling/multiclass/one_vs_rest_classifier.py,sha256=c8uxnEPim_ez3Fe3pAMqdy9sHUJGBpm4iPvZS7yi2xQ,51976
-snowflake/ml/modeling/multiclass/output_code_classifier.py,sha256=jUc8xYU1rW3T21IJDITK8Ju04ZA702CyMEb7Yhj5MtY,51306
+snowflake/ml/modeling/multiclass/one_vs_one_classifier.py,sha256=uTckORyZu2fXX-uBz2STZBFgoqKqH5MaDET6aDxy98U,52414
+snowflake/ml/modeling/multiclass/one_vs_rest_classifier.py,sha256=zUe8KBBhZyFZHSd9dVVrmanErHSW045iZTaxcvMxeJ0,53342
+snowflake/ml/modeling/multiclass/output_code_classifier.py,sha256=G03cM3PD0fdQpjF2gZnKuMktxRg3ikopCPnXurE8ylA,52672
 snowflake/ml/modeling/naive_bayes/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/naive_bayes/bernoulli_nb.py,sha256=r-jo5zsBrHEJTJF_yqxVMTkcJ_UyHt03Vl5Jg9pdAwY,51633
-snowflake/ml/modeling/naive_bayes/categorical_nb.py,sha256=rbzfzn1CbhWP0NUb_HuuJPqDXoGOh0q-U6pv1bQXJTQ,51954
-snowflake/ml/modeling/naive_bayes/complement_nb.py,sha256=sKFvVuVtZ5YkM9sn9hhSiJYPhu1XgfIpFlWCUZL6keo,51641
-snowflake/ml/modeling/naive_bayes/gaussian_nb.py,sha256=IJ_cLm2e1j9fIIJEuEqs73ErG5bTQadTyoXRPQdLSiA,50781
-snowflake/ml/modeling/naive_bayes/multinomial_nb.py,sha256=nw33nrEnVLho0AnOaP2X2wxQ9Sf7OFgzec1KIY153do,51398
+snowflake/ml/modeling/naive_bayes/bernoulli_nb.py,sha256=3vOEAj7kDxSNRRIfgs1Lq7OZHTXbmHPMZa8RNaDdYxg,52999
+snowflake/ml/modeling/naive_bayes/categorical_nb.py,sha256=WK3m0zx7_C8c1peHvC49F4YRr8EHfEMGJYzQEqBhOLY,53320
+snowflake/ml/modeling/naive_bayes/complement_nb.py,sha256=iFR2xR9klHkUJ0cdHHcGWtfrlw3R-2NuSf3piYa8ar4,53007
+snowflake/ml/modeling/naive_bayes/gaussian_nb.py,sha256=AFEdV5VNyqZm_YV71KAEP5F1uOPXHZ8ghRUPWCvKMrI,52147
+snowflake/ml/modeling/naive_bayes/multinomial_nb.py,sha256=xiZwZACAA5tHOK7DWhq4pCWWNKhp91zGKxDs5k7pa2I,52764
 snowflake/ml/modeling/neighbors/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/neighbors/k_neighbors_classifier.py,sha256=7slt_NxYYctTAtwfieDxAYnYl9ckZs4Q1UOizvdEBaE,54185
-snowflake/ml/modeling/neighbors/k_neighbors_regressor.py,sha256=17ofYkVFAIgbvXGMpNDg7ygvdBMXtxIDNp6QeJygdTI,53667
-snowflake/ml/modeling/neighbors/kernel_density.py,sha256=g1reFsR1aWBrsbSCzwGzifS4rCqKJY3H5MqRbnqjp9s,52144
-snowflake/ml/modeling/neighbors/local_outlier_factor.py,sha256=DhTrzk-WnYYXWCov8H-Bn3g8D-FoxvFkH7O_XbHHOwA,54425
-snowflake/ml/modeling/neighbors/nearest_centroid.py,sha256=0PAR1wLl8qwFTT4x44FLqOA5pj1x4twKe1ca0wkMkGQ,50948
-snowflake/ml/modeling/neighbors/nearest_neighbors.py,sha256=qTNrjb-vPzYQDs-CDhGlRPLe_bYhrJ8EfQEc2RaT96k,52857
-snowflake/ml/modeling/neighbors/neighborhood_components_analysis.py,sha256=b5aaw6oF-754dVsGkizkJHgtZCetKuDxb118ke8KzQQ,54333
-snowflake/ml/modeling/neighbors/radius_neighbors_classifier.py,sha256=j5mHbaSuJqLVfR6vdNmGWXut1UNAh5IxSgiO4HgVvGY,54814
-snowflake/ml/modeling/neighbors/radius_neighbors_regressor.py,sha256=GOxeGcAVyAWCRgxJvRO25guzDin5-tryWV2vuABel0U,53700
+snowflake/ml/modeling/neighbors/k_neighbors_classifier.py,sha256=N1Qb11dM2lAuEyP1RsXHZC9EZbVG5EaKUIy2gMTvQio,55551
+snowflake/ml/modeling/neighbors/k_neighbors_regressor.py,sha256=z-F3iOELgLHznQWpBpRb8NyQ_jflxJUL4DSLgAeyX1g,55033
+snowflake/ml/modeling/neighbors/kernel_density.py,sha256=kbJ3bfchkxqrtm_lodMFrrNBi2nUWPHWy7wJywonWh0,53510
+snowflake/ml/modeling/neighbors/local_outlier_factor.py,sha256=ej39Z2fyioeFjUsf5rfT093VUpmUbSS2kdSx3SSvMuY,55791
+snowflake/ml/modeling/neighbors/nearest_centroid.py,sha256=Soafun6CXIHSNW-v2w6XyEpxuPdGSEoQ2lwRXbzr3Ck,52314
+snowflake/ml/modeling/neighbors/nearest_neighbors.py,sha256=COicFuUJ2xgt6Cia4mrFLnTm4aRVmgQtbDl1Xh9RptI,54223
+snowflake/ml/modeling/neighbors/neighborhood_components_analysis.py,sha256=wDzqewngg8lGhRz5WaFgnimpjRo2arDRghU9_ajJCbg,55699
+snowflake/ml/modeling/neighbors/radius_neighbors_classifier.py,sha256=WhSbpXCjYY8IVh5FNbmBfBO5354gf5NaBYjctrTWG4g,56180
+snowflake/ml/modeling/neighbors/radius_neighbors_regressor.py,sha256=q8H7AdBOjsfxj0YHtyNw9Jjn_-zsxCS0L-7F9HEF2fY,55066
 snowflake/ml/modeling/neural_network/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/neural_network/bernoulli_rbm.py,sha256=joVaKo1byI-5MRmb0MarM7JBFn6Hg4jyqd37nu06Zhg,51349
-snowflake/ml/modeling/neural_network/mlp_classifier.py,sha256=ToJzxQly6m2g_WNHxXq0jJdM6tupA9w4m-L0Gvf6GW0,58847
-snowflake/ml/modeling/neural_network/mlp_regressor.py,sha256=Xi_YG8Ce0vVkIsHN1Zk82txQKDG-iHYFi-D2d_20J-U,58124
+snowflake/ml/modeling/neural_network/bernoulli_rbm.py,sha256=cs0JZu0r9M1HKwCJR7h85K-BW99ow0wxC2cTv7aeW2o,52715
+snowflake/ml/modeling/neural_network/mlp_classifier.py,sha256=crxBNA4nGPBFjJqTjHco87I9yKt-ztDqm6HhBjwj2z4,60213
+snowflake/ml/modeling/neural_network/mlp_regressor.py,sha256=_tnYTJs1zrMaVmjYngVEjI2m2U2iw0kCqfyOZrgnrZA,59490
 snowflake/ml/modeling/pipeline/__init__.py,sha256=dYtqk_GD_hAAZjGfH1maWlZQ30h4hu_KGaf-_y9_AD8,298
 snowflake/ml/modeling/pipeline/pipeline.py,sha256=kIvKahAyF7zQoT8eYVm9dJPafYLybGZ8ELaxrBIkQ34,23381
 snowflake/ml/modeling/preprocessing/__init__.py,sha256=dYtqk_GD_hAAZjGfH1maWlZQ30h4hu_KGaf-_y9_AD8,298
 snowflake/ml/modeling/preprocessing/binarizer.py,sha256=IoGdiZwqsLYRSkifmxzfCqCeOy5ir5Gq_ls_gsPu54I,6092
 snowflake/ml/modeling/preprocessing/k_bins_discretizer.py,sha256=upW9qxntwE0vZ8foc2J3BlVdKy61M7JBspZkKqAyKW0,20422
 snowflake/ml/modeling/preprocessing/label_encoder.py,sha256=r3S_-G5OIqjeBttyIicSar_4FNO68MOvRSyAi_6gzeA,6285
 snowflake/ml/modeling/preprocessing/max_abs_scaler.py,sha256=O2dXkX6PPJZaVbS7jIpC4DOfqUt85YFaDA-rLXz6pEc,8491
 snowflake/ml/modeling/preprocessing/min_max_scaler.py,sha256=1LDaOp-OJU-79B36ZxBhAMQe5AXDEU5f71PNVXwtLXU,10716
 snowflake/ml/modeling/preprocessing/normalizer.py,sha256=0pbgiOGqwC4Pv9MKnYfo_0vIUmBdyLFoPSd_Sr7Og4U,5951
 snowflake/ml/modeling/preprocessing/one_hot_encoder.py,sha256=ubZCjUhPdkqn_w4nuIpgozawjcV3HvnkqiKMYqo3ljA,66998
-snowflake/ml/modeling/preprocessing/ordinal_encoder.py,sha256=uryEQmMp45tHuuHI7k-D4CY9JCkFYJUuP6hWZcODoAQ,27848
-snowflake/ml/modeling/preprocessing/polynomial_features.py,sha256=aD_FCZlwUnGtAM5eexF8eGUqcOw6pC9TrSv_qadJ-n8,51483
+snowflake/ml/modeling/preprocessing/ordinal_encoder.py,sha256=HMJKJ6D-uGVWy3GWNGFBXOf98AuG_HzMgh0eRRNkuuw,27956
+snowflake/ml/modeling/preprocessing/polynomial_features.py,sha256=6qFvX4rjQh3C5iPwzY-fo0CYsBxBmHIHzFoPRqkcH0w,52849
 snowflake/ml/modeling/preprocessing/robust_scaler.py,sha256=JGgkPZfgezS4X8YECSjeWDQIoLbU98j43qbwqP2RzZE,11981
 snowflake/ml/modeling/preprocessing/standard_scaler.py,sha256=hu2VnATyizCz-QKv7aaGdATeU8Fyug8MeNxau3-CllQ,10672
 snowflake/ml/modeling/semi_supervised/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/semi_supervised/label_propagation.py,sha256=06dQseRmK25t9TjC06hHtG0Yx-w1QAgdg-iqofaR6FA,51820
-snowflake/ml/modeling/semi_supervised/label_spreading.py,sha256=4Y8eBB-lX8Jqo6aHSCDiAr6I392OtVknE0dis3qEtFQ,52184
+snowflake/ml/modeling/semi_supervised/label_propagation.py,sha256=mTboTfsYQEZJ0BCuhJLOwNRo2TH2jsI1TuZHacKnj84,53186
+snowflake/ml/modeling/semi_supervised/label_spreading.py,sha256=53caZidl9h0Vhay3jdJLscp1JAy4hNiqUvUdnrkMR5U,53550
 snowflake/ml/modeling/svm/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/svm/linear_svc.py,sha256=LLj2LrHZjcFf4A8V0MYLrQz9iWRkxE58XKoaSwMzyA0,54362
-snowflake/ml/modeling/svm/linear_svr.py,sha256=9YPXR-D_nsv8FJaZBuUACgqofeHuaQCJVLl4gKbH6V8,52777
-snowflake/ml/modeling/svm/nu_svc.py,sha256=2IlDpoENw5Q6OtKC7lK2ZLrHwVZBoSYfuF2eal18HbE,55076
-snowflake/ml/modeling/svm/nu_svr.py,sha256=zLfLSZATC_IikRI5lh3ySu_o0Rq5NmJCUeEB_0TcY-I,52152
-snowflake/ml/modeling/svm/svc.py,sha256=0ptSWOd54T5A8tO_3X8cIpthu-cDWELcZlGh4fiAVYI,55239
-snowflake/ml/modeling/svm/svr.py,sha256=XDymWoig3szXbPIiYyCs0sM-Q7lcCcZaD6f9-CE4lrg,52355
+snowflake/ml/modeling/svm/linear_svc.py,sha256=7jHsoPhMevfk_yff8PbfHlQIxPKL67Qnbr40Y_Qkzws,55728
+snowflake/ml/modeling/svm/linear_svr.py,sha256=24rFzxyckshDieb7YjkTvB1TfXkTDe28FUtPpOJ0j1Q,54143
+snowflake/ml/modeling/svm/nu_svc.py,sha256=7eQjwN5mrE86sqteDaI0VW0I28TfFjczxQ4itl85JB8,56442
+snowflake/ml/modeling/svm/nu_svr.py,sha256=3PNHsbgD6jPEzvrQKhwGvf2tDXPhXr9bcinC_l3Moig,53518
+snowflake/ml/modeling/svm/svc.py,sha256=JT99xjA5Kk9jcqfKRxMJOODxLmmgOlIoahLkJsdzUlU,56605
+snowflake/ml/modeling/svm/svr.py,sha256=6-iB8pZirEXlbYRe4UaQBWtRqL9Fp1tC9p9LKTNyrxg,53721
 snowflake/ml/modeling/tree/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/tree/decision_tree_classifier.py,sha256=Rej8fjFzDE4osX0waJdHjsw0DZ3Z6yg3VcfmdsA-V4Y,57438
-snowflake/ml/modeling/tree/decision_tree_regressor.py,sha256=kylQASQOkEtMWgGaTMO6vUqTImgdBhTEC5EbfZYl5i4,56134
-snowflake/ml/modeling/tree/extra_tree_classifier.py,sha256=aWV05GlwO653ndAmn3AtBmQA1R3KeVFjL3l2a8L2srQ,56801
-snowflake/ml/modeling/tree/extra_tree_regressor.py,sha256=zTo1vpjNvj65GiWCOPCv7iH93Jj8p66QPUZ0CMP0qN8,55506
+snowflake/ml/modeling/tree/decision_tree_classifier.py,sha256=kTzXtuyn6PBeYRwy_FzCcE_dBrFuk6NMFclXDcE5PmM,58804
+snowflake/ml/modeling/tree/decision_tree_regressor.py,sha256=ymKoxcTqnll58AmfpC6L0EOnLBjl0PWM4_QBzxHb-2w,57500
+snowflake/ml/modeling/tree/extra_tree_classifier.py,sha256=eXIPdPA-vyDxNPl_VzD8Jj0qE4xfhV-zAc5beMs-a8g,58167
+snowflake/ml/modeling/tree/extra_tree_regressor.py,sha256=HFNGqy5VOOSDOiR0bR7fkbWVTrD96DeQYIC7KaU0E-Y,56872
 snowflake/ml/modeling/xgboost/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/xgboost/xgb_classifier.py,sha256=KqK_zKRnWoTWqW82Un7CYGEgbHsloRwwOUbvhOk9SJQ,61208
-snowflake/ml/modeling/xgboost/xgb_regressor.py,sha256=4BrG1tjYvMk3wX3aK47PDiTmeMrCSB60Ke4BWud3vuk,60714
-snowflake/ml/modeling/xgboost/xgbrf_classifier.py,sha256=BQel82uG1f3o1XoODm6TIMQD3zRYSuXoD2G6oGSlX_A,61372
-snowflake/ml/modeling/xgboost/xgbrf_regressor.py,sha256=3EYUqQgFSFkiWRxmIYAw-Cav2CJl2pEejChVqAGgLDY,60905
+snowflake/ml/modeling/xgboost/xgb_classifier.py,sha256=LAhLngDv_uxgrzWCOozUBH0vVbPhASyayzsWGME7taU,62574
+snowflake/ml/modeling/xgboost/xgb_regressor.py,sha256=_pGLNmfTljuUCqahuGHI_U6x9xuWrgYSdwvwjFaLxPo,62080
+snowflake/ml/modeling/xgboost/xgbrf_classifier.py,sha256=ZQb5dHrKGvxg8cleZh9Jh64qPVCYbU6-ZKve9Zrytt8,62738
+snowflake/ml/modeling/xgboost/xgbrf_regressor.py,sha256=q1iAct-f1e-KFzukPlrQnu5fk-MCTCD-ava-Z2MrhNU,62271
 snowflake/ml/registry/_schema.py,sha256=7NezDozAqdbOjB9dYHSQQpxapSTKuXqnGrl394bDohc,1381
-snowflake/ml/registry/model_registry.py,sha256=jkeGWntSSKYRQZqoIX3_qAbAveMwoJhYzF_jgug3bxA,84697
+snowflake/ml/registry/model_registry.py,sha256=qoPhHaA_fbF-6xTT2jZiYbekaeCrd2xtDsAQl4fFR7U,85709
 snowflake/ml/utils/connection_params.py,sha256=W_MwEw1xUARgrDehP_Kz5dmqt1sBXct80xQ7N56qFCc,6138
 snowflake/ml/utils/sparse.py,sha256=1mI2lOm-nMQEwNfbDtHpkJ4SDkKKqsRFyGwSQJJZAiE,3893
-snowflake/ml/version.py,sha256=_MuTm0ZX7Fno8rwBkHygvwK4Mr2oy6nYbO-KOyECohQ,16
-snowflake_ml_python-1.0.2.dist-info/METADATA,sha256=5k_a2EQPGGkZZNUpSFLXSJ_WCEddktODr-aYUhz6gCc,11756
-snowflake_ml_python-1.0.2.dist-info/RECORD,,
-snowflake_ml_python-1.0.2.dist-info/WHEEL,sha256=sobxWSyDDkdg_rinUth-jxhXHqoNqlmNMJY3aTZn2Us,91
+snowflake/ml/version.py,sha256=YNTJssg_NQnP_MEsPYW475Gzld8FPxD_C1H5dD4ItLQ,16
+snowflake_ml_python-1.0.3.dist-info/METADATA,sha256=7J7r_mqwjnPPoFgxubEvGmd2yKTsTvsJlYFIm42Rcp4,13340
+snowflake_ml_python-1.0.3.dist-info/RECORD,,
+snowflake_ml_python-1.0.3.dist-info/WHEEL,sha256=sobxWSyDDkdg_rinUth-jxhXHqoNqlmNMJY3aTZn2Us,91
```

