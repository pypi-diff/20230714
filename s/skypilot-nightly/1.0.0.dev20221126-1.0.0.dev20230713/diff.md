# Comparing `tmp/skypilot-nightly-1.0.0.dev20221126.tar.gz` & `tmp/skypilot-nightly-1.0.0.dev20230713.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "skypilot-nightly-1.0.0.dev20221126.tar", last modified: Sat Nov 26 05:33:21 2022, max compression
+gzip compressed data, was "skypilot-nightly-1.0.0.dev20230713.tar", last modified: Thu Jul 13 16:17:35 2023, max compression
```

## Comparing `skypilot-nightly-1.0.0.dev20221126.tar` & `skypilot-nightly-1.0.0.dev20230713.tar`

### file list

```diff
@@ -1,157 +1,223 @@
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-11-26 05:33:21.762757 skypilot-nightly-1.0.0.dev20221126/
--rw-r--r--   0 runner    (1001) docker     (122)    12170 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/LICENSE
--rw-r--r--   0 runner    (1001) docker     (122)      353 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/MANIFEST.in
--rw-r--r--   0 runner    (1001) docker     (122)     7447 2022-11-26 05:33:21.762757 skypilot-nightly-1.0.0.dev20221126/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (122)     6531 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/README.md
--rw-r--r--   0 runner    (1001) docker     (122)      333 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/pyproject.toml
--rw-r--r--   0 runner    (1001) docker     (122)       38 2022-11-26 05:33:21.762757 skypilot-nightly-1.0.0.dev20221126/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (122)     6186 2022-11-26 05:33:14.000000 skypilot-nightly-1.0.0.dev20221126/setup.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-11-26 05:33:21.746757 skypilot-nightly-1.0.0.dev20221126/sky/
--rw-r--r--   0 runner    (1001) docker     (122)     1937 2022-11-26 05:33:14.000000 skypilot-nightly-1.0.0.dev20221126/sky/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-11-26 05:33:21.750757 skypilot-nightly-1.0.0.dev20221126/sky/adaptors/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/adaptors/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1443 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/adaptors/aws.py
--rw-r--r--   0 runner    (1001) docker     (122)      767 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/adaptors/azure.py
--rw-r--r--   0 runner    (1001) docker     (122)      852 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/adaptors/docker.py
--rw-r--r--   0 runner    (1001) docker     (122)     1872 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/adaptors/gcp.py
--rw-r--r--   0 runner    (1001) docker     (122)    11726 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/authentication.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-11-26 05:33:21.750757 skypilot-nightly-1.0.0.dev20221126/sky/backends/
--rw-r--r--   0 runner    (1001) docker     (122)      256 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/backends/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     5226 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/backends/backend.py
--rw-r--r--   0 runner    (1001) docker     (122)    86760 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/backends/backend_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)   151095 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/backends/cloud_vm_ray_backend.py
--rw-r--r--   0 runner    (1001) docker     (122)     7972 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/backends/docker_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    15495 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/backends/local_docker_backend.py
--rw-r--r--   0 runner    (1001) docker     (122)    21008 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/backends/onprem_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     5903 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/backends/wheel_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-11-26 05:33:21.750757 skypilot-nightly-1.0.0.dev20221126/sky/benchmark/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/benchmark/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     8723 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/benchmark/benchmark_state.py
--rw-r--r--   0 runner    (1001) docker     (122)    24479 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/benchmark/benchmark_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     2158 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/check.py
--rw-r--r--   0 runner    (1001) docker     (122)   134379 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/cli.py
--rw-r--r--   0 runner    (1001) docker     (122)     6082 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/cloud_stores.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-11-26 05:33:21.750757 skypilot-nightly-1.0.0.dev20221126/sky/clouds/
--rw-r--r--   0 runner    (1001) docker     (122)      426 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/clouds/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    14252 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/clouds/aws.py
--rw-r--r--   0 runner    (1001) docker     (122)    13043 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/clouds/azure.py
--rw-r--r--   0 runner    (1001) docker     (122)     7388 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/clouds/cloud.py
--rw-r--r--   0 runner    (1001) docker     (122)    22002 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/clouds/gcp.py
--rw-r--r--   0 runner    (1001) docker     (122)     6043 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/clouds/local.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-11-26 05:33:21.750757 skypilot-nightly-1.0.0.dev20221126/sky/clouds/service_catalog/
--rw-r--r--   0 runner    (1001) docker     (122)     9673 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/clouds/service_catalog/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3664 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/clouds/service_catalog/aws_catalog.py
--rw-r--r--   0 runner    (1001) docker     (122)     3245 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/clouds/service_catalog/azure_catalog.py
--rw-r--r--   0 runner    (1001) docker     (122)    13680 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/clouds/service_catalog/common.py
--rw-r--r--   0 runner    (1001) docker     (122)      282 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/clouds/service_catalog/constants.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-11-26 05:33:21.754757 skypilot-nightly-1.0.0.dev20221126/sky/clouds/service_catalog/data_fetchers/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/clouds/service_catalog/data_fetchers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    10732 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/clouds/service_catalog/data_fetchers/fetch_aws.py
--rw-r--r--   0 runner    (1001) docker     (122)     8399 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/clouds/service_catalog/data_fetchers/fetch_azure.py
--rw-r--r--   0 runner    (1001) docker     (122)    21521 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/clouds/service_catalog/data_fetchers/fetch_gcp.py
--rw-r--r--   0 runner    (1001) docker     (122)    16399 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/clouds/service_catalog/gcp_catalog.py
--rw-r--r--   0 runner    (1001) docker     (122)    30350 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/core.py
--rw-r--r--   0 runner    (1001) docker     (122)     2442 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/dag.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-11-26 05:33:21.754757 skypilot-nightly-1.0.0.dev20221126/sky/data/
--rw-r--r--   0 runner    (1001) docker     (122)      128 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/data/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     4870 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/data/data_transfer.py
--rw-r--r--   0 runner    (1001) docker     (122)     6980 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/data/data_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     2958 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/data/mounting_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    58154 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/data/storage.py
--rw-r--r--   0 runner    (1001) docker     (122)     1077 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/data/storage_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     3329 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/exceptions.py
--rw-r--r--   0 runner    (1001) docker     (122)    29657 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/execution.py
--rw-r--r--   0 runner    (1001) docker     (122)    13802 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/global_user_state.py
--rw-r--r--   0 runner    (1001) docker     (122)    37816 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/optimizer.py
--rw-r--r--   0 runner    (1001) docker     (122)     2281 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/registry.py
--rw-r--r--   0 runner    (1001) docker     (122)    29851 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/resources.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-11-26 05:33:21.754757 skypilot-nightly-1.0.0.dev20221126/sky/setup_files/
--rw-r--r--   0 runner    (1001) docker     (122)      353 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/setup_files/MANIFEST.in
--rw-r--r--   0 runner    (1001) docker     (122)     6186 2022-11-26 05:33:14.000000 skypilot-nightly-1.0.0.dev20221126/sky/setup_files/setup.py
--rw-r--r--   0 runner    (1001) docker     (122)     1256 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/sky_logging.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-11-26 05:33:21.754757 skypilot-nightly-1.0.0.dev20221126/sky/skylet/
--rw-r--r--   0 runner    (1001) docker     (122)    12568 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/skylet/LICENCE
--rw-r--r--   0 runner    (1001) docker     (122)        0 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/skylet/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     1932 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/skylet/autostop_lib.py
--rw-r--r--   0 runner    (1001) docker     (122)      751 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/skylet/configs.py
--rw-r--r--   0 runner    (1001) docker     (122)      408 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/skylet/constants.py
--rw-r--r--   0 runner    (1001) docker     (122)     5720 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/skylet/events.py
--rw-r--r--   0 runner    (1001) docker     (122)    25830 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/skylet/job_lib.py
--rw-r--r--   0 runner    (1001) docker     (122)    17641 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/skylet/log_lib.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-11-26 05:33:21.746757 skypilot-nightly-1.0.0.dev20221126/sky/skylet/providers/
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-11-26 05:33:21.754757 skypilot-nightly-1.0.0.dev20221126/sky/skylet/providers/aws/
--rw-r--r--   0 runner    (1001) docker     (122)       91 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/skylet/providers/aws/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-11-26 05:33:21.754757 skypilot-nightly-1.0.0.dev20221126/sky/skylet/providers/aws/cloudwatch/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/skylet/providers/aws/cloudwatch/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    31879 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/skylet/providers/aws/cloudwatch/cloudwatch_helper.py
--rw-r--r--   0 runner    (1001) docker     (122)    44115 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/skylet/providers/aws/config.py
--rw-r--r--   0 runner    (1001) docker     (122)    25702 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/skylet/providers/aws/node_provider.py
--rw-r--r--   0 runner    (1001) docker     (122)     5917 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/skylet/providers/aws/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-11-26 05:33:21.758757 skypilot-nightly-1.0.0.dev20221126/sky/skylet/providers/azure/
--rw-r--r--   0 runner    (1001) docker     (122)       97 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/skylet/providers/azure/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     3270 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/skylet/providers/azure/azure-config-template.json
--rw-r--r--   0 runner    (1001) docker     (122)    10157 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/skylet/providers/azure/azure-vm-template.json
--rw-r--r--   0 runner    (1001) docker     (122)     4371 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/skylet/providers/azure/config.py
--rw-r--r--   0 runner    (1001) docker     (122)    14471 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/skylet/providers/azure/node_provider.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-11-26 05:33:21.758757 skypilot-nightly-1.0.0.dev20221126/sky/skylet/providers/gcp/
--rw-r--r--   0 runner    (1001) docker     (122)       91 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/skylet/providers/gcp/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)    22429 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/skylet/providers/gcp/config.py
--rw-r--r--   0 runner    (1001) docker     (122)    25860 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/skylet/providers/gcp/node.py
--rw-r--r--   0 runner    (1001) docker     (122)    11309 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/skylet/providers/gcp/node_provider.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-11-26 05:33:21.758757 skypilot-nightly-1.0.0.dev20221126/sky/skylet/ray_patches/
--rw-r--r--   0 runner    (1001) docker     (122)     3467 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/skylet/ray_patches/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      294 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/skylet/ray_patches/autoscaler.py.patch
--rw-r--r--   0 runner    (1001) docker     (122)       86 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/skylet/ray_patches/azure_cli.py.patch
--rw-r--r--   0 runner    (1001) docker     (122)      391 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/skylet/ray_patches/cli.py.patch
--rw-r--r--   0 runner    (1001) docker     (122)      107 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/skylet/ray_patches/command_runner.py.patch
--rw-r--r--   0 runner    (1001) docker     (122)     1020 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/skylet/ray_patches/job_manager.py.patch
--rw-r--r--   0 runner    (1001) docker     (122)      512 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/skylet/ray_patches/log_monitor.py.patch
--rw-r--r--   0 runner    (1001) docker     (122)      658 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/skylet/ray_patches/resource_demand_scheduler.py.patch
--rw-r--r--   0 runner    (1001) docker     (122)      289 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/skylet/ray_patches/updater.py.patch
--rw-r--r--   0 runner    (1001) docker     (122)      549 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/skylet/ray_patches/worker.py.patch
--rw-r--r--   0 runner    (1001) docker     (122)      603 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/skylet/skylet.py
--rw-r--r--   0 runner    (1001) docker     (122)     2561 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/skylet/subprocess_daemon.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-11-26 05:33:21.758757 skypilot-nightly-1.0.0.dev20221126/sky/spot/
--rw-r--r--   0 runner    (1001) docker     (122)     1263 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/spot/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      561 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/spot/constants.py
--rw-r--r--   0 runner    (1001) docker     (122)    12129 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/spot/controller.py
--rw-r--r--   0 runner    (1001) docker     (122)    13768 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/spot/recovery_strategy.py
--rw-r--r--   0 runner    (1001) docker     (122)     8687 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/spot/spot_state.py
--rw-r--r--   0 runner    (1001) docker     (122)    18685 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/spot/spot_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    34439 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/task.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-11-26 05:33:21.762757 skypilot-nightly-1.0.0.dev20221126/sky/templates/
--rw-r--r--   0 runner    (1001) docker     (122)     8388 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/templates/aws-ray.yml.j2
--rw-r--r--   0 runner    (1001) docker     (122)     8389 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/templates/azure-ray.yml.j2
--rw-r--r--   0 runner    (1001) docker     (122)     9799 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/templates/gcp-ray.yml.j2
--rw-r--r--   0 runner    (1001) docker     (122)      480 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/templates/gcp-tpu-create.sh.j2
--rw-r--r--   0 runner    (1001) docker     (122)      328 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/templates/gcp-tpu-delete.sh.j2
--rw-r--r--   0 runner    (1001) docker     (122)     1424 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/templates/local-ray.yml.j2
--rw-r--r--   0 runner    (1001) docker     (122)     1804 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/templates/spot-controller.yaml.j2
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-11-26 05:33:21.762757 skypilot-nightly-1.0.0.dev20221126/sky/usage/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/usage/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)      629 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/usage/constants.py
--rw-r--r--   0 runner    (1001) docker     (122)    16886 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/usage/usage_lib.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-11-26 05:33:21.762757 skypilot-nightly-1.0.0.dev20221126/sky/utils/
--rw-r--r--   0 runner    (1001) docker     (122)       25 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     2813 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/utils/accelerator_registry.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-11-26 05:33:21.762757 skypilot-nightly-1.0.0.dev20221126/sky/utils/cli_utils/
--rw-r--r--   0 runner    (1001) docker     (122)        0 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/utils/cli_utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (122)     9288 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/utils/cli_utils/status_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)    13047 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/utils/command_runner.py
--rw-r--r--   0 runner    (1001) docker     (122)     9332 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/utils/common_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     1415 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/utils/db_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)      429 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/utils/env_options.py
--rw-r--r--   0 runner    (1001) docker     (122)     3675 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/utils/log_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     6169 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/utils/schemas.py
--rw-r--r--   0 runner    (1001) docker     (122)     3103 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/utils/subprocess_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)     3955 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/utils/timeline.py
--rw-r--r--   0 runner    (1001) docker     (122)      977 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/utils/tpu_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)      577 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/utils/ux_utils.py
--rw-r--r--   0 runner    (1001) docker     (122)      701 2022-11-26 05:33:11.000000 skypilot-nightly-1.0.0.dev20221126/sky/utils/validator.py
-drwxr-xr-x   0 runner    (1001) docker     (122)        0 2022-11-26 05:33:21.762757 skypilot-nightly-1.0.0.dev20221126/skypilot_nightly.egg-info/
--rw-r--r--   0 runner    (1001) docker     (122)     7447 2022-11-26 05:33:21.000000 skypilot-nightly-1.0.0.dev20221126/skypilot_nightly.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (122)     3870 2022-11-26 05:33:21.000000 skypilot-nightly-1.0.0.dev20221126/skypilot_nightly.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (122)        1 2022-11-26 05:33:21.000000 skypilot-nightly-1.0.0.dev20221126/skypilot_nightly.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (122)       36 2022-11-26 05:33:21.000000 skypilot-nightly-1.0.0.dev20221126/skypilot_nightly.egg-info/entry_points.txt
--rw-r--r--   0 runner    (1001) docker     (122)      539 2022-11-26 05:33:21.000000 skypilot-nightly-1.0.0.dev20221126/skypilot_nightly.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (122)        4 2022-11-26 05:33:21.000000 skypilot-nightly-1.0.0.dev20221126/skypilot_nightly.egg-info/top_level.txt
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-13 16:17:35.560316 skypilot-nightly-1.0.0.dev20230713/
+-rw-r--r--   0 runner    (1001) docker     (123)    12170 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/LICENSE
+-rw-r--r--   0 runner    (1001) docker     (123)      647 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/MANIFEST.in
+-rw-r--r--   0 runner    (1001) docker     (123)     8470 2023-07-13 16:17:35.556316 skypilot-nightly-1.0.0.dev20230713/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)     7687 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/README.md
+-rw-r--r--   0 runner    (1001) docker     (123)      519 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/pyproject.toml
+-rw-r--r--   0 runner    (1001) docker     (123)       38 2023-07-13 16:17:35.560316 skypilot-nightly-1.0.0.dev20230713/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (123)     7977 2023-07-13 16:17:27.000000 skypilot-nightly-1.0.0.dev20230713/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-13 16:17:35.536315 skypilot-nightly-1.0.0.dev20230713/sky/
+-rw-r--r--   0 runner    (1001) docker     (123)     2105 2023-07-13 16:17:27.000000 skypilot-nightly-1.0.0.dev20230713/sky/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-13 16:17:35.536315 skypilot-nightly-1.0.0.dev20230713/sky/adaptors/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/adaptors/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2607 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/adaptors/aws.py
+-rw-r--r--   0 runner    (1001) docker     (123)      989 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/adaptors/azure.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7726 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/adaptors/cloudflare.py
+-rw-r--r--   0 runner    (1001) docker     (123)      852 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/adaptors/docker.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2194 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/adaptors/gcp.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3052 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/adaptors/ibm.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2054 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/adaptors/oci.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15607 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/authentication.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-13 16:17:35.536315 skypilot-nightly-1.0.0.dev20230713/sky/backends/
+-rw-r--r--   0 runner    (1001) docker     (123)      414 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/backends/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5542 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/backends/backend.py
+-rw-r--r--   0 runner    (1001) docker     (123)   112593 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/backends/backend_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)   201662 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/backends/cloud_vm_ray_backend.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8321 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/backends/docker_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16267 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/backends/local_docker_backend.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-13 16:17:35.536315 skypilot-nightly-1.0.0.dev20230713/sky/backends/monkey_patches/
+-rw-r--r--   0 runner    (1001) docker     (123)     3410 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/backends/monkey_patches/monkey_patch_ray_up.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24422 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/backends/onprem_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5903 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/backends/wheel_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-13 16:17:35.540315 skypilot-nightly-1.0.0.dev20230713/sky/benchmark/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/benchmark/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8723 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/benchmark/benchmark_state.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24638 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/benchmark/benchmark_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3814 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/check.py
+-rw-r--r--   0 runner    (1001) docker     (123)   167453 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/cli.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8595 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/cloud_stores.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-13 16:17:35.540315 skypilot-nightly-1.0.0.dev20230713/sky/clouds/
+-rw-r--r--   0 runner    (1001) docker     (123)      701 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/clouds/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    41085 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/clouds/aws.py
+-rw-r--r--   0 runner    (1001) docker     (123)    25564 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/clouds/azure.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24158 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/clouds/cloud.py
+-rw-r--r--   0 runner    (1001) docker     (123)    42911 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/clouds/gcp.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20008 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/clouds/ibm.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11903 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/clouds/lambda_cloud.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7903 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/clouds/local.py
+-rw-r--r--   0 runner    (1001) docker     (123)    24297 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/clouds/oci.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14583 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/clouds/scp.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-13 16:17:35.540315 skypilot-nightly-1.0.0.dev20230713/sky/clouds/service_catalog/
+-rw-r--r--   0 runner    (1001) docker     (123)    13191 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/clouds/service_catalog/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11470 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/clouds/service_catalog/aws_catalog.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7050 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/clouds/service_catalog/azure_catalog.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22995 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/clouds/service_catalog/common.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1500 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/clouds/service_catalog/config.py
+-rw-r--r--   0 runner    (1001) docker     (123)      282 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/clouds/service_catalog/constants.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-13 16:17:35.540315 skypilot-nightly-1.0.0.dev20230713/sky/clouds/service_catalog/data_fetchers/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/clouds/service_catalog/data_fetchers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20092 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/clouds/service_catalog/data_fetchers/fetch_aws.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8679 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/clouds/service_catalog/data_fetchers/fetch_azure.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18601 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/clouds/service_catalog/data_fetchers/fetch_gcp.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4198 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/clouds/service_catalog/data_fetchers/fetch_lambda_cloud.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19812 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/clouds/service_catalog/gcp_catalog.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4656 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/clouds/service_catalog/ibm_catalog.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5414 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/clouds/service_catalog/lambda_catalog.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7582 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/clouds/service_catalog/oci_catalog.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5484 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/clouds/service_catalog/scp_catalog.py
+-rw-r--r--   0 runner    (1001) docker     (123)    40110 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/core.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2502 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/dag.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-13 16:17:35.544315 skypilot-nightly-1.0.0.dev20230713/sky/data/
+-rw-r--r--   0 runner    (1001) docker     (123)      128 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/data/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7273 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/data/data_transfer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7918 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/data/data_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3251 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/data/mounting_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    89247 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/data/storage.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1367 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/data/storage_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6017 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/exceptions.py
+-rw-r--r--   0 runner    (1001) docker     (123)    41325 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/execution.py
+-rw-r--r--   0 runner    (1001) docker     (123)    26274 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/global_user_state.py
+-rw-r--r--   0 runner    (1001) docker     (123)    45228 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/optimizer.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-13 16:17:35.544315 skypilot-nightly-1.0.0.dev20230713/sky/provision/
+-rw-r--r--   0 runner    (1001) docker     (123)     1612 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/provision/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-13 16:17:35.544315 skypilot-nightly-1.0.0.dev20230713/sky/provision/aws/
+-rw-r--r--   0 runner    (1001) docker     (123)      112 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/provision/aws/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3733 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/provision/aws/instance.py
+-rw-r--r--   0 runner    (1001) docker     (123)    45043 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/resources.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-13 16:17:35.544315 skypilot-nightly-1.0.0.dev20230713/sky/setup_files/
+-rw-r--r--   0 runner    (1001) docker     (123)      647 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/setup_files/MANIFEST.in
+-rw-r--r--   0 runner    (1001) docker     (123)     7977 2023-07-13 16:17:27.000000 skypilot-nightly-1.0.0.dev20230713/sky/setup_files/setup.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3216 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/sky_logging.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-13 16:17:35.544315 skypilot-nightly-1.0.0.dev20230713/sky/skylet/
+-rw-r--r--   0 runner    (1001) docker     (123)    12568 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/LICENSE
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1203 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/attempt_skylet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4378 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/autostop_lib.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1887 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/configs.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2064 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/constants.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8899 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/events.py
+-rw-r--r--   0 runner    (1001) docker     (123)    32722 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/job_lib.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19585 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/log_lib.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-13 16:17:35.532315 skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-13 16:17:35.544315 skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/aws/
+-rw-r--r--   0 runner    (1001) docker     (123)      110 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/aws/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-13 16:17:35.544315 skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/aws/cloudwatch/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/aws/cloudwatch/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    32698 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/aws/cloudwatch/cloudwatch_helper.py
+-rw-r--r--   0 runner    (1001) docker     (123)    52192 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/aws/config.py
+-rw-r--r--   0 runner    (1001) docker     (123)    29406 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/aws/node_provider.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5917 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/aws/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-13 16:17:35.548315 skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/azure/
+-rw-r--r--   0 runner    (1001) docker     (123)       97 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/azure/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4344 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/azure/azure-config-template.json
+-rw-r--r--   0 runner    (1001) docker     (123)    10633 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/azure/azure-vm-template.json
+-rw-r--r--   0 runner    (1001) docker     (123)     5020 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/azure/config.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17469 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/azure/node_provider.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-13 16:17:35.548315 skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/gcp/
+-rw-r--r--   0 runner    (1001) docker     (123)       91 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/gcp/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    34963 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/gcp/config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4118 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/gcp/constants.py
+-rw-r--r--   0 runner    (1001) docker     (123)    26192 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/gcp/node.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14292 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/gcp/node_provider.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-13 16:17:35.548315 skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/ibm/
+-rw-r--r--   0 runner    (1001) docker     (123)       94 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/ibm/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    38280 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/ibm/node_provider.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1290 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/ibm/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    34628 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/ibm/vpc_provider.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-13 16:17:35.548315 skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/lambda_cloud/
+-rw-r--r--   0 runner    (1001) docker     (123)      112 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/lambda_cloud/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8613 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/lambda_cloud/lambda_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13650 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/lambda_cloud/node_provider.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-13 16:17:35.548315 skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/oci/
+-rw-r--r--   0 runner    (1001) docker     (123)       91 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/oci/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4234 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/oci/config.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20136 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/oci/node_provider.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17048 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/oci/query_helper.py
+-rw-r--r--   0 runner    (1001) docker     (123)      508 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/oci/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-13 16:17:35.548315 skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/scp/
+-rw-r--r--   0 runner    (1001) docker     (123)       91 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/scp/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4683 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/scp/config.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22219 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/scp/node_provider.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15481 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/scp/scp_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-13 16:17:35.552316 skypilot-nightly-1.0.0.dev20230713/sky/skylet/ray_patches/
+-rw-r--r--   0 runner    (1001) docker     (123)     2904 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/ray_patches/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      294 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/ray_patches/autoscaler.py.patch
+-rw-r--r--   0 runner    (1001) docker     (123)      391 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/ray_patches/cli.py.patch
+-rw-r--r--   0 runner    (1001) docker     (123)      224 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/ray_patches/command_runner.py.patch
+-rw-r--r--   0 runner    (1001) docker     (123)      345 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/ray_patches/job_head.py.patch
+-rw-r--r--   0 runner    (1001) docker     (123)      528 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/ray_patches/log_monitor.py.patch
+-rw-r--r--   0 runner    (1001) docker     (123)      658 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/ray_patches/resource_demand_scheduler.py.patch
+-rw-r--r--   0 runner    (1001) docker     (123)      289 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/ray_patches/updater.py.patch
+-rw-r--r--   0 runner    (1001) docker     (123)      565 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/ray_patches/worker.py.patch
+-rw-r--r--   0 runner    (1001) docker     (123)      788 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/skylet.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2649 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skylet/subprocess_daemon.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5654 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/skypilot_config.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-13 16:17:35.552316 skypilot-nightly-1.0.0.dev20230713/sky/spot/
+-rw-r--r--   0 runner    (1001) docker     (123)     1356 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/spot/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      881 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/spot/constants.py
+-rw-r--r--   0 runner    (1001) docker     (123)    25102 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/spot/controller.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-13 16:17:35.552316 skypilot-nightly-1.0.0.dev20230713/sky/spot/dashboard/
+-rw-r--r--   0 runner    (1001) docker     (123)     2294 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/spot/dashboard/dashboard.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-13 16:17:35.552316 skypilot-nightly-1.0.0.dev20230713/sky/spot/dashboard/static/
+-rw-r--r--   0 runner    (1001) docker     (123)    15086 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/spot/dashboard/static/favicon.ico
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-13 16:17:35.552316 skypilot-nightly-1.0.0.dev20230713/sky/spot/dashboard/templates/
+-rw-r--r--   0 runner    (1001) docker     (123)     6247 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/spot/dashboard/templates/index.html
+-rw-r--r--   0 runner    (1001) docker     (123)    21256 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/spot/recovery_strategy.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22484 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/spot/spot_state.py
+-rw-r--r--   0 runner    (1001) docker     (123)    34333 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/spot/spot_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1457 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/status_lib.py
+-rw-r--r--   0 runner    (1001) docker     (123)    38424 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/task.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-13 16:17:35.552316 skypilot-nightly-1.0.0.dev20230713/sky/templates/
+-rw-r--r--   0 runner    (1001) docker     (123)    11671 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/templates/aws-ray.yml.j2
+-rw-r--r--   0 runner    (1001) docker     (123)     9639 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/templates/azure-ray.yml.j2
+-rw-r--r--   0 runner    (1001) docker     (123)    11400 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/templates/gcp-ray.yml.j2
+-rw-r--r--   0 runner    (1001) docker     (123)      505 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/templates/gcp-tpu-create.sh.j2
+-rw-r--r--   0 runner    (1001) docker     (123)      328 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/templates/gcp-tpu-delete.sh.j2
+-rw-r--r--   0 runner    (1001) docker     (123)     8040 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/templates/ibm-ray.yml.j2
+-rw-r--r--   0 runner    (1001) docker     (123)     6830 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/templates/lambda-ray.yml.j2
+-rw-r--r--   0 runner    (1001) docker     (123)     1424 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/templates/local-ray.yml.j2
+-rw-r--r--   0 runner    (1001) docker     (123)     8285 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/templates/oci-ray.yml.j2
+-rw-r--r--   0 runner    (1001) docker     (123)     6956 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/templates/scp-ray.yml.j2
+-rw-r--r--   0 runner    (1001) docker     (123)     2185 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/templates/spot-controller.yaml.j2
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-13 16:17:35.552316 skypilot-nightly-1.0.0.dev20230713/sky/usage/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/usage/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      633 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/usage/constants.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17294 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/usage/usage_lib.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-13 16:17:35.556316 skypilot-nightly-1.0.0.dev20230713/sky/utils/
+-rw-r--r--   0 runner    (1001) docker     (123)       25 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2806 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/utils/accelerator_registry.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-13 16:17:35.556316 skypilot-nightly-1.0.0.dev20230713/sky/utils/cli_utils/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/utils/cli_utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14891 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/utils/cli_utils/status_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14688 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/utils/command_runner.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12077 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/utils/common_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2941 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/utils/dag_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2777 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/utils/db_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)      852 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/utils/env_options.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4962 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/utils/log_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6927 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/utils/schemas.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5774 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/utils/subprocess_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3965 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/utils/timeline.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4186 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/utils/tpu_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1125 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/utils/ux_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)      701 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/sky/utils/validator.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-13 16:17:35.556316 skypilot-nightly-1.0.0.dev20230713/skypilot_nightly.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (123)     8470 2023-07-13 16:17:35.000000 skypilot-nightly-1.0.0.dev20230713/skypilot_nightly.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)     5675 2023-07-13 16:17:35.000000 skypilot-nightly-1.0.0.dev20230713/skypilot_nightly.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-07-13 16:17:35.000000 skypilot-nightly-1.0.0.dev20230713/skypilot_nightly.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (123)       36 2023-07-13 16:17:35.000000 skypilot-nightly-1.0.0.dev20230713/skypilot_nightly.egg-info/entry_points.txt
+-rw-r--r--   0 runner    (1001) docker     (123)     1234 2023-07-13 16:17:35.000000 skypilot-nightly-1.0.0.dev20230713/skypilot_nightly.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        4 2023-07-13 16:17:35.000000 skypilot-nightly-1.0.0.dev20230713/skypilot_nightly.egg-info/top_level.txt
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-13 16:17:35.556316 skypilot-nightly-1.0.0.dev20230713/tests/
+-rw-r--r--   0 runner    (1001) docker     (123)     4677 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/tests/test_cli.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5061 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/tests/test_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)      260 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/tests/test_global_user_state.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3620 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/tests/test_list_accelerators.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14008 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/tests/test_onprem.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20799 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/tests/test_optimizer_dryruns.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4663 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/tests/test_optimizer_random_dag.py
+-rw-r--r--   0 runner    (1001) docker     (123)       94 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/tests/test_pycryptodome_version.py
+-rw-r--r--   0 runner    (1001) docker     (123)   123678 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/tests/test_smoke.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7215 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/tests/test_spot.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4048 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/tests/test_storage.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1070 2023-07-13 16:17:25.000000 skypilot-nightly-1.0.0.dev20230713/tests/test_wheels.py
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/LICENSE` & `skypilot-nightly-1.0.0.dev20230713/LICENSE`

 * *Files identical despite different names*

### Comparing `skypilot-nightly-1.0.0.dev20221126/PKG-INFO` & `skypilot-nightly-1.0.0.dev20230713/README.md`

 * *Files 8% similar despite different names*

```diff
@@ -1,78 +1,74 @@
-Metadata-Version: 2.1
-Name: skypilot-nightly
-Version: 1.0.0.dev20221126
-Summary: SkyPilot: An intercloud broker for the clouds
-Author: SkyPilot Team
-License: Apache 2.0
-Project-URL: Homepage, https://github.com/skypilot-org/skypilot
-Project-URL: Issues, https://github.com/skypilot-org/skypilot/issues
-Project-URL: Discussion, https://github.com/skypilot-org/skypilot/discussions
-Project-URL: Documentation, https://skypilot.readthedocs.io/en/latest/
-Classifier: Programming Language :: Python :: 3.6
-Classifier: Programming Language :: Python :: 3.7
-Classifier: Programming Language :: Python :: 3.8
-Classifier: Programming Language :: Python :: 3.9
-Classifier: Programming Language :: Python :: 3.10
-Classifier: License :: OSI Approved :: Apache Software License
-Classifier: Operating System :: OS Independent
-Classifier: Topic :: Software Development :: Libraries :: Python Modules
-Classifier: Topic :: System :: Distributed Computing
-Description-Content-Type: text/markdown
-Provides-Extra: aws
-Provides-Extra: azure
-Provides-Extra: gcp
-Provides-Extra: docker
-Provides-Extra: all
-License-File: LICENSE
-
 <p align="center">
-  <img alt="SkyPilot" src="https://raw.githubusercontent.com/skypilot-org/skypilot/master/docs/source/images/skypilot-wide-light-1k.png" width=55%>
+  <picture>
+    <source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/skypilot-org/skypilot/master/docs/source/images/skypilot-wide-dark-1k.png">
+    <img alt="SkyPilot" src="https://raw.githubusercontent.com/skypilot-org/skypilot/master/docs/source/images/skypilot-wide-light-1k.png" width=55%>
+  </picture>
 </p>
 
 <p align="center">
   <a href="https://skypilot.readthedocs.io/en/latest/"> 
     <img alt="Documentation" src="https://readthedocs.org/projects/skypilot/badge/?version=latest">
   </a>
   
   <a href="https://github.com/skypilot-org/skypilot/releases"> 
     <img alt="GitHub Release" src="https://img.shields.io/github/release/skypilot-org/skypilot.svg">
   </a>
   
-  <a href="https://join.slack.com/t/skypilot-org/shared_invite/zt-1i4pa7lyc-g6Lo4_rqqCFWOSXdvwTs3Q"> 
+  <a href="http://slack.skypilot.co"> 
     <img alt="Join Slack" src="https://img.shields.io/badge/SkyPilot-Join%20Slack-blue?logo=slack">
   </a>
   
 </p>
 
 
 <h3 align="center">
-    Run jobs on any cloud, easily and cost effectively
+    Run LLMs and AI on Any Cloud
 </h3>
 
-SkyPilot is a framework for easily and cost effectively running ML workloads<sup>[1]</sup> on any cloud. 
-
-SkyPilot abstracts away cloud infra burden:
-- Launch jobs & clusters on any cloud (AWS, Azure, GCP)
-- Find scarce resources across zones/regions/clouds
-- Queue jobs & use cloud object stores
-
-SkyPilot cuts your cloud costs:
-* [Managed Spot](https://skypilot.readthedocs.io/en/latest/examples/spot-jobs.html): **3x cost savings** using spot VMs, with auto-recovery from preemptions
+----
+:fire: *News* :fire:
+- [June, 2023] Serving LLM **24x Faster On the Cloud** with vLLM and SkyPilot: [**example**](./llm/vllm/), [**blog post**](https://blog.skypilot.co/serving-llm-24x-faster-on-the-cloud-with-vllm-and-skypilot/)
+- [June, 2023] [**Two new clouds supported**](https://skypilot.readthedocs.io/en/latest/getting-started/installation.html): Samsung SCP and Oracle OCI!
+- [April, 2023] **[**SkyPilot YAMLs released**](./llm/vicuna/) for finetuning & serving the Vicuna model with a single command**!
+- [March, 2023] **[Vicuna LLM chatbot](https://lmsys.org/blog/2023-03-30-vicuna/) trained** [**using SkyPilot**](./llm/vicuna/) **for $300 on spot instances!** 
+- [March, 2023] Serve your own LLaMA LLM chatbot (not finetuned) on any cloud: [**example**](./llm/llama-chatbots/), [**repo**](https://github.com/skypilot-org/sky-llama)
+----
+
+SkyPilot is a framework for running LLMs, AI, and batch jobs on any cloud, offering maximum cost savings, highest GPU availability, and managed execution.
+
+SkyPilot **abstracts away cloud infra burdens**:
+- Launch jobs & clusters on any cloud 
+- Easy scale-out: queue and run many jobs, automatically managed
+- Easy access to object stores (S3, GCS, R2)
+
+SkyPilot **maximizes GPU availability for your jobs**:
+* Provision in all zones/regions/clouds you have access to ([the _Sky_](https://arxiv.org/abs/2205.07147)), with automatic failover
+
+SkyPilot **cuts your cloud costs**:
+* [Managed Spot](https://skypilot.readthedocs.io/en/latest/examples/spot-jobs.html): 3-6x cost savings using spot VMs, with auto-recovery from preemptions
+* Optimizer: 2x cost savings by auto-picking the cheapest VM/zone/region/cloud
 * [Autostop](https://skypilot.readthedocs.io/en/latest/reference/auto-stop.html): hands-free cleanup of idle clusters 
-* [Benchmark](https://skypilot.readthedocs.io/en/latest/reference/benchmark/index.html): find best VM types for your jobs
-* Optimizer: **2x cost savings** by auto-picking best prices across zones/regions/clouds
 
 SkyPilot supports your existing GPU, TPU, and CPU workloads, with no code changes. 
 
-Install with pip (choose your clouds) or [from source](https://skypilot.readthedocs.io/en/latest/getting-started/installation.html):
+Install with pip or [from source](https://skypilot.readthedocs.io/en/latest/getting-started/installation.html):
 ```
-pip install "skypilot[aws,gcp,azure]"
+pip install "skypilot[aws,gcp,azure,ibm,oci,scp,lambda]"  # choose your clouds
 ```
 
+Current supported providers (AWS, Azure, GCP, Lambda Cloud, IBM, Samsung, OCI, Cloudflare):
+<p align="center">
+  <picture>
+    <source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/skypilot-org/skypilot/master/docs/source/images/cloud-logos-dark.png">
+    <img alt="SkyPilot" src="https://raw.githubusercontent.com/skypilot-org/skypilot/master/docs/source/images/cloud-logos-light.png" width=80%>
+  </picture>
+</p>
+
+
 ## Getting Started
 You can find our documentation [here](https://skypilot.readthedocs.io/en/latest/).
 - [Installation](https://skypilot.readthedocs.io/en/latest/getting-started/installation.html)
 - [Quickstart](https://skypilot.readthedocs.io/en/latest/getting-started/quickstart.html)
 - [CLI reference](https://skypilot.readthedocs.io/en/latest/reference/cli.html)
 
 ## SkyPilot in 1 minute
@@ -106,15 +102,15 @@
 ```
 
 Prepare the workdir by cloning:
 ```bash
 git clone https://github.com/pytorch/examples.git ~/torch_examples
 ```
 
-Launch with `sky launch`:
+Launch with `sky launch` (note: [access to GPU instances](https://skypilot.readthedocs.io/en/latest/reference/quota.html) is needed for this example):
 ```bash
 sky launch my_task.yaml
 ```
 
 SkyPilot then performs the heavy-lifting for you, including:
 1. Find the lowest priced VM instance type across different clouds
 2. Provision the VM, with auto-failover if the cloud returned capacity errors
@@ -134,21 +130,20 @@
 - [Documentation](https://skypilot.readthedocs.io/en/latest/)
 - [Example: HuggingFace](https://skypilot.readthedocs.io/en/latest/getting-started/tutorial.html) 
 - [Tutorials](https://github.com/skypilot-org/skypilot-tutorial) 
 - [YAML reference](https://skypilot.readthedocs.io/en/latest/reference/yaml-spec.html)
 - Framework examples: [PyTorch DDP](https://github.com/skypilot-org/skypilot/blob/master/examples/resnet_distributed_torch.yaml),  [Distributed](https://github.com/skypilot-org/skypilot/blob/master/examples/resnet_distributed_tf_app.py) [TensorFlow](https://github.com/skypilot-org/skypilot/blob/master/examples/resnet_app_storage.yaml), [JAX/Flax on TPU](https://github.com/skypilot-org/skypilot/blob/master/examples/tpu/tpuvm_mnist.yaml), [Stable Diffusion](https://github.com/skypilot-org/skypilot/tree/master/examples/stable_diffusion), [Detectron2](https://github.com/skypilot-org/skypilot/blob/master/examples/detectron2_docker.yaml), [programmatic grid search](https://github.com/skypilot-org/skypilot/blob/master/examples/huggingface_glue_imdb_grid_search_app.py), [Docker](https://github.com/skypilot-org/skypilot/blob/master/examples/docker/echo_app.yaml), and [many more](./examples).
 
 More information:
-- [Introductory blog post](https://medium.com/@zongheng_yang/skypilot-ml-and-data-science-on-any-cloud-with-massive-cost-savings-244189cc7c0f)
+- [SkyPilot Blog](https://blog.skypilot.co/)
+  - [Introductory blog post](https://blog.skypilot.co/introducing-skypilot/)
+- [NSDI 2023 paper & talk](https://www.usenix.org/conference/nsdi23/presentation/yang-zongheng)
 
 ## Issues, feature requests, and questions
 We are excited to hear your feedback! 
 * For issues and feature requests, please [open a GitHub issue](https://github.com/skypilot-org/skypilot/issues/new).
 * For questions, please use [GitHub Discussions](https://github.com/skypilot-org/skypilot/discussions).
 
-For general discussions, join us on the [SkyPilot Slack](https://join.slack.com/t/skypilot-org/shared_invite/zt-1i4pa7lyc-g6Lo4_rqqCFWOSXdvwTs3Q).
+For general discussions, join us on the [SkyPilot Slack](http://slack.skypilot.co).
 
 ## Contributing
 We welcome and value all contributions to the project! Please refer to [CONTRIBUTING](CONTRIBUTING.md) for how to get involved.
-
-#
-<sup>[1]</sup>: While SkyPilot is currently targeted at machine learning workloads, it supports and has been used for other general batch workloads. We're excited to hear about your use case and how we can better support your requirements; please join us in [this discussion](https://github.com/skypilot-org/skypilot/discussions/1016)!
```

#### html2text {}

```diff
@@ -1,64 +1,67 @@
-Metadata-Version: 2.1 Name: skypilot-nightly Version: 1.0.0.dev20221126
-Summary: SkyPilot: An intercloud broker for the clouds Author: SkyPilot Team
-License: Apache 2.0 Project-URL: Homepage, https://github.com/skypilot-org/
-skypilot Project-URL: Issues, https://github.com/skypilot-org/skypilot/issues
-Project-URL: Discussion, https://github.com/skypilot-org/skypilot/discussions
-Project-URL: Documentation, https://skypilot.readthedocs.io/en/latest/
-Classifier: Programming Language :: Python :: 3.6 Classifier: Programming
-Language :: Python :: 3.7 Classifier: Programming Language :: Python :: 3.8
-Classifier: Programming Language :: Python :: 3.9 Classifier: Programming
-Language :: Python :: 3.10 Classifier: License :: OSI Approved :: Apache
-Software License Classifier: Operating System :: OS Independent Classifier:
-Topic :: Software Development :: Libraries :: Python Modules Classifier: Topic
-:: System :: Distributed Computing Description-Content-Type: text/markdown
-Provides-Extra: aws Provides-Extra: azure Provides-Extra: gcp Provides-Extra:
-docker Provides-Extra: all License-File: LICENSE
-                                  [SkyPilot]
+                                   [SkyPilot]
                  [Documentation] [GitHub_Release] [Join_Slack]
-         **** Run jobs on any cloud, easily and cost effectively ****
-SkyPilot is a framework for easily and cost effectively running ML workloads[1]
-on any cloud. SkyPilot abstracts away cloud infra burden: - Launch jobs &
-clusters on any cloud (AWS, Azure, GCP) - Find scarce resources across zones/
-regions/clouds - Queue jobs & use cloud object stores SkyPilot cuts your cloud
-costs: * [Managed Spot](https://skypilot.readthedocs.io/en/latest/examples/
-spot-jobs.html): **3x cost savings** using spot VMs, with auto-recovery from
-preemptions * [Autostop](https://skypilot.readthedocs.io/en/latest/reference/
-auto-stop.html): hands-free cleanup of idle clusters * [Benchmark](https://
-skypilot.readthedocs.io/en/latest/reference/benchmark/index.html): find best VM
-types for your jobs * Optimizer: **2x cost savings** by auto-picking best
-prices across zones/regions/clouds SkyPilot supports your existing GPU, TPU,
-and CPU workloads, with no code changes. Install with pip (choose your clouds)
-or [from source](https://skypilot.readthedocs.io/en/latest/getting-started/
-installation.html): ``` pip install "skypilot[aws,gcp,azure]" ``` ## Getting
-Started You can find our documentation [here](https://skypilot.readthedocs.io/
-en/latest/). - [Installation](https://skypilot.readthedocs.io/en/latest/
-getting-started/installation.html) - [Quickstart](https://
-skypilot.readthedocs.io/en/latest/getting-started/quickstart.html) - [CLI
-reference](https://skypilot.readthedocs.io/en/latest/reference/cli.html) ##
-SkyPilot in 1 minute A SkyPilot task specifies: resource requirements, data to
-be synced, setup commands, and the task commands. Once written in this
-[**unified interface**](https://skypilot.readthedocs.io/en/latest/reference/
-yaml-spec.html) (YAML or Python API), the task can be launched on any available
-cloud. This avoids vendor lock-in, and allows easily moving jobs to a different
-provider. Paste the following into a file `my_task.yaml`: ```yaml resources:
-accelerators: V100:1 # 1x NVIDIA V100 GPU num_nodes: 1 # Number of VMs to
-launch # Working directory (optional) containing the project codebase. # Its
-contents are synced to ~/sky_workdir/ on the cluster. workdir: ~/torch_examples
-# Commands to be run before executing the job. # Typical use: pip install -
-r requirements.txt, git clone, etc. setup: | pip install torch torchvision #
-Commands to run as a job. # Typical use: launch the main program. run: | cd
-mnist python main.py --epochs 1 ``` Prepare the workdir by cloning: ```bash git
-clone https://github.com/pytorch/examples.git ~/torch_examples ``` Launch with
-`sky launch`: ```bash sky launch my_task.yaml ``` SkyPilot then performs the
-heavy-lifting for you, including: 1. Find the lowest priced VM instance type
-across different clouds 2. Provision the VM, with auto-failover if the cloud
-returned capacity errors 3. Sync the local `workdir` to the VM 4. Run the
-task's `setup` commands to prepare the VM for running the task 5. Run the
-task's `run` commands
+                    **** Run LLMs and AI on Any Cloud ****
+---- :fire: *News* :fire: - [June, 2023] Serving LLM **24x Faster On the
+Cloud** with vLLM and SkyPilot: [**example**](./llm/vllm/), [**blog post**]
+(https://blog.skypilot.co/serving-llm-24x-faster-on-the-cloud-with-vllm-and-
+skypilot/) - [June, 2023] [**Two new clouds supported**](https://
+skypilot.readthedocs.io/en/latest/getting-started/installation.html): Samsung
+SCP and Oracle OCI! - [April, 2023] **[**SkyPilot YAMLs released**](./llm/
+vicuna/) for finetuning & serving the Vicuna model with a single command**! -
+[March, 2023] **[Vicuna LLM chatbot](https://lmsys.org/blog/2023-03-30-vicuna/
+) trained** [**using SkyPilot**](./llm/vicuna/) **for $300 on spot instances!**
+- [March, 2023] Serve your own LLaMA LLM chatbot (not finetuned) on any cloud:
+[**example**](./llm/llama-chatbots/), [**repo**](https://github.com/skypilot-
+org/sky-llama) ---- SkyPilot is a framework for running LLMs, AI, and batch
+jobs on any cloud, offering maximum cost savings, highest GPU availability, and
+managed execution. SkyPilot **abstracts away cloud infra burdens**: - Launch
+jobs & clusters on any cloud - Easy scale-out: queue and run many jobs,
+automatically managed - Easy access to object stores (S3, GCS, R2) SkyPilot
+**maximizes GPU availability for your jobs**: * Provision in all zones/regions/
+clouds you have access to ([the _Sky_](https://arxiv.org/abs/2205.07147)), with
+automatic failover SkyPilot **cuts your cloud costs**: * [Managed Spot](https:/
+/skypilot.readthedocs.io/en/latest/examples/spot-jobs.html): 3-6x cost savings
+using spot VMs, with auto-recovery from preemptions * Optimizer: 2x cost
+savings by auto-picking the cheapest VM/zone/region/cloud * [Autostop](https://
+skypilot.readthedocs.io/en/latest/reference/auto-stop.html): hands-free cleanup
+of idle clusters SkyPilot supports your existing GPU, TPU, and CPU workloads,
+with no code changes. Install with pip or [from source](https://
+skypilot.readthedocs.io/en/latest/getting-started/installation.html): ``` pip
+install "skypilot[aws,gcp,azure,ibm,oci,scp,lambda]" # choose your clouds ```
+Current supported providers (AWS, Azure, GCP, Lambda Cloud, IBM, Samsung, OCI,
+Cloudflare):
+                                   [SkyPilot]
+## Getting Started You can find our documentation [here](https://
+skypilot.readthedocs.io/en/latest/). - [Installation](https://
+skypilot.readthedocs.io/en/latest/getting-started/installation.html) -
+[Quickstart](https://skypilot.readthedocs.io/en/latest/getting-started/
+quickstart.html) - [CLI reference](https://skypilot.readthedocs.io/en/latest/
+reference/cli.html) ## SkyPilot in 1 minute A SkyPilot task specifies: resource
+requirements, data to be synced, setup commands, and the task commands. Once
+written in this [**unified interface**](https://skypilot.readthedocs.io/en/
+latest/reference/yaml-spec.html) (YAML or Python API), the task can be launched
+on any available cloud. This avoids vendor lock-in, and allows easily moving
+jobs to a different provider. Paste the following into a file `my_task.yaml`:
+```yaml resources: accelerators: V100:1 # 1x NVIDIA V100 GPU num_nodes: 1 #
+Number of VMs to launch # Working directory (optional) containing the project
+codebase. # Its contents are synced to ~/sky_workdir/ on the cluster. workdir:
+~/torch_examples # Commands to be run before executing the job. # Typical use:
+pip install -r requirements.txt, git clone, etc. setup: | pip install torch
+torchvision # Commands to run as a job. # Typical use: launch the main program.
+run: | cd mnist python main.py --epochs 1 ``` Prepare the workdir by cloning:
+```bash git clone https://github.com/pytorch/examples.git ~/torch_examples ```
+Launch with `sky launch` (note: [access to GPU instances](https://
+skypilot.readthedocs.io/en/latest/reference/quota.html) is needed for this
+example): ```bash sky launch my_task.yaml ``` SkyPilot then performs the heavy-
+lifting for you, including: 1. Find the lowest priced VM instance type across
+different clouds 2. Provision the VM, with auto-failover if the cloud returned
+capacity errors 3. Sync the local `workdir` to the VM 4. Run the task's `setup`
+commands to prepare the VM for running the task 5. Run the task's `run`
+commands
                                 [SkyPilot Demo]
 Refer to [Quickstart](https://skypilot.readthedocs.io/en/latest/getting-
 started/quickstart.html) to get started with SkyPilot. ## Learn more -
 [Documentation](https://skypilot.readthedocs.io/en/latest/) - [Example:
 HuggingFace](https://skypilot.readthedocs.io/en/latest/getting-started/
 tutorial.html) - [Tutorials](https://github.com/skypilot-org/skypilot-tutorial)
 - [YAML reference](https://skypilot.readthedocs.io/en/latest/reference/yaml-
@@ -70,22 +73,18 @@
 (https://github.com/skypilot-org/skypilot/blob/master/examples/tpu/
 tpuvm_mnist.yaml), [Stable Diffusion](https://github.com/skypilot-org/skypilot/
 tree/master/examples/stable_diffusion), [Detectron2](https://github.com/
 skypilot-org/skypilot/blob/master/examples/detectron2_docker.yaml),
 [programmatic grid search](https://github.com/skypilot-org/skypilot/blob/
 master/examples/huggingface_glue_imdb_grid_search_app.py), [Docker](https://
 github.com/skypilot-org/skypilot/blob/master/examples/docker/echo_app.yaml),
-and [many more](./examples). More information: - [Introductory blog post]
-(https://medium.com/@zongheng_yang/skypilot-ml-and-data-science-on-any-cloud-
-with-massive-cost-savings-244189cc7c0f) ## Issues, feature requests, and
+and [many more](./examples). More information: - [SkyPilot Blog](https://
+blog.skypilot.co/) - [Introductory blog post](https://blog.skypilot.co/
+introducing-skypilot/) - [NSDI 2023 paper & talk](https://www.usenix.org/
+conference/nsdi23/presentation/yang-zongheng) ## Issues, feature requests, and
 questions We are excited to hear your feedback! * For issues and feature
 requests, please [open a GitHub issue](https://github.com/skypilot-org/
 skypilot/issues/new). * For questions, please use [GitHub Discussions](https://
 github.com/skypilot-org/skypilot/discussions). For general discussions, join us
-on the [SkyPilot Slack](https://join.slack.com/t/skypilot-org/shared_invite/zt-
-1i4pa7lyc-g6Lo4_rqqCFWOSXdvwTs3Q). ## Contributing We welcome and value all
-contributions to the project! Please refer to [CONTRIBUTING](CONTRIBUTING.md)
-for how to get involved. # [1]: While SkyPilot is currently targeted at machine
-learning workloads, it supports and has been used for other general batch
-workloads. We're excited to hear about your use case and how we can better
-support your requirements; please join us in [this discussion](https://
-github.com/skypilot-org/skypilot/discussions/1016)!
+on the [SkyPilot Slack](http://slack.skypilot.co). ## Contributing We welcome
+and value all contributions to the project! Please refer to [CONTRIBUTING]
+(CONTRIBUTING.md) for how to get involved.
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/README.md` & `skypilot-nightly-1.0.0.dev20230713/PKG-INFO`

 * *Files 20% similar despite different names*

```diff
@@ -1,54 +1,99 @@
+Metadata-Version: 2.1
+Name: skypilot-nightly
+Version: 1.0.0.dev20230713
+Summary: SkyPilot: An intercloud broker for the clouds
+Author: SkyPilot Team
+License: Apache 2.0
+Project-URL: Homepage, https://github.com/skypilot-org/skypilot
+Project-URL: Issues, https://github.com/skypilot-org/skypilot/issues
+Project-URL: Discussion, https://github.com/skypilot-org/skypilot/discussions
+Project-URL: Documentation, https://skypilot.readthedocs.io/en/latest/
+Classifier: Programming Language :: Python :: 3.7
+Classifier: Programming Language :: Python :: 3.8
+Classifier: Programming Language :: Python :: 3.9
+Classifier: Programming Language :: Python :: 3.10
+Classifier: License :: OSI Approved :: Apache Software License
+Classifier: Operating System :: OS Independent
+Classifier: Topic :: Software Development :: Libraries :: Python Modules
+Classifier: Topic :: System :: Distributed Computing
+Description-Content-Type: text/markdown
+Provides-Extra: aws
+Provides-Extra: azure
+Provides-Extra: gcp
+Provides-Extra: ibm
+Provides-Extra: docker
+Provides-Extra: lambda
+Provides-Extra: cloudflare
+Provides-Extra: scp
+Provides-Extra: oci
+Provides-Extra: all
+License-File: LICENSE
+
 <p align="center">
-  <picture>
-    <source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/skypilot-org/skypilot/master/docs/source/images/skypilot-wide-dark-1k.png">
-    <img alt="SkyPilot" src="https://raw.githubusercontent.com/skypilot-org/skypilot/master/docs/source/images/skypilot-wide-light-1k.png" width=55%>
-  </picture>
+  <img alt="SkyPilot" src="https://raw.githubusercontent.com/skypilot-org/skypilot/master/docs/source/images/skypilot-wide-light-1k.png" width=55%>
 </p>
 
 <p align="center">
   <a href="https://skypilot.readthedocs.io/en/latest/"> 
     <img alt="Documentation" src="https://readthedocs.org/projects/skypilot/badge/?version=latest">
   </a>
   
   <a href="https://github.com/skypilot-org/skypilot/releases"> 
     <img alt="GitHub Release" src="https://img.shields.io/github/release/skypilot-org/skypilot.svg">
   </a>
   
-  <a href="https://join.slack.com/t/skypilot-org/shared_invite/zt-1i4pa7lyc-g6Lo4_rqqCFWOSXdvwTs3Q"> 
+  <a href="http://slack.skypilot.co"> 
     <img alt="Join Slack" src="https://img.shields.io/badge/SkyPilot-Join%20Slack-blue?logo=slack">
   </a>
   
 </p>
 
 
 <h3 align="center">
-    Run jobs on any cloud, easily and cost effectively
+    Run LLMs and AI on Any Cloud
 </h3>
 
-SkyPilot is a framework for easily and cost effectively running ML workloads[^1] on any cloud. 
-
-SkyPilot abstracts away cloud infra burden:
-- Launch jobs & clusters on any cloud (AWS, Azure, GCP)
-- Find scarce resources across zones/regions/clouds
-- Queue jobs & use cloud object stores
-
-SkyPilot cuts your cloud costs:
-* [Managed Spot](https://skypilot.readthedocs.io/en/latest/examples/spot-jobs.html): **3x cost savings** using spot VMs, with auto-recovery from preemptions
+----
+:fire: *News* :fire:
+- [June, 2023] Serving LLM **24x Faster On the Cloud** with vLLM and SkyPilot: [**example**](./llm/vllm/), [**blog post**](https://blog.skypilot.co/serving-llm-24x-faster-on-the-cloud-with-vllm-and-skypilot/)
+- [June, 2023] [**Two new clouds supported**](https://skypilot.readthedocs.io/en/latest/getting-started/installation.html): Samsung SCP and Oracle OCI!
+- [April, 2023] **[**SkyPilot YAMLs released**](./llm/vicuna/) for finetuning & serving the Vicuna model with a single command**!
+- [March, 2023] **[Vicuna LLM chatbot](https://lmsys.org/blog/2023-03-30-vicuna/) trained** [**using SkyPilot**](./llm/vicuna/) **for $300 on spot instances!** 
+- [March, 2023] Serve your own LLaMA LLM chatbot (not finetuned) on any cloud: [**example**](./llm/llama-chatbots/), [**repo**](https://github.com/skypilot-org/sky-llama)
+----
+
+SkyPilot is a framework for running LLMs, AI, and batch jobs on any cloud, offering maximum cost savings, highest GPU availability, and managed execution.
+
+SkyPilot **abstracts away cloud infra burdens**:
+- Launch jobs & clusters on any cloud 
+- Easy scale-out: queue and run many jobs, automatically managed
+- Easy access to object stores (S3, GCS, R2)
+
+SkyPilot **maximizes GPU availability for your jobs**:
+* Provision in all zones/regions/clouds you have access to ([the _Sky_](https://arxiv.org/abs/2205.07147)), with automatic failover
+
+SkyPilot **cuts your cloud costs**:
+* [Managed Spot](https://skypilot.readthedocs.io/en/latest/examples/spot-jobs.html): 3-6x cost savings using spot VMs, with auto-recovery from preemptions
+* Optimizer: 2x cost savings by auto-picking the cheapest VM/zone/region/cloud
 * [Autostop](https://skypilot.readthedocs.io/en/latest/reference/auto-stop.html): hands-free cleanup of idle clusters 
-* [Benchmark](https://skypilot.readthedocs.io/en/latest/reference/benchmark/index.html): find best VM types for your jobs
-* Optimizer: **2x cost savings** by auto-picking best prices across zones/regions/clouds
 
 SkyPilot supports your existing GPU, TPU, and CPU workloads, with no code changes. 
 
-Install with pip (choose your clouds) or [from source](https://skypilot.readthedocs.io/en/latest/getting-started/installation.html):
+Install with pip or [from source](https://skypilot.readthedocs.io/en/latest/getting-started/installation.html):
 ```
-pip install "skypilot[aws,gcp,azure]"
+pip install "skypilot[aws,gcp,azure,ibm,oci,scp,lambda]"  # choose your clouds
 ```
 
+Current supported providers (AWS, Azure, GCP, Lambda Cloud, IBM, Samsung, OCI, Cloudflare):
+<p align="center">
+  <img alt="SkyPilot" src="https://raw.githubusercontent.com/skypilot-org/skypilot/master/docs/source/images/cloud-logos-light.png" width=80%>
+</p>
+
+
 ## Getting Started
 You can find our documentation [here](https://skypilot.readthedocs.io/en/latest/).
 - [Installation](https://skypilot.readthedocs.io/en/latest/getting-started/installation.html)
 - [Quickstart](https://skypilot.readthedocs.io/en/latest/getting-started/quickstart.html)
 - [CLI reference](https://skypilot.readthedocs.io/en/latest/reference/cli.html)
 
 ## SkyPilot in 1 minute
@@ -82,15 +127,15 @@
 ```
 
 Prepare the workdir by cloning:
 ```bash
 git clone https://github.com/pytorch/examples.git ~/torch_examples
 ```
 
-Launch with `sky launch`:
+Launch with `sky launch` (note: [access to GPU instances](https://skypilot.readthedocs.io/en/latest/reference/quota.html) is needed for this example):
 ```bash
 sky launch my_task.yaml
 ```
 
 SkyPilot then performs the heavy-lifting for you, including:
 1. Find the lowest priced VM instance type across different clouds
 2. Provision the VM, with auto-failover if the cloud returned capacity errors
@@ -110,21 +155,20 @@
 - [Documentation](https://skypilot.readthedocs.io/en/latest/)
 - [Example: HuggingFace](https://skypilot.readthedocs.io/en/latest/getting-started/tutorial.html) 
 - [Tutorials](https://github.com/skypilot-org/skypilot-tutorial) 
 - [YAML reference](https://skypilot.readthedocs.io/en/latest/reference/yaml-spec.html)
 - Framework examples: [PyTorch DDP](https://github.com/skypilot-org/skypilot/blob/master/examples/resnet_distributed_torch.yaml),  [Distributed](https://github.com/skypilot-org/skypilot/blob/master/examples/resnet_distributed_tf_app.py) [TensorFlow](https://github.com/skypilot-org/skypilot/blob/master/examples/resnet_app_storage.yaml), [JAX/Flax on TPU](https://github.com/skypilot-org/skypilot/blob/master/examples/tpu/tpuvm_mnist.yaml), [Stable Diffusion](https://github.com/skypilot-org/skypilot/tree/master/examples/stable_diffusion), [Detectron2](https://github.com/skypilot-org/skypilot/blob/master/examples/detectron2_docker.yaml), [programmatic grid search](https://github.com/skypilot-org/skypilot/blob/master/examples/huggingface_glue_imdb_grid_search_app.py), [Docker](https://github.com/skypilot-org/skypilot/blob/master/examples/docker/echo_app.yaml), and [many more](./examples).
 
 More information:
-- [Introductory blog post](https://medium.com/@zongheng_yang/skypilot-ml-and-data-science-on-any-cloud-with-massive-cost-savings-244189cc7c0f)
+- [SkyPilot Blog](https://blog.skypilot.co/)
+  - [Introductory blog post](https://blog.skypilot.co/introducing-skypilot/)
+- [NSDI 2023 paper & talk](https://www.usenix.org/conference/nsdi23/presentation/yang-zongheng)
 
 ## Issues, feature requests, and questions
 We are excited to hear your feedback! 
 * For issues and feature requests, please [open a GitHub issue](https://github.com/skypilot-org/skypilot/issues/new).
 * For questions, please use [GitHub Discussions](https://github.com/skypilot-org/skypilot/discussions).
 
-For general discussions, join us on the [SkyPilot Slack](https://join.slack.com/t/skypilot-org/shared_invite/zt-1i4pa7lyc-g6Lo4_rqqCFWOSXdvwTs3Q).
+For general discussions, join us on the [SkyPilot Slack](http://slack.skypilot.co).
 
 ## Contributing
 We welcome and value all contributions to the project! Please refer to [CONTRIBUTING](CONTRIBUTING.md) for how to get involved.
-
-<!-- Footnote -->
-[^1]: While SkyPilot is currently targeted at machine learning workloads, it supports and has been used for other general batch workloads. We're excited to hear about your use case and how we can better support your requirements; please join us in [this discussion](https://github.com/skypilot-org/skypilot/discussions/1016)!
```

#### html2text {}

```diff
@@ -1,49 +1,83 @@
-                                   [SkyPilot]
+Metadata-Version: 2.1 Name: skypilot-nightly Version: 1.0.0.dev20230713
+Summary: SkyPilot: An intercloud broker for the clouds Author: SkyPilot Team
+License: Apache 2.0 Project-URL: Homepage, https://github.com/skypilot-org/
+skypilot Project-URL: Issues, https://github.com/skypilot-org/skypilot/issues
+Project-URL: Discussion, https://github.com/skypilot-org/skypilot/discussions
+Project-URL: Documentation, https://skypilot.readthedocs.io/en/latest/
+Classifier: Programming Language :: Python :: 3.7 Classifier: Programming
+Language :: Python :: 3.8 Classifier: Programming Language :: Python :: 3.9
+Classifier: Programming Language :: Python :: 3.10 Classifier: License :: OSI
+Approved :: Apache Software License Classifier: Operating System :: OS
+Independent Classifier: Topic :: Software Development :: Libraries :: Python
+Modules Classifier: Topic :: System :: Distributed Computing Description-
+Content-Type: text/markdown Provides-Extra: aws Provides-Extra: azure Provides-
+Extra: gcp Provides-Extra: ibm Provides-Extra: docker Provides-Extra: lambda
+Provides-Extra: cloudflare Provides-Extra: scp Provides-Extra: oci Provides-
+Extra: all License-File: LICENSE
+                                  [SkyPilot]
                  [Documentation] [GitHub_Release] [Join_Slack]
-         **** Run jobs on any cloud, easily and cost effectively ****
-SkyPilot is a framework for easily and cost effectively running ML workloads
-[^1] on any cloud. SkyPilot abstracts away cloud infra burden: - Launch jobs &
-clusters on any cloud (AWS, Azure, GCP) - Find scarce resources across zones/
-regions/clouds - Queue jobs & use cloud object stores SkyPilot cuts your cloud
-costs: * [Managed Spot](https://skypilot.readthedocs.io/en/latest/examples/
-spot-jobs.html): **3x cost savings** using spot VMs, with auto-recovery from
-preemptions * [Autostop](https://skypilot.readthedocs.io/en/latest/reference/
-auto-stop.html): hands-free cleanup of idle clusters * [Benchmark](https://
-skypilot.readthedocs.io/en/latest/reference/benchmark/index.html): find best VM
-types for your jobs * Optimizer: **2x cost savings** by auto-picking best
-prices across zones/regions/clouds SkyPilot supports your existing GPU, TPU,
-and CPU workloads, with no code changes. Install with pip (choose your clouds)
-or [from source](https://skypilot.readthedocs.io/en/latest/getting-started/
-installation.html): ``` pip install "skypilot[aws,gcp,azure]" ``` ## Getting
-Started You can find our documentation [here](https://skypilot.readthedocs.io/
-en/latest/). - [Installation](https://skypilot.readthedocs.io/en/latest/
-getting-started/installation.html) - [Quickstart](https://
-skypilot.readthedocs.io/en/latest/getting-started/quickstart.html) - [CLI
-reference](https://skypilot.readthedocs.io/en/latest/reference/cli.html) ##
-SkyPilot in 1 minute A SkyPilot task specifies: resource requirements, data to
-be synced, setup commands, and the task commands. Once written in this
-[**unified interface**](https://skypilot.readthedocs.io/en/latest/reference/
-yaml-spec.html) (YAML or Python API), the task can be launched on any available
-cloud. This avoids vendor lock-in, and allows easily moving jobs to a different
-provider. Paste the following into a file `my_task.yaml`: ```yaml resources:
-accelerators: V100:1 # 1x NVIDIA V100 GPU num_nodes: 1 # Number of VMs to
-launch # Working directory (optional) containing the project codebase. # Its
-contents are synced to ~/sky_workdir/ on the cluster. workdir: ~/torch_examples
-# Commands to be run before executing the job. # Typical use: pip install -
-r requirements.txt, git clone, etc. setup: | pip install torch torchvision #
-Commands to run as a job. # Typical use: launch the main program. run: | cd
-mnist python main.py --epochs 1 ``` Prepare the workdir by cloning: ```bash git
-clone https://github.com/pytorch/examples.git ~/torch_examples ``` Launch with
-`sky launch`: ```bash sky launch my_task.yaml ``` SkyPilot then performs the
-heavy-lifting for you, including: 1. Find the lowest priced VM instance type
-across different clouds 2. Provision the VM, with auto-failover if the cloud
-returned capacity errors 3. Sync the local `workdir` to the VM 4. Run the
-task's `setup` commands to prepare the VM for running the task 5. Run the
-task's `run` commands
+                    **** Run LLMs and AI on Any Cloud ****
+---- :fire: *News* :fire: - [June, 2023] Serving LLM **24x Faster On the
+Cloud** with vLLM and SkyPilot: [**example**](./llm/vllm/), [**blog post**]
+(https://blog.skypilot.co/serving-llm-24x-faster-on-the-cloud-with-vllm-and-
+skypilot/) - [June, 2023] [**Two new clouds supported**](https://
+skypilot.readthedocs.io/en/latest/getting-started/installation.html): Samsung
+SCP and Oracle OCI! - [April, 2023] **[**SkyPilot YAMLs released**](./llm/
+vicuna/) for finetuning & serving the Vicuna model with a single command**! -
+[March, 2023] **[Vicuna LLM chatbot](https://lmsys.org/blog/2023-03-30-vicuna/
+) trained** [**using SkyPilot**](./llm/vicuna/) **for $300 on spot instances!**
+- [March, 2023] Serve your own LLaMA LLM chatbot (not finetuned) on any cloud:
+[**example**](./llm/llama-chatbots/), [**repo**](https://github.com/skypilot-
+org/sky-llama) ---- SkyPilot is a framework for running LLMs, AI, and batch
+jobs on any cloud, offering maximum cost savings, highest GPU availability, and
+managed execution. SkyPilot **abstracts away cloud infra burdens**: - Launch
+jobs & clusters on any cloud - Easy scale-out: queue and run many jobs,
+automatically managed - Easy access to object stores (S3, GCS, R2) SkyPilot
+**maximizes GPU availability for your jobs**: * Provision in all zones/regions/
+clouds you have access to ([the _Sky_](https://arxiv.org/abs/2205.07147)), with
+automatic failover SkyPilot **cuts your cloud costs**: * [Managed Spot](https:/
+/skypilot.readthedocs.io/en/latest/examples/spot-jobs.html): 3-6x cost savings
+using spot VMs, with auto-recovery from preemptions * Optimizer: 2x cost
+savings by auto-picking the cheapest VM/zone/region/cloud * [Autostop](https://
+skypilot.readthedocs.io/en/latest/reference/auto-stop.html): hands-free cleanup
+of idle clusters SkyPilot supports your existing GPU, TPU, and CPU workloads,
+with no code changes. Install with pip or [from source](https://
+skypilot.readthedocs.io/en/latest/getting-started/installation.html): ``` pip
+install "skypilot[aws,gcp,azure,ibm,oci,scp,lambda]" # choose your clouds ```
+Current supported providers (AWS, Azure, GCP, Lambda Cloud, IBM, Samsung, OCI,
+Cloudflare):
+                                  [SkyPilot]
+## Getting Started You can find our documentation [here](https://
+skypilot.readthedocs.io/en/latest/). - [Installation](https://
+skypilot.readthedocs.io/en/latest/getting-started/installation.html) -
+[Quickstart](https://skypilot.readthedocs.io/en/latest/getting-started/
+quickstart.html) - [CLI reference](https://skypilot.readthedocs.io/en/latest/
+reference/cli.html) ## SkyPilot in 1 minute A SkyPilot task specifies: resource
+requirements, data to be synced, setup commands, and the task commands. Once
+written in this [**unified interface**](https://skypilot.readthedocs.io/en/
+latest/reference/yaml-spec.html) (YAML or Python API), the task can be launched
+on any available cloud. This avoids vendor lock-in, and allows easily moving
+jobs to a different provider. Paste the following into a file `my_task.yaml`:
+```yaml resources: accelerators: V100:1 # 1x NVIDIA V100 GPU num_nodes: 1 #
+Number of VMs to launch # Working directory (optional) containing the project
+codebase. # Its contents are synced to ~/sky_workdir/ on the cluster. workdir:
+~/torch_examples # Commands to be run before executing the job. # Typical use:
+pip install -r requirements.txt, git clone, etc. setup: | pip install torch
+torchvision # Commands to run as a job. # Typical use: launch the main program.
+run: | cd mnist python main.py --epochs 1 ``` Prepare the workdir by cloning:
+```bash git clone https://github.com/pytorch/examples.git ~/torch_examples ```
+Launch with `sky launch` (note: [access to GPU instances](https://
+skypilot.readthedocs.io/en/latest/reference/quota.html) is needed for this
+example): ```bash sky launch my_task.yaml ``` SkyPilot then performs the heavy-
+lifting for you, including: 1. Find the lowest priced VM instance type across
+different clouds 2. Provision the VM, with auto-failover if the cloud returned
+capacity errors 3. Sync the local `workdir` to the VM 4. Run the task's `setup`
+commands to prepare the VM for running the task 5. Run the task's `run`
+commands
                                 [SkyPilot Demo]
 Refer to [Quickstart](https://skypilot.readthedocs.io/en/latest/getting-
 started/quickstart.html) to get started with SkyPilot. ## Learn more -
 [Documentation](https://skypilot.readthedocs.io/en/latest/) - [Example:
 HuggingFace](https://skypilot.readthedocs.io/en/latest/getting-started/
 tutorial.html) - [Tutorials](https://github.com/skypilot-org/skypilot-tutorial)
 - [YAML reference](https://skypilot.readthedocs.io/en/latest/reference/yaml-
@@ -55,22 +89,18 @@
 (https://github.com/skypilot-org/skypilot/blob/master/examples/tpu/
 tpuvm_mnist.yaml), [Stable Diffusion](https://github.com/skypilot-org/skypilot/
 tree/master/examples/stable_diffusion), [Detectron2](https://github.com/
 skypilot-org/skypilot/blob/master/examples/detectron2_docker.yaml),
 [programmatic grid search](https://github.com/skypilot-org/skypilot/blob/
 master/examples/huggingface_glue_imdb_grid_search_app.py), [Docker](https://
 github.com/skypilot-org/skypilot/blob/master/examples/docker/echo_app.yaml),
-and [many more](./examples). More information: - [Introductory blog post]
-(https://medium.com/@zongheng_yang/skypilot-ml-and-data-science-on-any-cloud-
-with-massive-cost-savings-244189cc7c0f) ## Issues, feature requests, and
+and [many more](./examples). More information: - [SkyPilot Blog](https://
+blog.skypilot.co/) - [Introductory blog post](https://blog.skypilot.co/
+introducing-skypilot/) - [NSDI 2023 paper & talk](https://www.usenix.org/
+conference/nsdi23/presentation/yang-zongheng) ## Issues, feature requests, and
 questions We are excited to hear your feedback! * For issues and feature
 requests, please [open a GitHub issue](https://github.com/skypilot-org/
 skypilot/issues/new). * For questions, please use [GitHub Discussions](https://
 github.com/skypilot-org/skypilot/discussions). For general discussions, join us
-on the [SkyPilot Slack](https://join.slack.com/t/skypilot-org/shared_invite/zt-
-1i4pa7lyc-g6Lo4_rqqCFWOSXdvwTs3Q). ## Contributing We welcome and value all
-contributions to the project! Please refer to [CONTRIBUTING](CONTRIBUTING.md)
-for how to get involved.  [^1]: While SkyPilot is currently targeted at machine
-learning workloads, it supports and has been used for other general batch
-workloads. We're excited to hear about your use case and how we can better
-support your requirements; please join us in [this discussion](https://
-github.com/skypilot-org/skypilot/discussions/1016)!
+on the [SkyPilot Slack](http://slack.skypilot.co). ## Contributing We welcome
+and value all contributions to the project! Please refer to [CONTRIBUTING]
+(CONTRIBUTING.md) for how to get involved.
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/setup.py` & `skypilot-nightly-1.0.0.dev20230713/setup.py`

 * *Files 20% similar despite different names*

```diff
@@ -14,14 +14,15 @@
 """
 
 import io
 import os
 import platform
 import re
 import warnings
+from typing import Dict, List
 
 import setuptools
 
 ROOT_DIR = os.path.dirname(__file__)
 
 system = platform.system()
 if system == 'Darwin':
@@ -60,63 +61,93 @@
         re.MULTILINE)
     readme = mode_re.sub(r'<img\1>', readme)
     return readme
 
 
 install_requires = [
     'wheel',
-    # NOTE: ray 2.0.1 requires click<=8.0.4,>=7.0; We disable the
-    # shell completion for click<8.0 for backward compatibility.
+    # NOTE: ray requires click>=7.0. Also, click 8.1.x makes our rendered CLI
+    # docs display weird blockquotes.
+    # TODO(zongheng): investigate how to make click 8.1.x display nicely and
+    # remove the upper bound.
     'click<=8.0.4,>=7.0',
     # NOTE: required by awscli. To avoid ray automatically installing
     # the latest version.
     'colorama<0.4.5',
     'cryptography',
-    'jinja2',
+    # Jinja has a bug in older versions because of the lack of pinning
+    # the version of the underlying markupsafe package. See:
+    # https://github.com/pallets/jinja/issues/1585
+    'jinja2>=3.0',
     'jsonschema',
     'networkx',
     'oauth2client',
     'pandas',
     'pendulum',
-    'PrettyTable',
-    # Lower local ray version is not fully supported, due to the
-    # autoscaler issues (also tracked in #537).
-    'ray[default]>=1.9.0,<=2.0.1',
+    # PrettyTable with version >=2.0.0 is required for the support of
+    # `add_rows` method.
+    'PrettyTable>=2.0.0',
+    # Lower version of ray will cause dependency conflict for
+    # click/grpcio/protobuf.
+    'ray[default]>=2.2.0,<=2.4.0',
     'rich',
     'tabulate',
-    'filelock',  # TODO(mraheja): Enforce >=3.6.0 when python version is >= 3.7
-    # This is used by ray. The latest 1.44.0 will generate an error
-    # `Fork support is only compatible with the epoll1 and poll
-    # polling strategies`
-    'grpcio>=1.32.0,<=1.43.0',
+    # Light weight requirement, can be replaced with "typing" once
+    # we deprecate Python 3.7 (this will take a while).
+    "typing_extensions; python_version < '3.8'",
+    'filelock>=3.6.0',
+    # Adopted from ray's setup.py: https://github.com/ray-project/ray/blob/ray-2.4.0/python/setup.py
+    # SkyPilot: != 1.48.0 is required to avoid the error where ray dashboard fails to start when
+    # ray start is called (#2054).
+    # Tracking issue: https://github.com/ray-project/ray/issues/30984
+    "grpcio >= 1.32.0, <= 1.49.1, != 1.48.0; python_version < '3.10' and sys_platform == 'darwin'",  # noqa:E501
+    "grpcio >= 1.42.0, <= 1.49.1, != 1.48.0; python_version >= '3.10' and sys_platform == 'darwin'",  # noqa:E501
+    # Original issue: https://github.com/ray-project/ray/issues/33833
+    "grpcio >= 1.32.0, <= 1.51.3, != 1.48.0; python_version < '3.10' and sys_platform != 'darwin'",  # noqa:E501
+    "grpcio >= 1.42.0, <= 1.51.3, != 1.48.0; python_version >= '3.10' and sys_platform != 'darwin'",  # noqa:E501
     'packaging',
-    # The latest 4.21.1 will break ray. Enforce < 4.0.0 until Ray releases the
-    # fix.
-    # https://github.com/ray-project/ray/pull/25211
-    'protobuf<4.0.0',
+    # Adopted from ray's setup.py:
+    # https://github.com/ray-project/ray/blob/86fab1764e618215d8131e8e5068f0d493c77023/python/setup.py#L326
+    'protobuf >= 3.15.3, != 3.19.5',
     'psutil',
     'pulp',
+    # Ray job has an issue with pydantic>2.0.0, due to API changes of pydantic. See
+    # https://github.com/ray-project/ray/issues/36990
+    'pydantic<2.0'
 ]
 
-# NOTE: Change the templates/spot-controller.yaml.j2 file if any of the following
-# packages dependencies are changed.
-extras_require = {
-    'aws': [
-        'awscli',
-        'boto3',
-        # 'Crypto' module used in authentication.py for AWS.
-        'pycryptodome==3.12.0',
-    ],
+# NOTE: Change the templates/spot-controller.yaml.j2 file if any of the
+# following packages dependencies are changed.
+aws_dependencies = [
+    # NOTE: this installs CLI V1. To use AWS SSO (e.g., `aws sso login`), users
+    # should instead use CLI V2 which is not pip-installable. See
+    # https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html.
+    'awscli',
+    'boto3',
+    # 'Crypto' module used in authentication.py for AWS.
+    'pycryptodome==3.12.0',
+]
+extras_require: Dict[str, List[str]] = {
+    'aws': aws_dependencies,
     # TODO(zongheng): azure-cli is huge and takes a long time to install.
     # Tracked in: https://github.com/Azure/azure-cli/issues/7387
-    # azure-cli need to be pinned to 2.31.0 due to later versions
-    # do not have azure-identity (used in node_provider) installed
-    'azure': ['azure-cli==2.31.0', 'azure-core'],
+    # azure-identity is needed in node_provider.
+    # We need azure-identity>=1.13.0 to enable the customization of the
+    # timeout of AzureCliCredential.
+    'azure': [
+        'azure-cli>=2.31.0', 'azure-core', 'azure-identity>=1.13.0',
+        'azure-mgmt-network'
+    ],
     'gcp': ['google-api-python-client', 'google-cloud-storage'],
+    'ibm': ['ibm-cloud-sdk-core', 'ibm-vpc', 'ibm-platform-services'],
     'docker': ['docker'],
+    'lambda': [],
+    'cloudflare': aws_dependencies,
+    'scp': [],
+    'oci': ['oci'],
 }
 
 extras_require['all'] = sum(extras_require.values(), [])
 
 # Install aws requirements by default, as it is the most common cloud provider,
 # and the installation is quick.
 install_requires += extras_require['aws']
@@ -139,23 +170,22 @@
     author='SkyPilot Team',
     license='Apache 2.0',
     readme='README.md',
     description='SkyPilot: An intercloud broker for the clouds',
     long_description=long_description,
     long_description_content_type='text/markdown',
     setup_requires=['wheel'],
-    requires_python='>=3.6',
+    requires_python='>=3.7',
     install_requires=install_requires,
     extras_require=extras_require,
     entry_points={
         'console_scripts': ['sky = sky.cli:cli'],
     },
     include_package_data=True,
     classifiers=[
-        'Programming Language :: Python :: 3.6',
         'Programming Language :: Python :: 3.7',
         'Programming Language :: Python :: 3.8',
         'Programming Language :: Python :: 3.9',
         'Programming Language :: Python :: 3.10',
         'License :: OSI Approved :: Apache Software License',
         'Operating System :: OS Independent',
         'Topic :: Software Development :: Libraries :: Python Modules',
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/__init__.py` & `skypilot-nightly-1.0.0.dev20230713/sky/__init__.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,45 +1,54 @@
 """The SkyPilot package."""
 import os
 
 # Replaced with the current commit when building the wheels.
-__commit__ = 'af6629d9de3ce4b98f8da3c4da06f881f73785ef'
-__version__ = '1.0.0-dev20221126'
+__commit__ = '5864399925058b0ffdd68d4c9319bd2ea3610912'
+__version__ = '1.0.0-dev20230713'
 __root_dir__ = os.path.dirname(os.path.abspath(__file__))
 
 # Keep this order to avoid cyclic imports
 from sky import backends
 from sky import benchmark
 from sky import clouds
 from sky.clouds.service_catalog import list_accelerators
 from sky.dag import Dag
 from sky.execution import launch, exec, spot_launch  # pylint: disable=redefined-builtin
 from sky.resources import Resources
 from sky.task import Task
 from sky.optimizer import Optimizer, OptimizeTarget
 from sky.data import Storage, StorageMode, StoreType
-from sky.global_user_state import ClusterStatus
+from sky.status_lib import ClusterStatus
 from sky.skylet.job_lib import JobStatus
 from sky.core import (status, start, stop, down, autostop, queue, cancel,
                       tail_logs, download_logs, job_status, spot_queue,
-                      spot_status, spot_cancel, storage_ls, storage_delete)
+                      spot_status, spot_cancel, storage_ls, storage_delete,
+                      cost_report)
 
 # Aliases.
+IBM = clouds.IBM
 AWS = clouds.AWS
 Azure = clouds.Azure
 GCP = clouds.GCP
+Lambda = clouds.Lambda
+SCP = clouds.SCP
 Local = clouds.Local
+OCI = clouds.OCI
 optimize = Optimizer.optimize
 
 __all__ = [
     '__version__',
+    'IBM',
     'AWS',
     'Azure',
     'GCP',
+    'Lambda',
+    'SCP',
     'Local',
+    'OCI',
     'Optimizer',
     'OptimizeTarget',
     'backends',
     'benchmark',
     'list_accelerators',
     '__root_dir__',
     'Storage',
@@ -57,14 +66,15 @@
     'spot_launch',
     # core APIs
     'status',
     'start',
     'stop',
     'down',
     'autostop',
+    'cost_report',
     # core APIs Job Management
     'queue',
     'cancel',
     'tail_logs',
     'download_logs',
     'job_status',
     # core APIs Spot Job Management
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/adaptors/azure.py` & `skypilot-nightly-1.0.0.dev20230713/sky/adaptors/azure.py`

 * *Files 24% similar despite different names*

```diff
@@ -9,22 +9,29 @@
 def import_package(func):
 
     @wraps(func)
     def wrapper(*args, **kwargs):
         global azure
         if azure is None:
             try:
-                import azure as _azure
+                import azure as _azure  # type: ignore
                 azure = _azure
             except ImportError:
                 raise ImportError('Fail to import dependencies for Azure.'
                                   'Try pip install "skypilot[azure]"') from None
         return func(*args, **kwargs)
 
     return wrapper
 
 
 @import_package
 def get_subscription_id() -> str:
     """Get the default subscription id."""
     from azure.common import credentials
     return credentials.get_cli_profile().get_subscription_id()
+
+
+@import_package
+def get_current_account_user() -> str:
+    """Get the default account user."""
+    from azure.common import credentials
+    return credentials.get_cli_profile().get_current_account_user()
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/adaptors/docker.py` & `skypilot-nightly-1.0.0.dev20230713/sky/adaptors/docker.py`

 * *Files identical despite different names*

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/adaptors/gcp.py` & `skypilot-nightly-1.0.0.dev20230713/sky/adaptors/gcp.py`

 * *Files 14% similar despite different names*

```diff
@@ -15,16 +15,16 @@
         if googleapiclient is None or google is None:
             try:
                 import googleapiclient as _googleapiclient
                 import google as _google
                 googleapiclient = _googleapiclient
                 google = _google
             except ImportError:
-                raise ImportError('Fail to import dependencies for GCP.'
-                                  'Try pip install "skypilot[gcp]"') from None
+                raise ImportError('Failed to import dependencies for GCP. '
+                                  'Try: pip install "skypilot[gcp]"') from None
         return func(*args, **kwargs)
 
     return wrapper
 
 
 @import_package
 def build(service_name: str, version: str, *args, **kwargs):
@@ -64,7 +64,21 @@
 
 
 @import_package
 def forbidden_exception():
     """Forbidden exception."""
     from google.api_core import exceptions as gcs_exceptions
     return gcs_exceptions.Forbidden
+
+
+@import_package
+def http_error_exception():
+    """HttpError exception."""
+    from googleapiclient import errors
+    return errors.HttpError
+
+
+@import_package
+def credential_error_exception():
+    """CredentialError exception."""
+    from google.auth import exceptions
+    return exceptions.DefaultCredentialsError
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/authentication.py` & `skypilot-nightly-1.0.0.dev20230713/sky/utils/common_utils.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,296 +1,402 @@
-"""Module to enable a single SkyPilot key for all VMs in each cloud."""
-import copy
+"""Utils shared between all of sky"""
+
 import functools
+import getpass
+import hashlib
+import inspect
+import json
 import os
+import platform
+import random
 import re
-import subprocess
+import socket
 import sys
-import textwrap
 import time
-from typing import Any, Dict, Tuple
+from typing import Any, Callable, Dict, List, Optional, Union
+import uuid
+import yaml
 
 import colorama
-from cryptography.hazmat.primitives import serialization
-from cryptography.hazmat.primitives.asymmetric import rsa
-from cryptography.hazmat.backends import default_backend
 
-from sky import clouds
 from sky import sky_logging
-from sky.adaptors import gcp
-from sky.utils import common_utils
-from sky.utils import subprocess_utils
-from sky.utils import ux_utils
+
+_USER_HASH_FILE = os.path.expanduser('~/.sky/user_hash')
+USER_HASH_LENGTH = 8
+
+_COLOR_PATTERN = re.compile(r'\x1b[^m]*m')
+
+_PAYLOAD_PATTERN = re.compile(r'<sky-payload>(.*)</sky-payload>')
+_PAYLOAD_STR = '<sky-payload>{}</sky-payload>'
 
 logger = sky_logging.init_logger(__name__)
 
-# TODO: Should tolerate if gcloud is not installed. Also,
-# https://pypi.org/project/google-api-python-client/ recommends
-# using Cloud Client Libraries for Python, where possible, for new code
-# development.
-
-MAX_TRIALS = 64
-# TODO(zhwu): Support user specified key pair.
-PRIVATE_SSH_KEY_PATH = '~/.ssh/sky-key'
-PUBLIC_SSH_KEY_PATH = '~/.ssh/sky-key.pub'
-
-
-def _generate_rsa_key_pair() -> Tuple[str, str]:
-    key = rsa.generate_private_key(backend=default_backend(),
-                                   public_exponent=65537,
-                                   key_size=2048)
-
-    private_key = key.private_bytes(
-        encoding=serialization.Encoding.PEM,
-        format=serialization.PrivateFormat.TraditionalOpenSSL,
-        encryption_algorithm=serialization.NoEncryption()).decode('utf-8')
-
-    public_key = key.public_key().public_bytes(
-        serialization.Encoding.OpenSSH,
-        serialization.PublicFormat.OpenSSH).decode('utf-8')
-
-    return public_key, private_key
-
-
-def _save_key_pair(private_key_path: str, public_key_path: str,
-                   private_key: str, public_key: str) -> None:
-    private_key_dir = os.path.dirname(private_key_path)
-    os.makedirs(private_key_dir, exist_ok=True)
-
-    with open(
-            private_key_path,
-            'w',
-            opener=functools.partial(os.open, mode=0o600),
-    ) as f:
-        f.write(private_key)
-
-    with open(public_key_path, 'w') as f:
-        f.write(public_key)
-
-
-def get_or_generate_keys() -> Tuple[str, str]:
-    """Returns the aboslute public and private key paths."""
-    private_key_path = os.path.expanduser(PRIVATE_SSH_KEY_PATH)
-    public_key_path = os.path.expanduser(PUBLIC_SSH_KEY_PATH)
-    if not os.path.exists(private_key_path):
-        public_key, private_key = _generate_rsa_key_pair()
-        _save_key_pair(private_key_path, public_key_path, private_key,
-                       public_key)
-    else:
-        assert os.path.exists(public_key_path)
-    return private_key_path, public_key_path
+_usage_run_id = None
+
 
+def get_usage_run_id() -> str:
+    """Returns a unique run id for each 'run'.
 
-def setup_aws_authentication(config: Dict[str, Any]) -> Dict[str, Any]:
-    _, public_key_path = get_or_generate_keys()
-    with open(public_key_path, 'r') as f:
-        public_key = f.read()
-    # Use cloud init in UserData to set up the authorized_keys to get
-    # around the number of keys limit and permission issues with
-    # ec2.describe_key_pairs.
-    for node_type in config['available_node_types']:
-        config['available_node_types'][node_type]['node_config']['UserData'] = (
-            textwrap.dedent(f"""\
-            #cloud-config
-            users:
-            - name: {config['auth']['ssh_user']}
-              ssh-authorized-keys:
-                - {public_key}
-            """))
+    A run is defined as the lifetime of a process that has imported `sky`
+    and has called its CLI or programmatic APIs. For example, two successive
+    `sky launch` are two runs.
+    """
+    global _usage_run_id
+    if _usage_run_id is None:
+        _usage_run_id = str(uuid.uuid4())
+    return _usage_run_id
+
+
+def get_user_hash(default_value: Optional[str] = None) -> str:
+    """Returns a unique user-machine specific hash as a user id.
+
+    We cache the user hash in a file to avoid potential user_name or
+    hostname changes causing a new user hash to be generated.
+    """
+
+    def _is_valid_user_hash(user_hash: Optional[str]) -> bool:
+        if user_hash is None:
+            return False
+        try:
+            int(user_hash, 16)
+        except (TypeError, ValueError):
+            return False
+        return len(user_hash) == USER_HASH_LENGTH
+
+    user_hash = default_value
+    if _is_valid_user_hash(user_hash):
+        assert user_hash is not None
+        return user_hash
+
+    if os.path.exists(_USER_HASH_FILE):
+        # Read from cached user hash file.
+        with open(_USER_HASH_FILE, 'r') as f:
+            # Remove invalid characters.
+            user_hash = f.read().strip()
+        if _is_valid_user_hash(user_hash):
+            return user_hash
+
+    hash_str = user_and_hostname_hash()
+    user_hash = hashlib.md5(hash_str.encode()).hexdigest()[:USER_HASH_LENGTH]
+    if not _is_valid_user_hash(user_hash):
+        # A fallback in case the hash is invalid.
+        user_hash = uuid.uuid4().hex[:USER_HASH_LENGTH]
+    os.makedirs(os.path.dirname(_USER_HASH_FILE), exist_ok=True)
+    with open(_USER_HASH_FILE, 'w') as f:
+        f.write(user_hash)
+    return user_hash
+
+
+def get_global_job_id(job_timestamp: str,
+                      cluster_name: Optional[str],
+                      job_id: str,
+                      task_id: Optional[int] = None) -> str:
+    """Returns a unique job run id for each job run.
+
+    A job run is defined as the lifetime of a job that has been launched.
+    """
+    global_job_id = f'{job_timestamp}_{cluster_name}_id-{job_id}'
+    if task_id is not None:
+        global_job_id += f'-{task_id}'
+    return global_job_id
+
+
+class Backoff:
+    """Exponential backoff with jittering."""
+    MULTIPLIER = 1.6
+    JITTER = 0.4
+
+    def __init__(self, initial_backoff: int = 5, max_backoff_factor: int = 5):
+        self._initial = True
+        self._backoff = 0.0
+        self._inital_backoff = initial_backoff
+        self._max_backoff = max_backoff_factor * self._inital_backoff
+
+    # https://github.com/grpc/grpc/blob/2d4f3c56001cd1e1f85734b2f7c5ce5f2797c38a/doc/connection-backoff.md
+    # https://github.com/grpc/grpc/blob/5fc3ff82032d0ebc4bf252a170ebe66aacf9ed9d/src/core/lib/backoff/backoff.cc
+
+    def current_backoff(self) -> float:
+        """Backs off once and returns the current backoff in seconds."""
+        if self._initial:
+            self._initial = False
+            self._backoff = min(self._inital_backoff, self._max_backoff)
+        else:
+            self._backoff = min(self._backoff * self.MULTIPLIER,
+                                self._max_backoff)
+        self._backoff += random.uniform(-self.JITTER * self._backoff,
+                                        self.JITTER * self._backoff)
+        return self._backoff
+
+
+def get_pretty_entry_point() -> str:
+    """Returns the prettified entry point of this process (sys.argv).
+
+    Example return values:
+        $ sky launch app.yaml  # 'sky launch app.yaml'
+        $ sky gpunode  # 'sky gpunode'
+        $ python examples/app.py  # 'app.py'
+    """
+    argv = sys.argv
+    basename = os.path.basename(argv[0])
+    if basename == 'sky':
+        # Turn '/.../anaconda/envs/py36/bin/sky' into 'sky', but keep other
+        # things like 'examples/app.py'.
+        argv[0] = basename
+    return ' '.join(argv)
+
+
+def user_and_hostname_hash() -> str:
+    """Returns a string containing <user>-<hostname hash last 4 chars>.
+
+    For uniquefying user clusters on shared-account cloud providers. Also used
+    for AWS security group.
+
+    Using uuid.getnode() instead of gethostname() is incorrect; observed to
+    collide on Macs.
+
+    NOTE: BACKWARD INCOMPATIBILITY NOTES
+
+    Changing this string will render AWS clusters shown in `sky status`
+    unreusable and potentially cause leakage:
+
+    - If a cluster is STOPPED, any command restarting it (`sky launch`, `sky
+      start`) will launch a NEW cluster.
+    - If a cluster is UP, a `sky launch` command reusing it will launch a NEW
+      cluster. The original cluster will be stopped and thus leaked from Sky's
+      perspective.
+    - `sky down/stop/exec` on these pre-change clusters still works, if no new
+      clusters with the same name have been launched.
+
+    The reason is AWS security group names are derived from this string, and
+    thus changing the SG name makes these clusters unrecognizable.
+    """
+    hostname_hash = hashlib.md5(socket.gethostname().encode()).hexdigest()[-4:]
+    return f'{getpass.getuser()}-{hostname_hash}'
+
+
+def read_yaml(path) -> Dict[str, Any]:
+    with open(path, 'r') as f:
+        config = yaml.safe_load(f)
     return config
 
 
-# Reference:
-# https://github.com/ray-project/ray/blob/master/python/ray/autoscaler/_private/gcp/config.py
-def _wait_for_compute_global_operation(project_name: str, operation_name: str,
-                                       compute: Any) -> None:
-    """Poll for global compute operation until finished."""
-    logger.debug('wait_for_compute_global_operation: '
-                 'Waiting for operation {} to finish...'.format(operation_name))
-    max_polls = 10
-    poll_interval = 5
-    for _ in range(max_polls):
-        result = compute.globalOperations().get(
-            project=project_name,
-            operation=operation_name,
-        ).execute()
-        if 'error' in result:
-            raise Exception(result['error'])
-
-        if result['status'] == 'DONE':
-            logger.debug('wait_for_compute_global_operation: '
-                         'Operation done.')
-            break
-        time.sleep(poll_interval)
-    return result
-
-
-# Snippets of code inspired from
-# https://github.com/ray-project/ray/blob/master/python/ray/autoscaler/_private/gcp/config.py
-# Takes in config, a yaml dict and outputs a postprocessed dict
-# TODO(weilin): refactor the implementation to incorporate Ray autoscaler to
-# avoid duplicated codes.
-# Retry for the GCP as sometimes there will be connection reset by peer error.
-@common_utils.retry
-@gcp.import_package
-def setup_gcp_authentication(config: Dict[str, Any]) -> Dict[str, Any]:
-    private_key_path, public_key_path = get_or_generate_keys()
-    config = copy.deepcopy(config)
-
-    project_id = config['provider']['project_id']
-    compute = gcp.build('compute',
-                        'v1',
-                        credentials=None,
-                        cache_discovery=False)
-    user = config['auth']['ssh_user']
+def read_yaml_all(path: str) -> List[Dict[str, Any]]:
+    with open(path, 'r') as f:
+        config = yaml.safe_load_all(f)
+        configs = list(config)
+        if not configs:
+            # Empty YAML file.
+            return [{}]
+        return configs
+
+
+def dump_yaml(path, config) -> None:
+    with open(path, 'w') as f:
+        f.write(dump_yaml_str(config))
+
+
+def dump_yaml_str(config):
+    # https://github.com/yaml/pyyaml/issues/127
+    class LineBreakDumper(yaml.SafeDumper):
+
+        def write_line_break(self, data=None):
+            super().write_line_break(data)
+            if len(self.indents) == 1:
+                super().write_line_break()
 
-    try:
-        project = compute.projects().get(project=project_id).execute()
-    except gcp.googleapiclient.errors.HttpError as e:
-        # Can happen for a new project where Compute Engine API is disabled.
-        #
-        # Example message:
-        # 'Compute Engine API has not been used in project 123456 before
-        # or it is disabled. Enable it by visiting
-        # https://console.developers.google.com/apis/api/compute.googleapis.com/overview?project=123456
-        # then retry. If you enabled this API recently, wait a few minutes for
-        # the action to propagate to our systems and retry.'
-        if ' API has not been used in project' in e.reason:
-            match = re.fullmatch(r'(.+)(https://.*project=\d+) (.+)', e.reason)
-            if match is None:
-                raise  # This should not happen.
-            yellow = colorama.Fore.YELLOW
-            reset = colorama.Style.RESET_ALL
-            bright = colorama.Style.BRIGHT
-            dim = colorama.Style.DIM
-            logger.error(
-                f'{yellow}Certain GCP APIs are disabled for the GCP project '
-                f'{project_id}.{reset}')
-            logger.error(f'{yellow}Enable them by running:{reset} '
-                         f'{bright}sky check{reset}')
-            logger.error('Details:')
-            logger.error(f'{dim}{match.group(1)}{reset}\n'
-                         f'{dim}    {match.group(2)}{reset}\n'
-                         f'{dim}{match.group(3)}{reset}')
-            sys.exit(1)
-        else:
-            raise
+    if isinstance(config, list):
+        dump_func = yaml.dump_all
+    else:
+        dump_func = yaml.dump
+    return dump_func(config,
+                     Dumper=LineBreakDumper,
+                     sort_keys=False,
+                     default_flow_style=False)
+
+
+def make_decorator(cls, name_or_fn: Union[str, Callable], **ctx_kwargs):
+    """Make the cls a decorator.
+
+    class cls:
+        def __init__(self, name, **kwargs):
+            pass
+        def __enter__(self):
+            pass
+        def __exit__(self, exc_type, exc_value, traceback):
+            pass
+
+    Args:
+        name_or_fn: The name of the event or the function to be wrapped.
+        message: The message attached to the event.
+    """
+    if isinstance(name_or_fn, str):
+
+        def _wrapper(f):
+
+            @functools.wraps(f)
+            def _record(*args, **kwargs):
+                nonlocal name_or_fn
+                with cls(name_or_fn, **ctx_kwargs):
+                    return f(*args, **kwargs)
+
+            return _record
 
-    project_oslogin = next(
-        (item for item in project['commonInstanceMetadata'].get('items', [])
-         if item['key'] == 'enable-oslogin'), {}).get('value', 'False')
-
-    if project_oslogin.lower() == 'true':
-        # project.
-        logger.info(
-            f'OS Login is enabled for GCP project {project_id}. Running '
-            'additional authentication steps.')
-        # Read the account information from the credential file, since the user
-        # should be set according the account, when the oslogin is enabled.
-        config_path = os.path.expanduser(clouds.gcp.GCP_CONFIG_PATH)
-        sky_backup_config_path = os.path.expanduser(
-            clouds.gcp.GCP_CONFIG_SKY_BACKUP_PATH)
-        assert os.path.exists(sky_backup_config_path), (
-            'GCP credential backup file '
-            f'{sky_backup_config_path!r} does not exist.')
-
-        with open(sky_backup_config_path, 'r') as infile:
-            for line in infile:
-                if line.startswith('account'):
-                    account = line.split('=')[1].strip()
-                    break
+        return _wrapper
+    else:
+        if not inspect.isfunction(name_or_fn):
+            raise ValueError(
+                'Should directly apply the decorator to a function.')
+
+        @functools.wraps(name_or_fn)
+        def _record(*args, **kwargs):
+            nonlocal name_or_fn
+            f = name_or_fn
+            func_name = getattr(f, '__qualname__', f.__name__)
+            module_name = getattr(f, '__module__', '')
+            if module_name:
+                full_name = f'{module_name}.{func_name}'
             else:
-                with ux_utils.print_exception_no_traceback():
-                    raise RuntimeError(
-                        'GCP authentication failed, as the oslogin is enabled '
-                        f'but the file {config_path} does not contain the '
-                        'account information.')
-        config['auth']['ssh_user'] = account.replace('@', '_').replace('.', '_')
-
-        # Add ssh key to GCP with oslogin
-        subprocess.run(
-            'gcloud compute os-login ssh-keys add '
-            f'--key-file={public_key_path}',
-            check=True,
-            shell=True,
-            stdout=subprocess.DEVNULL)
-        # Enable ssh port for all the instances
-        enable_ssh_cmd = ('gcloud compute firewall-rules create '
-                          'allow-ssh-ingress-from-iap '
-                          '--direction=INGRESS '
-                          '--action=allow '
-                          '--rules=tcp:22 '
-                          '--source-ranges=0.0.0.0/0')
-        proc = subprocess.run(enable_ssh_cmd,
-                              check=False,
-                              shell=True,
-                              stdout=subprocess.DEVNULL,
-                              stderr=subprocess.PIPE)
-        if proc.returncode != 0 and 'already exists' not in proc.stderr.decode(
-                'utf-8'):
-            subprocess_utils.handle_returncode(proc.returncode, enable_ssh_cmd,
-                                               'Failed to enable ssh port.',
-                                               proc.stderr)
-        return config
-
-    # OS Login is not enabled for the project. Add the ssh key directly to the
-    # metadata.
-    # TODO(zhwu): Use cloud init to add ssh public key, to avoid the permission
-    # issue.
-    project_keys = next(
-        (item for item in project['commonInstanceMetadata'].get('items', [])
-         if item['key'] == 'ssh-keys'), {}).get('value', '')
-    ssh_keys = project_keys.split('\n') if project_keys else []
-
-    # Get public key from file.
-    with open(public_key_path, 'r') as f:
-        public_key = f.read()
-
-    # Check if ssh key in Google Project's metadata
-    public_key_token = public_key.split(' ')[1]
-
-    key_found = False
-    for key in ssh_keys:
-        key_list = key.split(' ')
-        if len(key_list) != 3:
-            continue
-        if user == key_list[-1] and os.path.exists(
-                private_key_path) and key_list[1] == public_key.split(' ')[1]:
-            key_found = True
-
-    if not key_found:
-        new_ssh_key = '{user}:ssh-rsa {public_key_token} {user}'.format(
-            user=user, public_key_token=public_key_token)
-        metadata = project['commonInstanceMetadata'].get('items', [])
-
-        ssh_key_index = [
-            k for k, v in enumerate(metadata) if v['key'] == 'ssh-keys'
-        ]
-        assert len(ssh_key_index) <= 1
+                full_name = func_name
+            with cls(full_name, **ctx_kwargs):
+                return f(*args, **kwargs)
 
-        if len(ssh_key_index) == 0:
-            metadata.append({'key': 'ssh-keys', 'value': new_ssh_key})
-        else:
-            ssh_key_index = ssh_key_index[0]
-            metadata[ssh_key_index]['value'] += '\n' + new_ssh_key
+        return _record
 
-        project['commonInstanceMetadata']['items'] = metadata
 
-        operation = compute.projects().setCommonInstanceMetadata(
-            project=project['name'],
-            body=project['commonInstanceMetadata']).execute()
-        _wait_for_compute_global_operation(project['name'], operation['name'],
-                                           compute)
-    return config
+def retry(method, max_retries=3, initial_backoff=1):
+    """Retry a function up to max_retries times with backoff between retries."""
 
+    @functools.wraps(method)
+    def method_with_retries(*args, **kwargs):
+        backoff = Backoff(initial_backoff)
+        try_count = 0
+        while try_count < max_retries:
+            try:
+                return method(*args, **kwargs)
+            except Exception as e:  # pylint: disable=broad-except
+                try_count += 1
+                if try_count < max_retries:
+                    logger.warning(f'Caught {e}. Retrying.')
+                    time.sleep(backoff.current_backoff())
+                else:
+                    raise
 
-def setup_azure_authentication(config: Dict[str, Any]) -> Dict[str, Any]:
-    get_or_generate_keys()
-    # Need to use ~ relative path because Ray uses the same
-    # path for finding the public key path on both local and head node.
-    config['auth']['ssh_public_key'] = PUBLIC_SSH_KEY_PATH
-
-    file_mounts = config['file_mounts']
-    file_mounts[PUBLIC_SSH_KEY_PATH] = PUBLIC_SSH_KEY_PATH
-    config['file_mounts'] = file_mounts
+    return method_with_retries
 
-    return config
+
+def encode_payload(payload: Any) -> str:
+    """Encode a payload to make it more robust for parsing.
+
+    The make the message transfer more robust to any additional
+    strings added to the message during transfering.
+
+    An example message that is polluted by the system warning:
+    "LC_ALL: cannot change locale (en_US.UTF-8)\n<sky-payload>hello, world</sky-payload>" # pylint: disable=line-too-long
+
+    Args:
+        payload: A str, dict or list to be encoded.
+
+    Returns:
+        A string that is encoded from the payload.
+    """
+    payload_str = json.dumps(payload)
+    payload_str = _PAYLOAD_STR.format(payload_str)
+    return payload_str
+
+
+def decode_payload(payload_str: str) -> Any:
+    """Decode a payload string.
+
+    Args:
+        payload_str: A string that is encoded from a payload.
+
+    Returns:
+        A str, dict or list that is decoded from the payload string.
+    """
+    matched = _PAYLOAD_PATTERN.findall(payload_str)
+    if not matched:
+        raise ValueError(f'Invalid payload string: \n{payload_str}')
+    payload_str = matched[0]
+    payload = json.loads(payload_str)
+    return payload
+
+
+def class_fullname(cls):
+    """Get the full name of a class.
+
+    Example:
+        >>> e = sky.exceptions.FetchIPError()
+        >>> class_fullname(e.__class__)
+        'sky.exceptions.FetchIPError'
+
+    Args:
+        cls: The class to get the full name.
+
+    Returns:
+        The full name of the class.
+    """
+    return f'{cls.__module__}.{cls.__name__}'
+
+
+def format_exception(e: Union[Exception, SystemExit],
+                     use_bracket: bool = False) -> str:
+    """Format an exception to a string.
+
+    Args:
+        e: The exception to format.
+
+    Returns:
+        A string that represents the exception.
+    """
+    bright = colorama.Style.BRIGHT
+    reset = colorama.Style.RESET_ALL
+    if use_bracket:
+        return f'{bright}[{class_fullname(e.__class__)}]{reset} {e}'
+    return f'{bright}{class_fullname(e.__class__)}:{reset} {e}'
+
+
+def remove_color(s: str):
+    """Remove color from a string.
+
+    Args:
+        s: The string to remove color.
+
+    Returns:
+        A string without color.
+    """
+    return _COLOR_PATTERN.sub('', s)
+
+
+def remove_file_if_exists(path: str):
+    """Delete a file if it exists.
+
+    Args:
+        path: The path to the file.
+    """
+    try:
+        os.remove(path)
+    except FileNotFoundError:
+        logger.debug(f'Tried to remove {path} but failed to find it. Skip.')
+        pass
+
+
+def is_wsl() -> bool:
+    """Detect if running under Windows Subsystem for Linux (WSL)."""
+    return 'microsoft' in platform.uname()[3].lower()
+
+
+def find_free_port(start_port: int) -> int:
+    """Finds first free local port starting with 'start_port'.
+
+    Returns: a free local port.
+
+    Raises:
+      OSError: If no free ports are available.
+    """
+    for port in range(start_port, 65535):
+        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
+            try:
+                s.bind(('', port))
+                return port
+            except OSError:
+                pass
+    raise OSError('No free ports available.')
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/backends/backend.py` & `skypilot-nightly-1.0.0.dev20230713/sky/backends/backend.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,145 +1,153 @@
 """Sky backend interface."""
 import typing
-from typing import Dict, Optional
+from typing import Dict, Generic, Optional
 
 import sky
 from sky.utils import timeline
 from sky.usage import usage_lib
 
 if typing.TYPE_CHECKING:
     from sky import resources
     from sky import task as task_lib
     from sky.data import storage as storage_lib
 
 Path = str
+_ResourceHandleType = typing.TypeVar('_ResourceHandleType',
+                                     bound='ResourceHandle')
 
 
-class Backend:
+# Backend-specific handle to the launched resources (e.g., a cluster).
+# Examples: 'cluster.yaml'; 'ray://...', 'k8s://...'.
+class ResourceHandle:
+
+    def get_cluster_name(self) -> str:
+        raise NotImplementedError
+
+
+class Backend(Generic[_ResourceHandleType]):
     """Backend interface: handles provisioning, setup, and scheduling."""
 
     # NAME is used to identify the backend class from cli/yaml.
     NAME = 'backend'
 
-    # Backend-specific handle to the launched resources (e.g., a cluster).
-    # Examples: 'cluster.yaml'; 'ray://...', 'k8s://...'.
-    class ResourceHandle:
-
-        def get_cluster_name(self) -> str:
-            raise NotImplementedError
+    # Backward compatibility, with the old name of the handle.
+    ResourceHandle = ResourceHandle  # pylint: disable=invalid-name
 
     # --- APIs ---
-    def check_resources_fit_cluster(self, handle: ResourceHandle,
+    def check_resources_fit_cluster(self, handle: _ResourceHandleType,
                                     task: 'task_lib.Task') -> None:
         """Check whether resources of the task are satisfied by cluster."""
         raise NotImplementedError
 
     @timeline.event
     @usage_lib.messages.usage.update_runtime('provision')
-    def provision(self,
-                  task: 'task_lib.Task',
-                  to_provision: Optional['resources.Resources'],
-                  dryrun: bool,
-                  stream_logs: bool,
-                  cluster_name: Optional[str] = None,
-                  retry_until_up: bool = False) -> ResourceHandle:
+    def provision(
+            self,
+            task: 'task_lib.Task',
+            to_provision: Optional['resources.Resources'],
+            dryrun: bool,
+            stream_logs: bool,
+            cluster_name: Optional[str] = None,
+            retry_until_up: bool = False) -> Optional[_ResourceHandleType]:
         if cluster_name is None:
             cluster_name = sky.backends.backend_utils.generate_cluster_name()
         usage_lib.record_cluster_name_for_current_operation(cluster_name)
         usage_lib.messages.usage.update_actual_task(task)
         return self._provision(task, to_provision, dryrun, stream_logs,
                                cluster_name, retry_until_up)
 
     @timeline.event
     @usage_lib.messages.usage.update_runtime('sync_workdir')
-    def sync_workdir(self, handle: ResourceHandle, workdir: Path) -> None:
+    def sync_workdir(self, handle: _ResourceHandleType, workdir: Path) -> None:
         return self._sync_workdir(handle, workdir)
 
     @timeline.event
     @usage_lib.messages.usage.update_runtime('sync_file_mounts')
     def sync_file_mounts(
         self,
-        handle: ResourceHandle,
+        handle: _ResourceHandleType,
         all_file_mounts: Dict[Path, Path],
         storage_mounts: Dict[Path, 'storage_lib.Storage'],
     ) -> None:
         return self._sync_file_mounts(handle, all_file_mounts, storage_mounts)
 
     @timeline.event
     @usage_lib.messages.usage.update_runtime('setup')
-    def setup(self, handle: ResourceHandle, task: 'task_lib.Task',
+    def setup(self, handle: _ResourceHandleType, task: 'task_lib.Task',
               detach_setup: bool) -> None:
         return self._setup(handle, task, detach_setup)
 
     def add_storage_objects(self, task: 'task_lib.Task') -> None:
         raise NotImplementedError
 
     @timeline.event
     @usage_lib.messages.usage.update_runtime('execute')
-    def execute(self, handle: ResourceHandle, task: 'task_lib.Task',
+    def execute(self, handle: _ResourceHandleType, task: 'task_lib.Task',
                 detach_run: bool) -> None:
         usage_lib.record_cluster_name_for_current_operation(
             handle.get_cluster_name())
         usage_lib.messages.usage.update_actual_task(task)
         return self._execute(handle, task, detach_run)
 
     @timeline.event
-    def post_execute(self, handle: ResourceHandle, down: bool) -> None:
+    def post_execute(self, handle: _ResourceHandleType, down: bool) -> None:
         """Post execute(): e.g., print helpful inspection messages."""
         return self._post_execute(handle, down)
 
     @timeline.event
     def teardown_ephemeral_storage(self, task: 'task_lib.Task') -> None:
         return self._teardown_ephemeral_storage(task)
 
     @timeline.event
     @usage_lib.messages.usage.update_runtime('teardown')
     def teardown(self,
-                 handle: ResourceHandle,
+                 handle: _ResourceHandleType,
                  terminate: bool,
                  purge: bool = False) -> None:
         self._teardown(handle, terminate, purge)
 
     def register_info(self, **kwargs) -> None:
         """Register backend-specific information."""
         pass
 
     # --- Implementations of the APIs ---
-    def _provision(self,
-                   task: 'task_lib.Task',
-                   to_provision: Optional['resources.Resources'],
-                   dryrun: bool,
-                   stream_logs: bool,
-                   cluster_name: str,
-                   retry_until_up: bool = False) -> ResourceHandle:
+    def _provision(
+            self,
+            task: 'task_lib.Task',
+            to_provision: Optional['resources.Resources'],
+            dryrun: bool,
+            stream_logs: bool,
+            cluster_name: str,
+            retry_until_up: bool = False) -> Optional[_ResourceHandleType]:
         raise NotImplementedError
 
-    def _sync_workdir(self, handle: ResourceHandle, workdir: Path) -> None:
+    def _sync_workdir(self, handle: _ResourceHandleType, workdir: Path) -> None:
         raise NotImplementedError
 
     def _sync_file_mounts(
         self,
-        handle: ResourceHandle,
+        handle: _ResourceHandleType,
         all_file_mounts: Dict[Path, Path],
         storage_mounts: Dict[Path, 'storage_lib.Storage'],
     ) -> None:
         raise NotImplementedError
 
-    def _setup(self, handle: ResourceHandle, task: 'task_lib.Task',
+    def _setup(self, handle: _ResourceHandleType, task: 'task_lib.Task',
                detach_setup: bool) -> None:
         raise NotImplementedError
 
-    def _execute(self, handle: ResourceHandle, task: 'task_lib.Task',
+    def _execute(self, handle: _ResourceHandleType, task: 'task_lib.Task',
                  detach_run: bool) -> None:
         raise NotImplementedError
 
-    def _post_execute(self, handle: ResourceHandle, down: bool) -> None:
+    def _post_execute(self, handle: _ResourceHandleType, down: bool) -> None:
         raise NotImplementedError
 
     def _teardown_ephemeral_storage(self, task: 'task_lib.Task') -> None:
         raise NotImplementedError
 
     def _teardown(self,
-                  handle: ResourceHandle,
+                  handle: _ResourceHandleType,
                   terminate: bool,
                   purge: bool = False):
         raise NotImplementedError
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/backends/backend_utils.py` & `skypilot-nightly-1.0.0.dev20230713/sky/backends/backend_utils.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,72 +1,74 @@
 """Util constants/functions for the backends."""
-import contextlib
-import copy
 from datetime import datetime
 import difflib
 import enum
 import getpass
 import json
 import os
 import pathlib
 import re
 import subprocess
 import tempfile
 import textwrap
-import threading
 import time
 import typing
-from typing import Any, Dict, List, Optional, Set, Tuple
+from typing import (Any, Dict, List, Optional, Sequence, Set, Tuple, Union)
+from typing_extensions import Literal
 import uuid
 
 import colorama
 import filelock
 import jinja2
 import jsonschema
 from packaging import version
 import requests
 from requests import adapters
 from requests.packages.urllib3.util import retry as retry_lib
-from ray.autoscaler._private import commands as ray_commands
-from ray.autoscaler._private import util as ray_util
-import rich.console as rich_console
 import rich.progress as rich_progress
 import yaml
 
 import sky
 from sky import authentication as auth
 from sky import backends
 from sky import check as sky_check
 from sky import clouds
 from sky import exceptions
 from sky import global_user_state
+from sky import skypilot_config
 from sky import sky_logging
 from sky import spot as spot_lib
+from sky import status_lib
 from sky.backends import onprem_utils
 from sky.skylet import constants
 from sky.skylet import log_lib
 from sky.utils import common_utils
 from sky.utils import command_runner
+from sky.utils import env_options
+from sky.utils import log_utils
 from sky.utils import subprocess_utils
 from sky.utils import timeline
+from sky.utils import tpu_utils
 from sky.utils import ux_utils
 from sky.utils import validator
 from sky.usage import usage_lib
 
 if typing.TYPE_CHECKING:
     from sky import resources
     from sky import task as task_lib
+    from sky.backends import cloud_vm_ray_backend
+    from sky.backends import local_docker_backend
 
 logger = sky_logging.init_logger(__name__)
-console = rich_console.Console()
 
 # NOTE: keep in sync with the cluster template 'file_mounts'.
 SKY_REMOTE_APP_DIR = '~/.sky/sky_app'
 SKY_RAY_YAML_REMOTE_PATH = '~/.sky/sky_ray.yml'
-IP_ADDR_REGEX = r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}'
+# Exclude subnet mask from IP address regex.
+IP_ADDR_REGEX = r'\b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}(?!/\d{1,2})\b'
 SKY_REMOTE_PATH = '~/.sky/wheels'
 SKY_USER_FILE_PATH = '~/.sky/generated'
 
 BOLD = '\033[1m'
 RESET_BOLD = '\033[0m'
 
 # Do not use /tmp because it gets cleared on VM restart.
@@ -78,57 +80,71 @@
 # Intentionally not using prefix 'rf' for the string format because yapf have a
 # bug with python=3.6.
 # 10.133.0.5: ray.worker.default,
 _LAUNCHING_IP_PATTERN = re.compile(
     r'({}): ray[._]worker[._]default'.format(IP_ADDR_REGEX))
 WAIT_HEAD_NODE_IP_MAX_ATTEMPTS = 3
 
-# We use fixed IP address to avoid DNS lookup blocking the check, for machine
-# with no internet connection.
+# We check network connection by going through _TEST_IP_LIST. We may need to
+# check multiple IPs because some IPs may be blocked on certain networks.
+# Fixed IP addresses are used to avoid DNS lookup blocking the check, for
+# machine with no internet connection.
 # Refer to: https://stackoverflow.com/questions/3764291/how-can-i-see-if-theres-an-available-and-active-network-connection-in-python # pylint: disable=line-too-long
-_TEST_IP = 'https://8.8.8.8'
-
-# GCP has a 63 char limit; however, Ray autoscaler adds many
-# characters. Through testing, this is the maximum length for the Sky cluster
-# name on GCP.  Ref:
-# https://cloud.google.com/compute/docs/naming-resources#resource-name-format
-# NOTE: actually 37 is maximum for a single-node cluster which gets the suffix
-# '-head', but 35 for a multinode cluster because workers get the suffix
-# '-worker'. Here we do not distinguish these cases and take the lower limit.
-_MAX_CLUSTER_NAME_LEN_FOR_GCP = 35
+_TEST_IP_LIST = ['https://1.1.1.1', 'https://8.8.8.8']
 
 # Allow each CPU thread take 2 tasks.
 # Note: This value cannot be too small, otherwise OOM issue may occur.
 DEFAULT_TASK_CPU_DEMAND = 0.5
 
-# Mapping from reserved cluster names to the corresponding group name (logging purpose).
+# Mapping from reserved cluster names to the corresponding group name (logging
+# purpose).
 # NOTE: each group can only have one reserved cluster name for now.
-SKY_RESERVED_CLUSTER_NAMES = {
+SKY_RESERVED_CLUSTER_NAMES: Dict[str, str] = {
     spot_lib.SPOT_CONTROLLER_NAME: 'Managed spot controller'
 }
 
 # Filelocks for the cluster status change.
 CLUSTER_STATUS_LOCK_PATH = os.path.expanduser('~/.sky/.{}.lock')
 CLUSTER_STATUS_LOCK_TIMEOUT_SECONDS = 20
 
 # Remote dir that holds our runtime files.
 _REMOTE_RUNTIME_FILES_DIR = '~/.sky/.runtime_files'
 
-# Include the fields that will be used for generating tags that  distinguishes the
-# cluster in ray, to avoid the stopped cluster being discarded due to updates in
-# the yaml template.
+# Include the fields that will be used for generating tags that distinguishes
+# the cluster in ray, to avoid the stopped cluster being discarded due to
+# updates in the yaml template.
 # Some notes on the fields:
-# - 'provider' fields will be used for bootstrapping and insert more new items in
-#   'node_config'.
-# - keeping the auth is not enough becuase the content of the key file will be used
-#   for calculating the hash.
+# - 'provider' fields will be used for bootstrapping and insert more new items
+#   in 'node_config'.
+# - keeping the auth is not enough becuase the content of the key file will be
+#   used for calculating the hash.
 # TODO(zhwu): Keep in sync with the fields used in https://github.com/ray-project/ray/blob/e4ce38d001dbbe09cd21c497fedd03d692b2be3e/python/ray/autoscaler/_private/commands.py#L687-L701
 _RAY_YAML_KEYS_TO_RESTORE_FOR_BACK_COMPATIBILITY = {
     'cluster_name', 'provider', 'auth', 'node_config'
 }
+# For these keys, don't use the old yaml's version and instead use the new yaml's.
+#  - zone: The zone field of the old yaml may be '1a,1b,1c' (AWS) while the actual
+#    zone of the launched cluster is '1a'. If we restore, then on capacity errors
+#    it's possible to failover to 1b, which leaves a leaked instance in 1a. Here,
+#    we use the new yaml's zone field, which is guaranteed to be the existing zone
+#    '1a'.
+# - UserData: The UserData field of the old yaml may be outdated, and we want to
+#   use the new yaml's UserData field, which contains the authorized key setup as
+#   well as the disabling of the auto-update with apt-get.
+_RAY_YAML_KEYS_TO_RESTORE_EXCEPTIONS = [
+    ('provider', 'availability_zone'),
+    ('available_node_types', 'ray.head.default', 'node_config', 'UserData'),
+    ('available_node_types', 'ray.worker.default', 'node_config', 'UserData'),
+]
+
+# Command that calls `ray status` with SkyPilot's Ray port set.
+RAY_STATUS_WITH_SKY_RAY_PORT_COMMAND = (
+    'RAY_PORT=$(python -c "from sky.skylet import job_lib; '
+    'print(job_lib.get_ray_port())" 2> /dev/null || echo 6379);'
+    'RAY_ADDRESS=127.0.0.1:$RAY_PORT ray status')
 
 
 def is_ip(s: str) -> bool:
     """Returns whether this string matches IP_ADDR_REGEX."""
     return len(re.findall(IP_ADDR_REGEX, s)) == 1
 
 
@@ -136,38 +152,31 @@
                                      prefix: str = SKY_USER_FILE_PATH) -> str:
     output_path = pathlib.Path(
         prefix).expanduser().resolve() / f'{cluster_name}.yml'
     os.makedirs(output_path.parents[0], exist_ok=True)
     return str(output_path)
 
 
-def fill_template(template_name: str,
-                  variables: Dict,
-                  output_path: Optional[str] = None,
-                  output_prefix: str = SKY_USER_FILE_PATH) -> str:
+def fill_template(template_name: str, variables: Dict,
+                  output_path: str) -> None:
     """Create a file from a Jinja template and return the filename."""
     assert template_name.endswith('.j2'), template_name
     template_path = os.path.join(sky.__root_dir__, 'templates', template_name)
     if not os.path.exists(template_path):
         raise FileNotFoundError(f'Template "{template_name}" does not exist.')
     with open(template_path) as fin:
         template = fin.read()
-    if output_path is None:
-        assert ('cluster_name' in variables), ('cluster_name is required.')
-        cluster_name = variables.get('cluster_name')
-        output_path = _get_yaml_path_from_cluster_name(cluster_name,
-                                                       output_prefix)
-    output_path = os.path.abspath(output_path)
+    output_path = os.path.abspath(os.path.expanduser(output_path))
+    os.makedirs(os.path.dirname(output_path), exist_ok=True)
 
     # Write out yaml config.
-    template = jinja2.Template(template)
-    content = template.render(**variables)
+    j2_template = jinja2.Template(template)
+    content = j2_template.render(**variables)
     with open(output_path, 'w') as fout:
         fout.write(content)
-    return output_path
 
 
 def _optimize_file_mounts(yaml_path: str) -> None:
     """Optimize file mounts in the given ray yaml file.
 
     Runtime files handling:
     List of runtime files to be uploaded to cluster:
@@ -182,33 +191,39 @@
     # Remove the file mounts added by the newline.
     if '' in file_mounts:
         assert file_mounts[''] == '', file_mounts['']
         file_mounts.pop('')
 
     # Putting these in file_mounts hurts provisioning speed, as each file
     # opens/closes an SSH connection.  Instead, we:
-    #  - cp locally them into a directory
+    #  - cp them locally into a directory, each with a unique name to avoid
+    #    basename conflicts
     #  - upload that directory as a file mount (1 connection)
     #  - use a remote command to move all runtime files to their right places.
 
     # Local tmp dir holding runtime files.
     local_runtime_files_dir = tempfile.mkdtemp()
     new_file_mounts = {_REMOTE_RUNTIME_FILES_DIR: local_runtime_files_dir}
 
+    # Generate local_src -> unique_name.
+    local_source_to_unique_name = {}
+    for local_src in file_mounts.values():
+        local_source_to_unique_name[local_src] = str(uuid.uuid4())
+
     # (For remote) Build a command that copies runtime files to their right
     # destinations.
     # NOTE: we copy rather than move, because when launching >1 node, head node
     # is fully set up first, and if moving then head node's files would already
     # move out of _REMOTE_RUNTIME_FILES_DIR, which would cause setting up
     # workers (from the head's files) to fail.  An alternative is softlink
     # (then we need to make sure the usage of runtime files follow links).
     commands = []
     basenames = set()
     for dst, src in file_mounts.items():
-        src_basename = os.path.basename(src)
+        src_basename = local_source_to_unique_name[src]
         dst_basename = os.path.basename(dst)
         dst_parent_dir = os.path.dirname(dst)
 
         # Validate by asserts here as these files are added by our backend.
         # Our runtime files (wheel, yaml, credentials) do not have backslashes.
         assert not src.endswith('/'), src
         assert not dst.endswith('/'), dst
@@ -216,16 +231,17 @@
             f'Duplicated src basename: {src_basename}; mounts: {file_mounts}')
         basenames.add(src_basename)
         # Our runtime files (wheel, yaml, credentials) are not relative paths.
         assert dst_parent_dir, f'Found relative destination path: {dst}'
 
         mkdir_parent = f'mkdir -p {dst_parent_dir}'
         if os.path.isdir(os.path.expanduser(src)):
-            # Special case for directories. If the dst already exists as a folder,
-            # directly copy the folder will create a subfolder under the dst.
+            # Special case for directories. If the dst already exists as a
+            # folder, directly copy the folder will create a subfolder under
+            # the dst.
             mkdir_parent = f'mkdir -p {dst}'
             src_basename = f'{src_basename}/*'
         mv = (f'cp -r {_REMOTE_RUNTIME_FILES_DIR}/{src_basename} '
               f'{dst_parent_dir}/{dst_basename}')
         fragment = f'({mkdir_parent} && {mv})'
         commands.append(fragment)
     postprocess_runtime_files_command = ' && '.join(commands)
@@ -236,45 +252,67 @@
             0] = f'{postprocess_runtime_files_command}; {setup_commands[0]}'
     else:
         setup_commands = [postprocess_runtime_files_command]
 
     yaml_config['file_mounts'] = new_file_mounts
     yaml_config['setup_commands'] = setup_commands
 
-    # (For local) Move all runtime files, including the just-written yaml, to
+    # (For local) Copy all runtime files, including the just-written yaml, to
     # local_runtime_files_dir/.
-    all_local_sources = ''
+    # < 0.3s to cp 6 clouds' credentials.
     for local_src in file_mounts.values():
+        # cp <local_src> <local_runtime_files_dir>/<unique name of local_src>.
         full_local_src = str(pathlib.Path(local_src).expanduser())
-        # Add quotes for paths containing spaces.
-        all_local_sources += f'{full_local_src!r} '
-    # Takes 10-20 ms on laptop incl. 3 clouds' credentials.
-    subprocess.run(f'cp -r {all_local_sources} {local_runtime_files_dir}/',
-                   shell=True,
-                   check=True)
+        unique_name = local_source_to_unique_name[local_src]
+        # !r to add quotes for paths containing spaces.
+        subprocess.run(
+            f'cp -r {full_local_src!r} {local_runtime_files_dir}/{unique_name}',
+            shell=True,
+            check=True)
 
     common_utils.dump_yaml(yaml_path, yaml_config)
 
 
 def path_size_megabytes(path: str) -> int:
-    """Returns the size of 'path' (directory or file) in megabytes."""
+    """Returns the size of 'path' (directory or file) in megabytes.
+
+    Returns:
+        If successful: the size of 'path' in megabytes, rounded down. Otherwise,
+        -1.
+    """
     resolved_path = pathlib.Path(path).expanduser().resolve()
     git_exclude_filter = ''
     if (resolved_path / command_runner.GIT_EXCLUDE).exists():
         # Ensure file exists; otherwise, rsync will error out.
         git_exclude_filter = command_runner.RSYNC_EXCLUDE_OPTION.format(
             str(resolved_path / command_runner.GIT_EXCLUDE))
-    rsync_output = str(
-        subprocess.check_output(
-            f'rsync {command_runner.RSYNC_DISPLAY_OPTION} '
-            f'{command_runner.RSYNC_FILTER_OPTION} '
-            f'{git_exclude_filter} --dry-run {path!r}',
-            shell=True).splitlines()[-1])
-    total_bytes = rsync_output.split(' ')[3].replace(',', '')
-    return int(total_bytes) // 10**6
+    rsync_command = (f'rsync {command_runner.RSYNC_DISPLAY_OPTION} '
+                     f'{command_runner.RSYNC_FILTER_OPTION} '
+                     f'{git_exclude_filter} --dry-run {path!r}')
+    rsync_output = ''
+    try:
+        rsync_output = str(subprocess.check_output(rsync_command, shell=True))
+    except subprocess.CalledProcessError:
+        logger.debug('Command failed, proceeding without estimating size: '
+                     f'{rsync_command}')
+        return -1
+    # 3.2.3:
+    #  total size is 250,957,728  speedup is 330.19 (DRY RUN)
+    # 2.6.9:
+    #  total size is 212627556  speedup is 2437.41
+    match = re.search(r'total size is ([\d,]+)', rsync_output)
+    if match is not None:
+        try:
+            total_bytes = int(float(match.group(1).replace(',', '')))
+            return total_bytes // (1024**2)
+        except ValueError:
+            logger.debug('Failed to find "total size" in rsync output. Inspect '
+                         f'output of the following command: {rsync_command}')
+            pass  # Maybe different rsync versions have different output.
+    return -1
 
 
 class FileMountHelper(object):
     """Helper for handling file mounts."""
 
     @classmethod
     def wrap_file_mount(cls, path: str) -> str:
@@ -357,26 +395,41 @@
 
     ssh_conf_path = '~/.ssh/config'
     ssh_conf_lock_path = os.path.expanduser('~/.sky/ssh_config.lock')
     ssh_multinode_path = SKY_USER_FILE_PATH + '/ssh/{}'
 
     @classmethod
     def _get_generated_config(cls, autogen_comment: str, host_name: str,
-                              ip: str, username: str, ssh_key_path: str):
+                              ip: str, username: str, ssh_key_path: str,
+                              proxy_command: Optional[str]):
+        if proxy_command is not None:
+            proxy = f'ProxyCommand {proxy_command}'
+        else:
+            proxy = ''
+        # StrictHostKeyChecking=no skips the host key check for the first
+        # time. UserKnownHostsFile=/dev/null and GlobalKnownHostsFile/dev/null
+        # prevent the host key from being added to the known_hosts file and
+        # always return an empty file for known hosts, making the ssh think
+        # this is a first-time connection, and thus skipping the host key
+        # check.
         codegen = textwrap.dedent(f"""\
             {autogen_comment}
             Host {host_name}
               HostName {ip}
               User {username}
               IdentityFile {ssh_key_path}
               IdentitiesOnly yes
               ForwardAgent yes
               StrictHostKeyChecking no
+              UserKnownHostsFile=/dev/null
+              GlobalKnownHostsFile=/dev/null
               Port 22
-            """)
+              {proxy}
+            """.rstrip())
+        codegen = codegen + '\n'
         return codegen
 
     @classmethod
     @timeline.FileLockEvent(ssh_conf_lock_path)
     def add_cluster(
         cls,
         cluster_name: str,
@@ -391,15 +444,16 @@
 
         If a host with `cluster_name` already exists and the configuration was
         added by sky (e.g. a spot instance), then the configuration is
         overwritten.
 
         Args:
             cluster_name: Cluster name (see `sky status`)
-            ips: List of IP addresses in the cluster. First IP is head node.
+            ips: List of public IP addresses in the cluster. First IP is head
+              node.
             auth_config: read_yaml(handle.cluster_yaml)['auth']
         """
         username = auth_config['ssh_user']
         key_path = os.path.expanduser(auth_config['ssh_private_key'])
         host_name = cluster_name
         sky_autogen_comment = ('# Added by sky (use `sky stop/down '
                                f'{cluster_name}` to remove)')
@@ -411,37 +465,38 @@
         if os.path.exists(config_path):
             with open(config_path) as f:
                 config = f.readlines()
 
             # If an existing config with `cluster_name` exists, raise a warning.
             for i, line in enumerate(config):
                 if line.strip() == f'Host {cluster_name}':
-                    prev_line = config[i - 1] if i - 1 > 0 else ''
+                    prev_line = config[i - 1] if i - 1 >= 0 else ''
                     if prev_line.strip().startswith(sky_autogen_comment):
                         overwrite = True
                         overwrite_begin_idx = i - 1
                     else:
                         logger.warning(f'{cls.ssh_conf_path} contains '
                                        f'host named {cluster_name}.')
                         host_name = ip
                         logger.warning(f'Using {ip} to identify host instead.')
 
                 if line.strip() == f'Host {ip}':
-                    prev_line = config[i - 1] if i - 1 > 0 else ''
+                    prev_line = config[i - 1] if i - 1 >= 0 else ''
                     if prev_line.strip().startswith(sky_autogen_comment):
                         overwrite = True
                         overwrite_begin_idx = i - 1
         else:
             config = ['\n']
             with open(config_path, 'w') as f:
                 f.writelines(config)
             os.chmod(config_path, 0o644)
 
+        proxy_command = auth_config.get('ssh_proxy_command', None)
         codegen = cls._get_generated_config(sky_autogen_comment, host_name, ip,
-                                            username, key_path)
+                                            username, key_path, proxy_command)
 
         # Add (or overwrite) the new config.
         if overwrite:
             assert overwrite_begin_idx is not None
             updated_lines = codegen.splitlines(keepends=True) + ['\n']
             config[overwrite_begin_idx:overwrite_begin_idx +
                    len(updated_lines)] = updated_lines
@@ -464,30 +519,35 @@
             SSHConfigHelper._add_multinode_config(cluster_name, ips[1:],
                                                   auth_config)
 
     @classmethod
     def _add_multinode_config(
         cls,
         cluster_name: str,
-        worker_ips: List[str],
+        external_worker_ips: List[str],
         auth_config: Dict[str, str],
     ):
         username = auth_config['ssh_user']
         key_path = os.path.expanduser(auth_config['ssh_private_key'])
         host_name = cluster_name
         sky_autogen_comment = ('# Added by sky (use `sky stop/down '
                                f'{cluster_name}` to remove)')
 
-        overwrites = [False] * len(worker_ips)
-        overwrite_begin_idxs = [None] * len(worker_ips)
-        codegens = [None] * len(worker_ips)
+        # Ensure stableness of the aliases worker-<i> by sorting based on
+        # public IPs.
+        external_worker_ips = list(sorted(external_worker_ips))
+
+        overwrites = [False] * len(external_worker_ips)
+        overwrite_begin_idxs: List[Optional[int]] = [None
+                                                    ] * len(external_worker_ips)
+        codegens: List[Optional[str]] = [None] * len(external_worker_ips)
         worker_names = []
         extra_path_name = cls.ssh_multinode_path.format(cluster_name)
 
-        for idx in range(len(worker_ips)):
+        for idx in range(len(external_worker_ips)):
             worker_names.append(cluster_name + f'-worker{idx+1}')
 
         config_path = os.path.expanduser(cls.ssh_conf_path)
         with open(config_path) as f:
             config = f.readlines()
 
         extra_config_path = os.path.expanduser(extra_path_name)
@@ -515,53 +575,56 @@
                     f.write(''.join(config).strip())
                     f.write('\n' * 2)
                 break
 
         with open(config_path) as f:
             config = f.readlines()
 
+        proxy_command = auth_config.get('ssh_proxy_command', None)
+
         # Check if ~/.ssh/config contains existing names
         host_lines = [f'Host {c_name}' for c_name in worker_names]
         for i, line in enumerate(config):
             if line.strip() in host_lines:
                 idx = host_lines.index(line.strip())
                 prev_line = config[i - 1] if i > 0 else ''
                 logger.warning(f'{cls.ssh_conf_path} contains '
                                f'host named {worker_names[idx]}.')
-                host_name = worker_ips[idx]
+                host_name = external_worker_ips[idx]
                 logger.warning(f'Using {host_name} to identify host instead.')
                 codegens[idx] = cls._get_generated_config(
-                    sky_autogen_comment, host_name, worker_ips[idx], username,
-                    key_path)
+                    sky_autogen_comment, host_name, external_worker_ips[idx],
+                    username, key_path, proxy_command)
 
         # All workers go to SKY_USER_FILE_PATH/ssh/{cluster_name}
         for i, line in enumerate(extra_config):
             if line.strip() in host_lines:
                 idx = host_lines.index(line.strip())
                 prev_line = extra_config[i - 1] if i > 0 else ''
                 if prev_line.strip().startswith(sky_autogen_comment):
                     host_name = worker_names[idx]
                     overwrites[idx] = True
                     overwrite_begin_idxs[idx] = i - 1
                 codegens[idx] = cls._get_generated_config(
-                    sky_autogen_comment, host_name, worker_ips[idx], username,
-                    key_path)
+                    sky_autogen_comment, host_name, external_worker_ips[idx],
+                    username, key_path, proxy_command)
 
         # This checks if all codegens have been created.
-        for idx, ip in enumerate(worker_ips):
+        for idx, ip in enumerate(external_worker_ips):
             if not codegens[idx]:
                 codegens[idx] = cls._get_generated_config(
                     sky_autogen_comment, worker_names[idx], ip, username,
-                    key_path)
+                    key_path, proxy_command)
 
-        for idx in range(len(worker_ips)):
+        for idx in range(len(external_worker_ips)):
             # Add (or overwrite) the new config.
             overwrite = overwrites[idx]
             overwrite_begin_idx = overwrite_begin_idxs[idx]
             codegen = codegens[idx]
+            assert codegen is not None, (codegens, idx)
             if overwrite:
                 assert overwrite_begin_idx is not None
                 updated_lines = codegen.splitlines(keepends=True) + ['\n']
                 extra_config[overwrite_begin_idx:overwrite_begin_idx +
                              len(updated_lines)] = updated_lines
                 with open(extra_config_path, 'w') as f:
                     f.write(''.join(extra_config).strip())
@@ -648,16 +711,15 @@
     ):
         config_path = os.path.expanduser(cls.ssh_conf_path)
         if not os.path.exists(config_path):
             return
 
         extra_path_name = cls.ssh_multinode_path.format(cluster_name)
         extra_config_path = os.path.expanduser(extra_path_name)
-        if os.path.exists(extra_config_path):
-            os.remove(extra_config_path)
+        common_utils.remove_file_if_exists(extra_config_path)
 
         # Delete include statement
         sky_autogen_comment = ('# Added by sky (use `sky stop/down '
                                f'{cluster_name}` to remove)')
         with open(config_path) as f:
             config = f.readlines()
 
@@ -674,65 +736,104 @@
                         del config[i - 1]
                     f.write(''.join(config))
                 break
             if 'Host' in config_str:
                 break
 
 
-def _replace_yaml_dicts(new_yaml: str, old_yaml: str,
-                        key_names: Set[str]) -> str:
-    """Replaces 'new' with 'old' for all keys in key_names.
+def _replace_yaml_dicts(
+        new_yaml: str, old_yaml: str, restore_key_names: Set[str],
+        restore_key_names_exceptions: Sequence[Tuple[str, ...]]) -> str:
+    """Replaces 'new' with 'old' for all keys in restore_key_names.
 
     The replacement will be applied recursively and only for the blocks
     with the key in key_names, and have the same ancestors in both 'new'
     and 'old' YAML tree.
+
+    The restore_key_names_exceptions is a list of key names that should not
+    be restored, i.e. those keys will be reset to the value in 'new' YAML
+    tree after the replacement.
     """
 
     def _restore_block(new_block: Dict[str, Any], old_block: Dict[str, Any]):
         for key, value in new_block.items():
-            if key in key_names:
+            if key in restore_key_names:
                 if key in old_block:
                     new_block[key] = old_block[key]
                 else:
                     del new_block[key]
             elif isinstance(value, dict):
                 if key in old_block:
                     _restore_block(value, old_block[key])
 
     new_config = yaml.safe_load(new_yaml)
     old_config = yaml.safe_load(old_yaml)
+    excluded_results = {}
+    # Find all key values excluded from restore
+    for exclude_restore_key_name_list in restore_key_names_exceptions:
+        excluded_result = new_config
+        found_excluded_key = True
+        for key in exclude_restore_key_name_list:
+            if (not isinstance(excluded_result, dict) or
+                    key not in excluded_result):
+                found_excluded_key = False
+                break
+            excluded_result = excluded_result[key]
+        if found_excluded_key:
+            excluded_results[exclude_restore_key_name_list] = excluded_result
+
+    # Restore from old config
     _restore_block(new_config, old_config)
+
+    # Revert the changes for the excluded key values
+    for exclude_restore_key_name, value in excluded_results.items():
+        curr = new_config
+        for key in exclude_restore_key_name[:-1]:
+            curr = curr[key]
+        curr[exclude_restore_key_name[-1]] = value
     return common_utils.dump_yaml_str(new_config)
 
 
 # TODO: too many things happening here - leaky abstraction. Refactor.
 @timeline.event
 def write_cluster_config(
         to_provision: 'resources.Resources',
         num_nodes: int,
         cluster_config_template: str,
         cluster_name: str,
         local_wheel_path: pathlib.Path,
         wheel_hash: str,
         region: Optional[clouds.Region] = None,
         zones: Optional[List[clouds.Zone]] = None,
-        auth_config: Optional[Dict[str, str]] = None,
         dryrun: bool = False,
         keep_launch_fields_in_existing_config: bool = True) -> Dict[str, str]:
     """Fills in cluster configuration templates and writes them out.
 
     Returns: {provisioner: path to yaml, the provisioning spec}.
       'provisioner' can be
         - 'ray'
         - 'tpu-create-script' (if TPU is requested)
         - 'tpu-delete-script' (if TPU is requested)
+    Raises:
+        exceptions.ResourcesUnavailableError: if the region/zones requested does
+            not appear in the catalog, or an ssh_proxy_command is specified but
+            not for the given region.
     """
     # task.best_resources may not be equal to to_provision if the user
     # is running a job with less resources than the cluster has.
     cloud = to_provision.cloud
+    # This can raise a ResourcesUnavailableError, when the region/zones
+    # requested does not appear in the catalog. It can be triggered when the
+    # user changed the catalog file, while there is a cluster in the removed
+    # region/zone.
+    #
+    # TODO(zhwu): We should change the exception type to a more specific one, as
+    # the ResourcesUnavailableError is overly used. Also, it would be better to
+    # move the check out of this function, i.e. the caller should be responsible
+    # for the validation.
     resources_vars = cloud.make_deploy_resources_variables(
         to_provision, region, zones)
     config_dict = {}
 
     azure_subscription_id = None
     if isinstance(cloud, clouds.Azure):
         azure_subscription_id = cloud.get_project_id(dryrun=dryrun)
@@ -749,36 +850,104 @@
     if isinstance(cloud, clouds.Local):
         ip_list = onprem_utils.get_local_ips(cluster_name)
         auth_config = onprem_utils.get_local_auth_config(cluster_name)
     region_name = resources_vars.get('region')
 
     yaml_path = _get_yaml_path_from_cluster_name(cluster_name)
 
-    # Use a tmp file path to avoid incomplete YAML file being re-used in the future.
+    # Retrieve the ssh_proxy_command for the given cloud / region.
+    ssh_proxy_command_config = skypilot_config.get_nested(
+        (str(cloud).lower(), 'ssh_proxy_command'), None)
+    if (isinstance(ssh_proxy_command_config, str) or
+            ssh_proxy_command_config is None):
+        ssh_proxy_command = ssh_proxy_command_config
+    else:
+        # ssh_proxy_command_config: Dict[str, str], region_name -> command
+        # This type check is done by skypilot_config at config load time.
+
+        # There are two cases:
+        if keep_launch_fields_in_existing_config:
+            # (1) We're re-provisioning an existing cluster.
+            #
+            # We use None for ssh_proxy_command, which will be restored to the
+            # cluster's original value later by _replace_yaml_dicts().
+            ssh_proxy_command = None
+        else:
+            # (2) We're launching a new cluster.
+            #
+            # Resources.get_valid_regions_for_launchable() respects the keys (regions)
+            # in ssh_proxy_command in skypilot_config. So here we add an assert.
+            assert region_name in ssh_proxy_command_config, (
+                region_name, ssh_proxy_command_config)
+            ssh_proxy_command = ssh_proxy_command_config[region_name]
+    logger.debug(f'Using ssh_proxy_command: {ssh_proxy_command!r}')
+
+    # User-supplied instance tags.
+    instance_tags = {}
+    instance_tags = skypilot_config.get_nested(
+        (str(cloud).lower(), 'instance_tags'), {})
+    if not isinstance(instance_tags, dict):
+        with ux_utils.print_exception_no_traceback():
+            raise ValueError('Custom instance_tags in config.yaml should '
+                             f'be a dict, but received {type(instance_tags)}.')
+
+    # Dump the Ray ports to a file for Ray job submission
+    dump_port_command = (
+        f'python -c \'import json, os; json.dump({constants.SKY_REMOTE_RAY_PORT_DICT_STR}, '
+        f'open(os.path.expanduser("{constants.SKY_REMOTE_RAY_PORT_FILE}"), "w"))\''
+    )
+
+    # Use a tmp file path to avoid incomplete YAML file being re-used in the
+    # future.
     tmp_yaml_path = yaml_path + '.tmp'
-    tmp_yaml_path = fill_template(
+    fill_template(
         cluster_config_template,
         dict(
             resources_vars,
             **{
                 'cluster_name': cluster_name,
                 'num_nodes': num_nodes,
                 'disk_size': to_provision.disk_size,
+                # If the current code is run by controller, propagate the real
+                # calling user which should've been passed in as the
+                # SKYPILOT_USER env var (see spot-controller.yaml.j2).
+                'user': os.environ.get('SKYPILOT_USER', getpass.getuser()),
+
+                # AWS only:
                 # Temporary measure, as deleting per-cluster SGs is too slow.
                 # See https://github.com/skypilot-org/skypilot/pull/742.
                 # Generate the name of the security group we're looking for.
                 # (username, last 4 chars of hash of hostname): for uniquefying
-                # users on shared-account cloud providers. Using uuid.getnode()
-                # is incorrect; observed to collide on Macs.
-                'security_group': f'sky-sg-{common_utils.user_and_hostname_hash()}',
-                # Azure only.
+                # users on shared-account scenarios.
+                'security_group': skypilot_config.get_nested(
+                    ('aws', 'security_group_name'),
+                    f'sky-sg-{common_utils.user_and_hostname_hash()}'),
+                'vpc_name': skypilot_config.get_nested(('aws', 'vpc_name'),
+                                                       None),
+                'use_internal_ips': skypilot_config.get_nested(
+                    ('aws', 'use_internal_ips'), False),
+                # Not exactly AWS only, but we only test it's supported on AWS
+                # for now:
+                'ssh_proxy_command': ssh_proxy_command,
+                # User-supplied instance tags.
+                'instance_tags': instance_tags,
+
+                # Azure only:
                 'azure_subscription_id': azure_subscription_id,
                 'resource_group': f'{cluster_name}-{region_name}',
-                # GCP only.
+
+                # GCP only:
                 'gcp_project_id': gcp_project_id,
+
+                # Port of Ray (GCS server).
+                # Ray's default port 6379 is conflicted with Redis.
+                'ray_port': constants.SKY_REMOTE_RAY_PORT,
+                'ray_dashboard_port': constants.SKY_REMOTE_RAY_DASHBOARD_PORT,
+                'ray_temp_dir': constants.SKY_REMOTE_RAY_TEMPDIR,
+                'dump_port_command': dump_port_command,
                 # Ray version.
                 'ray_version': constants.SKY_REMOTE_RAY_VERSION,
                 # Cloud credentials for cloud storage.
                 'credentials': credentials,
                 # Sky remote utils.
                 'sky_remote_path': SKY_REMOTE_PATH,
                 'sky_local_path': str(local_wheel_path),
@@ -799,59 +968,77 @@
     config_dict['cluster_name'] = cluster_name
     config_dict['ray'] = yaml_path
     if dryrun:
         # If dryrun, return the unfinished tmp yaml path.
         config_dict['ray'] = tmp_yaml_path
         return config_dict
     _add_auth_to_cluster_config(cloud, tmp_yaml_path)
-    # Delay the optimization of the config until the authentication files is added.
-    if not isinstance(cloud, clouds.Local):
-        # Only optimize the file mounts for public clouds now, as local has not
-        # been fully tested yet.
-        _optimize_file_mounts(tmp_yaml_path)
 
     # Restore the old yaml content for backward compatibility.
     if os.path.exists(yaml_path) and keep_launch_fields_in_existing_config:
         with open(yaml_path, 'r') as f:
             old_yaml_content = f.read()
         with open(tmp_yaml_path, 'r') as f:
             new_yaml_content = f.read()
         restored_yaml_content = _replace_yaml_dicts(
             new_yaml_content, old_yaml_content,
-            _RAY_YAML_KEYS_TO_RESTORE_FOR_BACK_COMPATIBILITY)
+            _RAY_YAML_KEYS_TO_RESTORE_FOR_BACK_COMPATIBILITY,
+            _RAY_YAML_KEYS_TO_RESTORE_EXCEPTIONS)
         with open(tmp_yaml_path, 'w') as f:
             f.write(restored_yaml_content)
 
+    # Optimization: copy the contents of source files in file_mounts to a
+    # special dir, and upload that as the only file_mount instead. Delay
+    # calling this optimization until now, when all source files have been
+    # written and their contents finalized.
+    #
+    # Note that the ray yaml file will be copied into that special dir (i.e.,
+    # uploaded as part of the file_mounts), so the restore for backward
+    # compatibility should go before this call.
+    if not isinstance(cloud, clouds.Local):
+        # Only optimize the file mounts for public clouds now, as local has not
+        # been fully tested yet.
+        _optimize_file_mounts(tmp_yaml_path)
+
     # Rename the tmp file to the final YAML path.
     os.rename(tmp_yaml_path, yaml_path)
-
     usage_lib.messages.usage.update_ray_yaml(yaml_path)
+
     # For TPU nodes. TPU VMs do not need TPU_NAME.
     if (resources_vars.get('tpu_type') is not None and
             resources_vars.get('tpu_vm') is None):
         tpu_name = resources_vars.get('tpu_name')
         if tpu_name is None:
             tpu_name = cluster_name
 
         user_file_dir = os.path.expanduser(f'{SKY_USER_FILE_PATH}/')
-        scripts = tuple(
+
+        from sky.skylet.providers.gcp import config as gcp_config  # pylint: disable=import-outside-toplevel
+        config = common_utils.read_yaml(os.path.expanduser(config_dict['ray']))
+        vpc_name = gcp_config.get_usable_vpc(config)
+
+        scripts = []
+        for template_name in ('gcp-tpu-create.sh.j2', 'gcp-tpu-delete.sh.j2'):
+            script_path = os.path.join(user_file_dir, template_name).replace(
+                '.sh.j2', f'.{cluster_name}.sh')
             fill_template(
                 template_name,
                 dict(
                     resources_vars, **{
                         'tpu_name': tpu_name,
                         'gcp_project_id': gcp_project_id,
+                        'vpc_name': vpc_name,
                     }),
                 # Use new names for TPU scripts so that different runs can use
                 # different TPUs.  Put in SKY_USER_FILE_PATH to be consistent
                 # with cluster yamls.
-                output_path=os.path.join(user_file_dir, template_name).replace(
-                    '.sh.j2', f'.{cluster_name}.sh'),
-            ) for template_name in
-            ['gcp-tpu-create.sh.j2', 'gcp-tpu-delete.sh.j2'])
+                output_path=script_path,
+            )
+            scripts.append(script_path)
+
         config_dict['tpu-create-script'] = scripts[0]
         config_dict['tpu-delete-script'] = scripts[1]
         config_dict['tpu_name'] = tpu_name
     return config_dict
 
 
 def _add_auth_to_cluster_config(cloud: clouds.Cloud, cluster_config_file: str):
@@ -863,14 +1050,22 @@
     # Check the availability of the cloud type.
     if isinstance(cloud, clouds.AWS):
         config = auth.setup_aws_authentication(config)
     elif isinstance(cloud, clouds.GCP):
         config = auth.setup_gcp_authentication(config)
     elif isinstance(cloud, clouds.Azure):
         config = auth.setup_azure_authentication(config)
+    elif isinstance(cloud, clouds.Lambda):
+        config = auth.setup_lambda_authentication(config)
+    elif isinstance(cloud, clouds.IBM):
+        config = auth.setup_ibm_authentication(config)
+    elif isinstance(cloud, clouds.SCP):
+        config = auth.setup_scp_authentication(config)
+    elif isinstance(cloud, clouds.OCI):
+        config = auth.setup_oci_authentication(config)
     else:
         assert isinstance(cloud, clouds.Local), cloud
         # Local cluster case, authentication is already filled by the user
         # in the local cluster config (in ~/.sky/local/...). There is no need
         # for Sky to generate authentication.
         pass
     common_utils.dump_yaml(cluster_config_file, config)
@@ -881,85 +1076,94 @@
 
 
 def get_timestamp_from_run_timestamp(run_timestamp: str) -> float:
     return datetime.strptime(
         run_timestamp.partition('-')[2], '%Y-%m-%d-%H-%M-%S-%f').timestamp()
 
 
+def _count_healthy_nodes_from_ray(output: str,
+                                  is_local_cloud: bool = False
+                                 ) -> Tuple[int, int]:
+    """Count the number of healthy nodes from the output of `ray status`."""
+
+    def get_ready_nodes(pattern, output):
+        result = pattern.findall(output)
+        # On-prem/local case is handled differently.
+        # `ray status` produces different output for local case, and
+        # we poll for number of nodes launched instead of counting for
+        # head and number of worker nodes separately (it is impossible
+        # to distinguish between head and worker node for local case).
+        if is_local_cloud:
+            # In the local case, ready_workers mean the total number
+            # of nodes launched, including head.
+            return len(result)
+        if len(result) == 0:
+            return 0
+        assert len(result) == 1, result
+        return int(result[0])
+
+    if is_local_cloud:
+        ready_head = 0
+        ready_workers = get_ready_nodes(_LAUNCHED_LOCAL_WORKER_PATTERN, output)
+    else:
+        ready_head = get_ready_nodes(_LAUNCHED_HEAD_PATTERN, output)
+        ready_workers = get_ready_nodes(_LAUNCHED_WORKER_PATTERN, output)
+    assert ready_head <= 1, f'#head node should be <=1 (Got {ready_head}).'
+    return ready_head, ready_workers
+
+
 @timeline.event
 def wait_until_ray_cluster_ready(
     cluster_config_file: str,
     num_nodes: int,
     log_path: str,
     is_local_cloud: bool = False,
     nodes_launching_progress_timeout: Optional[int] = None,
 ) -> bool:
     """Returns whether the entire ray cluster is ready."""
     if num_nodes <= 1:
-        return
+        return True
 
     # Manually fetching head ip instead of using `ray exec` to avoid the bug
     # that `ray exec` fails to connect to the head node after some workers
     # launched especially for Azure.
     try:
-        head_ip = query_head_ip_with_retries(
+        head_ip = _query_head_ip_with_retries(
             cluster_config_file, max_attempts=WAIT_HEAD_NODE_IP_MAX_ATTEMPTS)
     except RuntimeError as e:
         logger.error(e)
         return False  # failed
 
     ssh_credentials = ssh_credential_from_yaml(cluster_config_file)
     last_nodes_so_far = 0
     start = time.time()
     runner = command_runner.SSHCommandRunner(head_ip, **ssh_credentials)
-    with console.status('[bold cyan]Waiting for workers...') as worker_status:
+    with log_utils.console.status(
+            '[bold cyan]Waiting for workers...') as worker_status:
         while True:
-            rc, output, stderr = runner.run('ray status',
-                                            log_path=log_path,
-                                            stream_logs=False,
-                                            require_outputs=True,
-                                            separate_stderr=True)
+            rc, output, stderr = runner.run(
+                RAY_STATUS_WITH_SKY_RAY_PORT_COMMAND,
+                log_path=log_path,
+                stream_logs=False,
+                require_outputs=True,
+                separate_stderr=True)
             subprocess_utils.handle_returncode(
                 rc, 'ray status', 'Failed to run ray status on head node.',
                 stderr)
             logger.debug(output)
 
-            # Workers that are ready
-            ready_workers = 0
-            # On-prem/local case is handled differently.
-            # `ray status` produces different output for local case, and
-            # we poll for number of nodes launched instead of counting for
-            # head and number of worker nodes separately (it is impossible
-            # to distinguish between head and worker node for local case).
-            if is_local_cloud:
-                result = _LAUNCHED_LOCAL_WORKER_PATTERN.findall(output)
-                # In the local case, ready_workers mean the total number
-                # of nodes launched, including head.
-                ready_workers = len(result)
-            else:
-                result = _LAUNCHED_WORKER_PATTERN.findall(output)
-                if len(result) == 0:
-                    ready_workers = 0
-                else:
-                    assert len(result) == 1, result
-                    ready_workers = int(result[0])
-
-            result = _LAUNCHED_HEAD_PATTERN.findall(output)
-            ready_head = 0
-            if result:
-                assert len(result) == 1, result
-                ready_head = int(result[0])
-                assert ready_head <= 1, ready_head
+            ready_head, ready_workers = _count_healthy_nodes_from_ray(
+                output, is_local_cloud=is_local_cloud)
 
             worker_status.update('[bold cyan]'
                                  f'{ready_workers} out of {num_nodes - 1} '
                                  'workers ready')
 
-            # In the local case, ready_head=0 and ready_workers=num_nodes
-            # This is because there is no matching regex for _LAUNCHED_HEAD_PATTERN.
+            # In the local case, ready_head=0 and ready_workers=num_nodes. This
+            # is because there is no matching regex for _LAUNCHED_HEAD_PATTERN.
             if ready_head + ready_workers == num_nodes:
                 # All nodes are up.
                 break
 
             # Pending workers that have been launched by ray up.
             found_ips = _LAUNCHING_IP_PATTERN.findall(output)
             pending_workers = len(found_ips)
@@ -1006,39 +1210,41 @@
 def ssh_credential_from_yaml(cluster_yaml: str) -> Dict[str, str]:
     """Returns ssh_user, ssh_private_key and ssh_control name."""
     config = common_utils.read_yaml(cluster_yaml)
     auth_section = config['auth']
     ssh_user = auth_section['ssh_user'].strip()
     ssh_private_key = auth_section.get('ssh_private_key')
     ssh_control_name = config.get('cluster_name', '__default__')
+    ssh_proxy_command = auth_section.get('ssh_proxy_command')
     return {
         'ssh_user': ssh_user,
         'ssh_private_key': ssh_private_key,
-        'ssh_control_name': ssh_control_name
+        'ssh_control_name': ssh_control_name,
+        'ssh_proxy_command': ssh_proxy_command,
     }
 
 
 def parallel_data_transfer_to_nodes(
     runners: List[command_runner.SSHCommandRunner],
-    source: str,
+    source: Optional[str],
     target: str,
     cmd: Optional[str],
     run_rsync: bool,
     *,
     action_message: str,
     # Advanced options.
     log_path: str = os.devnull,
     stream_logs: bool = False,
 ):
     """Runs a command on all nodes and optionally runs rsync from src->dst.
 
     Args:
         runners: A list of SSHCommandRunner objects that represent multiple nodes.
-        source_target: Tuple[str, str]; Source for rsync on local node and
-            Destination on remote node for rsync
+        source: Optional[str]; Source for rsync on local node
+        target: str; Destination on remote node for rsync
         cmd: str; Command to be executed on all nodes
         action_message: str; Message to be printed while the command runs
         log_path: str; Path to the log file
         stream_logs: bool; Whether to stream logs to stdout
     """
     fore = colorama.Fore
     style = colorama.Style
@@ -1047,38 +1253,43 @@
 
     def _sync_node(runner: 'command_runner.SSHCommandRunner') -> None:
         if cmd is not None:
             rc, stdout, stderr = runner.run(cmd,
                                             log_path=log_path,
                                             stream_logs=stream_logs,
                                             require_outputs=True)
-            subprocess_utils.handle_returncode(
-                rc,
-                cmd,
-                f'Failed to run command before rsync {origin_source} -> {target}.',
-                stderr=stdout + stderr)
+            err_msg = ('Failed to run command before rsync '
+                       f'{origin_source} -> {target}. '
+                       'Ensure that the network is stable, then retry.')
+            if log_path != os.devnull:
+                err_msg += f' See logs in {log_path}'
+            subprocess_utils.handle_returncode(rc,
+                                               cmd,
+                                               err_msg,
+                                               stderr=stdout + stderr)
 
         if run_rsync:
+            assert source is not None
             # TODO(zhwu): Optimize for large amount of files.
-            # zip / transfer/ unzip
+            # zip / transfer / unzip
             runner.rsync(
                 source=source,
                 target=target,
                 up=True,
                 log_path=log_path,
                 stream_logs=stream_logs,
             )
 
     num_nodes = len(runners)
     plural = 's' if num_nodes > 1 else ''
     message = (f'{fore.CYAN}{action_message} (to {num_nodes} node{plural})'
                f': {style.BRIGHT}{origin_source}{style.RESET_ALL} -> '
                f'{style.BRIGHT}{target}{style.RESET_ALL}')
     logger.info(message)
-    with safe_console_status(f'[bold cyan]{action_message}[/]'):
+    with log_utils.safe_rich_status(f'[bold cyan]{action_message}[/]'):
         subprocess_utils.run_in_parallel(_sync_node, runners)
 
 
 def check_local_gpus() -> bool:
     """Checks if GPUs are available locally.
 
     Returns whether GPUs are available on the local machine by checking
@@ -1101,91 +1312,138 @@
         is_functional = execution_check.returncode == 0
     return is_functional
 
 
 def generate_cluster_name():
     # TODO: change this ID formatting to something more pleasant.
     # User name is helpful in non-isolated accounts, e.g., GCP, Azure.
-    return f'sky-{uuid.uuid4().hex[:4]}-{getpass.getuser()}'
+    return f'sky-{uuid.uuid4().hex[:4]}-{get_cleaned_username()}'
+
+
+def get_cleaned_username() -> str:
+    """Cleans the current username to be used as part of a cluster name.
+
+    Clean up includes:
+     1. Making all characters lowercase
+     2. Removing any non-alphanumeric characters (excluding hyphens)
+     3. Removing any numbers and/or hyphens at the start of the username.
+     4. Removing any hyphens at the end of the username
+
+    e.g. 1SkY-PiLot2- becomes sky-pilot2.
+
+    Returns:
+      A cleaned username that will pass the regex in
+      check_cluster_name_is_valid().
+    """
+    username = getpass.getuser()
+    username = username.lower()
+    username = re.sub(r'[^a-z0-9-]', '', username)
+    username = re.sub(r'^[0-9-]+', '', username)
+    username = re.sub(r'-$', '', username)
+    return username
+
 
+def _query_head_ip_with_retries(cluster_yaml: str,
+                                max_attempts: int = 1) -> str:
+    """Returns the IP of the head node by querying the cloud.
 
-def query_head_ip_with_retries(cluster_yaml: str, max_attempts: int = 1) -> str:
-    """Returns the ip of the head node from yaml file."""
+    Raises:
+      RuntimeError: if we failed to get the head IP.
+    """
     backoff = common_utils.Backoff(initial_backoff=5, max_backoff_factor=5)
     for i in range(max_attempts):
         try:
             full_cluster_yaml = str(pathlib.Path(cluster_yaml).expanduser())
             out = subprocess_utils.run(
                 f'ray get-head-ip {full_cluster_yaml!r}',
                 stdout=subprocess.PIPE,
                 stderr=subprocess.DEVNULL).stdout.decode().strip()
-            head_ip = re.findall(IP_ADDR_REGEX, out)
-            assert 1 == len(head_ip), out
-            head_ip = head_ip[0]
+            head_ip_list = re.findall(IP_ADDR_REGEX, out)
+            if len(head_ip_list) > 1:
+                # This could be triggered if e.g., some logging is added in
+                # skypilot_config, a module that has some code executed
+                # whenever `sky` is imported.
+                logger.warning(
+                    'Detected more than 1 IP from the output of '
+                    'the `ray get-head-ip` command. This could '
+                    'happen if there is extra output from it, '
+                    'which should be inspected below.\nProceeding with '
+                    f'the last detected IP ({head_ip_list[-1]}) as head IP.'
+                    f'\n== Output ==\n{out}'
+                    f'\n== Output ends ==')
+                head_ip_list = head_ip_list[-1:]
+            assert 1 == len(head_ip_list), (out, head_ip_list)
+            head_ip = head_ip_list[0]
             break
         except subprocess.CalledProcessError as e:
             if i == max_attempts - 1:
                 raise RuntimeError('Failed to get head ip') from e
             # Retry if the cluster is not up yet.
             logger.debug('Retrying to get head ip.')
             time.sleep(backoff.current_backoff())
     return head_ip
 
 
 @timeline.event
 def get_node_ips(cluster_yaml: str,
                  expected_num_nodes: int,
-                 handle: Optional[backends.Backend.ResourceHandle] = None,
+                 handle: Optional[
+                     'cloud_vm_ray_backend.CloudVmRayResourceHandle'] = None,
                  head_ip_max_attempts: int = 1,
                  worker_ip_max_attempts: int = 1,
                  get_internal_ips: bool = False) -> List[str]:
-    """Returns the IPs of all nodes in the cluster."""
-
+    """Returns the IPs of all nodes in the cluster, with head node at front."""
     # When ray up launches TPU VM Pod, Pod workers (except for the head)
     # won't be connected to Ray cluster. Thus "ray get-worker-ips"
     # won't work and we need to query the node IPs with gcloud as
     # implmented in _get_tpu_vm_pod_ips.
     ray_config = common_utils.read_yaml(cluster_yaml)
     use_tpu_vm = ray_config['provider'].get('_has_tpus', False)
     if use_tpu_vm:
-        return _get_tpu_vm_pod_ips(ray_config, get_internal_ips)
-
-    # Try optimize for the common case where we have 1 node.
-    if (expected_num_nodes == 1 and handle is not None and
-            handle.head_ip is not None):
-        return [handle.head_ip]
+        assert expected_num_nodes == 1, (
+            'TPU VM only supports single node for now.')
+        assert handle is not None, 'handle is required for TPU VM.'
+        try:
+            ips = _get_tpu_vm_pod_ips(ray_config, get_internal_ips)
+        except exceptions.CommandError as e:
+            raise exceptions.FetchIPError(
+                exceptions.FetchIPError.Reason.HEAD) from e
+        if len(ips) != tpu_utils.get_num_tpu_devices(handle.launched_resources):
+            raise exceptions.FetchIPError(exceptions.FetchIPError.Reason.HEAD)
+        return ips
 
     if get_internal_ips:
         with tempfile.NamedTemporaryFile(mode='w', delete=False) as f:
             ray_config['provider']['use_internal_ips'] = True
             yaml.dump(ray_config, f)
             cluster_yaml = f.name
 
     # Check the network connection first to avoid long hanging time for
     # ray get-head-ip below, if a long-lasting network connection failure
     # happens.
     check_network_connection()
     try:
-        head_ip = query_head_ip_with_retries(cluster_yaml,
-                                             max_attempts=head_ip_max_attempts)
+        head_ip = _query_head_ip_with_retries(cluster_yaml,
+                                              max_attempts=head_ip_max_attempts)
     except RuntimeError as e:
         raise exceptions.FetchIPError(
             exceptions.FetchIPError.Reason.HEAD) from e
-    head_ip = [head_ip]
+    head_ip_list = [head_ip]
     if expected_num_nodes > 1:
         backoff = common_utils.Backoff(initial_backoff=5, max_backoff_factor=5)
 
         for retry_cnt in range(worker_ip_max_attempts):
             try:
                 full_cluster_yaml = str(pathlib.Path(cluster_yaml).expanduser())
                 proc = subprocess_utils.run(
                     f'ray get-worker-ips {full_cluster_yaml!r}',
                     stdout=subprocess.PIPE,
                     stderr=subprocess.PIPE)
                 out = proc.stdout.decode()
+                break
             except subprocess.CalledProcessError as e:
                 if retry_cnt == worker_ip_max_attempts - 1:
                     raise exceptions.FetchIPError(
                         exceptions.FetchIPError.Reason.WORKER) from e
                 # Retry if the ssh is not ready for the workers yet.
                 backoff_time = backoff.current_backoff()
                 logger.debug('Retrying to get worker ip '
@@ -1199,603 +1457,864 @@
         if ((handle is not None and hasattr(handle, 'local_handle') and
              handle.local_handle is not None) or
                 onprem_utils.check_if_local_cloud(cluster_name)):
             out = proc.stderr.decode()
             worker_ips = re.findall(IP_ADDR_REGEX, out)
             # Remove head ip from worker ip list.
             for i, ip in enumerate(worker_ips):
-                if ip == head_ip[0]:
+                if ip == head_ip_list[0]:
                     del worker_ips[i]
                     break
         if len(worker_ips) != expected_num_nodes - 1:
-            raise exceptions.FetchIPError(exceptions.FetchIPError.Reason.WORKER)
+            n = expected_num_nodes - 1
+            if len(worker_ips) > n:
+                # This could be triggered if e.g., some logging is added in
+                # skypilot_config, a module that has some code executed whenever
+                # `sky` is imported.
+                logger.warning(
+                    f'Expected {n} worker IP(s); found '
+                    f'{len(worker_ips)}: {worker_ips}'
+                    '\nThis could happen if there is extra output from '
+                    '`ray get-worker-ips`, which should be inspected below.'
+                    f'\n== Output ==\n{out}'
+                    f'\n== Output ends ==')
+                logger.warning(f'\nProceeding with the last {n} '
+                               f'detected IP(s): {worker_ips[-n:]}.')
+                worker_ips = worker_ips[-n:]
+            else:
+                raise exceptions.FetchIPError(
+                    exceptions.FetchIPError.Reason.WORKER)
     else:
         worker_ips = []
-    return head_ip + worker_ips
+    return head_ip_list + worker_ips
 
 
 @timeline.event
 def _get_tpu_vm_pod_ips(ray_config: Dict[str, Any],
                         get_internal_ips: bool = False) -> List[str]:
     """Returns the IPs of all TPU VM Pod workers using gcloud."""
 
     cluster_name = ray_config['cluster_name']
     zone = ray_config['provider']['availability_zone']
     query_cmd = (f'gcloud compute tpus tpu-vm list --filter='
-                 f'\\(labels.ray-cluster-name={cluster_name}\\) '
-                 f'--zone={zone} --format=value\\(name\\)')
-    if not get_internal_ips:
-        tpuvm_cmd = (f'gcloud compute tpus tpu-vm describe $({query_cmd})'
-                     f' --zone {zone} --format="value[delimiter=\'\\n\']'
-                     '(networkEndpoints.accessConfig.externalIp)"')
-    else:
-        tpuvm_cmd = (f'gcloud compute tpus tpu-vm describe $({query_cmd})'
-                     f' --zone {zone} --format="value[delimiter=\'\\n\']'
-                     '(networkEndpoints.ipAddress)"')
-
-    rcode, stdout, stderr = log_lib.run_with_log(tpuvm_cmd,
-                                                 '/dev/null',
-                                                 shell=True,
-                                                 stream_logs=False,
-                                                 require_outputs=True)
-    if rcode != 0:
-        failure_massage = ('Failed to run gcloud to get TPU VM Pod IPs.\n'
-                           '**** STDOUT ****\n'
-                           '{stdout}\n'
-                           '**** STDERR ****\n'
-                           '{stderr}')
-        with ux_utils.print_exception_no_traceback():
-            raise RuntimeError(
-                failure_massage.format(stdout=stdout, stderr=stderr))
-    all_ips = re.findall(IP_ADDR_REGEX, stdout)
+                 f'"(labels.ray-cluster-name={cluster_name})" '
+                 f'--zone={zone} --format="value(name)"')
+    returncode, stdout, stderr = log_lib.run_with_log(query_cmd,
+                                                      '/dev/null',
+                                                      shell=True,
+                                                      stream_logs=False,
+                                                      require_outputs=True)
+    subprocess_utils.handle_returncode(
+        returncode,
+        query_cmd,
+        'Failed to run gcloud to get TPU VM IDs.',
+        stderr=stdout + stderr)
+    if len(stdout) == 0:
+        logger.debug('No TPU VMs found with cluster name '
+                     f'{cluster_name} in zone {zone}.')
+    if len(stdout.splitlines()) > 1:
+        # Rare case, this could mean resource leakage. Hint user.
+        logger.warning('Found more than one TPU VM/Pod with the same cluster '
+                       f'name {cluster_name} in zone {zone}.')
+
+    all_ips = []
+    for tpu_id in stdout.splitlines():
+        tpuvm_cmd = (f'gcloud compute tpus tpu-vm describe {tpu_id}'
+                     f' --zone {zone} --format=json')
+        returncode, stdout, stderr = log_lib.run_with_log(tpuvm_cmd,
+                                                          os.devnull,
+                                                          shell=True,
+                                                          stream_logs=False,
+                                                          require_outputs=True)
+        subprocess_utils.handle_returncode(
+            returncode,
+            tpuvm_cmd,
+            'Failed to run gcloud tpu-vm describe.',
+            stderr=stdout + stderr)
+
+        tpuvm_json = json.loads(stdout)
+        if tpuvm_json['state'] != 'READY':
+            # May be a leaked preempted resource, or terminated by user in the
+            # console, or still in the process of being created.
+            ux_utils.console_newline()
+            logger.debug(f'TPU VM {tpu_id} is in {tpuvm_json["state"]} '
+                         'state. Skipping IP query... '
+                         'Hint: make sure it is not leaked.')
+            continue
+
+        ips = []
+        for endpoint in tpuvm_json['networkEndpoints']:
+            # Note: if TPU VM is being preempted, its IP field may not exist.
+            # We use get() to avoid KeyError.
+            if get_internal_ips:
+                ip = endpoint.get('ipAddress', None)
+            else:
+                ip = endpoint['accessConfig'].get('externalIp', None)
+            if ip is not None:
+                ips.append(ip)
+        all_ips.extend(ips)
+
     return all_ips
 
 
 @timeline.event
 def get_head_ip(
-    handle: backends.Backend.ResourceHandle,
-    use_cached_head_ip: bool = True,
+    handle: 'cloud_vm_ray_backend.CloudVmRayResourceHandle',
     max_attempts: int = 1,
 ) -> str:
-    """Returns the ip of the head node."""
-    assert not use_cached_head_ip or max_attempts == 1, (
-        'Cannot use cached_head_ip when max_attempts is not 1')
-    if use_cached_head_ip:
-        if handle.head_ip is None:
-            # This happens for INIT clusters (e.g., exit 1 in setup).
-            with ux_utils.print_exception_no_traceback():
-                raise ValueError(
-                    'Cluster\'s head IP not found; is it up? To fix: '
-                    'run a successful launch first (`sky launch`) to ensure'
-                    ' the cluster status is UP (`sky status`).')
-        head_ip = handle.head_ip
-    else:
-        head_ip = query_head_ip_with_retries(handle.cluster_yaml, max_attempts)
-    return head_ip
+    """Returns the ip of the head node.
 
+    First try to use the cached head ip. If it is not available, query
+    the head ip from the cluster.
 
-def run_command_and_handle_ssh_failure(
-        runner: command_runner.SSHCommandRunner,
-        command: str,
-        failure_message: Optional[str] = None) -> str:
-    """Runs command remotely and returns output with proper error handling."""
-    rc, stdout, stderr = runner.run(command,
-                                    require_outputs=True,
-                                    stream_logs=False)
-    if rc == 255:
-        # SSH failed
-        raise RuntimeError(
-            f'SSH with user {runner.ssh_user} and key {runner.ssh_private_key} '
-            f'to {runner.ip} failed. This is most likely due to incorrect '
-            'credentials or incorrect permissions for the key file. Check '
-            'your credentials and try again.')
-    subprocess_utils.handle_returncode(rc,
-                                       command,
-                                       failure_message,
-                                       stderr=stderr)
-    return stdout
-
+    Args:
+        handle: The ResourceHandle of the cluster.
+        max_attempts: The maximum number of attempts to query the head ip.
 
-def do_filemounts_and_setup_on_local_workers(
-        cluster_config_file: str,
-        worker_ips: List[str] = None,
-        extra_setup_cmds: List[str] = None):
-    """Completes filemounting and setup on worker nodes.
-
-    Syncs filemounts and runs setup on worker nodes for a local cluster. This
-    is a workaround for a Ray Autoscaler bug where `ray up` does not perform
-    filemounting or setup for local cluster worker nodes.
+    Returns:
+        The ip of the head node.
     """
-    config = common_utils.read_yaml(cluster_config_file)
-
-    ssh_credentials = ssh_credential_from_yaml(cluster_config_file)
-    if worker_ips is None:
-        worker_ips = config['provider']['worker_ips']
-    file_mounts = config['file_mounts']
-
-    setup_cmds = config['setup_commands']
-    if extra_setup_cmds is not None:
-        setup_cmds += extra_setup_cmds
-    setup_script = log_lib.make_task_bash_script('\n'.join(setup_cmds))
-
-    worker_runners = command_runner.SSHCommandRunner.make_runner_list(
-        worker_ips, **ssh_credentials)
-
-    # Uploads setup script to the worker node
-    with tempfile.NamedTemporaryFile('w', prefix='sky_setup_') as f:
-        f.write(setup_script)
-        f.flush()
-        setup_sh_path = f.name
-        setup_file = os.path.basename(setup_sh_path)
-        file_mounts[f'/tmp/{setup_file}'] = setup_sh_path
-
-        # Ray Autoscaler Bug: Filemounting + Ray Setup
-        # does not happen on workers.
-        def _setup_local_worker(runner: command_runner.SSHCommandRunner):
-            for dst, src in file_mounts.items():
-                mkdir_dst = f'mkdir -p {os.path.dirname(dst)}'
-                run_command_and_handle_ssh_failure(
-                    runner,
-                    mkdir_dst,
-                    failure_message=f'Failed to run {mkdir_dst} on remote.')
-                if os.path.isdir(src):
-                    src = os.path.join(src, '')
-                runner.rsync(source=src, target=dst, up=True, stream_logs=False)
-
-            setup_cmd = f'/bin/bash -i /tmp/{setup_file} 2>&1'
-            rc, stdout, _ = runner.run(setup_cmd,
-                                       stream_logs=False,
-                                       require_outputs=True)
-            subprocess_utils.handle_returncode(
-                rc,
-                setup_cmd,
-                'Failed to setup Ray autoscaler commands on remote.',
-                stderr=stdout)
-
-        subprocess_utils.run_in_parallel(_setup_local_worker, worker_runners)
+    head_ip = handle.head_ip
+    if head_ip is not None:
+        return head_ip
+    head_ip = _query_head_ip_with_retries(handle.cluster_yaml, max_attempts)
+    return head_ip
 
 
 def check_network_connection():
     # Tolerate 3 retries as it is observed that connections can fail.
     adapter = adapters.HTTPAdapter(max_retries=retry_lib.Retry(total=3))
     http = requests.Session()
     http.mount('https://', adapter)
     http.mount('http://', adapter)
-    try:
-        http.head(_TEST_IP, timeout=3)
-    except requests.Timeout as e:
-        raise exceptions.NetworkError(
-            'Could not refresh the cluster. Network seems down.') from e
-
+    for i, ip in enumerate(_TEST_IP_LIST):
+        try:
+            http.head(ip, timeout=3)
+            return
+        except (requests.Timeout, requests.exceptions.ConnectionError) as e:
+            if i == len(_TEST_IP_LIST) - 1:
+                raise exceptions.NetworkError('Could not refresh the cluster. '
+                                              'Network seems down.') from e
 
-def _process_cli_query(
-    cloud: str, cluster: str, query_cmd: str, deliminiator: str,
-    status_map: Dict[str, global_user_state.ClusterStatus]
-) -> List[global_user_state.ClusterStatus]:
-    """Run the cloud CLI query and returns cluster status.
 
-    Args:
-        cloud: The cloud provider name.
-        cluster: The cluster name.
-        query_cmd: The cloud CLI query command.
-        deliminiator: The deliminiator separating the status in the output
-            of the query command.
-        status_map: A map from the CLI status string to the corresponding
-            global_user_state.ClusterStatus.
-    Returns:
-        A list of global_user_state.ClusterStatus of all existing nodes in the
-        cluster. The list can be empty if none of the nodes in the clusters are
-        found, i.e. the nodes are all terminated.
+def check_owner_identity(cluster_name: str) -> None:
+    """Check if current user is the same as the user who created the cluster.
+
+    Raises:
+        exceptions.ClusterOwnerIdentityMismatchError: if the current user is
+          not the same as the user who created the cluster.
+        exceptions.CloudUserIdentityError: if we fail to get the current user
+          identity.
     """
-    returncode, stdout, stderr = log_lib.run_with_log(query_cmd,
-                                                      '/dev/null',
-                                                      require_outputs=True,
-                                                      shell=True)
-    logger.debug(f'{query_cmd} returned {returncode}.\n'
-                 '**** STDOUT ****\n'
-                 f'{stdout}\n'
-                 '**** STDERR ****\n'
-                 f'{stderr}')
-    if (cloud == str(clouds.Azure()) and returncode == 2 and
-            'argument --ids: expected at least one argument' in stderr):
-        # Azure CLI has a returncode 2 when the cluster is not found, as
-        # --ids <empty> is passed to the query command. In that case, the
-        # cluster should be considered as DOWN.
-        return []
+    if env_options.Options.SKIP_CLOUD_IDENTITY_CHECK.get():
+        return
+    record = global_user_state.get_cluster_from_name(cluster_name)
+    if record is None:
+        return
+    handle = record['handle']
+    if not isinstance(handle, backends.CloudVmRayResourceHandle):
+        return
 
-    if returncode != 0:
+    cloud = handle.launched_resources.cloud
+    current_user_identity = cloud.get_current_user_identity()
+    owner_identity = record['owner']
+    if current_user_identity is None:
+        # Skip the check if the cloud does not support user identity.
+        return
+    # The user identity can be None, if the cluster is created by an older
+    # version of SkyPilot. In that case, we set the user identity to the
+    # current one.
+    # NOTE: a user who upgrades SkyPilot and switches to a new cloud identity
+    # immediately without `sky status --refresh` first, will cause a leakage
+    # of the existing cluster. We deem this an acceptable tradeoff mainly
+    # because multi-identity is not common (at least at the moment).
+    if owner_identity is None:
+        global_user_state.set_owner_identity_for_cluster(
+            cluster_name, current_user_identity)
+    else:
+        assert isinstance(owner_identity, list)
+        # It is OK if the owner identity is shorter, which will happen when
+        # the cluster is launched before #1808. In that case, we only check
+        # the same length (zip will stop at the shorter one).
+        for i, (owner,
+                current) in enumerate(zip(owner_identity,
+                                          current_user_identity)):
+            # Clean up the owner identiy for the backslash and newlines, caused
+            # by the cloud CLI output, e.g. gcloud.
+            owner = owner.replace('\n', '').replace('\\', '')
+            if owner == current:
+                if i != 0:
+                    logger.warning(
+                        f'The cluster was owned by {owner_identity}, but '
+                        f'a new identity {current_user_identity} is activated. We still '
+                        'allow the operation as the two identities are likely to have '
+                        'the same access to the cluster. Please be aware that this can '
+                        'cause unexpected cluster leakage if the two identities are not '
+                        'actually equivalent (e.g., belong to the same person).'
+                    )
+                if i != 0 or len(owner_identity) != len(current_user_identity):
+                    # We update the owner of a cluster, when:
+                    # 1. The strictest identty (i.e. the first one) does not
+                    # match, but the latter ones match.
+                    # 2. The length of the two identities are different, which
+                    # will only happen when the cluster is launched before #1808.
+                    # Update the user identity to avoid showing the warning above
+                    # again.
+                    global_user_state.set_owner_identity_for_cluster(
+                        cluster_name, current_user_identity)
+                return  # The user identity matches.
         with ux_utils.print_exception_no_traceback():
-            raise exceptions.ClusterStatusFetchingError(
-                f'Failed to query {cloud} cluster {cluster!r} status: {stdout + stderr}'
-            )
+            raise exceptions.ClusterOwnerIdentityMismatchError(
+                f'{cluster_name!r} ({cloud}) is owned by account '
+                f'{owner_identity!r}, but the activated account '
+                f'is {current_user_identity!r}.')
 
-    cluster_status = stdout.strip()
-    if cluster_status == '':
-        return []
-    return [
-        status_map[s]
-        for s in cluster_status.split(deliminiator)
-        if status_map[s] is not None
-    ]
-
-
-@contextlib.contextmanager
-def suppress_output():
-    """Suppress stdout and stderr."""
-    with open(os.devnull, 'w') as devnull:
-        with contextlib.redirect_stdout(devnull), contextlib.redirect_stderr(
-                devnull):
-            yield
-
-
-def _ray_launch_hash(cluster_name: str, ray_config: Dict[str, Any]) -> Set[str]:
-    """Returns a set of Ray launch config hashes, one per node type."""
-    # Use the cached Ray launch hashes if they exist.
-    metadata = global_user_state.get_cluster_metadata(cluster_name)
-    assert metadata is not None, cluster_name
-    ray_launch_hashes = metadata.get('ray_launch_hashes', None)
-    if ray_launch_hashes is not None:
-        logger.debug('Using cached launch_caches')
-        return set(ray_launch_hashes)
-    with suppress_output():
-        ray_config = ray_commands._bootstrap_config(ray_config)  # pylint: disable=protected-access
-    # Adopted from https://github.com/ray-project/ray/blob/ray-2.0.1/python/ray/autoscaler/_private/node_launcher.py#L87-L97
-    # TODO(zhwu): this logic is duplicated from the ray code above (keep in sync).
-    launch_hashes = set()
-    head_node_type = ray_config['head_node_type']
-    for node_type, node_config in ray_config['available_node_types'].items():
-        if node_type == head_node_type:
-            launch_config = ray_config.get('head_node', {})
-        else:
-            launch_config = ray_config.get('worker_nodes', {})
-        launch_config = copy.deepcopy(launch_config)
 
-        launch_config.update(node_config['node_config'])
-        with suppress_output():
-            current_hash = ray_util.hash_launch_conf(launch_config,
-                                                     ray_config['auth'])
-        launch_hashes.add(current_hash)
-    # Cache the launch hashes for the cluster.
-    metadata['ray_launch_hashes'] = list(launch_hashes)
-    global_user_state.set_cluster_metadata(cluster_name, metadata)
-    return launch_hashes
-
-
-def _query_status_aws(
-    cluster: str,
-    ray_config: Dict[str, Any],
-) -> List[global_user_state.ClusterStatus]:
-    status_map = {
-        'pending': global_user_state.ClusterStatus.INIT,
-        'running': global_user_state.ClusterStatus.UP,
-        # TODO(zhwu): stopping and shutting-down could occasionally fail
-        # due to internal errors of AWS. We should cover that case.
-        'stopping': global_user_state.ClusterStatus.STOPPED,
-        'stopped': global_user_state.ClusterStatus.STOPPED,
-        'shutting-down': None,
-        'terminated': None,
+def tag_filter_for_cluster(cluster_name: str) -> Dict[str, str]:
+    """Returns a tag filter for the cluster."""
+    return {
+        'ray-cluster-name': cluster_name,
     }
-    region = ray_config['provider']['region']
-    launch_hashes = _ray_launch_hash(cluster, ray_config)
-    hash_filter_str = ','.join(launch_hashes)
-    query_cmd = ('aws ec2 describe-instances --filters '
-                 f'Name=tag:ray-cluster-name,Values={cluster} '
-                 f'Name=tag:ray-launch-config,Values={hash_filter_str} '
-                 f'--region {region} '
-                 '--query "Reservations[].Instances[].State.Name" '
-                 '--output text')
-    return _process_cli_query('AWS', cluster, query_cmd, '\t', status_map)
-
-
-def _query_status_gcp(
-    cluster: str,
-    ray_config: Dict[str, Any],
-) -> List[global_user_state.ClusterStatus]:
-    launch_hashes = _ray_launch_hash(cluster, ray_config)
-    hash_filter_str = ' '.join(launch_hashes)
 
-    use_tpu_vm = ray_config['provider'].get('_has_tpus', False)
-    zone = ray_config['provider'].get('availability_zone', '')
-    if use_tpu_vm:
-        # TPU VM's state definition is different from compute VM
-        # https://cloud.google.com/tpu/docs/reference/rest/v2alpha1/projects.locations.nodes#State # pylint: disable=line-too-long
-        status_map = {
-            'CREATING': global_user_state.ClusterStatus.INIT,
-            'STARTING': global_user_state.ClusterStatus.INIT,
-            'RESTARTING': global_user_state.ClusterStatus.INIT,
-            'READY': global_user_state.ClusterStatus.UP,
-            'REPAIRING': global_user_state.ClusterStatus.INIT,
-            # 'STOPPED' in GCP TPU VM means stopped, with disk preserved.
-            'STOPPING': global_user_state.ClusterStatus.STOPPED,
-            'STOPPED': global_user_state.ClusterStatus.STOPPED,
-            'PREEMPTED': None,
-        }
-        check_gcp_cli_include_tpu_vm()
-        query_cmd = ('gcloud compute tpus tpu-vm list '
-                     f'--zone {zone} '
-                     f'--filter="(labels.ray-cluster-name={cluster} AND '
-                     f'labels.ray-launch-config=({hash_filter_str}))" '
-                     '--format="value(state)"')
-    else:
-        status_map = {
-            'PROVISIONING': global_user_state.ClusterStatus.INIT,
-            'STARTING': global_user_state.ClusterStatus.INIT,
-            'RUNNING': global_user_state.ClusterStatus.UP,
-            'REPAIRING': global_user_state.ClusterStatus.INIT,
-            # 'TERMINATED' in GCP means stopped, with disk preserved.
-            'STOPPING': global_user_state.ClusterStatus.STOPPED,
-            'TERMINATED': global_user_state.ClusterStatus.STOPPED,
-            # 'SUSPENDED' in GCP means stopped, with disk and OS memory preserved.
-            'SUSPENDING': global_user_state.ClusterStatus.STOPPED,
-            'SUSPENDED': global_user_state.ClusterStatus.STOPPED,
-        }
-        # TODO(zhwu): The status of the TPU attached to the cluster should also be
-        # checked, since TPUs are not part of the VMs.
-        query_cmd = ('gcloud compute instances list '
-                     f'--filter="(labels.ray-cluster-name={cluster} AND '
-                     f'labels.ray-launch-config=({hash_filter_str}))" '
-                     '--format="value(status)"')
-    status_list = _process_cli_query('GCP', cluster, query_cmd, '\n',
-                                     status_map)
 
+def _query_cluster_status_via_cloud_api(
+    handle: 'cloud_vm_ray_backend.CloudVmRayResourceHandle'
+) -> List[status_lib.ClusterStatus]:
+    """Returns the status of the cluster."""
+    cluster_name = handle.cluster_name
+    # Use region and zone from the cluster config, instead of the
+    # handle.launched_resources, because the latter may not be set
+    # correctly yet.
+    ray_config = common_utils.read_yaml(handle.cluster_yaml)
+    provider_config = ray_config['provider']
+    region = provider_config.get('region') or provider_config.get('location')
+    zone = ray_config['provider'].get('availability_zone')
+    kwargs = {}
+    if isinstance(handle.launched_resources.cloud, clouds.GCP):
+        kwargs['use_tpu_vm'] = ray_config['provider'].get('_has_tpus', False)
+
+    # Query the cloud provider.
+    node_statuses = handle.launched_resources.cloud.query_status(
+        cluster_name, tag_filter_for_cluster(cluster_name), region, zone,
+        **kwargs)
     # GCP does not clean up preempted TPU VMs. We remove it ourselves.
     # TODO(wei-lin): handle multi-node cases.
-    if use_tpu_vm and len(status_list) == 0:
+    # TODO(zhwu): this should be moved into the GCP class, after we refactor
+    # the cluster termination, as the preempted TPU VM should always be
+    # removed.
+    if kwargs.get('use_tpu_vm', False) and len(node_statuses) == 0:
+        logger.debug(f'Terminating preempted TPU VM cluster {cluster_name}')
         backend = backends.CloudVmRayBackend()
-        handle = global_user_state.get_handle_from_cluster_name(cluster)
+        # Do not use refresh cluster status during teardown, as that will
+        # cause infinite recursion by calling cluster status refresh
+        # again.
+
+        # Post teardown cleanup be done later in this function, which will
+        # remove the cluster entry from the status table & the ssh config file.
         backend.teardown_no_lock(handle,
                                  terminate=True,
                                  purge=False,
-                                 post_teardown_cleanup=False)
+                                 post_teardown_cleanup=False,
+                                 refresh_cluster_status=False)
+    return node_statuses
 
-    return status_list
 
+def check_can_clone_disk_and_override_task(
+    cluster_name: str, target_cluster_name: Optional[str], task: 'task_lib.Task'
+) -> Tuple['task_lib.Task', 'cloud_vm_ray_backend.CloudVmRayResourceHandle']:
+    """Check if the task is compatible to clone disk from the source cluster.
 
-def _query_status_azure(
-    cluster: str,
-    ray_config: Dict[str, Any],
-) -> List[global_user_state.ClusterStatus]:
-    status_map = {
-        'VM starting': global_user_state.ClusterStatus.INIT,
-        'VM running': global_user_state.ClusterStatus.UP,
-        # 'VM stopped' in Azure means Stopped (Allocated), which still bills
-        # for the VM.
-        'VM stopping': global_user_state.ClusterStatus.INIT,
-        'VM stopped': global_user_state.ClusterStatus.INIT,
-        # 'VM deallocated' in Azure means Stopped (Deallocated), which does not
-        # bill for the VM.
-        'VM deallocating': global_user_state.ClusterStatus.STOPPED,
-        'VM deallocated': global_user_state.ClusterStatus.STOPPED,
-    }
-    launch_hashes = _ray_launch_hash(cluster, ray_config)
-    hash_filter_str = ', '.join(f'\\"{h}\\"' for h in launch_hashes)
-    query_cmd = (
-        'az vm show -d --ids $(az vm list --query '
-        f'"[?tags.\\"ray-cluster-name\\" == \'{cluster}\' && '
-        f'contains(\'[{hash_filter_str}]\', tags.\\"ray-launch-config\\")].id" '
-        '-o tsv) --query "powerState" -o tsv')
-    # NOTE: Azure cli should be handled carefully. The query command above
-    # takes about 1 second to run.
-    # An alternative is the following command, but it will take more than
-    # 20 seconds to run.
-    # query_cmd = (
-    #     f'az vm list --show-details --query "['
-    #     f'?tags.\\"ray-cluster-name\\" == \'{handle.cluster_name}\' '
-    #     '&& tags.\\"ray-node-type\\" == \'head\'].powerState" -o tsv'
-    # )
-    return _process_cli_query('Azure', cluster, query_cmd, '\t', status_map)
-
-
-_QUERY_STATUS_FUNCS = {
-    'AWS': _query_status_aws,
-    'GCP': _query_status_gcp,
-    'Azure': _query_status_azure,
-}
+    Args:
+        cluster_name: The name of the cluster to clone disk from.
+        target_cluster_name: The name of the target cluster.
+        task: The task to check.
 
+    Returns:
+        The task to use and the resource handle of the source cluster.
 
-def _get_cluster_status_via_cloud_cli(
-    handle: 'backends.Backend.ResourceHandle'
-) -> List[global_user_state.ClusterStatus]:
-    """Returns the status of the cluster."""
-    resources: sky.Resources = handle.launched_resources
-    cloud = resources.cloud
-    ray_config = common_utils.read_yaml(handle.cluster_yaml)
-    return _QUERY_STATUS_FUNCS[str(cloud)](handle.cluster_name, ray_config)
+    Raises:
+        ValueError: If the source cluster does not exist.
+        exceptions.NotSupportedError: If the source cluster is not valid or the
+            task is not compatible to clone disk from the source cluster.
+    """
+    source_cluster_status, handle = refresh_cluster_status_handle(cluster_name)
+    if source_cluster_status is None:
+        with ux_utils.print_exception_no_traceback():
+            raise ValueError(
+                f'Cannot find cluster {cluster_name!r} to clone disk from.')
+
+    if not isinstance(handle, backends.CloudVmRayResourceHandle):
+        with ux_utils.print_exception_no_traceback():
+            raise exceptions.NotSupportedError(
+                f'Cannot clone disk from a non-cloud cluster {cluster_name!r}.')
+
+    if source_cluster_status != status_lib.ClusterStatus.STOPPED:
+        with ux_utils.print_exception_no_traceback():
+            raise exceptions.NotSupportedError(
+                f'Cannot clone disk from cluster {cluster_name!r} '
+                f'({source_cluster_status!r}). Please stop the '
+                f'cluster first: sky stop {cluster_name}')
+
+    if target_cluster_name is not None:
+        target_cluster_status, _ = refresh_cluster_status_handle(
+            target_cluster_name)
+        if target_cluster_status is not None:
+            with ux_utils.print_exception_no_traceback():
+                raise exceptions.NotSupportedError(
+                    f'The target cluster {target_cluster_name!r} already exists. Cloning '
+                    'disk is only supported when creating a new cluster. To fix: specify '
+                    'a new target cluster name.')
+
+    assert len(task.resources) == 1, task.resources
+    task_resources = list(task.resources)[0]
+    if handle.launched_resources.disk_size > task_resources.disk_size:
+        # The target cluster's disk should be at least as large as the source.
+        with ux_utils.print_exception_no_traceback():
+            target_cluster_name_str = f' {target_cluster_name!r}'
+            if target_cluster_name is None:
+                target_cluster_name_str = ''
+            raise exceptions.NotSupportedError(
+                f'The target cluster{target_cluster_name_str} should have a disk size '
+                f'of at least {handle.launched_resources.disk_size} GB to clone the '
+                f'disk from {cluster_name!r}.')
+    override_param = {}
+    original_cloud = handle.launched_resources.cloud
+    assert original_cloud is not None, handle.launched_resources
+    if task_resources.cloud is None:
+        override_param['cloud'] = original_cloud
+    else:
+        if not original_cloud.is_same_cloud(task_resources.cloud):
+            with ux_utils.print_exception_no_traceback():
+                raise ValueError(
+                    f'Cannot clone disk across cloud from {original_cloud} to '
+                    f'{task_resources.cloud}.')
+    original_cloud.check_features_are_supported(
+        {clouds.CloudImplementationFeatures.CLONE_DISK_FROM_CLUSTER})
+
+    if task_resources.region is None:
+        override_param['region'] = handle.launched_resources.region
+
+    if override_param:
+        logger.info(
+            f'No cloud/region specified for the task. Using the same region '
+            f'as source cluster {cluster_name!r}: '
+            f'{handle.launched_resources.cloud}'
+            f'({handle.launched_resources.region}).')
+        task_resources = task_resources.copy(**override_param)
+        task.set_resources({task_resources})
+        # Reset the best_resources to triger re-optimization
+        # later, so that the new task_resources will be used.
+        task.best_resources = None
+    return task, handle
 
 
 def _update_cluster_status_no_lock(
         cluster_name: str) -> Optional[Dict[str, Any]]:
     record = global_user_state.get_cluster_from_name(cluster_name)
     if record is None:
         return None
     handle = record['handle']
-    if not isinstance(handle, backends.CloudVmRayBackend.ResourceHandle):
+    if not isinstance(handle, backends.CloudVmRayResourceHandle):
         return record
-
     cluster_name = handle.cluster_name
-    try:
-        # TODO(zhwu): This function cannot distinguish transient network error
-        # in ray's get IPs vs. ray runtime failing.
-        ips = get_node_ips(handle.cluster_yaml, handle.launched_nodes)
-        # This happens to a stopped TPU VM as we use gcloud to query the IP.
-        if len(ips) == 0:
-            raise exceptions.FetchIPError(
-                reason=exceptions.FetchIPError.Reason.HEAD)
-        if handle.launched_nodes == 1:
-            # Check the ray cluster status. We have to check it for single node
-            # case, since the get_node_ips() does not require ray cluster to be
-            # running.
+
+    node_statuses = _query_cluster_status_via_cloud_api(handle)
+
+    all_nodes_up = (all(
+        status == status_lib.ClusterStatus.UP for status in node_statuses) and
+                    len(node_statuses) == handle.launched_nodes)
+
+    def run_ray_status_to_check_ray_cluster_healthy() -> bool:
+        try:
+            # TODO(zhwu): This function cannot distinguish transient network
+            # error in ray's get IPs vs. ray runtime failing.
+            #
+            # NOTE: using use_cached_ips=False is very slow as it calls into
+            # `ray get head-ip/worker-ips`. Setting it to True is safe because
+            # in the worst case we time out in the `ray status` SSH command
+            # below.
+            external_ips = handle.external_ips(use_cached_ips=True)
+            # This happens to a stopped TPU VM as we use gcloud to query the IP.
+            if external_ips is None or len(external_ips) == 0:
+                raise exceptions.FetchIPError(
+                    reason=exceptions.FetchIPError.Reason.HEAD)
+            # Check if ray cluster status is healthy.
             ssh_credentials = ssh_credential_from_yaml(handle.cluster_yaml)
-            runner = command_runner.SSHCommandRunner(ips[0], **ssh_credentials)
-            returncode = runner.run('ray status', stream_logs=False)
-            if returncode:
+            runner = command_runner.SSHCommandRunner(external_ips[0],
+                                                     **ssh_credentials)
+            rc, output, _ = runner.run(RAY_STATUS_WITH_SKY_RAY_PORT_COMMAND,
+                                       stream_logs=False,
+                                       require_outputs=True,
+                                       separate_stderr=True)
+            if rc:
                 raise exceptions.FetchIPError(
                     reason=exceptions.FetchIPError.Reason.HEAD)
-        # If we get node ips correctly, the cluster is UP. It is safe to
-        # set the status to UP, as the `get_node_ips` function uses ray
-        # to fetch IPs and starting ray is the final step of sky launch.
-        record['status'] = global_user_state.ClusterStatus.UP
-        handle.head_ip = ips[0]
+
+            ready_head, ready_workers = _count_healthy_nodes_from_ray(output)
+            if ready_head + ready_workers == handle.launched_nodes:
+                return True
+        except exceptions.FetchIPError:
+            logger.debug(
+                'Refreshing status: Failed to use `ray` to get IPs from cluster'
+                f' {cluster_name!r}.')
+        return False
+
+    # Determining if the cluster is healthy (UP):
+    #
+    # For non-spot clusters: If ray status shows all nodes are healthy, it is
+    # safe to set the status to UP as starting ray is the final step of sky
+    # launch. But we found that ray status is way too slow (see NOTE below) so
+    # we always query the cloud provider first which is faster.
+    #
+    # For spot clusters: the above can be unsafe because the Ray cluster may
+    # remain healthy for a while before the cloud completely preempts the VMs.
+    # We have mitigated this by again first querying the VM state from the cloud
+    # provider.
+    if all_nodes_up and run_ray_status_to_check_ray_cluster_healthy():
+        # NOTE: all_nodes_up calculation is fast due to calling cloud CLI;
+        # run_ray_status_to_check_all_nodes_up() is slow due to calling `ray get
+        # head-ip/worker-ips`.
+        record['status'] = status_lib.ClusterStatus.UP
         global_user_state.add_or_update_cluster(cluster_name,
                                                 handle,
+                                                requested_resources=None,
                                                 ready=True,
                                                 is_launch=False)
         return record
-    except exceptions.FetchIPError:
-        logger.debug('Refreshing status: Failed to get IPs from cluster '
-                     f'{cluster_name!r}, trying to fetch from provider.')
-    # For all code below, ray fails to get IPs for the cluster.
-    node_statuses = _get_cluster_status_via_cloud_cli(handle)
+
+    # All cases below are transitioning the cluster to non-UP states.
+
+    if len(node_statuses) > handle.launched_nodes:
+        # Unexpected: in the queried region more than 1 cluster with the same
+        # constructed name tag returned. This will typically not happen unless
+        # users manually create a cluster with that constructed name or there
+        # was a resource leak caused by different launch hash before #1671
+        # was merged.
+        #
+        # (Technically speaking, even if returned num nodes <= num
+        # handle.launched_nodes), not including the launch hash could mean the
+        # returned nodes contain some nodes that do not belong to the logical
+        # skypilot cluster. Doesn't seem to be a good way to handle this for
+        # now?)
+        #
+        # We have not experienced the above; adding as a safeguard.
+        #
+        # Since we failed to refresh, raise the status fetching error.
+        with ux_utils.print_exception_no_traceback():
+            raise exceptions.ClusterStatusFetchingError(
+                f'Found {len(node_statuses)} node(s) with the same cluster name'
+                f' tag in the cloud provider for cluster {cluster_name!r}, '
+                f'which should have {handle.launched_nodes} nodes. This '
+                f'normally should not happen. {colorama.Fore.RED}Please check '
+                'the cloud console and fix any possible resources leakage '
+                '(e.g., if there are any stopped nodes and they do not have '
+                'data or are unhealthy, terminate them).'
+                f'{colorama.Style.RESET_ALL}')
+    assert len(node_statuses) <= handle.launched_nodes
 
     # If the node_statuses is empty, all the nodes are terminated. We can
     # safely set the cluster status to TERMINATED. This handles the edge case
     # where the cluster is terminated by the user manually through the UI.
     to_terminate = not node_statuses
 
-    # A cluster is considered "abnormal", if not all nodes are TERMINATED or not all
-    # nodes are STOPPED. We check that with the following logic:
-    #   * not all nodes are terminated and there's at least one node terminated; or
-    #   * any of the non-TERMINATED nodes is in a non-STOPPED status.
+    # A cluster is considered "abnormal", if not all nodes are TERMINATED or
+    # not all nodes are STOPPED. We check that with the following logic:
+    #   * Not all nodes are terminated and there's at least one node
+    #     terminated; or
+    #   * Any of the non-TERMINATED nodes is in a non-STOPPED status.
     #
     # This includes these special cases:
-    # All stopped are considered normal and will be cleaned up at the end of the function.
-    # Some of the nodes UP should be considered abnormal, because the ray cluster is
-    # probably down.
-    # The cluster is partially terminated or stopped should be considered abnormal.
+    #   * All stopped are considered normal and will be cleaned up at the end
+    #     of the function.
+    #   * Some of the nodes UP should be considered abnormal, because the ray
+    #     cluster is probably down.
+    #   * The cluster is partially terminated or stopped should be considered
+    #     abnormal.
     #
-    # An abnormal cluster will transition to INIT and have any autostop setting reset.
-    is_abnormal = ((0 < len(node_statuses) < handle.launched_nodes) or
-                   any(status != global_user_state.ClusterStatus.STOPPED
-                       for status in node_statuses))
+    # An abnormal cluster will transition to INIT and have any autostop setting
+    # reset (unless it's autostopping/autodowning).
+    is_abnormal = ((0 < len(node_statuses) < handle.launched_nodes) or any(
+        status != status_lib.ClusterStatus.STOPPED for status in node_statuses))
     if is_abnormal:
-        # Reset the autostop to avoid false information with best effort.
-        # Side effect: if the status is refreshed during autostopping, the
-        # autostop field in the local cache will be reset, even though the
-        # cluster will still be correctly stopped.
-        try:
-            backend = backends.CloudVmRayBackend()
-            backend.set_autostop(handle, -1, stream_logs=False)
-        except (Exception, SystemExit) as e:  # pylint: disable=broad-except
-            logger.debug(f'Failed to reset autostop. Due to {type(e)}: {e}')
-        global_user_state.set_cluster_autostop_value(handle.cluster_name,
-                                                     -1,
-                                                     to_down=False)
-
-        # If the user starts part of a STOPPED cluster, we still need a status to
-        # represent the abnormal status. For spot cluster, it can also represent
-        # that the cluster is partially preempted.
+        backend = get_backend_from_handle(handle)
+        if isinstance(backend,
+                      backends.CloudVmRayBackend) and record['autostop'] >= 0:
+            if not backend.is_definitely_autostopping(handle,
+                                                      stream_logs=False):
+                # Reset the autostopping as the cluster is abnormal, and may
+                # not correctly autostop. Resetting the autostop will let
+                # the user know that the autostop may not happen to avoid
+                # leakages from the assumption that the cluster will autostop.
+                success = True
+                try:
+                    backend.set_autostop(handle, -1, stream_logs=False)
+                except (Exception, SystemExit) as e:  # pylint: disable=broad-except
+                    success = False
+                    logger.debug(f'Failed to reset autostop. Due to '
+                                 f'{common_utils.format_exception(e)}')
+                global_user_state.set_cluster_autostop_value(
+                    handle.cluster_name, -1, to_down=False)
+
+                # Friendly hint.
+                autostop = record['autostop']
+                maybe_down_str = ' --down' if record['to_down'] else ''
+                noun = 'autodown' if record['to_down'] else 'autostop'
+                if success:
+                    operation_str = (f'Canceled {noun} on the cluster '
+                                     f'{cluster_name!r}')
+                else:
+                    operation_str = (
+                        f'Attempted to cancel {noun} on the '
+                        f'cluster {cluster_name!r} with best effort')
+                yellow = colorama.Fore.YELLOW
+                bright = colorama.Style.BRIGHT
+                reset = colorama.Style.RESET_ALL
+                ux_utils.console_newline()
+                logger.warning(
+                    f'{yellow}{operation_str}, since it is found to be in an '
+                    f'abnormal state. To fix, try running: {reset}{bright}sky '
+                    f'start -f -i {autostop}{maybe_down_str} {cluster_name}'
+                    f'{reset}')
+            else:
+                ux_utils.console_newline()
+                operation_str = 'autodowning' if record[
+                    'to_down'] else 'autostopping'
+                logger.info(
+                    f'Cluster {cluster_name!r} is {operation_str}. Setting to '
+                    'INIT status; try refresh again in a while.')
+
+        # If the user starts part of a STOPPED cluster, we still need a status
+        # to represent the abnormal status. For spot cluster, it can also
+        # represent that the cluster is partially preempted.
         # TODO(zhwu): the definition of INIT should be audited/changed.
         # Adding a new status UNHEALTHY for abnormal status can be a choice.
-        global_user_state.set_cluster_status(
-            cluster_name, global_user_state.ClusterStatus.INIT)
+        global_user_state.add_or_update_cluster(cluster_name,
+                                                handle,
+                                                requested_resources=None,
+                                                ready=False,
+                                                is_launch=False)
         return global_user_state.get_cluster_from_name(cluster_name)
-    # Now is_abnormal is False: either node_statuses is empty or all nodes are STOPPED.
+    # Now is_abnormal is False: either node_statuses is empty or all nodes are
+    # STOPPED.
     backend = backends.CloudVmRayBackend()
-    # TODO(zhwu): adding output for the cluster removed by status refresh.
     backend.post_teardown_cleanup(handle, terminate=to_terminate, purge=False)
     return global_user_state.get_cluster_from_name(cluster_name)
 
 
 def _update_cluster_status(
         cluster_name: str,
         acquire_per_cluster_status_lock: bool) -> Optional[Dict[str, Any]]:
-    """Update the cluster status by checking ray cluster and real status from cloud.
+    """Update the cluster status.
+
+    The cluster status is updated by checking ray cluster and real status from
+    cloud.
 
-    The function will update the cached cluster status in the global state. For the
-    design of the cluster status and transition, please refer to the
+    The function will update the cached cluster status in the global state. For
+    the design of the cluster status and transition, please refer to the
     sky/design_docs/cluster_status.md
 
+    Args:
+        cluster_name: The name of the cluster.
+        acquire_per_cluster_status_lock: Whether to acquire the per-cluster lock
+            before updating the status.
+        need_owner_identity_check: Whether to check the owner identity before
+            updating
+
     Returns:
-      If the cluster is terminated or does not exist, return None.
-      Otherwise returns the input record with status and ip potentially updated.
+        If the cluster is terminated or does not exist, return None. Otherwise
+        returns the input record with status and handle potentially updated.
+
+    Raises:
+        exceptions.ClusterOwnerIdentityMismatchError: if the current user is
+          not the same as the user who created the cluster.
+        exceptions.CloudUserIdentityError: if we fail to get the current user
+          identity.
+        exceptions.ClusterStatusFetchingError: the cluster status cannot be
+          fetched from the cloud provider or there are leaked nodes causing
+          the node number larger than expected.
     """
     if not acquire_per_cluster_status_lock:
         return _update_cluster_status_no_lock(cluster_name)
 
     try:
         # TODO(mraheja): remove pylint disabling when filelock
         # version updated
         # pylint: disable=abstract-class-instantiated
         with filelock.FileLock(CLUSTER_STATUS_LOCK_PATH.format(cluster_name),
                                CLUSTER_STATUS_LOCK_TIMEOUT_SECONDS):
             return _update_cluster_status_no_lock(cluster_name)
     except filelock.Timeout:
-        logger.debug(
-            f'Refreshing status: Failed get the lock for cluster {cluster_name!r}.'
-            ' Using the cached status.')
+        logger.debug('Refreshing status: Failed get the lock for cluster '
+                     f'{cluster_name!r}. Using the cached status.')
         return global_user_state.get_cluster_from_name(cluster_name)
 
 
-@timeline.event
-def refresh_cluster_status_handle(
-    cluster_name: str,
-    *,
-    force_refresh: bool = False,
-    acquire_per_cluster_status_lock: bool = True,
-) -> Tuple[Optional[global_user_state.ClusterStatus],
-           Optional[backends.Backend.ResourceHandle]]:
+def _refresh_cluster_record(
+        cluster_name: str,
+        *,
+        force_refresh_statuses: Optional[Set[status_lib.ClusterStatus]] = None,
+        acquire_per_cluster_status_lock: bool = True
+) -> Optional[Dict[str, Any]]:
+    """Refresh the cluster, and return the possibly updated record.
+
+    This function will also check the owner identity of the cluster, and raise
+    exceptions if the current user is not the same as the user who created the
+    cluster.
+
+    Args:
+        cluster_name: The name of the cluster.
+        force_refresh_statuses: if specified, refresh the cluster if it has one of
+            the specified statuses. Additionally, clusters satisfying the
+            following conditions will always be refreshed no matter the
+            argument is specified or not:
+                1. is a spot cluster, or
+                2. is a non-spot cluster, is not STOPPED, and autostop is set.
+        acquire_per_cluster_status_lock: Whether to acquire the per-cluster lock
+            before updating the status.
+
+    Returns:
+        If the cluster is terminated or does not exist, return None.
+        Otherwise returns the cluster record.
+
+    Raises:
+        exceptions.ClusterOwnerIdentityMismatchError: if the current user is
+          not the same as the user who created the cluster.
+        exceptions.CloudUserIdentityError: if we fail to get the current user
+          identity.
+        exceptions.ClusterStatusFetchingError: the cluster status cannot be
+          fetched from the cloud provider or there are leaked nodes causing
+          the node number larger than expected.
+    """
+
     record = global_user_state.get_cluster_from_name(cluster_name)
     if record is None:
-        return None, None
+        return None
+    check_owner_identity(cluster_name)
 
     handle = record['handle']
-    if isinstance(handle, backends.CloudVmRayBackend.ResourceHandle):
-        if force_refresh or record['autostop'] >= 0:
-            # Refresh the status only when force_refresh is True or the cluster
-            # has autostopped turned on.
+    if isinstance(handle, backends.CloudVmRayResourceHandle):
+        use_spot = handle.launched_resources.use_spot
+        has_autostop = (record['status'] != status_lib.ClusterStatus.STOPPED and
+                        record['autostop'] >= 0)
+        force_refresh_for_cluster = (force_refresh_statuses is not None and
+                                     record['status'] in force_refresh_statuses)
+        if force_refresh_for_cluster or has_autostop or use_spot:
             record = _update_cluster_status(
                 cluster_name,
                 acquire_per_cluster_status_lock=acquire_per_cluster_status_lock)
-            if record is None:
-                return None, None
+    return record
+
+
+@timeline.event
+def refresh_cluster_status_handle(
+    cluster_name: str,
+    *,
+    force_refresh_statuses: Optional[Set[status_lib.ClusterStatus]] = None,
+    acquire_per_cluster_status_lock: bool = True,
+) -> Tuple[Optional[status_lib.ClusterStatus],
+           Optional[backends.ResourceHandle]]:
+    """Refresh the cluster, and return the possibly updated status and handle.
+
+    This is a wrapper of refresh_cluster_record, which returns the status and
+    handle of the cluster.
+    Please refer to the docstring of refresh_cluster_record for the details.
+    """
+    record = _refresh_cluster_record(
+        cluster_name,
+        force_refresh_statuses=force_refresh_statuses,
+        acquire_per_cluster_status_lock=acquire_per_cluster_status_lock)
+    if record is None:
+        return None, None
     return record['status'], record['handle']
 
 
+# =====================================
+
+
+@typing.overload
+def check_cluster_available(
+    cluster_name: str,
+    *,
+    operation: str,
+    check_cloud_vm_ray_backend: Literal[True] = True,
+) -> 'cloud_vm_ray_backend.CloudVmRayResourceHandle':
+    ...
+
+
+@typing.overload
+def check_cluster_available(
+    cluster_name: str,
+    *,
+    operation: str,
+    check_cloud_vm_ray_backend: Literal[False],
+) -> backends.ResourceHandle:
+    ...
+
+
+def check_cluster_available(
+    cluster_name: str,
+    *,
+    operation: str,
+    check_cloud_vm_ray_backend: bool = True,
+) -> backends.ResourceHandle:
+    """Check if the cluster is available.
+
+    Raises:
+        ValueError: if the cluster does not exist.
+        exceptions.ClusterNotUpError: if the cluster is not UP.
+        exceptions.NotSupportedError: if the cluster is not based on
+          CloudVmRayBackend.
+        exceptions.ClusterOwnerIdentityMismatchError: if the current user is
+          not the same as the user who created the cluster.
+        exceptions.CloudUserIdentityError: if we fail to get the current user
+          identity.
+    """
+    try:
+        cluster_status, handle = refresh_cluster_status_handle(cluster_name)
+    except exceptions.ClusterStatusFetchingError as e:
+        # Failed to refresh the cluster status is not fatal error as the callers
+        # can still be done by only using ssh, but the ssh can hang if the
+        # cluster is not up (e.g., autostopped).
+
+        # We do not catch the exception for cloud identity checking for now, in
+        # order to disable all operations on clusters created by another user
+        # identity.  That will make the design simpler and easier to
+        # understand, but it might be useful to allow the user to use
+        # operations that only involve ssh (e.g., sky exec, sky logs, etc) even
+        # if the user is not the owner of the cluster.
+        ux_utils.console_newline()
+        logger.warning(
+            f'Failed to refresh the status for cluster {cluster_name!r}. It is '
+            f'not fatal, but {operation} might hang if the cluster is not up.\n'
+            f'Detailed reason: {e}')
+        record = global_user_state.get_cluster_from_name(cluster_name)
+        if record is None:
+            cluster_status, handle = None, None
+        else:
+            cluster_status, handle = record['status'], record['handle']
+
+    bright = colorama.Style.BRIGHT
+    reset = colorama.Style.RESET_ALL
+    if handle is None:
+        with ux_utils.print_exception_no_traceback():
+            raise ValueError(
+                f'{colorama.Fore.YELLOW}Cluster {cluster_name!r} does not '
+                f'exist.{reset}')
+    backend = get_backend_from_handle(handle)
+    if check_cloud_vm_ray_backend and not isinstance(
+            backend, backends.CloudVmRayBackend):
+        with ux_utils.print_exception_no_traceback():
+            raise exceptions.NotSupportedError(
+                f'{colorama.Fore.YELLOW}{operation.capitalize()}: skipped for '
+                f'cluster {cluster_name!r}. It is only supported by backend: '
+                f'{backends.CloudVmRayBackend.NAME}.'
+                f'{reset}')
+    if cluster_status != status_lib.ClusterStatus.UP:
+        if onprem_utils.check_if_local_cloud(cluster_name):
+            raise exceptions.ClusterNotUpError(
+                constants.UNINITIALIZED_ONPREM_CLUSTER_MESSAGE.format(
+                    cluster_name),
+                cluster_status=cluster_status,
+                handle=handle)
+        with ux_utils.print_exception_no_traceback():
+            hint_for_init = ''
+            if cluster_status == status_lib.ClusterStatus.INIT:
+                hint_for_init = (
+                    f'{reset} Wait for a launch to finish, or use this command '
+                    f'to try to transition the cluster to UP: {bright}sky '
+                    f'start {cluster_name}{reset}')
+            raise exceptions.ClusterNotUpError(
+                f'{colorama.Fore.YELLOW}{operation.capitalize()}: skipped for '
+                f'cluster {cluster_name!r} (status: {cluster_status.value}). '
+                'It is only allowed for '
+                f'{status_lib.ClusterStatus.UP.value} clusters.'
+                f'{hint_for_init}'
+                f'{reset}',
+                cluster_status=cluster_status,
+                handle=handle)
+
+    if handle.head_ip is None:
+        with ux_utils.print_exception_no_traceback():
+            raise exceptions.ClusterNotUpError(
+                f'Cluster {cluster_name!r} has been stopped or not properly '
+                'set up. Please re-launch it with `sky start`.',
+                cluster_status=cluster_status,
+                handle=handle)
+    return handle
+
+
 class CloudFilter(enum.Enum):
     # Filter for all types of clouds.
     ALL = 'all'
     # Filter for Sky's main clouds (aws, gcp, azure, docker).
     CLOUDS_AND_DOCKER = 'clouds-and-docker'
     # Filter for only local clouds.
     LOCAL = 'local'
 
 
 def get_clusters(
-        include_reserved: bool,
-        refresh: bool,
-        cloud_filter: str = CloudFilter.CLOUDS_AND_DOCKER
+    include_reserved: bool,
+    refresh: bool,
+    cloud_filter: CloudFilter = CloudFilter.CLOUDS_AND_DOCKER,
+    cluster_names: Optional[Union[str, List[str]]] = None,
 ) -> List[Dict[str, Any]]:
-    """Returns a list of cached cluster records.
+    """Returns a list of cached or optionally refreshed cluster records.
 
     Combs through the database (in ~/.sky/state.db) to get a list of records
-    corresponding to launched clusters.
+    corresponding to launched clusters (filtered by `cluster_names` if it is
+    specified). The refresh flag can be used to force a refresh of the status
+    of the clusters.
 
     Args:
         include_reserved: Whether to include reserved clusters, e.g. spot
             controller.
         refresh: Whether to refresh the status of the clusters. (Refreshing will
             set the status to STOPPED if the cluster cannot be pinged.)
         cloud_filter: Sets which clouds to filer through from the global user
             state. Supports three values, 'all' for all clouds, 'public' for
             public clouds only, and 'local' for only local clouds.
+        cluster_names: If provided, only return records for the given cluster
+            names.
 
     Returns:
-        A list of cluster records.
+        A list of cluster records. If the cluster does not exist or has been
+        terminated, the record will be omitted from the returned list.
     """
     records = global_user_state.get_clusters()
 
     if not include_reserved:
         records = [
             record for record in records
             if record['name'] not in SKY_RESERVED_CLUSTER_NAMES
         ]
 
+    yellow = colorama.Fore.YELLOW
+    bright = colorama.Style.BRIGHT
+    reset = colorama.Style.RESET_ALL
+
+    if cluster_names is not None:
+        if isinstance(cluster_names, str):
+            cluster_names = [cluster_names]
+        new_records = []
+        not_exist_cluster_names = []
+        for cluster_name in cluster_names:
+            for record in records:
+                if record['name'] == cluster_name:
+                    new_records.append(record)
+                    break
+            else:
+                not_exist_cluster_names.append(cluster_name)
+        if not_exist_cluster_names:
+            clusters_str = ', '.join(not_exist_cluster_names)
+            logger.info(f'Cluster(s) not found: {bright}{clusters_str}{reset}.')
+        records = new_records
+
     def _is_local_cluster(record):
         handle = record['handle']
-        if isinstance(handle, backends.LocalDockerBackend.ResourceHandle):
+        if isinstance(handle, backends.LocalDockerResourceHandle):
             return False
         cluster_resources = handle.launched_resources
         return isinstance(cluster_resources.cloud, clouds.Local)
 
     if cloud_filter == CloudFilter.LOCAL:
         records = [record for record in records if _is_local_cluster(record)]
     elif cloud_filter == CloudFilter.CLOUDS_AND_DOCKER:
@@ -1813,89 +2332,110 @@
                                       redirect_stdout=False,
                                       redirect_stderr=False)
     task = progress.add_task(
         f'[bold cyan]Refreshing status for {len(records)} cluster{plural}[/]',
         total=len(records))
 
     def _refresh_cluster(cluster_name):
-        record = _update_cluster_status(cluster_name,
-                                        acquire_per_cluster_status_lock=True)
+        try:
+            record = _refresh_cluster_record(
+                cluster_name,
+                force_refresh_statuses=set(status_lib.ClusterStatus),
+                acquire_per_cluster_status_lock=True)
+        except (exceptions.ClusterStatusFetchingError,
+                exceptions.CloudUserIdentityError,
+                exceptions.ClusterOwnerIdentityMismatchError) as e:
+            # Do not fail the entire refresh process. The caller will
+            # handle the 'UNKNOWN' status, and collect the errors into
+            # a table.
+            record = {'status': 'UNKNOWN', 'error': e}
         progress.update(task, advance=1)
         return record
 
     cluster_names = [record['name'] for record in records]
     with progress:
         updated_records = subprocess_utils.run_in_parallel(
             _refresh_cluster, cluster_names)
 
     # Show information for removed clusters.
-    autodown_clusters, remaining_clusters = [], []
+    kept_records = []
+    autodown_clusters, remaining_clusters, failed_clusters = [], [], []
     for i, record in enumerate(records):
         if updated_records[i] is None:
             if record['to_down']:
                 autodown_clusters.append(cluster_names[i])
             else:
                 remaining_clusters.append(cluster_names[i])
+        elif updated_records[i]['status'] == 'UNKNOWN':
+            failed_clusters.append(
+                (cluster_names[i], updated_records[i]['error']))
+            # Keep the original record if the status is unknown,
+            # so that the user can still see the cluster.
+            kept_records.append(record)
+        else:
+            kept_records.append(updated_records[i])
 
-    yellow = colorama.Fore.YELLOW
-    bright = colorama.Style.BRIGHT
-    reset = colorama.Style.RESET_ALL
     if autodown_clusters:
         plural = 's' if len(autodown_clusters) > 1 else ''
         cluster_str = ', '.join(autodown_clusters)
         logger.info(f'Autodowned cluster{plural}: '
                     f'{bright}{cluster_str}{reset}')
     if remaining_clusters:
         plural = 's' if len(remaining_clusters) > 1 else ''
         cluster_str = ', '.join(name for name in remaining_clusters)
         logger.warning(f'{yellow}Cluster{plural} terminated on '
                        f'the cloud: {reset}{bright}{cluster_str}{reset}')
 
-    # Filter out removed clusters.
-    updated_records = [
-        record for record in updated_records if record is not None
-    ]
-    return updated_records
+    if failed_clusters:
+        plural = 's' if len(failed_clusters) > 1 else ''
+        logger.warning(f'{yellow}Failed to refresh status for '
+                       f'{len(failed_clusters)} cluster{plural}:{reset}')
+        for cluster_name, e in failed_clusters:
+            logger.warning(f'  {bright}{cluster_name}{reset}: {e}')
+    return kept_records
 
 
+@typing.overload
 def get_backend_from_handle(
-        handle: backends.Backend.ResourceHandle) -> backends.Backend:
+    handle: 'cloud_vm_ray_backend.CloudVmRayResourceHandle'
+) -> 'cloud_vm_ray_backend.CloudVmRayBackend':
+    ...
+
+
+@typing.overload
+def get_backend_from_handle(
+    handle: 'local_docker_backend.LocalDockerResourceHandle'
+) -> 'local_docker_backend.LocalDockerBackend':
+    ...
+
+
+@typing.overload
+def get_backend_from_handle(
+        handle: backends.ResourceHandle) -> backends.Backend:
+    ...
+
+
+def get_backend_from_handle(
+        handle: backends.ResourceHandle) -> backends.Backend:
     """Gets a Backend object corresponding to a handle.
 
     Inspects handle type to infer the backend used for the resource.
     """
-    if isinstance(handle, backends.CloudVmRayBackend.ResourceHandle):
+    backend: backends.Backend
+    if isinstance(handle, backends.CloudVmRayResourceHandle):
         backend = backends.CloudVmRayBackend()
-    elif isinstance(handle, backends.LocalDockerBackend.ResourceHandle):
+    elif isinstance(handle, backends.LocalDockerResourceHandle):
         backend = backends.LocalDockerBackend()
     else:
         raise NotImplementedError(
             f'Handle type {type(handle)} is not supported yet.')
     return backend
 
 
-class NoOpConsole:
-    """An empty class for multi-threaded console.status."""
-
-    def __enter__(self):
-        pass
-
-    def __exit__(self, exc_type, exc_val, exc_tb):
-        pass
-
-
-def safe_console_status(msg: str):
-    """A wrapper for multi-threaded console.status."""
-    if threading.current_thread() is threading.main_thread():
-        return console.status(msg)
-    return NoOpConsole()
-
-
-def get_task_demands_dict(
-        task: 'task_lib.Task') -> Optional[Tuple[Optional[str], int]]:
+def get_task_demands_dict(task: 'task_lib.Task') -> Optional[Dict[str, float]]:
     """Returns the accelerator dict of the task"""
     # TODO: CPU and other memory resources are not supported yet.
     accelerator_dict = None
     if task.best_resources is not None:
         resources = task.best_resources
     else:
         # Task may (e.g., sky launch) or may not (e.g., sky exec) have undergone
@@ -1913,93 +2453,33 @@
         resources_str = f'CPU:{DEFAULT_TASK_CPU_DEMAND}'
     else:
         resources_str = ', '.join(f'{k}:{v}' for k, v in resources_dict.items())
     resources_str = f'{task.num_nodes}x [{resources_str}]'
     return resources_str
 
 
-def check_cluster_name_is_valid(cluster_name: str,
-                                cloud: Optional[clouds.Cloud] = None) -> None:
-    """Errors out on invalid cluster names not supported by cloud providers.
-
-    Bans (including but not limited to) names that:
-    - are digits-only
-    - contain underscore (_)
-    """
-    if cluster_name is None:
-        return
-    # GCP errors return this exact regex.  An informal description is at:
-    # https://cloud.google.com/compute/docs/naming-resources#resource-name-format
-    valid_regex = '[a-z]([-a-z0-9]{0,61}[a-z0-9])?'
-    if re.fullmatch(valid_regex, cluster_name) is None:
-        with ux_utils.print_exception_no_traceback():
-            raise exceptions.InvalidClusterNameError(
-                f'Cluster name "{cluster_name}" is invalid; '
-                f'ensure it is fully matched by regex: {valid_regex}')
-    if isinstance(cloud, clouds.GCP):
-        # GCP has too restrictive of a length limit. Don't check for other
-        # clouds.
-        if len(cluster_name) > _MAX_CLUSTER_NAME_LEN_FOR_GCP:
-            with ux_utils.print_exception_no_traceback():
-                raise exceptions.InvalidClusterNameError(
-                    f'Cluster name {cluster_name!r} has {len(cluster_name)} '
-                    f'chars; maximum length is {_MAX_CLUSTER_NAME_LEN_FOR_GCP} '
-                    'chars.')
-
-
 def check_cluster_name_not_reserved(
         cluster_name: Optional[str],
         operation_str: Optional[str] = None) -> None:
-    """Errors out if cluster name is reserved by sky.
+    """Errors out if the cluster is a reserved cluster (spot controller).
+
+    Raises:
+      sky.exceptions.NotSupportedError: if the cluster name is reserved, raise
+        with an error message explaining 'operation_str' is not allowed.
 
-    If the cluster name is reserved, return the error message. Otherwise,
-    return None.
+    Returns:
+      None, if the cluster name is not reserved.
     """
     if cluster_name in SKY_RESERVED_CLUSTER_NAMES:
-        msg = (f'Cluster {cluster_name!r} is reserved for '
+        msg = (f'Cluster {cluster_name!r} is reserved for the '
                f'{SKY_RESERVED_CLUSTER_NAMES[cluster_name].lower()}.')
         if operation_str is not None:
             msg += f' {operation_str} is not allowed.'
         with ux_utils.print_exception_no_traceback():
-            raise ValueError(msg)
-
-
-def check_gcp_cli_include_tpu_vm() -> None:
-    # TPU VM API available with gcloud version >= 382.0.0
-    version_cmd = 'gcloud version --format=json'
-    rcode, stdout, stderr = log_lib.run_with_log(version_cmd,
-                                                 '/dev/null',
-                                                 shell=True,
-                                                 stream_logs=False,
-                                                 require_outputs=True)
-
-    if rcode != 0:
-        failure_massage = ('Failed to run "gcloud version".\n'
-                           '**** STDOUT ****\n'
-                           '{stdout}\n'
-                           '**** STDERR ****\n'
-                           '{stderr}')
-        with ux_utils.print_exception_no_traceback():
-            raise RuntimeError(
-                failure_massage.format(stdout=stdout, stderr=stderr))
-
-    sdk_ver = json.loads(stdout).get('Google Cloud SDK', None)
-
-    if sdk_ver is None:
-        with ux_utils.print_exception_no_traceback():
-            raise RuntimeError('Failed to get Google Cloud SDK version from'
-                               f' "gcloud version": {stdout}')
-    else:
-        major_ver = sdk_ver.split('.')[0]
-        major_ver = int(major_ver)
-        if major_ver < 382:
-            with ux_utils.print_exception_no_traceback():
-                raise RuntimeError(
-                    'Google Cloud SDK version must be >= 382.0.0 to use'
-                    ' TPU VM APIs, check "gcloud version" for details.')
+            raise exceptions.NotSupportedError(msg)
 
 
 # Handle ctrl-c
 def interrupt_handler(signum, frame):
     del signum, frame
     subprocess_utils.kill_children_processes()
     # Avoid using logger here, as it will print the stack trace for broken
@@ -2019,49 +2499,113 @@
     print(f'{colorama.Style.DIM}Tip: The job will keep '
           f'running after Ctrl-Z.{colorama.Style.RESET_ALL}')
     with ux_utils.print_exception_no_traceback():
         raise KeyboardInterrupt(exceptions.SIGTSTP_CODE)
 
 
 def validate_schema(obj, schema, err_msg_prefix=''):
+    """Validates an object against a JSON schema.
+
+    Raises:
+        ValueError: if the object does not match the schema.
+    """
     err_msg = None
     try:
         validator.SchemaValidator(schema).validate(obj)
     except jsonschema.ValidationError as e:
         if e.validator == 'additionalProperties':
-            err_msg = err_msg_prefix + 'The following fields are invalid:'
-            known_fields = set(e.schema.get('properties', {}).keys())
-            for field in e.instance:
-                if field not in known_fields:
-                    most_similar_field = difflib.get_close_matches(
-                        field, known_fields, 1)
-                    if most_similar_field:
-                        err_msg += (f'\nInstead of {field!r}, did you mean '
-                                    f'{most_similar_field[0]!r}?')
-                    else:
-                        err_msg += f'\nFound unsupported field {field!r}.'
+            if tuple(e.schema_path) == ('properties', 'envs',
+                                        'additionalProperties'):
+                # Hack. Here the error is Task.envs having some invalid keys. So
+                # we should not print "unsupported field".
+                #
+                # This will print something like:
+                # 'hello world' does not match any of the regexes: <regex>
+                err_msg = (err_msg_prefix +
+                           'The `envs` field contains invalid keys:\n' +
+                           e.message)
+            else:
+                err_msg = err_msg_prefix + 'The following fields are invalid:'
+                known_fields = set(e.schema.get('properties', {}).keys())
+                for field in e.instance:
+                    if field not in known_fields:
+                        most_similar_field = difflib.get_close_matches(
+                            field, known_fields, 1)
+                        if most_similar_field:
+                            err_msg += (f'\nInstead of {field!r}, did you mean '
+                                        f'{most_similar_field[0]!r}?')
+                        else:
+                            err_msg += f'\nFound unsupported field {field!r}.'
         else:
-            err_msg = err_msg_prefix + e.message
+            # Example e.json_path value: '$.resources'
+            err_msg = (err_msg_prefix + e.message +
+                       f'. Check problematic field(s): {e.json_path}')
 
     if err_msg:
         with ux_utils.print_exception_no_traceback():
             raise ValueError(err_msg)
 
 
 def check_public_cloud_enabled():
-    """Checks if any of the public clouds is enabled."""
+    """Checks if any of the public clouds is enabled.
+
+    Exceptions:
+        exceptions.NoCloudAccessError: if no public cloud is enabled.
+    """
 
     def _no_public_cloud():
         enabled_clouds = global_user_state.get_enabled_clouds()
         return (len(enabled_clouds) == 0 or
                 (len(enabled_clouds) == 1 and
                  isinstance(enabled_clouds[0], clouds.Local)))
 
     if not _no_public_cloud():
         return
 
     sky_check.check(quiet=True)
     if _no_public_cloud():
         with ux_utils.print_exception_no_traceback():
-            raise RuntimeError(
+            raise exceptions.NoCloudAccessError(
                 'Cloud access is not set up. Run: '
                 f'{colorama.Style.BRIGHT}sky check{colorama.Style.RESET_ALL}')
+
+
+def run_command_and_handle_ssh_failure(runner: command_runner.SSHCommandRunner,
+                                       command: str,
+                                       failure_message: str) -> str:
+    """Runs command remotely and returns output with proper error handling."""
+    rc, stdout, stderr = runner.run(command,
+                                    require_outputs=True,
+                                    stream_logs=False)
+    if rc == 255:
+        # SSH failed
+        raise RuntimeError(
+            f'SSH with user {runner.ssh_user} and key {runner.ssh_private_key} '
+            f'to {runner.ip} failed. This is most likely due to incorrect '
+            'credentials or incorrect permissions for the key file. Check '
+            'your credentials and try again.')
+    subprocess_utils.handle_returncode(rc,
+                                       command,
+                                       failure_message,
+                                       stderr=stderr)
+    return stdout
+
+
+def check_rsync_installed() -> None:
+    """Checks if rsync is installed.
+
+    Raises:
+        RuntimeError: if rsync is not installed in the machine.
+    """
+    try:
+        subprocess.run('rsync --version',
+                       shell=True,
+                       check=True,
+                       stdout=subprocess.PIPE,
+                       stderr=subprocess.PIPE)
+    except subprocess.CalledProcessError:
+        with ux_utils.print_exception_no_traceback():
+            raise RuntimeError(
+                '`rsync` is required for provisioning and'
+                ' it is not installed. For Debian/Ubuntu system, '
+                'install it with:\n'
+                '  $ sudo apt install rsync') from None
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/backends/cloud_vm_ray_backend.py` & `skypilot-nightly-1.0.0.dev20230713/sky/backends/cloud_vm_ray_backend.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,38 +1,43 @@
 """Backend: runs on cloud virtual machines, managed by Ray."""
 import ast
+import copy
 import enum
 import getpass
 import inspect
 import math
 import json
 import os
 import pathlib
 import re
 import signal
 import subprocess
+import sys
 import tempfile
 import textwrap
 import time
 import typing
-from typing import Dict, List, Optional, Tuple, Union
+from typing import Dict, Iterable, List, Optional, Tuple, Union, Set
 
 import colorama
 import filelock
 
 import sky
 from sky import backends
 from sky import clouds
 from sky import cloud_stores
 from sky import exceptions
 from sky import global_user_state
+from sky import provision as provision_api
 from sky import resources as resources_lib
 from sky import sky_logging
 from sky import optimizer
+from sky import skypilot_config
 from sky import spot as spot_lib
+from sky import status_lib
 from sky import task as task_lib
 from sky.data import data_utils
 from sky.data import storage as storage_lib
 from sky.backends import backend_utils
 from sky.backends import onprem_utils
 from sky.backends import wheel_utils
 from sky.skylet import autostop_lib
@@ -43,39 +48,46 @@
 from sky.utils import common_utils
 from sky.utils import command_runner
 from sky.utils import log_utils
 from sky.utils import subprocess_utils
 from sky.utils import timeline
 from sky.utils import tpu_utils
 from sky.utils import ux_utils
+from sky.skylet.providers.scp.node_provider import SCPNodeProvider, SCPError
 
 if typing.TYPE_CHECKING:
     from sky import dag
 
-OptimizeTarget = optimizer.OptimizeTarget
 Path = str
 
 SKY_REMOTE_APP_DIR = backend_utils.SKY_REMOTE_APP_DIR
 SKY_REMOTE_WORKDIR = constants.SKY_REMOTE_WORKDIR
 
 logger = sky_logging.init_logger(__name__)
 
 _PATH_SIZE_MEGABYTES_WARN_THRESHOLD = 256
 
 # Timeout (seconds) for provision progress: if in this duration no new nodes
 # are launched, abort and failover.
-_NODES_LAUNCHING_PROGRESS_TIMEOUT = 90
+_NODES_LAUNCHING_PROGRESS_TIMEOUT = {
+    clouds.AWS: 90,
+    clouds.Azure: 90,
+    clouds.GCP: 120,
+    clouds.Lambda: 150,
+    clouds.IBM: 160,
+    clouds.Local: 90,
+    clouds.OCI: 300,
+}
 
 # Time gap between retries after failing to provision in all possible places.
 # Used only if --retry-until-up is set.
 _RETRY_UNTIL_UP_INIT_GAP_SECONDS = 60
 
 # The maximum retry count for fetching IP address.
-_HEAD_IP_MAX_ATTEMPTS = 5
-_WORKER_IP_MAX_ATTEMPTS = 5
+_FETCH_IP_MAX_ATTEMPTS = 3
 
 _TEARDOWN_FAILURE_MESSAGE = (
     f'\n{colorama.Fore.RED}Failed to terminate '
     '{cluster_name}. {extra_reason}'
     'If you want to ignore this error and remove the cluster '
     'from the status table, use `sky down --purge`.'
     f'{colorama.Style.RESET_ALL}\n'
@@ -83,37 +95,78 @@
     '{stdout}\n'
     '**** STDERR ****\n'
     '{stderr}')
 
 _TEARDOWN_PURGE_WARNING = (
     f'{colorama.Fore.YELLOW}'
     'WARNING: Received non-zero exit code from {reason}. '
-    'Make sure resources are manually deleted.'
+    'Make sure resources are manually deleted.\n'
+    'Details: {details}'
     f'{colorama.Style.RESET_ALL}')
 
 _TPU_NOT_FOUND_ERROR = 'ERROR: (gcloud.compute.tpus.delete) NOT_FOUND'
 
 _CTRL_C_TIP_MESSAGE = ('INFO: Tip: use Ctrl-C to exit log streaming '
                        '(task will not be killed).')
 
 _MAX_RAY_UP_RETRY = 5
 
+# Number of retries for getting zones.
+_MAX_GET_ZONE_RETRY = 3
+
 _JOB_ID_PATTERN = re.compile(r'Job ID: ([0-9]+)')
 
+# Path to the monkey-patched ray up script.
+# We don't do import then __file__ because that script needs to be filled in
+# (so import would fail).
+_RAY_UP_WITH_MONKEY_PATCHED_HASH_LAUNCH_CONF_PATH = (
+    pathlib.Path(sky.__file__).resolve().parent / 'backends' /
+    'monkey_patches' / 'monkey_patch_ray_up.py')
+
+# Restart skylet when the version does not match to keep the skylet up-to-date.
+_MAYBE_SKYLET_RESTART_CMD = 'python3 -m sky.skylet.attempt_skylet'
+
 
 def _get_cluster_config_template(cloud):
     cloud_to_template = {
         clouds.AWS: 'aws-ray.yml.j2',
         clouds.Azure: 'azure-ray.yml.j2',
         clouds.GCP: 'gcp-ray.yml.j2',
+        clouds.Lambda: 'lambda-ray.yml.j2',
+        clouds.IBM: 'ibm-ray.yml.j2',
         clouds.Local: 'local-ray.yml.j2',
+        clouds.SCP: 'scp-ray.yml.j2',
+        clouds.OCI: 'oci-ray.yml.j2',
     }
     return cloud_to_template[type(cloud)]
 
 
+def write_ray_up_script_with_patched_launch_hash_fn(
+    cluster_config_path: str,
+    ray_up_kwargs: Dict[str, bool],
+) -> str:
+    """Writes a Python script that runs `ray up` with our launch hash func.
+
+    Our patched launch hash has one difference from the non-patched version: it
+    does not include any `ssh_proxy_command` under `auth` as part of the hash
+    calculation.
+    """
+    with open(_RAY_UP_WITH_MONKEY_PATCHED_HASH_LAUNCH_CONF_PATH, 'r') as f:
+        ray_up_no_restart_script = f.read().format(
+            ray_yaml_path=repr(cluster_config_path),
+            ray_up_kwargs=ray_up_kwargs)
+    with tempfile.NamedTemporaryFile('w',
+                                     prefix='skypilot_ray_up_',
+                                     suffix='.py',
+                                     delete=False) as f:
+        f.write(ray_up_no_restart_script)
+        logger.debug(f'`ray up` script: {f.name}')
+    return f.name
+
+
 class RayCodeGen:
     """Code generator of a Ray program that executes a sky.Task.
 
     Usage:
 
       >> codegen = RayCodegen()
       >> codegen.add_prologue()
@@ -144,28 +197,23 @@
         # and monotonically increasing starting from 1.
         # To generate the job ID, we use the following logic:
         #   code = job_lib.JobLibCodeGen.add_job(username,
         #                                              run_timestamp)
         #   job_id = get_output(run_on_cluster(code))
         self.job_id = None
 
-    def add_prologue(self,
-                     job_id: int,
-                     spot_task: Optional['task_lib.Task'] = None,
-                     setup_cmd: Optional[str] = None,
-                     envs: Optional[Dict[str, str]] = None,
-                     setup_log_path: Optional[str] = None,
-                     is_local: bool = False) -> None:
+    def add_prologue(self, job_id: int, is_local: bool = False) -> None:
         assert not self._has_prologue, 'add_prologue() called twice?'
         self._has_prologue = True
         self.job_id = job_id
+        self.is_local = is_local
         # Should use 'auto' or 'ray://<internal_head_ip>:10001' rather than
         # 'ray://localhost:10001', or 'ray://127.0.0.1:10001', for public cloud.
-        # Otherwise, it will a bug of ray job failed to get the placement group
-        # in ray <= 2.0.1.
+        # Otherwise, ray will fail to get the placement group because of a bug
+        # in ray job.
         # TODO(mluo): Check why 'auto' not working with on-prem cluster and
         # whether the placement group issue also occurs in on-prem cluster.
         ray_address = 'ray://localhost:10001' if is_local else 'auto'
         self._code = [
             textwrap.dedent(f"""\
             import getpass
             import hashlib
@@ -179,101 +227,79 @@
             import textwrap
             import time
             from typing import Dict, List, Optional, Tuple, Union
 
             import ray
             import ray.util as ray_util
 
+            from sky.skylet import autostop_lib
             from sky.skylet import constants
             from sky.skylet import job_lib
             from sky.utils import log_utils
 
             SKY_REMOTE_WORKDIR = {constants.SKY_REMOTE_WORKDIR!r}
 
-            ray.init(address={ray_address!r}, namespace='__sky__{job_id}__', log_to_driver=True)
+            kwargs = dict()
+            # Only set the `_temp_dir` to SkyPilot's ray cluster directory when the directory
+            # exists for backward compatibility for the VM launched before #1790.
+            if os.path.exists({constants.SKY_REMOTE_RAY_TEMPDIR!r}):
+                kwargs['_temp_dir'] = {constants.SKY_REMOTE_RAY_TEMPDIR!r}
+            ray.init(
+                address={ray_address!r},
+                namespace='__sky__{job_id}__',
+                log_to_driver=True,
+                **kwargs
+            )
             run_fn = None
             futures = []
             """),
             # FIXME: This is a hack to make sure that the functions can be found
             # by ray.remote. This should be removed once we have a better way to
             # specify dependencies for ray.
+            inspect.getsource(log_lib._ProcessingArgs),  # pylint: disable=protected-access
+            inspect.getsource(log_lib._handle_io_stream),  # pylint: disable=protected-access
             inspect.getsource(log_lib.process_subprocess_stream),
             inspect.getsource(log_lib.run_with_log),
             inspect.getsource(log_lib.make_task_bash_script),
             inspect.getsource(log_lib.add_ray_env_vars),
             inspect.getsource(log_lib.run_bash_command_with_log),
             'run_bash_command_with_log = ray.remote(run_bash_command_with_log)',
-            f'setup_cmd = {setup_cmd!r}'
         ]
-        if setup_cmd is not None:
-            self._code += [
-                textwrap.dedent(f"""\
-                _SETUP_CPUS = 0.0001
-                # The setup command will be run as a ray task with num_cpus=_SETUP_CPUS as the
-                # requirement; this means Ray will set CUDA_VISIBLE_DEVICES to an empty string.
-                # We unset it so that user setup command may properly use this env var.
-                setup_cmd = 'unset CUDA_VISIBLE_DEVICES; ' + setup_cmd
-                job_lib.set_status({job_id!r}, job_lib.JobStatus.SETTING_UP)
-                print({_CTRL_C_TIP_MESSAGE!r}, file=sys.stderr, flush=True)
-                total_num_nodes = len(ray.nodes())
-                setup_bundles = [{{"CPU": _SETUP_CPUS}} for _ in range(total_num_nodes)]
-                setup_pg = ray.util.placement_group(setup_bundles, strategy='STRICT_SPREAD')
-                ray.get(setup_pg.ready())
-                setup_workers = [run_bash_command_with_log \\
-                    .options(name='setup', num_cpus=_SETUP_CPUS, placement_group=setup_pg, placement_group_bundle_index=i) \\
-                    .remote(
-                        setup_cmd,
-                        os.path.expanduser({setup_log_path!r}),
-                        getpass.getuser(),
-                        job_id={self.job_id},
-                        env_vars={envs!r},
-                        stream_logs=True,
-                        with_ray=True,
-                        use_sudo={is_local},
-                    ) for i in range(total_num_nodes)]
-                setup_returncodes = ray.get(setup_workers)
-                if sum(setup_returncodes) != 0:
-                    job_lib.set_status({self.job_id!r}, job_lib.JobStatus.FAILED_SETUP)
-                    # This waits for all streaming logs to finish.
-                    time.sleep(1)
-                    print('ERROR: {colorama.Fore.RED}Job {self.job_id}\\'s setup failed with '
-                        'return code list:{colorama.Style.RESET_ALL}',
-                        setup_returncodes,
-                        file=sys.stderr,
-                        flush=True)
-                    # Need this to set the job status in ray job to be FAILED.
-                    sys.exit(1)
-                """)
-            ]
+        # Currently, the codegen program is/can only be submitted to the head
+        # node, due to using job_lib for updating job statuses, and using
+        # autostop_lib here.
+        self._code.append(
+            # Use hasattr to handle backward compatibility.
+            # TODO(zongheng): remove in ~1-2 minor releases (currently 0.2.x).
+            textwrap.dedent("""\
+              if hasattr(autostop_lib, 'set_last_active_time_to_now'):
+                  autostop_lib.set_last_active_time_to_now()
+            """))
         self._code += [
             f'job_lib.set_status({job_id!r}, job_lib.JobStatus.PENDING)',
         ]
-        if spot_task is not None:
-            # Add the spot job to spot queue table.
-            resources_str = backend_utils.get_task_resources_str(spot_task)
-            self._code += [
-                'from sky.spot import spot_state',
-                f'spot_state.set_pending('
-                f'{job_id}, {spot_task.name!r}, {resources_str!r})',
-            ]
 
-    def add_gang_scheduling_placement_group(
+    def add_gang_scheduling_placement_group_and_setup(
         self,
         num_nodes: int,
-        accelerator_dict: Dict[str, int],
-        cluster_ips_sorted: Optional[List[str]] = None,
+        accelerator_dict: Optional[Dict[str, float]],
+        stable_cluster_internal_ips: List[str],
+        setup_cmd: Optional[str] = None,
+        setup_log_path: Optional[str] = None,
+        envs: Optional[Dict[str, str]] = None,
     ) -> None:
         """Create the gang scheduling placement group for a Task.
 
         cluster_ips_sorted is used to ensure that the SKY_NODE_RANK environment
         variable is assigned in a deterministic order whenever a new task is
         added.
         """
-        assert self._has_prologue, ('Call add_prologue() before '
-                                    'add_gang_scheduling_placement_group().')
+        assert self._has_prologue, (
+            'Call add_prologue() before '
+            'add_gang_scheduling_placement_group_and_setup().')
         self._has_gang_scheduling = True
         self._num_nodes = num_nodes
 
         # Set CPU to avoid ray hanging the resources allocation
         # for remote functions, since the task will request 1 CPU
         # by default.
         bundles = [{
@@ -284,180 +310,251 @@
             acc_name = list(accelerator_dict.keys())[0]
             acc_count = list(accelerator_dict.values())[0]
             gpu_dict = {'GPU': acc_count}
             # gpu_dict should be empty when the accelerator is not GPU.
             # FIXME: This is a hack to make sure that we do not reserve
             # GPU when requesting TPU.
             if 'tpu' in acc_name.lower():
-                gpu_dict = dict()
+                gpu_dict = {}
             for bundle in bundles:
                 bundle.update({
                     **accelerator_dict,
                     # Set the GPU to avoid ray hanging the resources allocation
                     **gpu_dict,
                 })
 
         self._code += [
             textwrap.dedent(f"""\
                 pg = ray_util.placement_group({json.dumps(bundles)}, 'STRICT_SPREAD')
                 plural = 's' if {num_nodes} > 1 else ''
                 node_str = f'{num_nodes} node{{plural}}'
-                
-                message = '' if setup_cmd is not None else {_CTRL_C_TIP_MESSAGE!r} + '\\n'
+
+                message = {_CTRL_C_TIP_MESSAGE!r} + '\\n'
                 message += f'INFO: Waiting for task resources on {{node_str}}. This will block if the cluster is full.'
                 print(message,
                       file=sys.stderr,
                       flush=True)
                 # FIXME: This will print the error message from autoscaler if
                 # it is waiting for other task to finish. We should hide the
                 # error message.
                 ray.get(pg.ready())
                 print('INFO: All task resources reserved.',
                       file=sys.stderr,
                       flush=True)
-                job_lib.set_job_started({self.job_id!r})
                 """)
         ]
 
+        job_id = self.job_id
+        if setup_cmd is not None:
+            self._code += [
+                textwrap.dedent(f"""\
+                setup_cmd = {setup_cmd!r}
+                _SETUP_CPUS = 0.0001
+                # The setup command will be run as a ray task with num_cpus=_SETUP_CPUS as the
+                # requirement; this means Ray will set CUDA_VISIBLE_DEVICES to an empty string.
+                # We unset it so that user setup command may properly use this env var.
+                setup_cmd = 'unset CUDA_VISIBLE_DEVICES; ' + setup_cmd
+                job_lib.set_status({job_id!r}, job_lib.JobStatus.SETTING_UP)
+
+                # The schedule_step should be called after the job status is set to non-PENDING,
+                # otherwise, the scheduler will think the current job is not submitted yet, and
+                # skip the scheduling step.
+                job_lib.scheduler.schedule_step()
+
+                total_num_nodes = len(ray.nodes())
+                setup_bundles = [{{"CPU": _SETUP_CPUS}} for _ in range(total_num_nodes)]
+                setup_pg = ray.util.placement_group(setup_bundles, strategy='STRICT_SPREAD')
+                setup_workers = [run_bash_command_with_log \\
+                    .options(
+                        name='setup',
+                        num_cpus=_SETUP_CPUS,
+                        scheduling_strategy=ray.util.scheduling_strategies.PlacementGroupSchedulingStrategy(
+                            placement_group=setup_pg,
+                            placement_group_bundle_index=i)
+                    ) \\
+                    .remote(
+                        setup_cmd,
+                        os.path.expanduser({setup_log_path!r}),
+                        getpass.getuser(),
+                        job_id={self.job_id},
+                        env_vars={envs!r},
+                        stream_logs=True,
+                        with_ray=True,
+                        use_sudo={self.is_local},
+                    ) for i in range(total_num_nodes)]
+                setup_returncodes = ray.get(setup_workers)
+                if sum(setup_returncodes) != 0:
+                    job_lib.set_status({self.job_id!r}, job_lib.JobStatus.FAILED_SETUP)
+                    # This waits for all streaming logs to finish.
+                    time.sleep(1)
+                    print('ERROR: {colorama.Fore.RED}Job {self.job_id}\\'s setup failed with '
+                        'return code list:{colorama.Style.RESET_ALL}',
+                        setup_returncodes,
+                        file=sys.stderr,
+                        flush=True)
+                    # Need this to set the job status in ray job to be FAILED.
+                    sys.exit(1)
+                """)
+            ]
+
+        self._code.append(f'job_lib.set_job_started({self.job_id!r})')
+        if setup_cmd is None:
+            # Need to call schedule_step() to make sure the scheduler
+            # schedule the next pending job.
+            self._code.append('job_lib.scheduler.schedule_step()')
+
         # Export IP and node rank to the environment variables.
         self._code += [
             textwrap.dedent(f"""\
                 @ray.remote
                 def check_ip():
                     return ray.util.get_node_ip_address()
                 gang_scheduling_id_to_ip = ray.get([
-                    check_ip.options(num_cpus={backend_utils.DEFAULT_TASK_CPU_DEMAND},
-                                     placement_group=pg,
-                                     placement_group_bundle_index=i).remote()
+                    check_ip.options(
+                            num_cpus={backend_utils.DEFAULT_TASK_CPU_DEMAND},
+                            scheduling_strategy=ray.util.scheduling_strategies.PlacementGroupSchedulingStrategy(
+                                placement_group=pg,
+                                placement_group_bundle_index=i
+                            )).remote()
                     for i in range(pg.bundle_count)
                 ])
                 print('INFO: Reserved IPs:', gang_scheduling_id_to_ip)
 
-                if {cluster_ips_sorted!r} is not None:
-                    cluster_ips_map = {{ip: i for i, ip in enumerate({cluster_ips_sorted!r})}}
-                    ip_rank_list = sorted(gang_scheduling_id_to_ip, key=cluster_ips_map.get)
-                    ip_rank_map = {{ip: i for i, ip in enumerate(ip_rank_list)}}
-                    ip_list_str = '\\n'.join(ip_rank_list)
-                else:
-                    ip_rank_map = {{ip: i for i, ip in enumerate(gang_scheduling_id_to_ip)}}
-                    ip_list_str = '\\n'.join(gang_scheduling_id_to_ip)
+                cluster_ips_to_node_id = {{ip: i for i, ip in enumerate({stable_cluster_internal_ips!r})}}
+                job_ip_rank_list = sorted(gang_scheduling_id_to_ip, key=cluster_ips_to_node_id.get)
+                job_ip_rank_map = {{ip: i for i, ip in enumerate(job_ip_rank_list)}}
+                job_ip_list_str = '\\n'.join(job_ip_rank_list)
                 """),
         ]
 
     def register_run_fn(self, run_fn: str, run_fn_name: str) -> None:
         """Register the run function to be run on the remote cluster.
 
         Args:
             run_fn: The run function to be run on the remote cluster.
         """
         assert self._has_gang_scheduling, (
-            'Call add_gang_scheduling_placement_group() '
+            'Call add_gang_scheduling_placement_group_and_setup() '
             'before register_run_fn().')
         assert not self._has_register_run_fn, (
             'register_run_fn() called twice?')
         self._has_register_run_fn = True
 
         self._code += [
             run_fn,
             f'run_fn = {run_fn_name}',
         ]
 
     def add_ray_task(self,
-                     bash_script: str,
+                     bash_script: Optional[str],
                      task_name: Optional[str],
                      job_run_id: Optional[str],
                      ray_resources_dict: Optional[Dict[str, float]],
-                     log_path: str,
-                     env_vars: Dict[str, str] = None,
+                     log_dir: str,
+                     env_vars: Optional[Dict[str, str]] = None,
                      gang_scheduling_id: int = 0,
                      use_sudo: bool = False) -> None:
         """Generates code for a ray remote task that runs a bash command."""
         assert self._has_gang_scheduling, (
-            'Call add_gang_schedule_placement_group() before add_ray_task().')
+            'Call add_gang_scheduling_placement_group_and_setup() before '
+            'add_ray_task().')
         assert (not self._has_register_run_fn or
                 bash_script is None), ('bash_script should '
                                        'be None when run_fn is registered.')
         # Build remote_task.options(...)
-        #   name=...
         #   resources=...
         #   num_gpus=...
-        name_str = f'name=\'{task_name}\''
-        if task_name is None:
-            # Make the task name more meaningful in ray log.
-            name_str = 'name=\'task\''
-        cpu_str = f', num_cpus={backend_utils.DEFAULT_TASK_CPU_DEMAND}'
-
-        resources_str = ''
-        num_gpus = 0
-        num_gpus_str = ''
+        options = []
+        options.append(f'num_cpus={backend_utils.DEFAULT_TASK_CPU_DEMAND}')
+
+        num_gpus = 0.0
         if ray_resources_dict is not None:
             assert len(ray_resources_dict) == 1, \
                 ('There can only be one type of accelerator per instance.'
                  f' Found: {ray_resources_dict}.')
             num_gpus = list(ray_resources_dict.values())[0]
-            resources_str = f', resources={json.dumps(ray_resources_dict)}'
+            options.append(f'resources={json.dumps(ray_resources_dict)}')
 
-            # Passing this ensures that the Ray remote task gets
-            # CUDA_VISIBLE_DEVICES set correctly.  If not passed, that flag
-            # would be force-set to empty by Ray.
-            num_gpus_str = f', num_gpus={num_gpus}'
-            # `num_gpus` should be empty when the accelerator is not GPU.
-            # FIXME: use a set of GPU types.
             resources_key = list(ray_resources_dict.keys())[0]
-            if 'tpu' in resources_key.lower():
-                num_gpus_str = ''
-        resources_str += ', placement_group=pg'
-        resources_str += f', placement_group_bundle_index={gang_scheduling_id}'
+            if 'tpu' not in resources_key.lower():
+                # `num_gpus` should be empty when the accelerator is not GPU.
+                # FIXME: use a set of GPU types, instead of 'tpu' in the key.
+
+                # Passing this ensures that the Ray remote task gets
+                # CUDA_VISIBLE_DEVICES set correctly.  If not passed, that flag
+                # would be force-set to empty by Ray.
+                options.append(f'num_gpus={num_gpus}')
+        options.append(
+            'scheduling_strategy=ray.util.scheduling_strategies.PlacementGroupSchedulingStrategy('  # pylint: disable=line-too-long
+            'placement_group=pg, '
+            f'placement_group_bundle_index={gang_scheduling_id})')
 
         sky_env_vars_dict_str = [
             textwrap.dedent("""\
-            sky_env_vars_dict = dict()
-            sky_env_vars_dict['SKYPILOT_NODE_IPS'] = ip_list_str
+            sky_env_vars_dict = {}
+            sky_env_vars_dict['SKYPILOT_NODE_IPS'] = job_ip_list_str
             # Environment starting with `SKY_` is deprecated.
-            sky_env_vars_dict['SKY_NODE_IPS'] = ip_list_str
+            sky_env_vars_dict['SKY_NODE_IPS'] = job_ip_list_str
             """)
         ]
 
         if env_vars is not None:
             sky_env_vars_dict_str.extend(f'sky_env_vars_dict[{k!r}] = {v!r}'
                                          for k, v in env_vars.items())
         if job_run_id is not None:
             sky_env_vars_dict_str += [
-                f'sky_env_vars_dict[{constants.JOB_ID_ENV_VAR!r}]'
+                f'sky_env_vars_dict[{constants.TASK_ID_ENV_VAR!r}]'
+                f' = {job_run_id!r}',
+                # TODO(zhwu): remove this deprecated env var in later release
+                # (after 0.5).
+                f'sky_env_vars_dict[{constants.TASK_ID_ENV_VAR_DEPRECATED!r}]'
                 f' = {job_run_id!r}'
             ]
         sky_env_vars_dict_str = '\n'.join(sky_env_vars_dict_str)
 
+        options_str = ', '.join(options)
         logger.debug('Added Task with options: '
-                     f'{name_str}{cpu_str}{resources_str}{num_gpus_str}')
+                     f'{options_str}')
         self._code += [
             sky_env_vars_dict_str,
             textwrap.dedent(f"""\
         script = {bash_script!r}
         if run_fn is not None:
             script = run_fn({gang_scheduling_id}, gang_scheduling_id_to_ip)
 
-        log_path = os.path.expanduser({log_path!r})
 
         if script is not None:
             sky_env_vars_dict['SKYPILOT_NUM_GPUS_PER_NODE'] = {int(math.ceil(num_gpus))!r}
             # Environment starting with `SKY_` is deprecated.
             sky_env_vars_dict['SKY_NUM_GPUS_PER_NODE'] = {int(math.ceil(num_gpus))!r}
 
             ip = gang_scheduling_id_to_ip[{gang_scheduling_id!r}]
-            sky_env_vars_dict['SKYPILOT_NODE_RANK'] = ip_rank_map[ip]
+            rank = job_ip_rank_map[ip]
+
+            if len(cluster_ips_to_node_id) == 1: # Single-node task on single-node cluter
+                name_str = '{task_name},' if {task_name!r} != None else 'task,'
+                log_path = os.path.expanduser(os.path.join({log_dir!r}, 'run.log'))
+            else: # Single-node or multi-node task on multi-node cluster
+                idx_in_cluster = cluster_ips_to_node_id[ip]
+                if cluster_ips_to_node_id[ip] == 0:
+                    node_name = 'head'
+                else:
+                    node_name = f'worker{{idx_in_cluster}}'
+                name_str = f'{{node_name}}, rank={{rank}},'
+                log_path = os.path.expanduser(os.path.join({log_dir!r}, f'{{rank}}-{{node_name}}.log'))
+            sky_env_vars_dict['SKYPILOT_NODE_RANK'] = rank
             # Environment starting with `SKY_` is deprecated.
-            sky_env_vars_dict['SKY_NODE_RANK'] = ip_rank_map[ip]
-            
+            sky_env_vars_dict['SKY_NODE_RANK'] = rank
+
             sky_env_vars_dict['SKYPILOT_INTERNAL_JOB_ID'] = {self.job_id}
             # Environment starting with `SKY_` is deprecated.
             sky_env_vars_dict['SKY_INTERNAL_JOB_ID'] = {self.job_id}
 
             futures.append(run_bash_command_with_log \\
-                    .options({name_str}{cpu_str}{resources_str}{num_gpus_str}) \\
+                    .options(name=name_str, {options_str}) \\
                     .remote(
                         script,
                         log_path,
                         getpass.getuser(),
                         job_id={self.job_id},
                         env_vars=sky_env_vars_dict,
                         stream_logs=True,
@@ -474,28 +571,30 @@
 
         self._code += [
             textwrap.dedent(f"""\
             returncodes = ray.get(futures)
             if sum(returncodes) != 0:
                 job_lib.set_status({self.job_id!r}, job_lib.JobStatus.FAILED)
                 # This waits for all streaming logs to finish.
-                time.sleep(1)
+                job_lib.scheduler.schedule_step()
+                time.sleep(0.5)
                 print('ERROR: {colorama.Fore.RED}Job {self.job_id} failed with '
                       'return code list:{colorama.Style.RESET_ALL}',
                       returncodes,
                       file=sys.stderr,
                       flush=True)
                 # Need this to set the job status in ray job to be FAILED.
                 sys.exit(1)
             else:
                 sys.stdout.flush()
                 sys.stderr.flush()
                 job_lib.set_status({self.job_id!r}, job_lib.JobStatus.SUCCEEDED)
                 # This waits for all streaming logs to finish.
-                time.sleep(1)
+                job_lib.scheduler.schedule_step()
+                time.sleep(0.5)
             """)
         ]
 
     def build(self) -> str:
         """Returns the entire generated program."""
         assert self._has_epilogue, 'Call add_epilogue() before build().'
         return '\n'.join(self._code)
@@ -503,81 +602,75 @@
 
 class RetryingVmProvisioner(object):
     """A provisioner that retries different cloud/regions/zones."""
 
     class ToProvisionConfig:
         """Resources to be provisioned."""
 
-        def __init__(self,
-                     cluster_name: str,
-                     resources: Optional[resources_lib.Resources],
-                     num_nodes: int,
-                     cluster_exists: bool = False) -> None:
+        def __init__(
+                self, cluster_name: str, resources: resources_lib.Resources,
+                num_nodes: int,
+                prev_cluster_status: Optional[status_lib.ClusterStatus]
+        ) -> None:
             assert cluster_name is not None, 'cluster_name must be specified.'
             self.cluster_name = cluster_name
             self.resources = resources
             self.num_nodes = num_nodes
-            # Whether the cluster exists in the clusters table. It may be
-            # actually live or down.
-            self.cluster_exists = cluster_exists
+            self.prev_cluster_status = prev_cluster_status
 
     class GangSchedulingStatus(enum.Enum):
         """Enum for gang scheduling status."""
         CLUSTER_READY = 0
         GANG_FAILED = 1
         HEAD_FAILED = 2
 
     def __init__(self, log_dir: str, dag: 'dag.Dag',
-                 optimize_target: OptimizeTarget,
+                 optimize_target: 'optimizer.OptimizeTarget',
+                 requested_features: Set[clouds.CloudImplementationFeatures],
                  local_wheel_path: pathlib.Path, wheel_hash: str):
-        self._blocked_regions = set()
-        self._blocked_zones = set()
-        self._blocked_launchable_resources = set()
+        self._blocked_resources: Set[resources_lib.Resources] = set()
 
         self.log_dir = os.path.expanduser(log_dir)
         self._dag = dag
         self._optimize_target = optimize_target
+        self._requested_features = requested_features
         self._local_wheel_path = local_wheel_path
         self._wheel_hash = wheel_hash
 
-        colorama.init()
-
-    def _in_blocklist(self, cloud, region, zones):
-        if region.name in self._blocked_regions:
-            return True
-        # We do not keep track of zones in Azure and Local,
-        # as both clouds do not have zones.
-        if isinstance(cloud, (clouds.Azure, clouds.Local)):
-            return False
-        assert zones, (cloud, region, zones)
-        for zone in zones:
-            if zone.name not in self._blocked_zones:
-                return False
-        return True
-
-    def _clear_blocklist(self):
-        self._blocked_regions.clear()
-        self._blocked_zones.clear()
+    def _update_blocklist_on_gcp_error(
+            self, launchable_resources: 'resources_lib.Resources',
+            region: 'clouds.Region', zones: Optional[List['clouds.Zone']],
+            stdout: str, stderr: str):
 
-    def _update_blocklist_on_gcp_error(self, region, zones, stdout, stderr):
+        del region  # unused
         style = colorama.Style
-        assert len(zones) == 1, zones
+        assert zones and len(zones) == 1, zones
         zone = zones[0]
         splits = stderr.split('\n')
-        exception_str = [s for s in splits if s.startswith('Exception: ')]
+        exception_list = [s for s in splits if s.startswith('Exception: ')]
         httperror_str = [
             s for s in splits
             if s.startswith('googleapiclient.errors.HttpError: ')
         ]
-        if len(exception_str) == 1:
+        if len(exception_list) == 1:
             # Parse structured response {'errors': [...]}.
-            exception_str = exception_str[0][len('Exception: '):]
+            exception_str = exception_list[0][len('Exception: '):]
             try:
                 exception_dict = ast.literal_eval(exception_str)
             except Exception as e:
+                if 'wait_ready timeout exceeded' in exception_str:
+                    # This error seems to occur when the provisioning process
+                    # went through partially (e.g., for spot, initial
+                    # provisioning succeeded, but while waiting for ssh/setting
+                    # up it got preempted).
+                    logger.error('Got the following exception, continuing: '
+                                 f'{exception_list[0]}')
+                    self._blocked_resources.add(
+                        launchable_resources.copy(zone=zone.name))
+                    return
                 raise RuntimeError(
                     f'Failed to parse exception: {exception_str}') from e
             # TPU VM returns a different structured response.
             if 'errors' not in exception_dict:
                 exception_dict = {'errors': [exception_dict]}
             for error in exception_dict['errors']:
                 code = error['code']
@@ -589,77 +682,123 @@
                     if '\'GPUS_ALL_REGIONS\' exceeded' in message:
                         # Global quota.  All regions in GCP will fail.  Ex:
                         # Quota 'GPUS_ALL_REGIONS' exceeded.  Limit: 1.0
                         # globally.
                         # This skip is only correct if we implement "first
                         # retry the region/zone of an existing cluster with the
                         # same name" correctly.
-                        for r, _ in clouds.GCP.region_zones_provision_loop():
-                            self._blocked_regions.add(r.name)
+                        self._blocked_resources.add(
+                            launchable_resources.copy(region=None, zone=None))
                     else:
                         # Per region.  Ex: Quota 'CPUS' exceeded.  Limit: 24.0
                         # in region us-west1.
-                        self._blocked_regions.add(region.name)
+                        self._blocked_resources.add(
+                            launchable_resources.copy(zone=None))
                 elif code in [
                         'ZONE_RESOURCE_POOL_EXHAUSTED',
                         'ZONE_RESOURCE_POOL_EXHAUSTED_WITH_DETAILS',
                         'UNSUPPORTED_OPERATION'
                 ]:  # Per zone.
                     # Return codes can be found at https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-vm-creation # pylint: disable=line-too-long
                     # However, UNSUPPORTED_OPERATION is observed empirically
                     # when VM is preempted during creation.  This seems to be
                     # not documented by GCP.
-                    self._blocked_zones.add(zone.name)
+                    self._blocked_resources.add(
+                        launchable_resources.copy(zone=zone.name))
                 elif code in ['RESOURCE_NOT_READY']:
                     # This code is returned when the VM is still STOPPING.
-                    self._blocked_zones.add(zone.name)
-                elif code == 8:
+                    self._blocked_resources.add(
+                        launchable_resources.copy(zone=zone.name))
+                elif code in [8, 9]:
                     # Error code 8 means TPU resources is out of
                     # capacity. Example:
                     # {'code': 8, 'message': 'There is no more capacity in the zone "europe-west4-a"; you can try in another zone where Cloud TPU Nodes are offered (see https://cloud.google.com/tpu/docs/regions) [EID: 0x1bc8f9d790be9142]'} # pylint: disable=line-too-long
-                    self._blocked_zones.add(zone.name)
+                    # Error code 9 means TPU resources is insufficient reserved
+                    # capacity. Example:
+                    # {'code': 9, 'message': 'Insufficient reserved capacity. Contact customer support to increase your reservation. [EID: 0x2f8bc266e74261a]'} # pylint: disable=line-too-long
+                    self._blocked_resources.add(
+                        launchable_resources.copy(zone=zone.name))
+                elif code == 'RESOURCE_NOT_FOUND':
+                    # https://github.com/skypilot-org/skypilot/issues/1797
+                    # In the inner provision loop we have used retries to
+                    # recover but failed. This indicates this zone is most
+                    # likely out of capacity. The provision loop will terminate
+                    # any potentially live VMs before moving onto the next
+                    # zone.
+                    self._blocked_resources.add(
+                        launchable_resources.copy(zone=zone.name))
                 else:
                     assert False, error
         elif len(httperror_str) >= 1:
-            # Parse HttpError for unauthorized regions. Example:
-            # googleapiclient.errors.HttpError: <HttpError 403 when requesting ... returned "Location us-east1-d is not found or access is unauthorized.". # pylint: disable=line-too-long
-            # Details: "Location us-east1-d is not found or access is
-            # unauthorized.">
             logger.info(f'Got {httperror_str[0]}')
-            self._blocked_zones.add(zone.name)
+            if ('Requested disk size cannot be smaller than the image size'
+                    in httperror_str[0]):
+                logger.info('Skipping all regions due to disk size issue.')
+                self._blocked_resources.add(
+                    launchable_resources.copy(region=None, zone=None))
+            elif ('Policy update access denied.' in httperror_str[0] or
+                  'IAM_PERMISSION_DENIED' in httperror_str[0]):
+                logger.info(
+                    'Skipping all regions due to service account not '
+                    'having the required permissions and the user '
+                    'account does not have enough permission to '
+                    'update it. Please contact your administrator and '
+                    'check out: https://skypilot.readthedocs.io/en/latest/cloud-setup/cloud-permissions.html#gcp\n'  # pylint: disable=line-too-long
+                    f'Details: {httperror_str[0]}')
+                self._blocked_resources.add(
+                    launchable_resources.copy(region=None, zone=None))
+
+            else:
+                # Parse HttpError for unauthorized regions. Example:
+                # googleapiclient.errors.HttpError: <HttpError 403 when requesting ... returned "Location us-east1-d is not found or access is unauthorized.". # pylint: disable=line-too-long
+                # Details: "Location us-east1-d is not found or access is
+                # unauthorized.">
+                self._blocked_resources.add(
+                    launchable_resources.copy(zone=zone.name))
         else:
             # No such structured error response found.
-            assert not exception_str, stderr
+            assert not exception_list, stderr
             if 'was not found' in stderr:
                 # Example: The resource
                 # 'projects/<id>/zones/zone/acceleratorTypes/nvidia-tesla-v100'
                 # was not found.
                 logger.warning(f'Got \'resource not found\' in {zone.name}.')
-                self._blocked_zones.add(zone.name)
+                self._blocked_resources.add(
+                    launchable_resources.copy(zone=zone.name))
             else:
                 logger.info('====== stdout ======')
                 for s in stdout.split('\n'):
                     print(s)
                 logger.info('====== stderr ======')
                 for s in splits:
                     print(s)
 
                 with ux_utils.print_exception_no_traceback():
                     raise RuntimeError('Errors occurred during provision; '
                                        'check logs above.')
 
-    def _update_blocklist_on_aws_error(self, region, zones, stdout, stderr):
-        del zones  # Unused.
+    def _update_blocklist_on_aws_error(
+            self, launchable_resources: 'resources_lib.Resources',
+            region: 'clouds.Region', zones: Optional[List['clouds.Zone']],
+            stdout: str, stderr: str):
+        assert launchable_resources.is_launchable()
+        assert zones is not None, 'AWS should always have zones.'
+
         style = colorama.Style
         stdout_splits = stdout.split('\n')
         stderr_splits = stderr.split('\n')
         errors = [
             s.strip()
             for s in stdout_splits + stderr_splits
-            if 'An error occurred' in s.strip()
+            # 'An error occurred': boto3 errors
+            # 'SKYPILOT_ERROR_NO_NODES_LAUNCHED': skypilot's changes to the AWS
+            #    node provider; for errors prior to provisioning like VPC
+            #    setup.
+            if 'An error occurred' in s or
+            'SKYPILOT_ERROR_NO_NODES_LAUNCHED: ' in s
         ]
         # Need to handle boto3 printing error but its retry succeeded:
         #   error occurred (Unsupported) .. not supported in your requested
         #   Availability Zone (us-west-2d)...retrying
         #   --> it automatically succeeded in another zone
         #   --> failed in [4/7] Running initialization commands due to user cmd
         # In this case, we should error out.
@@ -675,22 +814,42 @@
                 print(s)
             logger.info('====== stderr ======')
             for s in stderr_splits:
                 print(s)
             with ux_utils.print_exception_no_traceback():
                 raise RuntimeError('Errors occurred during provision; '
                                    'check logs above.')
-        # The underlying ray autoscaler / boto3 will try all zones of a region
-        # at once.
-        logger.warning(f'Got error(s) in all zones of {region.name}:')
+
+        # Fill in the zones field in the Region.
+        region_with_zones_list = clouds.AWS.regions_with_offering(
+            launchable_resources.instance_type,
+            launchable_resources.accelerators,
+            launchable_resources.use_spot,
+            region.name,
+            zone=None)
+        assert len(region_with_zones_list) == 1, region_with_zones_list
+        region_with_zones = region_with_zones_list[0]
+        assert region_with_zones.zones is not None, region_with_zones
+        if set(zones) == set(region_with_zones.zones):
+            # The underlying AWS NodeProvider will try all specified zones of a
+            # region. (Each boto3 request takes one zone.)
+            logger.warning(f'Got error(s) in all zones of {region.name}:')
+        else:
+            zones_str = ', '.join(z.name for z in zones)
+            logger.warning(f'Got error(s) in {zones_str}:')
         messages = '\n\t'.join(errors)
         logger.warning(f'{style.DIM}\t{messages}{style.RESET_ALL}')
-        self._blocked_regions.add(region.name)
+        for zone in zones:
+            self._blocked_resources.add(
+                launchable_resources.copy(zone=zone.name))
 
-    def _update_blocklist_on_azure_error(self, region, zones, stdout, stderr):
+    def _update_blocklist_on_azure_error(
+            self, launchable_resources: 'resources_lib.Resources',
+            region: 'clouds.Region', zones: Optional[List['clouds.Zone']],
+            stdout: str, stderr: str):
         del zones  # Unused.
         # The underlying ray autoscaler will try all zones of a region at once.
         style = colorama.Style
         stdout_splits = stdout.split('\n')
         stderr_splits = stderr.split('\n')
         errors = [
             s.strip()
@@ -709,20 +868,127 @@
                 raise RuntimeError('Errors occurred during provision; '
                                    'check logs above.')
 
         logger.warning(f'Got error(s) in {region.name}:')
         messages = '\n\t'.join(errors)
         logger.warning(f'{style.DIM}\t{messages}{style.RESET_ALL}')
         if any('(ReadOnlyDisabledSubscription)' in s for s in errors):
-            for r in sky.Azure.regions():
-                self._blocked_regions.add(r.name)
+            self._blocked_resources.add(
+                resources_lib.Resources(cloud=clouds.Azure()))
         else:
-            self._blocked_regions.add(region.name)
+            self._blocked_resources.add(launchable_resources.copy(zone=None))
 
-    def _update_blocklist_on_local_error(self, region, zones, stdout, stderr):
+    def _update_blocklist_on_lambda_error(
+            self, launchable_resources: 'resources_lib.Resources',
+            region: 'clouds.Region', zones: Optional[List['clouds.Zone']],
+            stdout: str, stderr: str):
+        del zones  # Unused.
+        style = colorama.Style
+        stdout_splits = stdout.split('\n')
+        stderr_splits = stderr.split('\n')
+        errors = [
+            s.strip()
+            for s in stdout_splits + stderr_splits
+            if 'LambdaCloudError:' in s.strip()
+        ]
+        if not errors:
+            logger.info('====== stdout ======')
+            for s in stdout_splits:
+                print(s)
+            logger.info('====== stderr ======')
+            for s in stderr_splits:
+                print(s)
+            with ux_utils.print_exception_no_traceback():
+                raise RuntimeError('Errors occurred during provision; '
+                                   'check logs above.')
+
+        logger.warning(f'Got error(s) in {region.name}:')
+        messages = '\n\t'.join(errors)
+        logger.warning(f'{style.DIM}\t{messages}{style.RESET_ALL}')
+        self._blocked_resources.add(launchable_resources.copy(zone=None))
+
+        # Sometimes, LambdaCloudError will list available regions.
+        for e in errors:
+            if e.find('Regions with capacity available:') != -1:
+                for r in clouds.Lambda.regions():
+                    if e.find(r.name) == -1:
+                        self._blocked_resources.add(
+                            launchable_resources.copy(region=r.name, zone=None))
+
+    def _update_blocklist_on_scp_error(
+            self, launchable_resources: 'resources_lib.Resources', region,
+            zones, stdout, stderr):
+        del zones  # Unused.
+        style = colorama.Style
+        stdout_splits = stdout.split('\n')
+        stderr_splits = stderr.split('\n')
+        errors = [
+            s.strip()
+            for s in stdout_splits + stderr_splits
+            if 'SCPError:' in s.strip()
+        ]
+        if not errors:
+            logger.info('====== stdout ======')
+            for s in stdout_splits:
+                print(s)
+            logger.info('====== stderr ======')
+            for s in stderr_splits:
+                print(s)
+            with ux_utils.print_exception_no_traceback():
+                raise RuntimeError('Errors occurred during provision; '
+                                   'check logs above.')
+
+        logger.warning(f'Got error(s) in {region.name}:')
+        messages = '\n\t'.join(errors)
+        logger.warning(f'{style.DIM}\t{messages}{style.RESET_ALL}')
+        self._blocked_resources.add(launchable_resources.copy(zone=None))
+
+        # Sometimes, SCPError will list available regions.
+        for e in errors:
+            if e.find('Regions with capacity available:') != -1:
+                for r in clouds.SCP.regions():
+                    if e.find(r.name) == -1:
+                        self._blocked_resources.add(
+                            launchable_resources.copy(region=r.name, zone=None))
+
+    def _update_blocklist_on_ibm_error(
+            self, launchable_resources: 'resources_lib.Resources',
+            region: 'clouds.Region', zones: Optional[List['clouds.Zone']],
+            stdout: str, stderr: str):
+
+        style = colorama.Style
+        stdout_splits = stdout.split('\n')
+        stderr_splits = stderr.split('\n')
+        errors = [
+            s.strip()
+            for s in stdout_splits + stderr_splits
+            if 'ERR' in s.strip() or 'PANIC' in s.strip()
+        ]
+        if not errors:
+            logger.info('====== stdout ======')
+            for s in stdout_splits:
+                print(s)
+            logger.info('====== stderr ======')
+            for s in stderr_splits:
+                print(s)
+            with ux_utils.print_exception_no_traceback():
+                raise RuntimeError('Errors occurred during provision; '
+                                   'check logs above.')
+        logger.warning(f'Got error(s) on IBM cluster, in {region.name}:')
+        messages = '\n\t'.join(errors)
+        logger.warning(f'{style.DIM}\t{messages}{style.RESET_ALL}')
+
+        for zone in zones:  # type: ignore[union-attr]
+            self._blocked_resources.add(
+                launchable_resources.copy(zone=zone.name))
+
+    def _update_blocklist_on_local_error(
+            self, launchable_resources: 'resources_lib.Resources',
+            region: 'clouds.Region', zones: Optional[List['clouds.Zone']],
+            stdout: str, stderr: str):
         del zones  # Unused.
         style = colorama.Style
         stdout_splits = stdout.split('\n')
         stderr_splits = stderr.split('\n')
         errors = [
             s.strip()
             for s in stdout_splits + stderr_splits
@@ -738,162 +1004,239 @@
             with ux_utils.print_exception_no_traceback():
                 raise RuntimeError(
                     'Errors occurred during launching of cluster services; '
                     'check logs above.')
         logger.warning('Got error(s) on local cluster:')
         messages = '\n\t'.join(errors)
         logger.warning(f'{style.DIM}\t{messages}{style.RESET_ALL}')
-        self._blocked_regions.add(region.name)
+        self._blocked_resources.add(
+            launchable_resources.copy(region=region.name, zone=None))
+
+    # Apr, 2023 by Hysun(hysun.he@oracle.com): Added support for OCI
+    def _update_blocklist_on_oci_error(
+            self, launchable_resources: 'resources_lib.Resources',
+            region: 'clouds.Region', zones: Optional[List['clouds.Zone']],
+            stdout: str, stderr: str):
+
+        style = colorama.Style
+        stdout_splits = stdout.split('\n')
+        stderr_splits = stderr.split('\n')
+        errors = [
+            s.strip()
+            for s in stdout_splits + stderr_splits
+            if ('VcnSubnetNotFound' in s.strip()) or
+            ('oci.exceptions.ServiceError' in s.strip() and
+             ('NotAuthorizedOrNotFound' in s.strip() or 'CannotParseRequest' in
+              s.strip() or 'InternalError' in s.strip() or
+              'LimitExceeded' in s.strip() or 'NotAuthenticated' in s.strip()))
+        ]
+        if not errors:
+            logger.info('====== stdout ======')
+            for s in stdout_splits:
+                print(s)
+            logger.info('====== stderr ======')
+            for s in stderr_splits:
+                print(s)
+            with ux_utils.print_exception_no_traceback():
+                raise RuntimeError('Errors occurred during provision; '
+                                   'check logs above.')
 
-    def _update_blocklist_on_error(self, cloud, region, zones, stdout,
-                                   stderr) -> None:
+        logger.warning(f'Got error(s) in {region.name}:')
+        messages = '\n\t'.join(errors)
+        logger.warning(f'{style.DIM}\t{messages}{style.RESET_ALL}')
+
+        if zones is not None:
+            for zone in zones:
+                self._blocked_resources.add(
+                    launchable_resources.copy(zone=zone.name))
+
+    def _update_blocklist_on_error(
+            self, launchable_resources: 'resources_lib.Resources',
+            region: 'clouds.Region', zones: Optional[List['clouds.Zone']],
+            stdout: Optional[str], stderr: Optional[str]) -> bool:
         """Handles cloud-specific errors and updates the block list.
 
         This parses textual stdout/stderr because we don't directly use the
         underlying clouds' SDKs.  If we did that, we could catch proper
         exceptions instead.
+
+        Returns:
+          definitely_no_nodes_launched: bool, True if definitely no nodes
+            launched (e.g., due to VPC errors we have never sent the provision
+            request), False otherwise.
         """
+        assert launchable_resources.region == region.name, (
+            launchable_resources, region)
         if stdout is None:
-            # Gang scheduling failure.  Simply block the region.
+            # Gang scheduling failure (head node is definitely up, but some
+            # workers' provisioning failed).  Simply block the zones.
             assert stderr is None, stderr
-            self._blocked_regions.add(region.name)
-            return
-
-        if isinstance(cloud, clouds.GCP):
-            return self._update_blocklist_on_gcp_error(region, zones, stdout,
-                                                       stderr)
-
-        if isinstance(cloud, clouds.AWS):
-            return self._update_blocklist_on_aws_error(region, zones, stdout,
-                                                       stderr)
-
-        if isinstance(cloud, clouds.Azure):
-            return self._update_blocklist_on_azure_error(
-                region, zones, stdout, stderr)
-
-        if isinstance(cloud, clouds.Local):
-            return self._update_blocklist_on_local_error(
-                region, zones, stdout, stderr)
-
-        assert False, f'Unknown cloud: {cloud}.'
+            if zones is not None:
+                for zone in zones:
+                    self._blocked_resources.add(
+                        launchable_resources.copy(zone=zone.name))
+            return False  # definitely_no_nodes_launched
+        assert stdout is not None and stderr is not None, (stdout, stderr)
+
+        # TODO(zongheng): refactor into Cloud interface?
+        handlers = {
+            clouds.AWS: self._update_blocklist_on_aws_error,
+            clouds.Azure: self._update_blocklist_on_azure_error,
+            clouds.GCP: self._update_blocklist_on_gcp_error,
+            clouds.Lambda: self._update_blocklist_on_lambda_error,
+            clouds.IBM: self._update_blocklist_on_ibm_error,
+            clouds.SCP: self._update_blocklist_on_scp_error,
+            clouds.Local: self._update_blocklist_on_local_error,
+            clouds.OCI: self._update_blocklist_on_oci_error,
+        }
+        cloud = launchable_resources.cloud
+        cloud_type = type(cloud)
+        if cloud_type not in handlers:
+            raise NotImplementedError(
+                f'Cloud {cloud} unknown, or has not added '
+                'support for parsing and handling provision failures.')
+        handler = handlers[cloud_type]
+        handler(launchable_resources, region, zones, stdout, stderr)
 
-    def _yield_region_zones(self, to_provision: resources_lib.Resources,
-                            cluster_name: str, cluster_exists: bool):
+        stdout_splits = stdout.split('\n')
+        stderr_splits = stderr.split('\n')
+        # Determining whether head node launch *may* have been requested based
+        # on outputs is tricky. We are conservative here by choosing an "early
+        # enough" output line in the following:
+        # https://github.com/ray-project/ray/blob/03b6bc7b5a305877501110ec04710a9c57011479/python/ray/autoscaler/_private/commands.py#L704-L737  # pylint: disable=line-too-long
+        # This is okay, because we mainly want to use the return value of this
+        # func to skip cleaning up never-launched clusters that encountered VPC
+        # errors; their launch should not have printed any such outputs.
+        head_node_launch_may_have_been_requested = any(
+            'Acquiring an up-to-date head node' in line
+            for line in stdout_splits + stderr_splits)
+        # If head node request has definitely not been sent (this happens when
+        # there are errors during node provider "bootstrapping", e.g.,
+        # VPC-not-found errors), then definitely no nodes are launched.
+        definitely_no_nodes_launched = (
+            not head_node_launch_may_have_been_requested)
+
+        return definitely_no_nodes_launched
+
+    def _yield_zones(
+        self, to_provision: resources_lib.Resources, num_nodes: int,
+        cluster_name: str,
+        prev_cluster_status: Optional[status_lib.ClusterStatus]
+    ) -> Iterable[Optional[List[clouds.Zone]]]:
+        """Yield zones within the given region to try for provisioning.
+
+        Yields:
+            Zones to try for provisioning within the given to_provision.region.
+              - None means the cloud does not support zones, but the region does
+                offer the requested resources (so the outer loop should issue a
+                request to that region).
+              - Non-empty list means the cloud supports zones, and the zones
+                do offer the requested resources. If a list is yielded, it is
+                guaranteed to be non-empty.
+              - Nothing yielded means the region does not offer the requested
+                resources.
+        """
+        assert (to_provision.cloud is not None and
+                to_provision.region is not None and to_provision.instance_type
+                is not None), (to_provision,
+                               'cloud, region and instance_type must have been '
+                               'set by optimizer')
         cloud = to_provision.cloud
-        region = None
+        region = clouds.Region(to_provision.region)
         zones = None
-        # Try loading previously launched region/zones and try them first,
-        # because we may have an existing cluster there.
-        # Get the *previous* cluster status and handle.
-        cluster_status, handle = backend_utils.refresh_cluster_status_handle(
-            cluster_name, acquire_per_cluster_status_lock=False)
-        if handle is not None:
-            try:
-                config = common_utils.read_yaml(handle.cluster_yaml)
-                prev_resources = handle.launched_resources
-                if prev_resources is not None and cloud.is_same_cloud(
-                        prev_resources.cloud):
-                    if cloud.is_same_cloud(clouds.GCP()) or cloud.is_same_cloud(
-                            clouds.AWS()):
-                        region = config['provider']['region']
-                        zones = config['provider']['availability_zone']
-                    elif cloud.is_same_cloud(clouds.Azure()):
-                        region = config['provider']['location']
-                        zones = None
-                    elif cloud.is_same_cloud(clouds.Local()):
-                        local_regions = clouds.Local.regions()
-                        region = local_regions[0].name
-                        zones = None
-                    else:
-                        assert False, cloud
-                    assert region == prev_resources.region, (
-                        f'Region mismatch. The region in '
-                        f'{handle.cluster_yaml} '
-                        'has been changed from '
-                        f'{prev_resources.region} to {region}.')
-                    assert (zones is None or prev_resources.zone is None or
-                            prev_resources.zone
-                            in zones), (f'{prev_resources.zone} not found in '
-                                        f'zones of {handle.cluster_yaml}.')
-                    # Note that we don't overwrite the zone field in Ray YAML
-                    # even if prev_resources.zone != zones.
-                    # This is because Ray will consider the YAML hash changed
-                    # and not reuse the existing cluster.
-            except FileNotFoundError:
-                # Happens if no previous cluster.yaml exists.
-                pass
-        if region is not None and cluster_exists:
 
-            region = clouds.Region(name=region)
-            if zones is not None:
-                zones = [clouds.Zone(name=zone) for zone in zones.split(',')]
-                region.set_zones(zones)
+        def _get_previously_launched_zones() -> Optional[List[clouds.Zone]]:
+            # When the cluster exists, the to_provision should have been set
+            # to the previous cluster's resources.
+            zones = [
+                clouds.Zone(name=to_provision.zone),
+            ] if to_provision.zone is not None else None
+            if zones is None:
+                # Reuse the zone field in the ray yaml as the
+                # prev_resources.zone field may not be set before the previous
+                # cluster is launched.
+                handle = global_user_state.get_handle_from_cluster_name(
+                    cluster_name)
+                assert isinstance(handle, CloudVmRayResourceHandle), (
+                    'handle should be CloudVmRayResourceHandle (found: '
+                    f'{type(handle)}) {cluster_name!r}')
+                config = common_utils.read_yaml(handle.cluster_yaml)
+                # This is for the case when the zone field is not set in the
+                # launched resources in a previous launch (e.g., ctrl-c during
+                # launch and multi-node cluster before PR #1700).
+                zones_str = config.get('provider', {}).get('availability_zone')
+                if zones_str is not None:
+                    zones = [
+                        clouds.Zone(name=zone) for zone in zones_str.split(',')
+                    ]
+            return zones
+
+        if prev_cluster_status is not None:
+            # If the cluster is previously launched, we should relaunch in the
+            # same region and zone.
+            zones = _get_previously_launched_zones()
 
-            if cluster_status != global_user_state.ClusterStatus.UP:
+            if prev_cluster_status != status_lib.ClusterStatus.UP:
                 logger.info(
-                    f'Cluster {cluster_name!r} (status: {cluster_status.value})'
-                    f' was previously launched in {cloud} '
-                    f'({region.name}). Relaunching in that region.')
-            # This should be handled in the check_resources_match function.
-            assert (to_provision.region is None or
-                    region.name == to_provision.region), (
-                        f'Cluster {cluster_name!r} was previously launched in '
-                        f'{cloud} ({region.name}). It does not match the '
-                        f'required region {to_provision.region}.')
+                    f'Cluster {cluster_name!r} (status: '
+                    f'{prev_cluster_status.value}) was previously launched '
+                    f'in {cloud} {region.name}. Relaunching in that region.')
             # TODO(zhwu): The cluster being killed by cloud provider should
             # be tested whether re-launching a cluster killed spot instance
             # will recover the data.
-            yield (region, zones)  # Ok to yield again in the next loop.
+            yield zones
 
+            # TODO(zhwu): update the following logics, since we have added
+            # the support for refreshing the cluster status from the cloud
+            # provider.
+            # If it reaches here: the cluster status in the database gets
+            # set to INIT, since a launch request was issued but failed.
+            #
             # Cluster with status UP can reach here, if it was killed by the
             # cloud provider and no available resources in that region to
             # relaunch, which can happen to spot instance.
-            if cluster_status == global_user_state.ClusterStatus.UP:
+            if prev_cluster_status == status_lib.ClusterStatus.UP:
                 message = (
                     f'Failed to connect to the cluster {cluster_name!r}. '
                     'It is possibly killed by cloud provider or manually '
                     'in the cloud provider console. To remove the cluster '
                     f'please run: sky down {cluster_name}')
                 # Reset to UP (rather than keeping it at INIT), as INIT
                 # mode will enable failover to other regions, causing
                 # data lose.
                 # TODO(zhwu): This is set to UP to be more conservative,
                 # we may need to confirm whether the cluster is down in all
                 # cases.
                 global_user_state.set_cluster_status(
-                    cluster_name, global_user_state.ClusterStatus.UP)
+                    cluster_name, status_lib.ClusterStatus.UP)
                 with ux_utils.print_exception_no_traceback():
                     raise exceptions.ResourcesUnavailableError(message,
                                                                no_failover=True)
 
-            # If it reaches here: the cluster status gets set to INIT, since
-            # a launch request was issued but failed.
-            #
             # Check the *previous* cluster status. If the cluster is previously
             # stopped, we should not retry other regions, since the previously
             # attached volumes are not visible on another region.
-            if cluster_status == global_user_state.ClusterStatus.STOPPED:
+            elif prev_cluster_status == status_lib.ClusterStatus.STOPPED:
                 message = (
                     'Failed to acquire resources to restart the stopped '
-                    f'cluster {cluster_name} on {region}. Please retry again '
-                    'later.')
+                    f'cluster {cluster_name} in {region.name}. Please retry '
+                    'again later.')
 
                 # Reset to STOPPED (rather than keeping it at INIT), because
                 # (1) the cluster is not up (2) it ensures future `sky start`
                 # will disable auto-failover too.
                 global_user_state.set_cluster_status(
-                    cluster_name, global_user_state.ClusterStatus.STOPPED)
+                    cluster_name, status_lib.ClusterStatus.STOPPED)
 
                 with ux_utils.print_exception_no_traceback():
                     raise exceptions.ResourcesUnavailableError(message,
                                                                no_failover=True)
-
-            assert cluster_status == global_user_state.ClusterStatus.INIT
+            assert prev_cluster_status == status_lib.ClusterStatus.INIT
             message = (f'Failed to launch cluster {cluster_name!r} '
-                       f'(previous status: {cluster_status.value}) '
+                       f'(previous status: {prev_cluster_status.value}) '
                        f'with the original resources: {to_provision}.')
             # We attempted re-launching a previously INIT cluster with the
             # same cloud/region/resources, but failed. Here no_failover=False,
             # so we will retry provisioning it with the current requested
             # resources in the outer loop.
             #
             # This condition can be triggered for previously INIT cluster by
@@ -902,37 +1245,49 @@
             # After (1), the cluster exists with INIT, and may or may not be
             # live.  And if it hits here, it's definitely not alive (because
             # step (2) failed).  Hence it's ok to retry with different
             # cloud/region and with current resources.
             with ux_utils.print_exception_no_traceback():
                 raise exceptions.ResourcesUnavailableError(message)
 
-        for region, zones in cloud.region_zones_provision_loop(
+        # If it reaches here, it means the cluster did not exist, as all the
+        # cases when the cluster exists have been handled above (either the
+        # provision succeeded in the caller and no need to retry, or this
+        # function raised an ResourcesUnavailableError).
+        for zones in cloud.zones_provision_loop(
+                region=to_provision.region,
+                num_nodes=num_nodes,
                 instance_type=to_provision.instance_type,
                 accelerators=to_provision.accelerators,
                 use_spot=to_provision.use_spot,
         ):
-            # Only retry requested region/zones or all if not specified.
-            zone_names = [zone.name for zone in zones]
-            if not to_provision.valid_on_region_zones(region.name, zone_names):
-                continue
-            if to_provision.zone is not None:
-                zones = [clouds.Zone(name=to_provision.zone)]
-            yield (region, zones)
+            if zones is None:
+                yield None
+            else:
+                assert zones, (
+                    'Either None or a non-empty list of zones should '
+                    'be yielded')
+                # Only retry requested region/zones or all if not specified.
+                zone_names = [zone.name for zone in zones]
+                if not to_provision.valid_on_region_zones(
+                        region.name, zone_names):
+                    continue
+                if to_provision.zone is not None:
+                    zones = [clouds.Zone(name=to_provision.zone)]
+                yield zones
 
     def _try_provision_tpu(self, to_provision: resources_lib.Resources,
                            config_dict: Dict[str, str]) -> bool:
         """Returns whether the provision is successful."""
         tpu_name = config_dict['tpu_name']
         assert 'tpu-create-script' in config_dict, \
             'Expect TPU provisioning with gcloud.'
         try:
-            with backend_utils.safe_console_status(
-                    '[bold cyan]Provisioning TPU '
-                    f'[green]{tpu_name}[/]'):
+            with log_utils.safe_rich_status('[bold cyan]Provisioning TPU '
+                                            f'[green]{tpu_name}[/]'):
                 subprocess_utils.run(f'bash {config_dict["tpu-create-script"]}',
                                      stdout=subprocess.PIPE,
                                      stderr=subprocess.PIPE)
             return True
         except subprocess.CalledProcessError as e:
             stderr = e.stderr.decode('ascii')
             if 'ALREADY_EXISTS' in stderr:
@@ -967,97 +1322,175 @@
                 logger.info(
                     f'  TPU type {tpu_type} is not available in this zone.')
                 return False
 
             logger.error(stderr)
             raise e
 
-    def _retry_region_zones(self,
-                            to_provision: resources_lib.Resources,
-                            num_nodes: int,
-                            dryrun: bool,
-                            stream_logs: bool,
-                            cluster_name: str,
-                            cluster_exists: bool = False):
+    def _retry_zones(
+        self,
+        to_provision: resources_lib.Resources,
+        num_nodes: int,
+        requested_resources: Set[resources_lib.Resources],
+        dryrun: bool,
+        stream_logs: bool,
+        cluster_name: str,
+        cloud_user_identity: Optional[List[str]],
+        prev_cluster_status: Optional[status_lib.ClusterStatus],
+    ):
         """The provision retry loop."""
         style = colorama.Style
         fore = colorama.Fore
         # Get log_path name
         log_path = os.path.join(self.log_dir, 'provision.log')
         log_abs_path = os.path.abspath(log_path)
+        if not dryrun:
+            os.makedirs(os.path.expanduser(self.log_dir), exist_ok=True)
+            os.system(f'touch {log_path}')
         tail_cmd = f'tail -n100 -f {log_path}'
         logger.info('To view detailed progress: '
                     f'{style.BRIGHT}{tail_cmd}{style.RESET_ALL}')
 
         # Get previous cluster status
-        prev_cluster_status = backend_utils.refresh_cluster_status_handle(
-            cluster_name, acquire_per_cluster_status_lock=False)[0]
+        cluster_exists = prev_cluster_status is not None
+        is_prev_cluster_healthy = prev_cluster_status in [
+            status_lib.ClusterStatus.STOPPED, status_lib.ClusterStatus.UP
+        ]
+
+        assert to_provision.region is not None, (
+            to_provision, 'region should have been set by the optimizer.')
+        region = clouds.Region(to_provision.region)
+
+        # Optimization - check if user has non-zero quota for
+        # the instance type in the target region. If not, fail early
+        # instead of trying to provision and failing later.
+        try:
+            need_provision = to_provision.cloud.check_quota_available(
+                to_provision.region, to_provision.instance_type,
+                to_provision.use_spot)
+
+        except Exception as e:  # pylint: disable=broad-except
+            need_provision = True
+            logger.info(f'Error occurred when trying to check quota. '
+                        f'Proceeding assuming quotas are available. Error: '
+                        f'{common_utils.format_exception(e, use_bracket=True)}')
+
+        if not need_provision:
+            # if quota is found to be zero, raise exception and skip to
+            # the next region
+            if to_provision.use_spot:
+                instance_descriptor = 'spot'
+            else:
+                instance_descriptor = 'on-demand'
+            raise exceptions.ResourcesUnavailableError(
+                f'{colorama.Fore.YELLOW}Found no quota for '
+                f'{to_provision.instance_type} {instance_descriptor} '
+                f'instances in region {to_provision.region}. '
+                f'{colorama.Style.RESET_ALL}'
+                f'To request quotas, check the instruction: '
+                f'https://skypilot.readthedocs.io/en/latest/cloud-setup/quota.html.'  # pylint: disable=line-too-long
+            )
 
-        self._clear_blocklist()
-        for region, zones in self._yield_region_zones(to_provision,
-                                                      cluster_name,
-                                                      cluster_exists):
-            if self._in_blocklist(to_provision.cloud, region, zones):
+        for zones in self._yield_zones(to_provision, num_nodes, cluster_name,
+                                       prev_cluster_status):
+            # Filter out zones that are blocked, if any.
+            # This optimize the provision loop by skipping zones that are
+            # indicated to be unavailable from previous provision attempts.
+            # It can happen for the provisioning on GCP, as the
+            # yield_region_zones will return zones from a region one by one,
+            # but the optimizer that does the filtering will not be involved
+            # until the next region.
+            if zones is not None:
+                remaining_unblocked_zones = copy.deepcopy(zones)
+                for zone in zones:
+                    for blocked_resources in self._blocked_resources:
+                        if to_provision.copy(
+                                region=region.name,
+                                zone=zone.name).should_be_blocked_by(
+                                    blocked_resources):
+                            remaining_unblocked_zones.remove(zone)
+                            break
+                if not remaining_unblocked_zones:
+                    # Skip the region if all zones are blocked.
+                    continue
+                zones = remaining_unblocked_zones
+
+            if zones is None:
+                # For clouds that don't have a zone concept or cloud
+                # provisioners that do not support zone-based provisioning
+                # (e.g., Azure, Lambda).
+                zone_str = ''
+            else:
+                zone_str = ','.join(z.name for z in zones)
+                zone_str = f' ({zone_str})'
+            try:
+                config_dict = backend_utils.write_cluster_config(
+                    to_provision,
+                    num_nodes,
+                    _get_cluster_config_template(to_provision.cloud),
+                    cluster_name,
+                    self._local_wheel_path,
+                    self._wheel_hash,
+                    region=region,
+                    zones=zones,
+                    dryrun=dryrun,
+                    keep_launch_fields_in_existing_config=cluster_exists)
+            except exceptions.ResourcesUnavailableError as e:
+                # Failed due to catalog issue, e.g. image not found.
+                logger.info(
+                    f'Failed to find catalog in region {region.name}: {e}')
                 continue
-            zone_str = ','.join(
-                z.name for z in zones) if zones is not None else 'all zones'
-            config_dict = backend_utils.write_cluster_config(
-                to_provision,
-                num_nodes,
-                _get_cluster_config_template(to_provision.cloud),
-                cluster_name,
-                self._local_wheel_path,
-                self._wheel_hash,
-                region=region,
-                zones=zones,
-                dryrun=dryrun,
-                keep_launch_fields_in_existing_config=prev_cluster_status
-                is not None)
             if dryrun:
                 return
             cluster_config_file = config_dict['ray']
 
             # Record early, so if anything goes wrong, 'sky status' will show
             # the cluster name and users can appropriately 'sky down'.  It also
             # means a second 'sky launch -c <name>' will attempt to reuse.
-            handle = CloudVmRayBackend.ResourceHandle(
+            handle = CloudVmRayResourceHandle(
                 cluster_name=cluster_name,
                 cluster_yaml=cluster_config_file,
                 launched_nodes=num_nodes,
                 # OK for this to be shown in CLI as status == INIT.
                 launched_resources=to_provision.copy(region=region.name),
                 tpu_create_script=config_dict.get('tpu-create-script'),
                 tpu_delete_script=config_dict.get('tpu-delete-script'))
             usage_lib.messages.usage.update_final_cluster_status(
-                global_user_state.ClusterStatus.INIT)
+                status_lib.ClusterStatus.INIT)
 
             # This sets the status to INIT (even for a normal, UP cluster).
-            global_user_state.add_or_update_cluster(cluster_name,
-                                                    cluster_handle=handle,
-                                                    ready=False)
+            global_user_state.add_or_update_cluster(
+                cluster_name,
+                cluster_handle=handle,
+                requested_resources=requested_resources,
+                ready=False,
+            )
+
+            global_user_state.set_owner_identity_for_cluster(
+                cluster_name, cloud_user_identity)
 
             tpu_name = config_dict.get('tpu_name')
             if tpu_name is not None:
                 logger.info(
                     f'{colorama.Style.BRIGHT}Provisioning TPU on '
                     f'{to_provision.cloud} '
-                    f'{region.name}{colorama.Style.RESET_ALL} ({zone_str})')
+                    f'{region.name}{colorama.Style.RESET_ALL}{zone_str}')
 
                 success = self._try_provision_tpu(to_provision, config_dict)
                 if not success:
                     continue
 
             logging_info = {
                 'cluster_name': cluster_name,
                 'region_name': region.name,
                 'zone_str': zone_str,
             }
             status, stdout, stderr, head_ip = self._gang_schedule_ray_up(
-                to_provision.cloud, num_nodes, cluster_config_file, handle,
-                log_abs_path, stream_logs, logging_info, to_provision.use_spot)
+                to_provision.cloud, cluster_config_file, handle, log_abs_path,
+                stream_logs, logging_info, to_provision.use_spot)
 
             if status == self.GangSchedulingStatus.CLUSTER_READY:
                 if cluster_exists:
                     # Guard against the case where there's an existing cluster
                     # with ray runtime messed up (e.g., manually killed) by (1)
                     # querying ray status (2) restarting ray if needed.
                     #
@@ -1080,178 +1513,199 @@
                 config_dict['zones'] = zones
                 plural = '' if num_nodes == 1 else 's'
                 if not isinstance(to_provision.cloud, clouds.Local):
                     logger.info(f'{fore.GREEN}Successfully provisioned or found'
                                 f' existing VM{plural}.{style.RESET_ALL}')
                 return config_dict
 
-            # The cluster is not ready.
+            # The cluster is not ready. We must perform error recording and/or
+            # cleanup.
 
             # If cluster was previously UP or STOPPED, stop it; otherwise
             # terminate.
             # FIXME(zongheng): terminating a potentially live cluster is
             # scary. Say: users have an existing cluster that got into INIT, do
             # sky launch, somehow failed, then we may be terminating it here.
-            need_terminate = prev_cluster_status not in [
-                global_user_state.ClusterStatus.STOPPED,
-                global_user_state.ClusterStatus.UP
-            ]
+            terminate_or_stop = not is_prev_cluster_healthy
+            definitely_no_nodes_launched = False
             if status == self.GangSchedulingStatus.HEAD_FAILED:
                 # ray up failed for the head node.
-                self._update_blocklist_on_error(to_provision.cloud, region,
-                                                zones, stdout, stderr)
+                definitely_no_nodes_launched = self._update_blocklist_on_error(
+                    to_provision, region, zones, stdout, stderr)
             else:
                 # gang scheduling failed.
                 assert status == self.GangSchedulingStatus.GANG_FAILED, status
                 # The stdout/stderr of ray up is not useful here, since
                 # head node is successfully provisioned.
-                self._update_blocklist_on_error(
-                    to_provision.cloud,
-                    region,
-                    # Ignored and block region:
-                    zones=None,
-                    stdout=None,
-                    stderr=None)
+                definitely_no_nodes_launched = self._update_blocklist_on_error(
+                    to_provision, region, zones=zones, stdout=None, stderr=None)
+                # GANG_FAILED means head is up, workers failed.
+                assert definitely_no_nodes_launched is False, (
+                    definitely_no_nodes_launched)
 
                 # Only log the errors for GANG_FAILED, since HEAD_FAILED may
                 # not have created any resources (it can happen however) and
                 # HEAD_FAILED can happen in "normal" failover cases.
                 logger.error('*** Failed provisioning the cluster. ***')
-                terminate_str = 'Terminating' if need_terminate else 'Stopping'
+                terminate_str = ('Terminating'
+                                 if terminate_or_stop else 'Stopping')
                 logger.error(f'*** {terminate_str} the failed cluster. ***')
 
-            # There may exists partial nodes (e.g., head node) so we must
+            # If these conditions hold, it *should* be safe to skip the cleanup
+            # action. This is a UX optimization.
+            #
+            # We want to skip mainly for VPC/subnets errors thrown during node
+            # provider bootstrapping: if users encountered "No VPC with name
+            # 'xxx' is found in <region>.", then going ahead to down the
+            # non-existent cluster will itself print out a (caught, harmless)
+            # error with the same message.  This was found to be
+            # confusing. Thus we skip termination.
+            skip_cleanup = not cluster_exists and definitely_no_nodes_launched
+            if skip_cleanup:
+                continue
+
+            # There may exist partial nodes (e.g., head node) so we must
             # terminate or stop before moving on to other regions.
             #
-            # NOTE: even HEAD_FAILED could've left a live head node there, so
-            # we must terminate/stop here too. E.g., node is up, and ray
+            # NOTE: even HEAD_FAILED could've left a live head node there,
+            # so we must terminate/stop here too. E.g., node is up, and ray
             # autoscaler proceeds to setup commands, which may fail:
             #   ERR updater.py:138 -- New status: update-failed
             CloudVmRayBackend().teardown_no_lock(handle,
-                                                 terminate=need_terminate)
+                                                 terminate=terminate_or_stop)
 
-        message = ('Failed to acquire resources in all regions/zones of '
-                   f'{to_provision.cloud}. '
-                   'Try changing resource requirements or use another cloud.')
-        raise exceptions.ResourcesUnavailableError(message)
+        if to_provision.zone is not None:
+            message = (
+                f'Failed to acquire resources in {to_provision.zone}. '
+                'Try changing resource requirements or use another zone.')
+        elif to_provision.region is not None:
+            # For public clouds, provision.region is always set.
+            message = ('Failed to acquire resources in all zones in '
+                       f'{to_provision.region}. Try changing resource '
+                       'requirements or use another region.')
+        else:
+            message = (f'Failed to acquire resources in {to_provision.cloud}. '
+                       'Try changing resource requirements or use another '
+                       'cloud provider.')
+        # Do not failover to other clouds if the cluster was previously
+        # UP or STOPPED, since the user can have some data on the cluster.
+        raise exceptions.ResourcesUnavailableError(
+            message, no_failover=is_prev_cluster_healthy)
 
     def _tpu_pod_setup(self, cluster_yaml: str,
-                       cluster_handle: 'backends.Backend.ResourceHandle',
-                       num_nodes: int):
+                       cluster_handle: 'backends.CloudVmRayResourceHandle'):
         """Completes setup and start Ray cluster on TPU VM Pod nodes.
 
         This is a workaround for Ray Autoscaler where `ray up` does not
         run setup or launch ray cluster on TPU VM Pod nodes.
         """
         ssh_credentials = backend_utils.ssh_credential_from_yaml(cluster_yaml)
-        all_ips = backend_utils.get_node_ips(cluster_yaml,
-                                             num_nodes,
-                                             handle=cluster_handle)
+        all_ips = cluster_handle.external_ips(use_cached_ips=False)
         num_tpu_devices = tpu_utils.get_num_tpu_devices(
             cluster_handle.launched_resources)
-        if len(all_ips) != num_tpu_devices:
+        if all_ips is None or len(all_ips) != num_tpu_devices:
             raise RuntimeError(
-                f'Number of nodes IPs: {len(all_ips)} does not'
+                f'Nodes IPs: {all_ips} does not'
                 f'match number of TPU devices: {num_tpu_devices}.')
 
         # Get the private IP of head node for connecting Ray cluster.
         head_runner = command_runner.SSHCommandRunner(all_ips[0],
                                                       **ssh_credentials)
         cmd_str = 'python3 -c \"import ray; print(ray._private.services.get_node_ip_address())\"'  # pylint: disable=line-too-long
         rc, stdout, stderr = head_runner.run(cmd_str,
-                                             stream_logs=False,
-                                             require_outputs=True)
+                                             require_outputs=True,
+                                             stream_logs=False)
         subprocess_utils.handle_returncode(
             rc,
             cmd_str,
             'Failed to get private IP from head node.',
             stderr=stdout + stderr)
         head_ip_private = stdout.strip()
 
         ray_config = common_utils.read_yaml(cluster_yaml)
         worker_start_ray_commands = [f'echo "export RAY_HEAD_IP={head_ip_private}" >> ~/.bashrc && source ~/.bashrc']  # pylint: disable=line-too-long
         worker_start_ray_commands += ray_config['worker_start_ray_commands']
 
         # Setup TPU VM Pod workers and launch Ray cluster.
-        backend_utils.do_filemounts_and_setup_on_local_workers(
+        onprem_utils.do_filemounts_and_setup_on_local_workers(
             cluster_yaml,
             worker_ips=all_ips[1:],
             extra_setup_cmds=worker_start_ray_commands)
 
     @timeline.event
     def _gang_schedule_ray_up(
-            self, to_provision_cloud: clouds.Cloud, num_nodes: int,
-            cluster_config_file: str,
-            cluster_handle: 'backends.Backend.ResourceHandle',
+            self, to_provision_cloud: clouds.Cloud, cluster_config_file: str,
+            cluster_handle: 'backends.CloudVmRayResourceHandle',
             log_abs_path: str, stream_logs: bool, logging_info: dict,
             use_spot: bool
     ) -> Tuple[GangSchedulingStatus, str, str, Optional[str]]:
         """Provisions a cluster via 'ray up' and wait until fully provisioned.
 
         Returns:
           (GangSchedulingStatus; stdout; stderr; optional head_ip).
         """
         # FIXME(zhwu,zongheng): ray up on multiple nodes ups the head node then
         # waits for all workers; turn it into real gang scheduling.
         # FIXME: refactor code path to remove use of stream_logs
         del stream_logs
 
-        style = colorama.Style
-
         def ray_up():
+            # Runs `ray up <kwargs>` with our monkey-patched launch hash
+            # calculation. See the monkey patch file for why.
+            #
+            # NOTE: --no-restart solves the following bug.  Without it, if 'ray
+            # up' (sky launch) twice on a cluster with >1 node, the worker node
+            # gets disconnected/killed by ray autoscaler; the whole task will
+            # just freeze.  (Doesn't affect 1-node clusters.)  With this flag,
+            # ray processes no longer restart and this bug doesn't show.
+            # Downside is existing tasks on the cluster will keep running
+            # (which may be ok with the semantics of 'sky launch' twice).
+            # Tracked in https://github.com/ray-project/ray/issues/20402.
+            # Ref: https://github.com/ray-project/ray/blob/releases/2.4.0/python/ray/autoscaler/sdk/sdk.py#L16-L49  # pylint: disable=line-too-long
+            script_path = write_ray_up_script_with_patched_launch_hash_fn(
+                cluster_config_file, ray_up_kwargs={'no_restart': True})
+
             # Redirect stdout/err to the file and streaming (if stream_logs).
             # With stdout/err redirected, 'ray up' will have no color and
             # different order from directly running in the console. The
             # `--log-style` and `--log-color` flags do not work. To reproduce,
             # `ray up --log-style pretty --log-color true | tee tmp.out`.
-
             returncode, stdout, stderr = log_lib.run_with_log(
-                # NOTE: --no-restart solves the following bug.  Without it, if
-                # 'ray up' (sky launch) twice on a cluster with >1 node, the
-                # worker node gets disconnected/killed by ray autoscaler; the
-                # whole task will just freeze.  (Doesn't affect 1-node
-                # clusters.)  With this flag, ray processes no longer restart
-                # and this bug doesn't show.  Downside is existing tasks on the
-                # cluster will keep running (which may be ok with the semantics
-                # of 'sky launch' twice).
-                # Tracked in https://github.com/ray-project/ray/issues/20402.
-                ['ray', 'up', '-y', '--no-restart', cluster_config_file],
+                [sys.executable, script_path],
                 log_abs_path,
                 stream_logs=False,
                 start_streaming_at='Shared connection to',
                 line_processor=log_utils.RayUpLineProcessor(),
                 # Reduce BOTO_MAX_RETRIES from 12 to 5 to avoid long hanging
                 # time during 'ray up' if insufficient capacity occurs.
                 env=dict(
                     os.environ,
                     BOTO_MAX_RETRIES='5',
-                    # Use environment variables to disable the ray usage stats
-                    # (to avoid the 10 second wait for usage collection
-                    # confirmation), as the ray version on the user's machine
-                    # may be lower version that does not support the
-                    # `--disable-usage-stats` flag.
+                    # Use environment variables to disable the ray usage collection
+                    # (to avoid overheads and potential issues with the usage)
+                    # as sdk does not take the argument for disabling the usage
+                    # collection.
                     RAY_USAGE_STATS_ENABLED='0'),
                 require_outputs=True,
                 # Disable stdin to avoid ray outputs mess up the terminal with
                 # misaligned output when multithreading/multiprocessing are used
-                # Refer to: https://github.com/ray-project/ray/blob/d462172be7c5779abf37609aed08af112a533e1e/python/ray/autoscaler/_private/subprocess_output_util.py#L264 # pylint: disable=line-too-long
+                # Refer to: https://github.com/ray-project/ray/blob/d462172be7c5779abf37609aed08af112a533e1e/python/ray/autoscaler/_private/subprocess_output_util.py#L264  # pylint: disable=line-too-long
                 stdin=subprocess.DEVNULL)
             return returncode, stdout, stderr
 
         region_name = logging_info['region_name']
         zone_str = logging_info['zone_str']
-
+        style = colorama.Style
         if isinstance(to_provision_cloud, clouds.Local):
             cluster_name = logging_info['cluster_name']
-            logger.info(f'{colorama.Style.BRIGHT}Launching on local cluster '
+            logger.info(f'{style.BRIGHT}Launching on local cluster '
                         f'{cluster_name!r}.')
         else:
-            logger.info(
-                f'{colorama.Style.BRIGHT}Launching on {to_provision_cloud} '
-                f'{region_name}{colorama.Style.RESET_ALL} ({zone_str})')
+            logger.info(f'{style.BRIGHT}Launching on {to_provision_cloud} '
+                        f'{region_name}{style.RESET_ALL}{zone_str}')
         start = time.time()
 
         # Edge case: /tmp/ray does not exist, so autoscaler can't create/store
         # cluster lock and cluster state.
         os.makedirs('/tmp/ray', exist_ok=True)
 
         # Launch the cluster with ray up
@@ -1288,14 +1742,26 @@
             if isinstance(to_provision_cloud, clouds.GCP):
                 if ('Quota exceeded for quota metric \'List requests\' and '
                         'limit \'List requests per minute\'' in stderr):
                     logger.info(
                         'Retrying due to list request rate limit exceeded.')
                     return True
 
+                # https://github.com/skypilot-org/skypilot/issues/1797
+                # "The resource 'projects/xxx/zones/us-central1-b/instances/ray-yyy-head-<hash>-compute' was not found" # pylint: disable=line-too-long
+                pattern = (r'\'code\': \'RESOURCE_NOT_FOUND\'.*The resource'
+                           r'.*instances\/.*-compute\' was not found')
+                result = re.search(pattern, stderr)
+                if result is not None:
+                    # Retry. Unlikely will succeed if it's due to no capacity.
+                    logger.info(
+                        'Retrying due to the possibly flaky RESOURCE_NOT_FOUND '
+                        'error.')
+                    return True
+
             if ('Processing file mounts' in stdout and
                     'Running setup commands' not in stdout and
                     'Failed to setup head node.' in stderr):
                 logger.info(
                     'Retrying runtime setup due to ssh connection issue.')
                 return True
 
@@ -1320,27 +1786,27 @@
                     'Retrying launching in {:.1f} seconds.'.format(sleep))
                 time.sleep(sleep)
             ray_up_return_value = ray_up()
 
         assert ray_up_return_value is not None
         returncode, stdout, stderr = ray_up_return_value
 
-        logger.debug(f'Ray up takes {time.time() - start} seconds with '
+        logger.debug(f'`ray up` takes {time.time() - start:.1f} seconds with '
                      f'{retry_cnt} retries.')
         if returncode != 0:
             return self.GangSchedulingStatus.HEAD_FAILED, stdout, stderr, None
 
         resources = cluster_handle.launched_resources
         if tpu_utils.is_tpu_vm_pod(resources):
             logger.info(f'{style.BRIGHT}Setting up TPU VM Pod workers...'
                         f'{style.RESET_ALL}')
-            self._tpu_pod_setup(cluster_config_file, cluster_handle, num_nodes)
+            self._tpu_pod_setup(cluster_config_file, cluster_handle)
 
         # Only 1 node or head node provisioning failure.
-        if num_nodes == 1 and returncode == 0:
+        if cluster_handle.launched_nodes == 1 and returncode == 0:
             # Optimization: Try parse head ip from 'ray up' stdout.
             # Last line looks like: 'ssh ... <user>@<public head_ip>\n'
             position = stdout.rfind('@')
             # Use a regex to extract the IP address.
             ip_list = re.findall(backend_utils.IP_ADDR_REGEX,
                                  stdout[position + 1:])
             # If something's wrong. Ok to not return a head_ip.
@@ -1360,25 +1826,26 @@
                     f'Waiting for workers.{style.RESET_ALL}')
 
         # Special handling is needed for the local case. This is due to a Ray
         # autoscaler bug, where filemounting and setup does not run on worker
         # nodes. Hence, this method here replicates what the Ray autoscaler
         # would do were it for public cloud.
         if isinstance(to_provision_cloud, clouds.Local):
-            backend_utils.do_filemounts_and_setup_on_local_workers(
+            onprem_utils.do_filemounts_and_setup_on_local_workers(
                 cluster_config_file)
 
         # FIXME(zongheng): the below requires ray processes are up on head. To
         # repro it failing: launch a 2-node cluster, log into head and ray
         # stop, then launch again.
         cluster_ready = backend_utils.wait_until_ray_cluster_ready(
             cluster_config_file,
-            num_nodes,
+            cluster_handle.launched_nodes,
             log_path=log_abs_path,
-            nodes_launching_progress_timeout=_NODES_LAUNCHING_PROGRESS_TIMEOUT,
+            nodes_launching_progress_timeout=_NODES_LAUNCHING_PROGRESS_TIMEOUT[
+                type(to_provision_cloud)],
             is_local_cloud=isinstance(to_provision_cloud, clouds.Local))
         if cluster_ready:
             cluster_status = self.GangSchedulingStatus.CLUSTER_READY
             # ray up --no-restart again with upscaling_speed=0 after cluster is
             # ready to ensure cluster will not scale up after preemption (spot).
             # Skip for non-spot as this takes extra time to provision (~1min).
             if use_spot:
@@ -1395,56 +1862,54 @@
         else:
             cluster_status = self.GangSchedulingStatus.GANG_FAILED
 
         # Do not need stdout/stderr if gang scheduling failed.
         # gang_succeeded = False, if head OK, but workers failed.
         return cluster_status, '', '', None
 
-    def _ensure_cluster_ray_started(self,
-                                    handle: 'CloudVmRayBackend.ResourceHandle',
+    def _ensure_cluster_ray_started(self, handle: 'CloudVmRayResourceHandle',
                                     log_abs_path) -> None:
         """Ensures ray processes are up on a just-provisioned cluster."""
         if handle.launched_nodes > 1:
             # FIXME(zongheng): this has NOT been tested with multinode
             # clusters; mainly because this function will not be reached in
             # that case.  See #140 for details.  If it were reached, the
             # following logic might work:
             #   - get all node ips
             #   - for all nodes: ray stop
             #   - ray up --restart-only
             return
         backend = CloudVmRayBackend()
 
         returncode = backend.run_on_head(
-            handle,
-            'ray status',
-            # At this state, an erroneous cluster may not have cached
-            # handle.head_ip (global_user_state.add_or_update_cluster(...,
-            # ready=True)).
-            use_cached_head_ip=False)
+            handle, backend_utils.RAY_STATUS_WITH_SKY_RAY_PORT_COMMAND)
         if returncode == 0:
             return
         launched_resources = handle.launched_resources
         # Ray cluster should already be running if the system admin has setup
         # Ray.
         if isinstance(launched_resources.cloud, clouds.Local):
             raise RuntimeError(
                 'The command `ray status` errored out on the head node '
-                'of the local cluster. Check if ray[default]==2.0.1 '
+                'of the local cluster. Check if ray[default]==2.4.0 '
                 'is installed or running correctly.')
-        backend.run_on_head(handle, 'ray stop', use_cached_head_ip=False)
+        backend.run_on_head(handle, 'ray stop')
 
+        # Runs `ray up <kwargs>` with our monkey-patched launch hash
+        # calculation. See the monkey patch file for why.
+        script_path = write_ray_up_script_with_patched_launch_hash_fn(
+            handle.cluster_yaml, ray_up_kwargs={'restart_only': True})
         log_lib.run_with_log(
-            ['ray', 'up', '-y', '--restart-only', handle.cluster_yaml],
+            [sys.executable, script_path],
             log_abs_path,
             stream_logs=False,
             # Use environment variables to disable the ray usage collection
-            # (avoid the 10 second wait for usage collection confirmation),
-            # as the ray version on the user's machine may be lower version
-            # that does not support the `--disable-usage-stats` flag.
+            # (to avoid overheads and potential issues with the usage)
+            # as sdk does not take the argument for disabling the usage
+            # collection.
             env=dict(os.environ, RAY_USAGE_STATS_ENABLED='0'),
             # Disable stdin to avoid ray outputs mess up the terminal with
             # misaligned output when multithreading/multiprocessing is used.
             # Refer to: https://github.com/ray-project/ray/blob/d462172be7c5779abf37609aed08af112a533e1e/python/ray/autoscaler/_private/subprocess_output_util.py#L264 # pylint: disable=line-too-long
             stdin=subprocess.DEVNULL)
 
     @timeline.event
@@ -1455,255 +1920,405 @@
         dryrun: bool,
         stream_logs: bool,
     ):
         """Provision with retries for all launchable resources."""
         cluster_name = to_provision_config.cluster_name
         to_provision = to_provision_config.resources
         num_nodes = to_provision_config.num_nodes
-        cluster_exists = to_provision_config.cluster_exists
+        prev_cluster_status = to_provision_config.prev_cluster_status
         launchable_retries_disabled = (self._dag is None or
                                        self._optimize_target is None)
 
+        failover_history: List[Exception] = list()
+
         style = colorama.Style
         # Retrying launchable resources.
-        provision_failed = True
-        while provision_failed:
-            provision_failed = False
+        while True:
             try:
-                try:
-                    # Recheck cluster name as the 'except:' block below may
-                    # change the cloud assignment.
-                    backend_utils.check_cluster_name_is_valid(
-                        cluster_name, to_provision.cloud)
-                except exceptions.InvalidClusterNameError as e:
-                    # Let failover below handle this (i.e., block this cloud).
-                    raise exceptions.ResourcesUnavailableError(str(e)) from e
-                config_dict = self._retry_region_zones(
+                # Recheck cluster name as the 'except:' block below may
+                # change the cloud assignment.
+                to_provision.cloud.check_cluster_name_is_valid(cluster_name)
+                if dryrun:
+                    cloud_user = None
+                else:
+                    cloud_user = to_provision.cloud.get_current_user_identity()
+                # Skip if to_provision.cloud does not support requested features
+                to_provision.cloud.check_features_are_supported(
+                    self._requested_features)
+
+                config_dict = self._retry_zones(
                     to_provision,
                     num_nodes,
+                    requested_resources=task.resources,
                     dryrun=dryrun,
                     stream_logs=stream_logs,
                     cluster_name=cluster_name,
-                    cluster_exists=cluster_exists)
+                    cloud_user_identity=cloud_user,
+                    prev_cluster_status=prev_cluster_status)
                 if dryrun:
                     return
+            except (exceptions.InvalidClusterNameError,
+                    exceptions.NotSupportedError,
+                    exceptions.CloudUserIdentityError) as e:
+                # InvalidClusterNameError: cluster name is invalid,
+                # NotSupportedError: cloud does not support requested features,
+                # CloudUserIdentityError: cloud user identity is invalid.
+                # The exceptions above should be applicable to the whole
+                # cloud, so we do add the cloud to the blocked resources.
+                logger.warning(common_utils.format_exception(e))
+                self._blocked_resources.add(
+                    resources_lib.Resources(cloud=to_provision.cloud))
+                failover_history.append(e)
             except exceptions.ResourcesUnavailableError as e:
+                failover_history.append(e)
                 if e.no_failover:
-                    raise e
+                    raise e.with_failover_history(failover_history)
                 if launchable_retries_disabled:
                     logger.warning(
                         'DAG and optimize_target needs to be registered first '
                         'to enable cross-cloud retry. '
                         'To fix, call backend.register_info(dag=dag, '
                         'optimize_target=sky.OptimizeTarget.COST)')
-                    raise e
+                    raise e.with_failover_history(failover_history)
 
-                logger.warning(e)
-                provision_failed = True
-                logger.warning(
-                    f'\n{style.BRIGHT}Provision failed for {num_nodes}x '
-                    f'{to_provision}. Trying other launchable resources '
-                    f'(if any).{style.RESET_ALL}')
-                if not cluster_exists:
-                    # Add failed resources to the blocklist, only when it
-                    # is in fallback mode.
-                    self._blocked_launchable_resources.add(to_provision)
-                else:
-                    logger.info(
-                        'Retrying provisioning with requested resources '
-                        f'{task.num_nodes}x {task.resources}')
-                    # Retry with the current, potentially "smaller" resources:
-                    # to_provision == the current new resources (e.g., V100:1),
-                    # which may be "smaller" than the original (V100:8).
-                    # num_nodes is not part of a Resources so must be updated
-                    # separately.
-                    num_nodes = task.num_nodes
-                    cluster_exists = False
-
-                # Set to None so that sky.optimize() will assign a new one
-                # (otherwise will skip re-optimizing this task).
-                # TODO: set all remaining tasks' best_resources to None.
-                task.best_resources = None
-                self._dag = sky.optimize(self._dag,
-                                         minimize=self._optimize_target,
-                                         blocked_launchable_resources=self.
-                                         _blocked_launchable_resources)
-                to_provision = task.best_resources
-                assert task in self._dag.tasks, 'Internal logic error.'
-                assert to_provision is not None, task
+                logger.warning(common_utils.format_exception(e))
+            else:
+                # Provisioning succeeded.
+                break
+
+            if to_provision.zone is None:
+                region_or_zone_str = str(to_provision.region)
+            else:
+                region_or_zone_str = str(to_provision.zone)
+            logger.warning(f'\n{style.BRIGHT}Provision failed for {num_nodes}x '
+                           f'{to_provision} in {region_or_zone_str}. '
+                           f'Trying other locations (if any).{style.RESET_ALL}')
+            if prev_cluster_status is None:
+                # Add failed resources to the blocklist, only when it
+                # is in fallback mode.
+                self._blocked_resources.add(to_provision)
+            else:
+                # If we reach here, it means that the existing cluster must have
+                # a previous status of INIT, because other statuses (UP,
+                # STOPPED) will not trigger the failover due to `no_failover`
+                # flag; see _yield_zones(). Also, the cluster should have been
+                # terminated by _retry_zones().
+                assert (prev_cluster_status == status_lib.ClusterStatus.INIT
+                       ), prev_cluster_status
+                assert global_user_state.get_handle_from_cluster_name(
+                    cluster_name) is None, cluster_name
+                logger.info('Retrying provisioning with requested resources '
+                            f'{task.num_nodes}x {task.resources}')
+                # Retry with the current, potentially "smaller" resources:
+                # to_provision == the current new resources (e.g., V100:1),
+                # which may be "smaller" than the original (V100:8).
+                # num_nodes is not part of a Resources so must be updated
+                # separately.
+                num_nodes = task.num_nodes
+                prev_cluster_status = None
+
+            # Set to None so that sky.optimize() will assign a new one
+            # (otherwise will skip re-optimizing this task).
+            # TODO: set all remaining tasks' best_resources to None.
+            task.best_resources = None
+            try:
+                self._dag = sky.optimize(
+                    self._dag,
+                    minimize=self._optimize_target,
+                    blocked_resources=self._blocked_resources)
+            except exceptions.ResourcesUnavailableError as e:
+                # Optimizer failed to find a feasible resources for the task,
+                # either because the previous failovers have blocked all the
+                # possible resources or the requested resources is too
+                # restrictive. If we reach here, our failover logic finally
+                # ends here.
+                raise e.with_failover_history(failover_history)
+            to_provision = task.best_resources
+            assert task in self._dag.tasks, 'Internal logic error.'
+            assert to_provision is not None, task
         return config_dict
 
 
-class CloudVmRayBackend(backends.Backend):
-    """Backend: runs on cloud virtual machines, managed by Ray.
+class CloudVmRayResourceHandle(backends.backend.ResourceHandle):
+    """A pickle-able tuple of:
 
-    Changing this class may also require updates to:
-      * Cloud providers' templates under config/
-      * Cloud providers' implementations under clouds/
+    - (required) Cluster name.
+    - (required) Path to a cluster.yaml file.
+    - (optional) A cached head node public IP.  Filled in after a
+        successful provision().
+    - (optional) A cached stable list of (internal IP, external IP) tuples
+        for all nodes in a cluster. Filled in after successful task execution.
+    - (optional) Launched num nodes
+    - (optional) Launched resources
+    - (optional) If TPU(s) are managed, a path to a deletion script.
     """
+    _VERSION = 3
 
-    NAME = 'cloudvmray'
+    def __init__(self,
+                 *,
+                 cluster_name: str,
+                 cluster_yaml: str,
+                 launched_nodes: int,
+                 launched_resources: resources_lib.Resources,
+                 stable_internal_external_ips: Optional[List[Tuple[
+                     str, str]]] = None,
+                 tpu_create_script: Optional[str] = None,
+                 tpu_delete_script: Optional[str] = None) -> None:
+        self._version = self._VERSION
+        self.cluster_name = cluster_name
+        self._cluster_yaml = cluster_yaml.replace(os.path.expanduser('~'), '~',
+                                                  1)
+        # List of (internal_ip, external_ip) tuples for all the nodes
+        # in the cluster, sorted by the external ips.
+        self.stable_internal_external_ips = stable_internal_external_ips
+        self.launched_nodes = launched_nodes
+        self.launched_resources = launched_resources
+        self.tpu_create_script = tpu_create_script
+        self.tpu_delete_script = tpu_delete_script
+        self._maybe_make_local_handle()
+
+    def __repr__(self):
+        return (f'ResourceHandle('
+                f'\n\tcluster_name={self.cluster_name},'
+                f'\n\thead_ip={self.head_ip},'
+                '\n\tstable_internal_external_ips='
+                f'{self.stable_internal_external_ips},'
+                '\n\tcluster_yaml='
+                f'{self.cluster_yaml}, '
+                f'\n\tlaunched_resources={self.launched_nodes}x '
+                f'{self.launched_resources}, '
+                f'\n\ttpu_create_script={self.tpu_create_script}, '
+                f'\n\ttpu_delete_script={self.tpu_delete_script})')
+
+    def get_cluster_name(self):
+        return self.cluster_name
+
+    def _maybe_make_local_handle(self):
+        """Adds local handle for the local cloud case.
+
+        For public cloud, the first time sky launch is ran, task resources
+        = cluster resources. For the local cloud case, sky launch is ran,
+        task resources != cluster resources; hence, this method is needed
+        to correct this.
+        """
+        self.local_handle = None
+        local_file = os.path.expanduser(
+            onprem_utils.SKY_USER_LOCAL_CONFIG_PATH.format(self.cluster_name))
+        # Local cluster case requires several modifications:
+        #   1) Create local_handle to store local cluster IPs and
+        #      custom accelerators for each node.
+        #   2) Replace launched_resources to represent a generic local
+        #      node (without accelerator specifications).
+        #   3) Replace launched_nodes to represent the total nodes in the
+        #      local cluster.
+        if os.path.isfile(local_file):
+            config = onprem_utils.get_local_cluster_config_or_error(
+                self.cluster_name)
+            self.local_handle = {}
+            cluster_config = config['cluster']
+            auth_config = config['auth']
+            ips = cluster_config['ips']
+            local_region = clouds.Local.regions()[0].name
+            # Convert existing ResourceHandle fields to specify local
+            # cluster resources.
+            self.launched_resources = resources_lib.Resources(
+                cloud=clouds.Local(), region=local_region)
+            self.launched_nodes = len(ips)
+            self.local_handle['ips'] = ips
+            cluster_accs = onprem_utils.get_local_cluster_accelerators(
+                ips, auth_config)
+            self.local_handle['cluster_resources'] = [
+                resources_lib.Resources(
+                    cloud=clouds.Local(),
+                    accelerators=acc_dict if acc_dict else None,
+                    region=local_region) for acc_dict in cluster_accs
+            ]
+
+    def _update_cluster_region(self):
+        if self.launched_resources.region is not None:
+            return
 
-    class ResourceHandle(object):
-        """A pickle-able tuple of:
+        config = common_utils.read_yaml(self.cluster_yaml)
+        provider = config['provider']
+        cloud = self.launched_resources.cloud
+        if cloud.is_same_cloud(clouds.Azure()):
+            region = provider['location']
+        elif cloud.is_same_cloud(clouds.GCP()) or cloud.is_same_cloud(
+                clouds.AWS()):
+            region = provider['region']
+        elif cloud.is_same_cloud(clouds.Local()):
+            # There is only 1 region for Local cluster, 'Local'.
+            local_regions = clouds.Local.regions()
+            region = local_regions[0].name
+
+        self.launched_resources = self.launched_resources.copy(region=region)
+
+    def _update_stable_cluster_ips(self, max_attempts: int = 1) -> None:
+        cluster_external_ips = backend_utils.get_node_ips(
+            self.cluster_yaml,
+            self.launched_nodes,
+            handle=self,
+            head_ip_max_attempts=max_attempts,
+            worker_ip_max_attempts=max_attempts,
+            get_internal_ips=False)
+
+        if self.external_ips() == cluster_external_ips:
+            # Optimization: If the cached external IPs are the same as the
+            # retrieved external IPs, then we can skip retrieving internal
+            # IPs since the cached IPs are up-to-date.
+            return
 
-        - (required) Cluster name.
-        - (required) Path to a cluster.yaml file.
-        - (optional) A cached head node public IP.  Filled in after a
-            successful provision().
-        - (optional) Launched num nodes
-        - (optional) Launched resources
-        - (optional) If TPU(s) are managed, a path to a deletion script.
-        """
-        _VERSION = 2
+        is_cluster_aws = (self.launched_resources is not None and
+                          isinstance(self.launched_resources.cloud, clouds.AWS))
+        if is_cluster_aws and skypilot_config.get_nested(
+                keys=('aws', 'use_internal_ips'), default_value=False):
+            # Optimization: if we know use_internal_ips is True (currently
+            # only exposed for AWS), then our AWS NodeProvider is
+            # guaranteed to pick subnets that will not assign public IPs,
+            # thus the first list of IPs returned above are already private
+            # IPs. So skip the second query.
+            cluster_internal_ips = list(cluster_external_ips)
+        else:
+            cluster_internal_ips = backend_utils.get_node_ips(
+                self.cluster_yaml,
+                self.launched_nodes,
+                handle=self,
+                head_ip_max_attempts=max_attempts,
+                worker_ip_max_attempts=max_attempts,
+                get_internal_ips=True)
+
+        assert len(cluster_external_ips) == len(cluster_internal_ips), (
+            f'Cluster {self.cluster_name!r}:'
+            f'Expected same number of internal IPs {cluster_internal_ips}'
+            f' and external IPs {cluster_external_ips}.')
+
+        internal_external_ips = list(
+            zip(cluster_internal_ips, cluster_external_ips))
+
+        # Ensure head node is the first element, then sort based on the
+        # external IPs for stableness
+        stable_internal_external_ips = [internal_external_ips[0]] + sorted(
+            internal_external_ips[1:], key=lambda x: x[1])
+        self.stable_internal_external_ips = stable_internal_external_ips
+
+    def internal_ips(self,
+                     max_attempts: int = _FETCH_IP_MAX_ATTEMPTS,
+                     use_cached_ips: bool = True) -> Optional[List[str]]:
+        if not use_cached_ips:
+            self._update_stable_cluster_ips(max_attempts=max_attempts)
+        if self.stable_internal_external_ips is not None:
+            return [ips[0] for ips in self.stable_internal_external_ips]
+        return None
+
+    def external_ips(self,
+                     max_attempts: int = _FETCH_IP_MAX_ATTEMPTS,
+                     use_cached_ips: bool = True) -> Optional[List[str]]:
+        if not use_cached_ips:
+            self._update_stable_cluster_ips(max_attempts=max_attempts)
+        if self.stable_internal_external_ips is not None:
+            return [ips[1] for ips in self.stable_internal_external_ips]
+        return None
+
+    def get_hourly_price(self) -> float:
+        hourly_cost = (self.launched_resources.get_cost(3600) *
+                       self.launched_nodes)
+        return hourly_cost
+
+    @property
+    def cluster_yaml(self):
+        return os.path.expanduser(self._cluster_yaml)
+
+    @property
+    def head_ip(self):
+        external_ips = self.external_ips()
+        if external_ips is not None:
+            return external_ips[0]
+        return None
+
+    def __setstate__(self, state):
+        self._version = self._VERSION
+
+        version = state.pop('_version', None)
+        if version is None:
+            version = -1
+            state.pop('cluster_region', None)
+        if version < 2:
+            state['_cluster_yaml'] = state.pop('cluster_yaml')
+        if version < 3:
+            head_ip = state.pop('head_ip', None)
+            state['stable_internal_external_ips'] = None
+
+        self.__dict__.update(state)
+
+        # Because the _update_stable_cluster_ips function uses the handle,
+        # we call it on the current instance after the state is updated
+        if version < 3 and head_ip is not None:
+            try:
+                self._update_stable_cluster_ips()
+            except exceptions.FetchIPError:
+                # This occurs when an old cluster from was autostopped,
+                # so the head IP in the database is not updated.
+                pass
 
-        def __init__(self,
-                     *,
-                     cluster_name: str,
-                     cluster_yaml: str,
-                     head_ip: Optional[str] = None,
-                     launched_nodes: Optional[int] = None,
-                     launched_resources: Optional[
-                         resources_lib.Resources] = None,
-                     tpu_create_script: Optional[str] = None,
-                     tpu_delete_script: Optional[str] = None) -> None:
-            self._version = self._VERSION
-            self.cluster_name = cluster_name
-            self._cluster_yaml = cluster_yaml.replace(os.path.expanduser('~'),
-                                                      '~', 1)
-            self.head_ip = head_ip
-            self.launched_nodes = launched_nodes
-            self.launched_resources = launched_resources
-            self.tpu_create_script = tpu_create_script
-            self.tpu_delete_script = tpu_delete_script
-            self._maybe_make_local_handle()
-
-        def __repr__(self):
-            return (f'ResourceHandle('
-                    f'\n\tcluster_name={self.cluster_name},'
-                    f'\n\thead_ip={self.head_ip},'
-                    '\n\tcluster_yaml='
-                    f'{self.cluster_yaml}, '
-                    f'\n\tlaunched_resources={self.launched_nodes}x '
-                    f'{self.launched_resources}, '
-                    f'\n\ttpu_create_script={self.tpu_create_script}, '
-                    f'\n\ttpu_delete_script={self.tpu_delete_script})')
-
-        def get_cluster_name(self):
-            return self.cluster_name
-
-        def _maybe_make_local_handle(self):
-            """Adds local handle for the local cloud case.
-
-            For public cloud, the first time sky launch is ran, task resources
-            = cluster resources. For the local cloud case, sky launch is ran,
-            task resources != cluster resources; hence, this method is needed
-            to correct this.
-            """
-            self.local_handle = None
-            local_file = os.path.expanduser(
-                onprem_utils.SKY_USER_LOCAL_CONFIG_PATH.format(
-                    self.cluster_name))
-            # Local cluster case requires several modifications:
-            #   1) Create local_handle to store local cluster IPs and
-            #      custom accelerators for each node.
-            #   2) Replace launched_resources to represent a generic local
-            #      node (without accelerator specifications).
-            #   3) Replace launched_nodes to represent the total nodes in the
-            #      local cluster.
-            if os.path.isfile(local_file):
-                config = onprem_utils.get_local_cluster_config_or_error(
-                    self.cluster_name)
-                self.local_handle = {}
-                cluster_config = config['cluster']
-                auth_config = config['auth']
-                ips = cluster_config['ips']
-                local_region = clouds.Local.regions()[0].name
-                # Convert existing ResourceHandle fields to specify local
-                # cluster resources.
-                self.launched_resources = resources_lib.Resources(
-                    cloud=clouds.Local(), region=local_region)
-                self.launched_nodes = len(ips)
-                self.local_handle['ips'] = ips
-                cluster_accs = onprem_utils.get_local_cluster_accelerators(
-                    ips, auth_config)
-                self.local_handle['cluster_resources'] = [
-                    resources_lib.Resources(
-                        cloud=clouds.Local(),
-                        accelerators=acc_dict if acc_dict else None,
-                        region=local_region) for acc_dict in cluster_accs
-                ]
+        self._update_cluster_region()
 
-        def _update_cluster_region(self):
-            if self.launched_resources.region is not None:
-                return
 
-            config = common_utils.read_yaml(self.cluster_yaml)
-            provider = config['provider']
-            cloud = self.launched_resources.cloud
-            if cloud.is_same_cloud(clouds.Azure()):
-                region = provider['location']
-            elif cloud.is_same_cloud(clouds.GCP()) or cloud.is_same_cloud(
-                    clouds.AWS()):
-                region = provider['region']
-            elif cloud.is_same_cloud(clouds.Local()):
-                # There is only 1 region for Local cluster, 'Local'.
-                local_regions = clouds.Local.regions()
-                region = local_regions[0].name
-
-            self.launched_resources = self.launched_resources.copy(
-                region=region)
-
-        @property
-        def cluster_yaml(self):
-            return os.path.expanduser(self._cluster_yaml)
-
-        def __setstate__(self, state):
-            self._version = self._VERSION
-
-            version = state.pop('_version', None)
-            if version is None:
-                version = -1
-                state.pop('cluster_region', None)
-            if version < 2:
-                state['_cluster_yaml'] = state.pop('cluster_yaml')
+class CloudVmRayBackend(backends.Backend['CloudVmRayResourceHandle']):
+    """Backend: runs on cloud virtual machines, managed by Ray.
+
+    Changing this class may also require updates to:
+      * Cloud providers' templates under config/
+      * Cloud providers' implementations under clouds/
+    """
+
+    NAME = 'cloudvmray'
 
-            self.__dict__.update(state)
-            self._update_cluster_region()
+    # Backward compatibility, with the old name of the handle.
+    ResourceHandle = CloudVmRayResourceHandle  # pylint: disable=invalid-name
 
     def __init__(self):
         self.run_timestamp = backend_utils.get_run_timestamp()
+        # NOTE: do not expanduser() here, as this '~/...' path is used for
+        # remote as well to be expanded on the remote side.
         self.log_dir = os.path.join(constants.SKY_LOGS_DIRECTORY,
                                     self.run_timestamp)
         # Do not make directories to avoid create folder for commands that
         # do not need it (`sky status`, `sky logs` ...)
         # os.makedirs(self.log_dir, exist_ok=True)
 
         self._dag = None
         self._optimize_target = None
+        self._requested_features = set()
 
         # Command for running the setup script. It is only set when the
         # setup needs to be run outside the self._setup() and as part of
         # a job (--detach-setup).
         self._setup_cmd = None
 
     # --- Implementation of Backend APIs ---
 
     def register_info(self, **kwargs) -> None:
         self._dag = kwargs.pop('dag', self._dag)
         self._optimize_target = kwargs.pop(
-            'optimize_target', self._optimize_target) or OptimizeTarget.COST
+            'optimize_target',
+            self._optimize_target) or optimizer.OptimizeTarget.COST
+        self._requested_features = kwargs.pop('requested_features',
+                                              self._requested_features)
         assert len(kwargs) == 0, f'Unexpected kwargs: {kwargs}'
 
-    def check_resources_fit_cluster(self, handle: ResourceHandle,
+    def check_resources_fit_cluster(self, handle: CloudVmRayResourceHandle,
                                     task: task_lib.Task):
         """Check if resources requested by the task fit the cluster.
 
         The resources requested by the task should be smaller than the existing
         cluster.
+
+        Raises:
+            exceptions.ResourcesMismatchError: If the resources in the task
+                does not match the existing cluster.
         """
         assert len(task.resources) == 1, task.resources
 
         launched_resources = handle.launched_resources
         task_resources = list(task.resources)[0]
         cluster_name = handle.cluster_name
         usage_lib.messages.usage.update_cluster_resources(
@@ -1753,67 +2368,96 @@
                 raise exceptions.ResourcesMismatchError(
                     'Requested resources do not match the existing cluster.\n'
                     f'  Requested:\t{task.num_nodes}x {task_resources} \n'
                     f'  Existing:\t{handle.launched_nodes}x '
                     f'{handle.launched_resources}\n'
                     f'{mismatch_str}')
 
-    def _provision(self,
-                   task: task_lib.Task,
-                   to_provision: Optional[resources_lib.Resources],
-                   dryrun: bool,
-                   stream_logs: bool,
-                   cluster_name: str,
-                   retry_until_up: bool = False) -> ResourceHandle:
-        """Provisions using 'ray up'."""
+    def _provision(
+            self,
+            task: task_lib.Task,
+            to_provision: Optional[resources_lib.Resources],
+            dryrun: bool,
+            stream_logs: bool,
+            cluster_name: str,
+            retry_until_up: bool = False) -> Optional[CloudVmRayResourceHandle]:
+        """Provisions using 'ray up'.
+
+        Raises:
+            exceptions.ClusterOwnerIdentityMismatchError: if the cluster
+                'cluster_name' exists and is owned by another user.
+            exceptions.InvalidClusterNameError: if the cluster name is invalid.
+            exceptions.ResourcesMismatchError: if the requested resources
+                do not match the existing cluster.
+            exceptions.ResourcesUnavailableError: if the requested resources
+                cannot be satisfied. The failover_history of the exception
+                will be set as at least 1 exception from either our pre-checks
+                (e.g., cluster name invalid) or a region/zone throwing
+                resource unavailability.
+            exceptions.CommandError: any ssh command error.
+            RuntimeErorr: raised when 'rsync' is not installed.
+            # TODO(zhwu): complete the list of exceptions.
+        """
         # FIXME: ray up for Azure with different cluster_names will overwrite
         # each other.
+        # When rsync is not installed in the user's machine, Ray will
+        # silently retry to up the node for _MAX_RAY_UP_RETRY number
+        # of times. This is time consuming so we fail early.
+        backend_utils.check_rsync_installed()
+        # Check if the cluster is owned by the current user. Raise
+        # exceptions.ClusterOwnerIdentityMismatchError
+        backend_utils.check_owner_identity(cluster_name)
         lock_path = os.path.expanduser(
             backend_utils.CLUSTER_STATUS_LOCK_PATH.format(cluster_name))
         with timeline.FileLockEvent(lock_path):
             to_provision_config = RetryingVmProvisioner.ToProvisionConfig(
-                cluster_name, to_provision, task.num_nodes)
-            prev_cluster_status = None
+                cluster_name,
+                to_provision,
+                task.num_nodes,
+                prev_cluster_status=None)
             if not dryrun:  # dry run doesn't need to check existing cluster.
                 # Try to launch the exiting cluster first
                 to_provision_config = self._check_existing_cluster(
                     task, to_provision, cluster_name)
-                prev_cluster_status, _ = (
-                    backend_utils.refresh_cluster_status_handle(
-                        cluster_name, acquire_per_cluster_status_lock=False))
             assert to_provision_config.resources is not None, (
                 'to_provision should not be None', to_provision_config)
 
+            prev_cluster_status = to_provision_config.prev_cluster_status
             usage_lib.messages.usage.update_cluster_resources(
                 to_provision_config.num_nodes, to_provision_config.resources)
             usage_lib.messages.usage.update_cluster_status(prev_cluster_status)
 
             # TODO(suquark): once we have sky on PyPI, we should directly
             # install sky from PyPI.
             # NOTE: can take ~2s.
             with timeline.Event('backend.provision.wheel_build'):
                 # TODO(suquark): once we have sky on PyPI, we should directly
                 # install sky from PyPI.
                 local_wheel_path, wheel_hash = wheel_utils.build_sky_wheel()
             backoff = common_utils.Backoff(_RETRY_UNTIL_UP_INIT_GAP_SECONDS)
             attempt_cnt = 1
             while True:
-                # RetryingVmProvisioner will retry within a cloud's regions
-                # first (if a region is not explicitly requested), then
-                # optionally retry on all other clouds (if
-                # backend.register_info() has been called).  After this "round"
-                # of optimization across clouds, provisioning may still have
-                # not succeeded. This while loop will then kick in if
-                # retry_until_up is set, which will kick off new "rounds" of
-                # optimization infinitely.
+                # For on-demand instances, RetryingVmProvisioner will retry
+                # within the given region first, then optionally retry on all
+                # other clouds and regions (if backend.register_info()
+                # has been called).
+                # For spot instances, each provisioning request is made for a
+                # single zone and the provisioner will retry on all other
+                # clouds, regions, and zones.
+                # See optimizer.py#_make_launchables_for_valid_region_zones()
+                # for detailed reasons.
+
+                # After this "round" of optimization across clouds, provisioning
+                # may still have not succeeded. This while loop will then kick
+                # in if retry_until_up is set, which will kick off new "rounds"
+                # of optimization infinitely.
                 try:
-                    provisioner = RetryingVmProvisioner(self.log_dir, self._dag,
-                                                        self._optimize_target,
-                                                        local_wheel_path,
-                                                        wheel_hash)
+                    provisioner = RetryingVmProvisioner(
+                        self.log_dir, self._dag, self._optimize_target,
+                        self._requested_features, local_wheel_path, wheel_hash)
                     config_dict = provisioner.provision_with_retries(
                         task, to_provision_config, dryrun, stream_logs)
                     break
                 except exceptions.ResourcesUnavailableError as e:
                     # Do not remove the stopped cluster from the global state
                     # if failed to start.
                     if e.no_failover:
@@ -1824,15 +2468,15 @@
                                                          terminate=True)
                         usage_lib.messages.usage.update_final_cluster_status(
                             None)
                         error_message = (
                             'Failed to provision all possible launchable '
                             'resources.'
                             f' Relax the task\'s resource requirements: '
-                            f'{task.num_nodes}x {task.resources}')
+                            f'{task.num_nodes}x {list(task.resources)[0]}')
                     if retry_until_up:
                         logger.error(error_message)
                         # Sleep and retry.
                         gap_seconds = backoff.current_backoff()
                         plural = 's' if attempt_cnt > 1 else ''
                         logger.info(
                             f'{colorama.Style.BRIGHT}=== Retry until up ==='
@@ -1844,138 +2488,160 @@
                         time.sleep(gap_seconds)
                         continue
                     error_message += (
                         '\nTo keep retrying until the cluster is up, use the '
                         '`--retry-until-up` flag.')
                     with ux_utils.print_exception_no_traceback():
                         raise exceptions.ResourcesUnavailableError(
-                            error_message) from None
+                            error_message,
+                            failover_history=e.failover_history) from None
             if dryrun:
-                return
+                return None
             cluster_config_file = config_dict['ray']
 
-            if 'tpu_name' in config_dict:
-                self._set_tpu_name(cluster_config_file,
-                                   config_dict['launched_nodes'],
-                                   config_dict['tpu_name'])
-
-            if config_dict['launched_nodes'] == 1 and config_dict[
-                    'head_ip'] is not None:
-                # Optimization for 1-node: we may have parsed the stdout of
-                # 'ray up' to get the head_ip already.
-                head_ip = config_dict['head_ip']
-                ip_list = [head_ip]
-            else:
-                # NOTE: querying node_ips is expensive, observed 1node GCP >=4s.
-                with timeline.Event('backend.provision.get_node_ips'):
-                    ip_list = backend_utils.get_node_ips(
-                        cluster_config_file,
-                        config_dict['launched_nodes'],
-                        head_ip_max_attempts=_HEAD_IP_MAX_ATTEMPTS,
-                        worker_ip_max_attempts=_WORKER_IP_MAX_ATTEMPTS)
-                    head_ip = ip_list[0]
-
-            handle = self.ResourceHandle(
+            handle = CloudVmRayResourceHandle(
                 cluster_name=cluster_name,
                 cluster_yaml=cluster_config_file,
-                # Cache head ip in the handle to speed up ssh operations.
-                head_ip=head_ip,
                 launched_nodes=config_dict['launched_nodes'],
                 launched_resources=config_dict['launched_resources'],
                 # TPU.
                 tpu_create_script=config_dict.get('tpu-create-script'),
                 tpu_delete_script=config_dict.get('tpu-delete-script'))
 
+            ip_list = handle.external_ips(max_attempts=_FETCH_IP_MAX_ATTEMPTS,
+                                          use_cached_ips=False)
+            assert ip_list is not None, handle
+
+            if 'tpu_name' in config_dict:
+                self._set_tpu_name(handle, config_dict['tpu_name'])
+
             # Get actual zone info and save it into handle.
             # NOTE: querying zones is expensive, observed 1node GCP >=4s.
             zones = config_dict['zones']
             if zones is not None and len(zones) == 1:  # zones is None for Azure
                 # Optimization for if the provision request was for 1 zone
                 # (currently happens only for GCP since it uses per-zone
                 # provisioning), then we know the exact zone already.
                 handle.launched_resources = handle.launched_resources.copy(
                     zone=zones[0].name)
-            elif (task.num_nodes == 1 or
-                  handle.launched_resources.zone is not None):
-                # Query zone if the cluster has 1 node, or a zone is
-                # specifically requested for a multinode cluster.  Otherwise
-                # leave the zone field to None because head and worker nodes
-                # can be launched in different zones.
+            else:
                 get_zone_cmd = (
                     handle.launched_resources.cloud.get_zone_shell_cmd())
                 if get_zone_cmd is not None:
-                    returncode, stdout, stderr = self.run_on_head(
-                        handle, get_zone_cmd, require_outputs=True)
-                    subprocess_utils.handle_returncode(returncode,
-                                                       get_zone_cmd,
-                                                       'Failed to get zone',
-                                                       stderr=stderr,
-                                                       stream_logs=stream_logs)
-                    # zone will be checked during Resources cls
-                    # initialization.
-                    handle.launched_resources = handle.launched_resources.copy(
-                        zone=stdout.strip())
-
-            usage_lib.messages.usage.update_cluster_resources(
-                handle.launched_nodes, handle.launched_resources)
+                    ssh_credentials = backend_utils.ssh_credential_from_yaml(
+                        handle.cluster_yaml)
+                    runners = command_runner.SSHCommandRunner.make_runner_list(
+                        ip_list, **ssh_credentials)
+
+                    def _get_zone(runner):
+                        retry_count = 0
+                        backoff = common_utils.Backoff(initial_backoff=1,
+                                                       max_backoff_factor=3)
+                        while True:
+                            returncode, stdout, stderr = runner.run(
+                                get_zone_cmd,
+                                require_outputs=True,
+                                stream_logs=False)
+                            if returncode == 0:
+                                break
+                            retry_count += 1
+                            if retry_count <= _MAX_GET_ZONE_RETRY:
+                                time.sleep(backoff.current_backoff())
+                                continue
+                        subprocess_utils.handle_returncode(
+                            returncode,
+                            get_zone_cmd,
+                            f'Failed to get zone for {cluster_name!r}',
+                            stderr=stderr,
+                            stream_logs=stream_logs)
+                        return stdout.strip()
+
+                    zones = subprocess_utils.run_in_parallel(_get_zone, runners)
+                    if len(set(zones)) == 1:
+                        # zone will be checked during Resources cls
+                        # initialization.
+                        handle.launched_resources = (
+                            handle.launched_resources.copy(zone=zones[0]))
+                    # If the number of zones > 1, nodes in the cluster are
+                    # launched in different zones (legacy clusters before
+                    # #1700), leave the zone field of handle.launched_resources
+                    # to None.
+            self._update_after_cluster_provisioned(handle, task,
+                                                   prev_cluster_status, ip_list,
+                                                   lock_path)
+            return handle
+
+    def _update_after_cluster_provisioned(
+            self, handle: CloudVmRayResourceHandle, task: task_lib.Task,
+            prev_cluster_status: Optional[status_lib.ClusterStatus],
+            ip_list: List[str], lock_path: str) -> None:
+        usage_lib.messages.usage.update_cluster_resources(
+            handle.launched_nodes, handle.launched_resources)
+        usage_lib.messages.usage.update_final_cluster_status(
+            status_lib.ClusterStatus.UP)
+
+        # For backward compatibility and robustness of skylet, it is restarted
+        with log_utils.safe_rich_status('Updating remote skylet'):
+            self.run_on_head(handle, _MAYBE_SKYLET_RESTART_CMD)
+
+        # Update job queue to avoid stale jobs (when restarted), before
+        # setting the cluster to be ready.
+        if prev_cluster_status == status_lib.ClusterStatus.INIT:
+            # update_status will query the ray job status for all INIT /
+            # PENDING / RUNNING jobs for the real status, since we do not
+            # know the actual previous status of the cluster.
+            job_owner = onprem_utils.get_job_owner(handle.cluster_yaml)
+            cmd = job_lib.JobLibCodeGen.update_status(job_owner)
+            with log_utils.safe_rich_status('[bold cyan]Preparing Job Queue'):
+                returncode, _, stderr = self.run_on_head(handle,
+                                                         cmd,
+                                                         require_outputs=True)
+            subprocess_utils.handle_returncode(returncode, cmd,
+                                               'Failed to update job status.',
+                                               stderr)
+        if prev_cluster_status == status_lib.ClusterStatus.STOPPED:
+            # Safely set all the previous jobs to FAILED since the cluster
+            # is restarted
+            # An edge case here due to racing:
+            # 1. A job finishes RUNNING, but right before it update itself
+            # to SUCCEEDED, the cluster is STOPPED by `sky stop`.
+            # 2. On next `sky start`, it gets reset to FAILED.
+            cmd = job_lib.JobLibCodeGen.fail_all_jobs_in_progress()
+            returncode, stdout, stderr = self.run_on_head(handle,
+                                                          cmd,
+                                                          require_outputs=True)
+            subprocess_utils.handle_returncode(
+                returncode, cmd,
+                'Failed to set previously in-progress jobs to FAILED',
+                stdout + stderr)
+
+        with timeline.Event('backend.provision.post_process'):
+            global_user_state.add_or_update_cluster(
+                handle.cluster_name,
+                handle,
+                task.resources,
+                ready=True,
+            )
             usage_lib.messages.usage.update_final_cluster_status(
-                global_user_state.ClusterStatus.UP)
+                status_lib.ClusterStatus.UP)
+            auth_config = common_utils.read_yaml(handle.cluster_yaml)['auth']
+            backend_utils.SSHConfigHelper.add_cluster(handle.cluster_name,
+                                                      ip_list, auth_config)
 
-            # Update job queue to avoid stale jobs (when restarted), before
-            # setting the cluster to be ready.
-            if prev_cluster_status == global_user_state.ClusterStatus.INIT:
-                # update_status will query the ray job status for all INIT /
-                # PENDING / RUNNING jobs for the real status, since we do not
-                # know the actual previous status of the cluster.
-                job_owner = onprem_utils.get_job_owner(handle.cluster_yaml)
-                cmd = job_lib.JobLibCodeGen.update_status(job_owner)
-                with backend_utils.safe_console_status(
-                        '[bold cyan]Preparing Job Queue'):
-                    returncode, _, stderr = self.run_on_head(
-                        handle, cmd, require_outputs=True)
-                subprocess_utils.handle_returncode(
-                    returncode, cmd, 'Failed to update job status.', stderr)
-            if prev_cluster_status == global_user_state.ClusterStatus.STOPPED:
-                # Safely set all the previous jobs to FAILED since the cluster
-                # is restarted
-                # An edge case here due to racing:
-                # 1. A job finishes RUNNING, but right before it update itself
-                # to SUCCEEDED, the cluster is STOPPED by `sky stop`.
-                # 2. On next `sky start`, it gets reset to FAILED.
-                cmd = job_lib.JobLibCodeGen.fail_all_jobs_in_progress()
-                returncode, stdout, stderr = self.run_on_head(
-                    handle, cmd, require_outputs=True)
-                subprocess_utils.handle_returncode(
-                    returncode, cmd,
-                    'Failed to set previously in-progress jobs to FAILED',
-                    stdout + stderr)
-
-            with timeline.Event('backend.provision.post_process'):
-                global_user_state.add_or_update_cluster(cluster_name,
-                                                        handle,
-                                                        ready=True)
-                usage_lib.messages.usage.update_final_cluster_status(
-                    global_user_state.ClusterStatus.UP)
-                auth_config = common_utils.read_yaml(
-                    handle.cluster_yaml)['auth']
-                backend_utils.SSHConfigHelper.add_cluster(
-                    cluster_name, ip_list, auth_config)
+            common_utils.remove_file_if_exists(lock_path)
 
-                os.remove(lock_path)
-                return handle
-
-    def _sync_workdir(self, handle: ResourceHandle, workdir: Path) -> None:
+    def _sync_workdir(self, handle: CloudVmRayResourceHandle,
+                      workdir: Path) -> None:
         # Even though provision() takes care of it, there may be cases where
         # this function is called in isolation, without calling provision(),
         # e.g., in CLI.  So we should rerun rsync_up.
         fore = colorama.Fore
         style = colorama.Style
-        ip_list = backend_utils.get_node_ips(handle.cluster_yaml,
-                                             handle.launched_nodes,
-                                             handle=handle)
+        ip_list = handle.external_ips()
+        assert ip_list is not None, 'external_ips is not cached in handle'
         full_workdir = os.path.abspath(os.path.expanduser(workdir))
 
         # These asserts have been validated at Task construction time.
         assert os.path.exists(full_workdir), f'{full_workdir} does not exist'
         if os.path.islink(full_workdir):
             logger.warning(
                 f'{fore.YELLOW}Workdir {workdir!r} is a symlink. '
@@ -2014,32 +2680,34 @@
         num_nodes = handle.launched_nodes
         plural = 's' if num_nodes > 1 else ''
         logger.info(
             f'{fore.CYAN}Syncing workdir (to {num_nodes} node{plural}): '
             f'{style.BRIGHT}{workdir}{style.RESET_ALL}'
             f' -> '
             f'{style.BRIGHT}{SKY_REMOTE_WORKDIR}{style.RESET_ALL}')
+        os.makedirs(os.path.expanduser(self.log_dir), exist_ok=True)
+        os.system(f'touch {log_path}')
         tail_cmd = f'tail -n100 -f {log_path}'
         logger.info('To view detailed progress: '
                     f'{style.BRIGHT}{tail_cmd}{style.RESET_ALL}')
-        with backend_utils.safe_console_status('[bold cyan]Syncing[/]'):
+        with log_utils.safe_rich_status('[bold cyan]Syncing[/]'):
             subprocess_utils.run_in_parallel(_sync_workdir_node, runners)
 
     def _sync_file_mounts(
         self,
-        handle: ResourceHandle,
+        handle: CloudVmRayResourceHandle,
         all_file_mounts: Dict[Path, Path],
         storage_mounts: Dict[Path, storage_lib.Storage],
     ) -> None:
         """Mounts all user files to the remote nodes."""
         self._execute_file_mounts(handle, all_file_mounts)
         self._execute_storage_mounts(handle, storage_mounts)
 
-    def _setup(self, handle: ResourceHandle, task: task_lib.Task,
-               detach_setup: bool) -> Optional[str]:
+    def _setup(self, handle: CloudVmRayResourceHandle, task: task_lib.Task,
+               detach_setup: bool) -> None:
         start = time.time()
         style = colorama.Style
         fore = colorama.Fore
 
         if task.setup is None:
             return
 
@@ -2047,33 +2715,29 @@
                                                      env_vars=task.envs)
         with tempfile.NamedTemporaryFile('w', prefix='sky_setup_') as f:
             f.write(setup_script)
             f.flush()
             setup_sh_path = f.name
             setup_file = os.path.basename(setup_sh_path)
             # Sync the setup script up and run it.
-            ip_list = backend_utils.get_node_ips(
-                handle.cluster_yaml,
-                handle.launched_nodes,
-                handle=handle,
-                head_ip_max_attempts=_HEAD_IP_MAX_ATTEMPTS,
-                worker_ip_max_attempts=_WORKER_IP_MAX_ATTEMPTS)
+            ip_list = handle.external_ips()
+            assert ip_list is not None, 'external_ips is not cached in handle'
             ssh_credentials = backend_utils.ssh_credential_from_yaml(
                 handle.cluster_yaml)
             # Disable connection sharing for setup script to avoid old
             # connections being reused, which may cause stale ssh agent
             # forwarding.
             ssh_credentials.pop('ssh_control_name')
             runners = command_runner.SSHCommandRunner.make_runner_list(
                 ip_list, **ssh_credentials)
 
             # Need this `-i` option to make sure `source ~/.bashrc` work
             setup_cmd = f'/bin/bash -i /tmp/{setup_file} 2>&1'
 
-            def _setup_node(runner: command_runner.SSHCommandRunner) -> int:
+            def _setup_node(runner: command_runner.SSHCommandRunner) -> None:
                 runner.rsync(source=setup_sh_path,
                              target=f'/tmp/{setup_file}',
                              up=True,
                              stream_logs=False)
                 if detach_setup:
                     return
                 setup_log_path = os.path.join(self.log_dir,
@@ -2130,22 +2794,22 @@
             return
         logger.info(f'{fore.GREEN}Setup completed.{style.RESET_ALL}')
         end = time.time()
         logger.debug(f'Setup took {end - start} seconds.')
 
     def _exec_code_on_head(
         self,
-        handle: ResourceHandle,
+        handle: CloudVmRayResourceHandle,
         codegen: str,
         job_id: int,
         executable: str,
         detach_run: bool = False,
+        spot_dag: Optional['dag.Dag'] = None,
     ) -> None:
         """Executes generated code on the head node."""
-        colorama.init()
         style = colorama.Style
         fore = colorama.Fore
         ssh_credentials = backend_utils.ssh_credential_from_yaml(
             handle.cluster_yaml)
         runner = command_runner.SSHCommandRunner(handle.head_ip,
                                                  **ssh_credentials)
         with tempfile.NamedTemporaryFile('w', prefix='sky_app_') as fp:
@@ -2175,23 +2839,57 @@
             executable = onprem_utils.get_python_executable(handle.cluster_name)
             ray_command = (f'{cd} && {executable} -u {script_path} '
                            f'> {remote_log_path} 2>&1')
             job_submit_cmd = self._setup_and_create_job_cmd_on_local_head(
                 handle, ray_command, ray_job_id)
         else:
             job_submit_cmd = (
-                f'{cd} && mkdir -p {remote_log_dir} && ray job submit '
-                f'--address=http://127.0.0.1:8265 --submission-id {ray_job_id} '
-                '--no-wait '
+                'RAY_DASHBOARD_PORT=$(python -c "from sky.skylet import job_lib; print(job_lib.get_job_submission_port())" 2> /dev/null || echo 8265);'  # pylint: disable=line-too-long
+                f'{cd} && ray job submit '
+                '--address=http://127.0.0.1:$RAY_DASHBOARD_PORT '
+                f'--submission-id {ray_job_id} --no-wait '
                 f'"{executable} -u {script_path} > {remote_log_path} 2>&1"')
 
+            mkdir_code = (f'{cd} && mkdir -p {remote_log_dir} && '
+                          f'touch {remote_log_path}')
+            code = job_lib.JobLibCodeGen.queue_job(job_id, job_submit_cmd)
+
+            job_submit_cmd = mkdir_code + ' && ' + code
+
+            if spot_dag is not None:
+                # Add the spot job to spot queue table.
+                spot_codegen = spot_lib.SpotCodeGen()
+                spot_code = spot_codegen.set_pending(job_id, spot_dag)
+                # Set the spot job to PENDING state to make sure that this spot
+                # job appears in the `sky spot queue`, when there are already 16
+                # controller process jobs running on the controller VM with 8
+                # CPU cores.
+                # The spot job should be set to PENDING state *after* the
+                # controller process job has been queued, as our skylet on spot
+                # controller will set the spot job in FAILED state if the
+                # controller process job does not exist.
+                # We cannot set the spot job to PENDING state in the codegen for
+                # the controller process job, as it will stay in the job pending
+                # table and not be executed until there is an empty slot.
+                job_submit_cmd = job_submit_cmd + ' && ' + spot_code
+
         returncode, stdout, stderr = self.run_on_head(handle,
                                                       job_submit_cmd,
                                                       stream_logs=False,
                                                       require_outputs=True)
+
+        if 'has no attribute' in stdout:
+            # Happens when someone calls `sky exec` but remote is outdated
+            # necessicating calling `sky launch`
+            with ux_utils.print_exception_no_traceback():
+                raise RuntimeError(
+                    f'{colorama.Fore.RED}SkyPilot runtime is stale on the '
+                    'remote cluster. To update, run: sky launch -c '
+                    f'{handle.cluster_name}{colorama.Style.RESET_ALL}')
+
         subprocess_utils.handle_returncode(returncode,
                                            job_submit_cmd,
                                            f'Failed to submit job {job_id}.',
                                            stderr=stdout + stderr)
 
         logger.info('Job submitted with Job ID: '
                     f'{style.BRIGHT}{job_id}{style.RESET_ALL}')
@@ -2203,78 +2901,87 @@
                 else:
                     # Sky logs. Not using subprocess.run since it will make the
                     # ssh keep connected after ctrl-c.
                     self.tail_logs(handle, job_id)
         finally:
             name = handle.cluster_name
             if name == spot_lib.SPOT_CONTROLLER_NAME:
-                logger.info(f'{fore.CYAN}Spot Job ID: '
-                            f'{style.BRIGHT}{job_id}{style.RESET_ALL}'
-                            '\nTo cancel the job:\t\t'
-                            f'{backend_utils.BOLD}sky spot cancel {job_id}'
-                            f'{backend_utils.RESET_BOLD}'
-                            '\nTo stream the logs:\t\t'
-                            f'{backend_utils.BOLD}sky spot logs {job_id}'
-                            f'{backend_utils.RESET_BOLD}'
-                            f'\nTo stream controller logs:\t'
-                            f'{backend_utils.BOLD}sky logs {name} {job_id}'
-                            f'{backend_utils.RESET_BOLD}'
-                            '\nTo view all spot jobs:\t\t'
-                            f'{backend_utils.BOLD}sky spot queue'
-                            f'{backend_utils.RESET_BOLD}')
+                logger.info(
+                    f'{fore.CYAN}Spot Job ID: '
+                    f'{style.BRIGHT}{job_id}{style.RESET_ALL}'
+                    '\nTo cancel the job:\t\t'
+                    f'{backend_utils.BOLD}sky spot cancel {job_id}'
+                    f'{backend_utils.RESET_BOLD}'
+                    '\nTo stream job logs:\t\t'
+                    f'{backend_utils.BOLD}sky spot logs {job_id}'
+                    f'{backend_utils.RESET_BOLD}'
+                    f'\nTo stream controller logs:\t'
+                    f'{backend_utils.BOLD}sky spot logs --controller {job_id}'
+                    f'{backend_utils.RESET_BOLD}'
+                    '\nTo view all spot jobs:\t\t'
+                    f'{backend_utils.BOLD}sky spot queue'
+                    f'{backend_utils.RESET_BOLD}'
+                    '\nTo view the spot job dashboard:\t'
+                    f'{backend_utils.BOLD}sky spot dashboard'
+                    f'{backend_utils.RESET_BOLD}')
             else:
                 logger.info(f'{fore.CYAN}Job ID: '
                             f'{style.BRIGHT}{job_id}{style.RESET_ALL}'
                             '\nTo cancel the job:\t'
                             f'{backend_utils.BOLD}sky cancel {name} {job_id}'
                             f'{backend_utils.RESET_BOLD}'
-                            '\nTo stream the logs:\t'
+                            '\nTo stream job logs:\t'
                             f'{backend_utils.BOLD}sky logs {name} {job_id}'
                             f'{backend_utils.RESET_BOLD}'
                             '\nTo view the job queue:\t'
                             f'{backend_utils.BOLD}sky queue {name}'
                             f'{backend_utils.RESET_BOLD}')
 
     def _setup_and_create_job_cmd_on_local_head(
         self,
-        handle: ResourceHandle,
+        handle: CloudVmRayResourceHandle,
         ray_command: str,
         ray_job_id: str,
     ):
         """Generates and prepares job submission code for local clusters."""
         ssh_credentials = backend_utils.ssh_credential_from_yaml(
             handle.cluster_yaml)
         ssh_user = ssh_credentials['ssh_user']
         runner = command_runner.SSHCommandRunner(handle.head_ip,
                                                  **ssh_credentials)
         remote_log_dir = self.log_dir
         with tempfile.NamedTemporaryFile('w', prefix='sky_local_app_') as fp:
             fp.write(ray_command)
             fp.flush()
             run_file = os.path.basename(fp.name)
-            remote_run_file = f'/tmp/{run_file}'
+            remote_run_file = f'/tmp/sky_local/{run_file}'
+            # Ensures remote_run_file directory is created.
+            runner.run(f'mkdir -p {os.path.dirname(remote_run_file)}',
+                       stream_logs=False)
             # We choose to sync code + exec, so that Ray job submission API will
             # work for the multitenant case.
             runner.rsync(source=fp.name,
                          target=remote_run_file,
                          up=True,
                          stream_logs=False)
         runner.run(f'mkdir -p {remote_log_dir}; chmod a+rwx {remote_run_file}',
                    stream_logs=False)
         switch_user_cmd = job_lib.make_job_command_with_user_switching(
             ssh_user, remote_run_file)
         switch_user_cmd = ' '.join(switch_user_cmd)
         job_submit_cmd = (
             'ray job submit '
-            f'--address=http://127.0.0.1:8265 --submission-id {ray_job_id} '
+            '--address='
+            f'http://127.0.0.1:{constants.SKY_REMOTE_RAY_DASHBOARD_PORT} '
+            f'--submission-id {ray_job_id} '
             f'--no-wait -- {switch_user_cmd}')
         return job_submit_cmd
 
-    def _add_job(self, handle: ResourceHandle, job_name: str,
-                 resources_str: str) -> int:
+    def _add_job(self, handle: CloudVmRayResourceHandle,
+                 job_name: Optional[str], resources_str: str) -> int:
         username = getpass.getuser()
         code = job_lib.JobLibCodeGen.add_job(job_name, username,
                                              self.run_timestamp, resources_str)
         returncode, job_id_str, stderr = self.run_on_head(handle,
                                                           code,
                                                           stream_logs=False,
                                                           require_outputs=True,
@@ -2294,15 +3001,15 @@
             logger.error(stderr)
             raise ValueError(f'Failed to parse job id: {job_id_str}; '
                              f'Returncode: {returncode}') from e
         return job_id
 
     def _execute(
         self,
-        handle: ResourceHandle,
+        handle: CloudVmRayResourceHandle,
         task: task_lib.Task,
         detach_run: bool,
     ) -> None:
         if task.run is None:
             logger.info('Run commands not specified or empty.')
             return
         # Check the task resources vs the cluster resources. Since `sky exec`
@@ -2316,16 +3023,16 @@
         # Case: task_lib.Task(run, num_nodes=N) or TPU VM Pods
         if task.num_nodes > 1 or is_tpu_vm_pod:
             self._execute_task_n_nodes(handle, task, job_id, detach_run)
         else:
             # Case: task_lib.Task(run, num_nodes=1)
             self._execute_task_one_node(handle, task, job_id, detach_run)
 
-    def _post_execute(self, handle: ResourceHandle, down: bool) -> None:
-        colorama.init()
+    def _post_execute(self, handle: CloudVmRayResourceHandle,
+                      down: bool) -> None:
         fore = colorama.Fore
         style = colorama.Style
         name = handle.cluster_name
         if name == spot_lib.SPOT_CONTROLLER_NAME or down:
             return
         stop_str = ('\nTo stop the cluster:'
                     f'\t{backend_utils.BOLD}sky stop {name}'
@@ -2351,72 +3058,117 @@
         storage_mounts = task.storage_mounts
         if storage_mounts is not None:
             for _, storage in storage_mounts.items():
                 if not storage.persistent:
                     storage.delete()
 
     def _teardown(self,
-                  handle: ResourceHandle,
+                  handle: CloudVmRayResourceHandle,
                   terminate: bool,
                   purge: bool = False):
+        """Tear down/ Stop the cluster.
+
+        Args:
+            handle: The handle to the cluster.
+            terminate: Terminate or stop the cluster.
+            purge: Purge the cluster record from the cluster table, even if
+                the teardown fails.
+        Raises:
+            exceptions.ClusterOwnerIdentityMismatchError: If the cluster is
+                owned by another user.
+            exceptions.CloudUserIdentityError: if we fail to get the current
+                user identity.
+            RuntimeError: If the cluster fails to be terminated/stopped.
+        """
         cluster_name = handle.cluster_name
+        # Check if the cluster is owned by the current user. Raise
+        # exceptions.ClusterOwnerIdentityMismatchError
+        yellow = colorama.Fore.YELLOW
+        reset = colorama.Style.RESET_ALL
+        is_identity_mismatch_and_purge = False
+        try:
+            backend_utils.check_owner_identity(cluster_name)
+        except exceptions.ClusterOwnerIdentityMismatchError as e:
+            if purge:
+                logger.error(e)
+                verbed = 'terminated' if terminate else 'stopped'
+                logger.warning(
+                    f'{yellow}Purge (-p/--purge) is set, ignoring the '
+                    f'identity mismatch error and removing '
+                    f'the cluser record from cluster table.{reset}\n{yellow}It '
+                    'is the user\'s responsibility to ensure that this '
+                    f'cluster is actually {verbed} on the cloud.{reset}')
+                is_identity_mismatch_and_purge = True
+            else:
+                raise
+
         lock_path = os.path.expanduser(
             backend_utils.CLUSTER_STATUS_LOCK_PATH.format(cluster_name))
 
         try:
             # TODO(mraheja): remove pylint disabling when filelock
             # version updated
             # pylint: disable=abstract-class-instantiated
             with filelock.FileLock(
                     lock_path,
                     backend_utils.CLUSTER_STATUS_LOCK_TIMEOUT_SECONDS):
-                success = self.teardown_no_lock(handle, terminate, purge)
-            if success and terminate:
-                os.remove(lock_path)
+                self.teardown_no_lock(
+                    handle,
+                    terminate,
+                    purge,
+                    # When --purge is set and we already see an ID mismatch
+                    # error, we skip the refresh codepath. This is because
+                    # refresh checks current user identity can throw
+                    # ClusterOwnerIdentityMismatchError. The argument/flag
+                    # `purge` should bypass such ID mismatch errors.
+                    refresh_cluster_status=not is_identity_mismatch_and_purge)
+            if terminate:
+                common_utils.remove_file_if_exists(lock_path)
         except filelock.Timeout as e:
             raise RuntimeError(
                 f'Cluster {cluster_name!r} is locked by {lock_path}. '
                 'Check to see if it is still being launched.') from e
 
     # --- CloudVMRayBackend Specific APIs ---
 
     def get_job_status(
         self,
-        handle: ResourceHandle,
-        job_ids: Optional[List[str]] = None,
+        handle: CloudVmRayResourceHandle,
+        job_ids: Optional[List[int]] = None,
         stream_logs: bool = True
-    ) -> Dict[Optional[str], Optional[job_lib.JobStatus]]:
+    ) -> Dict[Optional[int], Optional[job_lib.JobStatus]]:
         code = job_lib.JobLibCodeGen.get_job_status(job_ids)
         returncode, stdout, stderr = self.run_on_head(handle,
                                                       code,
                                                       stream_logs=stream_logs,
                                                       require_outputs=True,
                                                       separate_stderr=True)
         subprocess_utils.handle_returncode(returncode, code,
                                            'Failed to get job status.', stderr)
         statuses = job_lib.load_statuses_payload(stdout)
         return statuses
 
-    def cancel_jobs(self, handle: ResourceHandle, jobs: Optional[List[int]]):
+    def cancel_jobs(self, handle: CloudVmRayResourceHandle,
+                    jobs: Optional[List[int]]):
         job_owner = onprem_utils.get_job_owner(handle.cluster_yaml)
         code = job_lib.JobLibCodeGen.cancel_jobs(job_owner, jobs)
 
         # All error messages should have been redirected to stdout.
         returncode, stdout, _ = self.run_on_head(handle,
                                                  code,
                                                  stream_logs=False,
                                                  require_outputs=True)
         subprocess_utils.handle_returncode(
             returncode, code,
             f'Failed to cancel jobs on cluster {handle.cluster_name}.', stdout)
 
     def sync_down_logs(
             self,
-            handle: ResourceHandle,
-            job_ids: Optional[str],
+            handle: CloudVmRayResourceHandle,
+            job_ids: Optional[List[str]],
             local_dir: str = constants.SKY_LOGS_DIRECTORY) -> Dict[str, str]:
         """Sync down logs for the given job_ids.
 
         Returns:
             A dictionary mapping job_id to log path.
         """
         code = job_lib.JobLibCodeGen.get_run_timestamp_with_globbing(job_ids)
@@ -2429,15 +3181,15 @@
         subprocess_utils.handle_returncode(returncode, code,
                                            'Failed to sync logs.', stderr)
         run_timestamps = common_utils.decode_payload(run_timestamps)
         if not run_timestamps:
             logger.info(f'{colorama.Fore.YELLOW}'
                         'No matching log directories found'
                         f'{colorama.Style.RESET_ALL}')
-            return
+            return {}
 
         job_ids = list(run_timestamps.keys())
         run_timestamps = list(run_timestamps.values())
         remote_log_dirs = [
             os.path.join(constants.SKY_LOGS_DIRECTORY, run_timestamp)
             for run_timestamp in run_timestamps
         ]
@@ -2448,17 +3200,16 @@
 
         style = colorama.Style
         fore = colorama.Fore
         for job_id, log_dir in zip(job_ids, local_log_dirs):
             logger.info(f'{fore.CYAN}Job {job_id} logs: {log_dir}'
                         f'{style.RESET_ALL}')
 
-        ip_list = backend_utils.get_node_ips(handle.cluster_yaml,
-                                             handle.launched_nodes,
-                                             handle=handle)
+        ip_list = handle.external_ips()
+        assert ip_list is not None, 'external_ips is not cached in handle'
         ssh_credentials = backend_utils.ssh_credential_from_yaml(
             handle.cluster_yaml)
         runners = command_runner.SSHCommandRunner.make_runner_list(
             ip_list, **ssh_credentials)
 
         def _rsync_down(args) -> None:
             """Rsync down logs from remote nodes.
@@ -2486,15 +3237,15 @@
         parallel_args = [[runner, *item]
                          for item in zip(local_log_dirs, remote_log_dirs)
                          for runner in runners]
         subprocess_utils.run_in_parallel(_rsync_down, parallel_args)
         return dict(zip(job_ids, local_log_dirs))
 
     def tail_logs(self,
-                  handle: ResourceHandle,
+                  handle: CloudVmRayResourceHandle,
                   job_id: Optional[int],
                   spot_job_id: Optional[int] = None,
                   follow: bool = True) -> int:
         job_owner = onprem_utils.get_job_owner(handle.cluster_yaml)
         code = job_lib.JobLibCodeGen.tail_logs(job_owner,
                                                job_id,
                                                spot_job_id=spot_job_id,
@@ -2522,15 +3273,15 @@
                 stdin=subprocess.DEVNULL,
             )
         except SystemExit as e:
             returncode = e.code
         return returncode
 
     def tail_spot_logs(self,
-                       handle: ResourceHandle,
+                       handle: CloudVmRayResourceHandle,
                        job_id: Optional[int] = None,
                        job_name: Optional[str] = None,
                        follow: bool = True) -> None:
         # if job_name is not None, job_id should be None
         assert job_name is None or job_id is None, (job_name, job_id)
         if job_name is not None:
             code = spot_lib.SpotCodeGen.stream_logs_by_name(job_name, follow)
@@ -2549,92 +3300,216 @@
             stream_logs=True,
             process_stream=False,
             ssh_mode=command_runner.SshMode.INTERACTIVE,
             stdin=subprocess.DEVNULL,
         )
 
     def teardown_no_lock(self,
-                         handle: ResourceHandle,
+                         handle: CloudVmRayResourceHandle,
                          terminate: bool,
                          purge: bool = False,
-                         post_teardown_cleanup: bool = True) -> bool:
+                         post_teardown_cleanup: bool = True,
+                         refresh_cluster_status: bool = True) -> None:
         """Teardown the cluster without acquiring the cluster status lock.
 
         NOTE: This method should not be called without holding the cluster
         status lock already.
+
+        refresh_cluster_status is only used internally in the status refresh
+        process, and should not be set to False in other cases.
+
+        Raises:
+            RuntimeError: If the cluster fails to be terminated/stopped.
         """
+        if refresh_cluster_status:
+            prev_cluster_status, _ = (
+                backend_utils.refresh_cluster_status_handle(
+                    handle.cluster_name, acquire_per_cluster_status_lock=False))
+        else:
+            record = global_user_state.get_cluster_from_name(
+                handle.cluster_name)
+            prev_cluster_status = record[
+                'status'] if record is not None else None
+        if prev_cluster_status is None:
+            # When the cluster is not in the cluster table, we guarantee that
+            # all related resources / cache / config are cleaned up, i.e. it
+            # is safe to skip and return True.
+            ux_utils.console_newline()
+            logger.warning(
+                f'Cluster {handle.cluster_name!r} is already terminated. '
+                'Skipped.')
+            return
         log_path = os.path.join(os.path.expanduser(self.log_dir),
                                 'teardown.log')
         log_abs_path = os.path.abspath(log_path)
         cloud = handle.launched_resources.cloud
         config = common_utils.read_yaml(handle.cluster_yaml)
-        prev_status, _ = backend_utils.refresh_cluster_status_handle(
-            handle.cluster_name, acquire_per_cluster_status_lock=False)
         cluster_name = handle.cluster_name
         use_tpu_vm = config['provider'].get('_has_tpus', False)
+
+        # Avoid possibly unbound warnings. Code below must overwrite these vars:
+        returncode = 0
+        stdout = ''
+        stderr = ''
+
+        # Use the new provisioner for AWS.
+        if isinstance(cloud, clouds.AWS):
+            region = config['provider']['region']
+            # Stop the ray autoscaler first to avoid the head node trying to
+            # re-launch the worker nodes, during the termination of the
+            # cluster.
+            try:
+                # We do not check the return code, since Ray returns
+                # non-zero return code when calling Ray stop,
+                # even when the command was executed successfully.
+                self.run_on_head(handle, 'ray stop --force')
+            except RuntimeError:
+                # This error is expected if the previous cluster IP is
+                # failed to be found,
+                # i.e., the cluster is already stopped/terminated.
+                if prev_cluster_status == status_lib.ClusterStatus.UP:
+                    logger.warning(
+                        'Failed to take down Ray autoscaler on the head node. '
+                        'It might be because the cluster\'s head node has '
+                        'already been terminated. It is fine to skip this.')
+            try:
+                if terminate:
+                    provision_api.terminate_instances(repr(cloud), region,
+                                                      cluster_name)
+                else:
+                    provision_api.stop_instances(repr(cloud), region,
+                                                 cluster_name)
+            except Exception as e:  # pylint: disable=broad-except
+                if purge:
+                    logger.warning(
+                        _TEARDOWN_PURGE_WARNING.format(
+                            reason='stopping/terminating cluster nodes',
+                            details=common_utils.format_exception(
+                                e, use_bracket=True)))
+                else:
+                    raise
+
+            if post_teardown_cleanup:
+                self.post_teardown_cleanup(handle, terminate, purge)
+            return
+
         if terminate and isinstance(cloud, clouds.Azure):
             # Here we handle termination of Azure by ourselves instead of Ray
             # autoscaler.
             resource_group = config['provider']['resource_group']
             terminate_cmd = f'az group delete -y --name {resource_group}'
-            with backend_utils.safe_console_status(f'[bold cyan]Terminating '
-                                                   f'[green]{cluster_name}'):
+            with log_utils.safe_rich_status(f'[bold cyan]Terminating '
+                                            f'[green]{cluster_name}'):
                 returncode, stdout, stderr = log_lib.run_with_log(
                     terminate_cmd,
                     log_abs_path,
                     shell=True,
                     stream_logs=False,
                     require_outputs=True)
+
+        elif (isinstance(cloud, clouds.IBM) and terminate and
+              prev_cluster_status == status_lib.ClusterStatus.STOPPED):
+            # pylint: disable= W0622 W0703 C0415
+            from sky.adaptors import ibm
+            from sky.skylet.providers.ibm.vpc_provider import IBMVPCProvider
+
+            config_provider = common_utils.read_yaml(
+                handle.cluster_yaml)['provider']
+            region = config_provider['region']
+            cluster_name = handle.cluster_name
+            search_client = ibm.search_client()
+            vpc_found = False
+            # pylint: disable=unsubscriptable-object
+            vpcs_filtered_by_tags_and_region = search_client.search(
+                query=f'type:vpc AND tags:{cluster_name} AND region:{region}',
+                fields=['tags', 'region', 'type'],
+                limit=1000).get_result()['items']
+            vpc_id = None
+            try:
+                # pylint: disable=line-too-long
+                vpc_id = vpcs_filtered_by_tags_and_region[0]['crn'].rsplit(
+                    ':', 1)[-1]
+                vpc_found = True
+            except Exception:
+                logger.critical('failed to locate vpc for ibm cloud')
+                returncode = -1
+
+            if vpc_found:
+                # # pylint: disable=line-too-long E1136
+                # Delete VPC and it's associated resources
+                vpc_provider = IBMVPCProvider(
+                    config_provider['resource_group_id'], region, cluster_name)
+                vpc_provider.delete_vpc(vpc_id, region)
+                # successfully removed cluster as no exception was raised
+                returncode = 0
+
+        elif terminate and isinstance(cloud, clouds.SCP):
+            config['provider']['cache_stopped_nodes'] = not terminate
+            provider = SCPNodeProvider(config['provider'], handle.cluster_name)
+            try:
+                if not os.path.exists(provider.metadata.path):
+                    raise SCPError('SKYPILOT_ERROR_NO_NODES_LAUNCHED: '
+                                   'Metadata file does not exist.')
+
+                with open(provider.metadata.path, 'r') as f:
+                    metadata = json.load(f)
+                    node_id = next(iter(metadata.values())).get(
+                        'creation', {}).get('virtualServerId', None)
+                    provider.terminate_node(node_id)
+                returncode = 0
+            except SCPError as e:
+                returncode = 1
+                stdout = ''
+                stderr = str(e)
+
+        # Apr, 2023 by Hysun(hysun.he@oracle.com): Added support for OCI
+        # May, 2023 by Hysun: Allow terminate INIT cluster which may have
+        # some instances provisioning in background but not completed.
+        elif (isinstance(cloud, clouds.OCI) and terminate and
+              prev_cluster_status in (status_lib.ClusterStatus.STOPPED,
+                                      status_lib.ClusterStatus.INIT)):
+            region = config['provider']['region']
+
+            # pylint: disable=import-outside-toplevel
+            from sky.skylet.providers.oci.query_helper import oci_query_helper
+            from ray.autoscaler.tags import TAG_RAY_CLUSTER_NAME
+
+            # 0: All terminated successfully, failed count otherwise
+            returncode = oci_query_helper.terminate_instances_by_tags(
+                {TAG_RAY_CLUSTER_NAME: cluster_name}, region)
+
+            # To avoid undefined local variables error.
+            stdout = stderr = ''
         elif (terminate and
-              (prev_status == global_user_state.ClusterStatus.STOPPED or
+              (prev_cluster_status == status_lib.ClusterStatus.STOPPED or
                use_tpu_vm)):
             # For TPU VMs, gcloud CLI is used for VM termination.
-            if isinstance(cloud, clouds.AWS):
-                # TODO(zhwu): Room for optimization. We can move these cloud
-                # specific handling to the cloud class.
-                # The stopped instance on AWS will not be correctly terminated
-                # due to ray's bug.
-                region = config['provider']['region']
-                query_cmd = (
-                    f'aws ec2 describe-instances --region {region} --filters '
-                    f'Name=tag:ray-cluster-name,Values={handle.cluster_name} '
-                    f'--query Reservations[].Instances[].InstanceId '
-                    '--output text')
-                terminate_cmd = (
-                    f'aws ec2 terminate-instances --region {region} '
-                    f'--instance-ids $({query_cmd})')
-            elif isinstance(cloud, clouds.GCP):
+            if isinstance(cloud, clouds.GCP):
                 zone = config['provider']['availability_zone']
                 # TODO(wei-lin): refactor by calling functions of node provider
                 # that uses Python API rather than CLI
                 if use_tpu_vm:
-                    # check if gcloud includes TPU VM API
-                    backend_utils.check_gcp_cli_include_tpu_vm()
-
-                    query_cmd = (
-                        f'gcloud compute tpus tpu-vm list --filter='
-                        f'\\(labels.ray-cluster-name={cluster_name}\\) '
-                        f'--zone={zone} --format=value\\(name\\)')
-                    terminate_cmd = (
-                        f'gcloud compute tpus tpu-vm delete --zone={zone}'
-                        f' --quiet $({query_cmd})')
+                    terminate_cmd = tpu_utils.terminate_tpu_vm_cluster_cmd(
+                        cluster_name, zone, log_abs_path)
                 else:
-                    query_cmd = (
-                        f'gcloud compute instances list --filter='
-                        f'\\(labels.ray-cluster-name={cluster_name}\\) '
-                        f'--zones={zone} --format=value\\(name\\)')
+                    query_cmd = (f'gcloud compute instances list --filter='
+                                 f'"(labels.ray-cluster-name={cluster_name})" '
+                                 f'--zones={zone} --format=value\\(name\\)')
+                    # If there are no instances, exit with 0 rather than causing
+                    # the delete command to fail.
                     terminate_cmd = (
-                        f'gcloud compute instances delete --zone={zone}'
-                        f' --quiet $({query_cmd})')
+                        f'VMS=$({query_cmd}) && [ -n "$VMS" ] && '
+                        f'gcloud compute instances delete --zone={zone} --quiet'
+                        ' $VMS || echo "No instances to delete."')
             else:
                 with ux_utils.print_exception_no_traceback():
                     raise ValueError(f'Unsupported cloud {cloud} for stopped '
                                      f'cluster {cluster_name!r}.')
-            with backend_utils.safe_console_status(f'[bold cyan]Terminating '
-                                                   f'[green]{cluster_name}'):
+            with log_utils.safe_rich_status(f'[bold cyan]Terminating '
+                                            f'[green]{cluster_name}'):
                 returncode, stdout, stderr = log_lib.run_with_log(
                     terminate_cmd,
                     log_abs_path,
                     shell=True,
                     stream_logs=False,
                     require_outputs=True)
         else:
@@ -2643,17 +3518,16 @@
                                              prefix='sky_',
                                              delete=False,
                                              suffix='.yml') as f:
                 common_utils.dump_yaml(f.name, config)
                 f.flush()
 
                 teardown_verb = 'Terminating' if terminate else 'Stopping'
-                with backend_utils.safe_console_status(
-                        f'[bold cyan]{teardown_verb} '
-                        f'[green]{cluster_name}'):
+                with log_utils.safe_rich_status(f'[bold cyan]{teardown_verb} '
+                                                f'[green]{cluster_name}'):
                     # FIXME(zongheng): support retries. This call can fail for
                     # example due to GCP returning list requests per limit
                     # exceeded.
                     returncode, stdout, stderr = log_lib.run_with_log(
                         ['ray', 'down', '-y', f.name],
                         log_abs_path,
                         stream_logs=False,
@@ -2663,100 +3537,133 @@
                         # multiprocessing are used.
                         # Refer to: https://github.com/ray-project/ray/blob/d462172be7c5779abf37609aed08af112a533e1e/python/ray/autoscaler/_private/subprocess_output_util.py#L264 # pylint: disable=line-too-long
                         stdin=subprocess.DEVNULL)
         if returncode != 0:
             if purge:
                 logger.warning(
                     _TEARDOWN_PURGE_WARNING.format(
-                        reason='stopping/terminating cluster nodes'))
-            # This error returns when we call "gcloud delete" with an empty VM
-            # list where no instance exists. Safe to ignore it and do cleanup
-            # locally.
-            # TODO(wei-lin): refactor error handling mechanism.
-            elif 'TPU must be specified.' not in stderr:
-                logger.error(
+                        reason='stopping/terminating cluster nodes',
+                        details=stderr))
+            # 'TPU must be specified.': This error returns when we call "gcloud
+            #   delete" with an empty VM list where no instance exists. Safe to
+            #   ignore it and do cleanup locally. TODO(wei-lin): refactor error
+            #   handling mechanism.
+            #
+            # 'SKYPILOT_ERROR_NO_NODES_LAUNCHED': this indicates nodes are
+            #   never launched and the errors are related to pre-launch
+            #   configurations (such as VPC not found). So it's safe & good UX
+            #   to not print a failure message.
+            #
+            # '(ResourceGroupNotFound)': this indicates the resource group on
+            #   Azure is not found. That means the cluster is already deleted
+            #   on the cloud. So it's safe & good UX to not print a failure
+            #   message.
+            elif ('TPU must be specified.' not in stderr and
+                  'SKYPILOT_ERROR_NO_NODES_LAUNCHED: ' not in stderr and
+                  '(ResourceGroupNotFound)' not in stderr):
+                raise RuntimeError(
                     _TEARDOWN_FAILURE_MESSAGE.format(
                         extra_reason='',
                         cluster_name=handle.cluster_name,
                         stdout=stdout,
                         stderr=stderr))
-                return False
 
+        # No need to clean up if the cluster is already terminated
+        # (i.e., prev_status is None), as the cleanup has already been done
+        # if the cluster is removed from the status table.
         if post_teardown_cleanup:
-            return self.post_teardown_cleanup(handle, terminate, purge)
-        else:
-            return True
+            self.post_teardown_cleanup(handle, terminate, purge)
 
     def post_teardown_cleanup(self,
-                              handle: ResourceHandle,
+                              handle: CloudVmRayResourceHandle,
                               terminate: bool,
-                              purge: bool = False) -> bool:
+                              purge: bool = False) -> None:
         """Cleanup local configs/caches and delete TPUs after teardown.
 
         This method will handle the following cleanup steps:
         * Deleting the TPUs;
         * Removing ssh configs for the cluster;
         * Updating the local state of the cluster;
         * Removing the terminated cluster's scripts and ray yaml files.
+
+        Raises:
+            RuntimeError: If it fails to delete the TPU.
         """
         log_path = os.path.join(os.path.expanduser(self.log_dir),
                                 'teardown.log')
         log_abs_path = os.path.abspath(log_path)
 
         if (handle.tpu_delete_script is not None and
                 os.path.exists(handle.tpu_delete_script)):
-            with backend_utils.safe_console_status(
-                    '[bold cyan]Terminating TPU...'):
+            with log_utils.safe_rich_status('[bold cyan]Terminating TPU...'):
                 tpu_rc, tpu_stdout, tpu_stderr = log_lib.run_with_log(
                     ['bash', handle.tpu_delete_script],
                     log_abs_path,
                     stream_logs=False,
                     require_outputs=True)
             if tpu_rc != 0:
                 if _TPU_NOT_FOUND_ERROR in tpu_stderr:
                     logger.info('TPU not found. '
                                 'It should have been deleted already.')
                 elif purge:
                     logger.warning(
                         _TEARDOWN_PURGE_WARNING.format(
-                            reason='stopping/terminating TPU'))
+                            reason='stopping/terminating TPU',
+                            details=tpu_stderr))
                 else:
-                    logger.error(
+                    raise RuntimeError(
                         _TEARDOWN_FAILURE_MESSAGE.format(
                             extra_reason='It is caused by TPU failure.',
                             cluster_name=handle.cluster_name,
                             stdout=tpu_stdout,
                             stderr=tpu_stderr))
-                    return False
+        if (terminate and handle.launched_resources.is_image_managed is True):
+            # Delete the image when terminating a "cloned" cluster, i.e.,
+            # whose image is created by SkyPilot (--clone-disk-from)
+            logger.debug(f'Deleting image {handle.launched_resources.image_id}')
+            cluster_resources = handle.launched_resources
+            cluster_cloud = cluster_resources.cloud
+            image_dict = cluster_resources.image_id
+            assert cluster_cloud is not None, cluster_resources
+            assert image_dict is not None and len(image_dict) == 1
+            image_id = list(image_dict.values())[0]
+            try:
+                cluster_cloud.delete_image(image_id,
+                                           handle.launched_resources.region)
+            except exceptions.CommandError as e:
+                logger.warning(
+                    f'Failed to delete cloned image {image_id}. Please '
+                    'remove it manually to avoid image leakage. Details: '
+                    f'{common_utils.format_exception(e, use_bracket=True)}')
 
         # The cluster file must exist because the cluster_yaml will only
         # be removed after the cluster entry in the database is removed.
         config = common_utils.read_yaml(handle.cluster_yaml)
         auth_config = config['auth']
         backend_utils.SSHConfigHelper.remove_cluster(handle.cluster_name,
                                                      handle.head_ip,
                                                      auth_config)
+
         global_user_state.remove_cluster(handle.cluster_name,
                                          terminate=terminate)
 
         if terminate:
             # Clean up TPU creation/deletion scripts
             if handle.tpu_delete_script is not None:
                 assert handle.tpu_create_script is not None
-                os.remove(handle.tpu_create_script)
-                os.remove(handle.tpu_delete_script)
+                common_utils.remove_file_if_exists(handle.tpu_create_script)
+                common_utils.remove_file_if_exists(handle.tpu_delete_script)
 
             # Clean up generated config
             # No try-except is needed since Ray will fail to teardown the
             # cluster if the cluster_yaml is missing.
-            os.remove(handle.cluster_yaml)
-        return True
+            common_utils.remove_file_if_exists(handle.cluster_yaml)
 
     def set_autostop(self,
-                     handle: ResourceHandle,
+                     handle: CloudVmRayResourceHandle,
                      idle_minutes_to_autostop: Optional[int],
                      down: bool = False,
                      stream_logs: bool = True) -> None:
         if idle_minutes_to_autostop is not None:
             code = autostop_lib.AutostopCodeGen.set_autostop(
                 idle_minutes_to_autostop, self.NAME, down)
             returncode, _, stderr = self.run_on_head(handle,
@@ -2767,38 +3674,81 @@
                                                code,
                                                'Failed to set autostop',
                                                stderr=stderr,
                                                stream_logs=stream_logs)
             global_user_state.set_cluster_autostop_value(
                 handle.cluster_name, idle_minutes_to_autostop, down)
 
+    def is_definitely_autostopping(self,
+                                   handle: CloudVmRayResourceHandle,
+                                   stream_logs: bool = True) -> bool:
+        """Check if the cluster is autostopping.
+
+        Returns:
+            True if the cluster is definitely autostopping. It is possible
+            that the cluster is still autostopping when False is returned,
+            due to errors like transient network issues.
+        """
+        if handle.head_ip is None:
+            # The head node of the cluster is not UP or in an abnormal state.
+            # We cannot check if the cluster is autostopping.
+            return False
+        code = autostop_lib.AutostopCodeGen.is_autostopping()
+        returncode, stdout, stderr = self.run_on_head(handle,
+                                                      code,
+                                                      require_outputs=True,
+                                                      stream_logs=stream_logs)
+
+        if returncode == 0:
+            return common_utils.decode_payload(stdout)
+        logger.debug(f'Failed to check if cluster is autostopping: {stderr}')
+        return False
+
     # TODO(zhwu): Refactor this to a CommandRunner class, so different backends
     # can support its own command runner.
     @timeline.event
     def run_on_head(
         self,
-        handle: ResourceHandle,
+        handle: CloudVmRayResourceHandle,
         cmd: str,
         *,
-        port_forward: Optional[List[str]] = None,
+        port_forward: Optional[List[int]] = None,
         log_path: str = '/dev/null',
-        process_stream: bool = True,
         stream_logs: bool = False,
-        use_cached_head_ip: bool = True,
         ssh_mode: command_runner.SshMode = command_runner.SshMode.
         NON_INTERACTIVE,
         under_remote_workdir: bool = False,
         require_outputs: bool = False,
         separate_stderr: bool = False,
+        process_stream: bool = True,
         **kwargs,
     ) -> Union[int, Tuple[int, str, str]]:
-        """Runs 'cmd' on the cluster's head node."""
-        max_attempts = 1 if use_cached_head_ip else _HEAD_IP_MAX_ATTEMPTS
-        head_ip = backend_utils.get_head_ip(handle, use_cached_head_ip,
-                                            max_attempts)
+        """Runs 'cmd' on the cluster's head node.
+
+        Args:
+            handle: The ResourceHandle to the cluster.
+            cmd: The command to run.
+
+            Advanced options:
+
+            port_forward: A list of ports to forward.
+            log_path: The path to the log file.
+            stream_logs: Whether to stream the logs to stdout/stderr.
+            ssh_mode: The mode to use for ssh.
+                See command_runner.SSHCommandRunner.SSHMode for more details.
+            under_remote_workdir: Whether to run the command under the remote
+                workdir ~/sky_workdir.
+            require_outputs: Whether to return the stdout and stderr of the
+                command.
+            separate_stderr: Whether to separate stderr from stdout.
+            process_stream: Whether to post-process the stdout/stderr of the
+                command, such as replacing or skipping lines on the fly. If
+                enabled, lines are printed only when '\r' or '\n' is found.
+        """
+        head_ip = backend_utils.get_head_ip(handle, _FETCH_IP_MAX_ATTEMPTS)
         ssh_credentials = backend_utils.ssh_credential_from_yaml(
             handle.cluster_yaml)
         runner = command_runner.SSHCommandRunner(head_ip, **ssh_credentials)
         if under_remote_workdir:
             cmd = f'cd {SKY_REMOTE_WORKDIR} && {cmd}'
 
         return runner.run(
@@ -2813,34 +3763,89 @@
             **kwargs,
         )
 
     # --- Utilities ---
 
     @timeline.event
     def _check_existing_cluster(
-            self, task: task_lib.Task, to_provision: resources_lib.Resources,
+            self, task: task_lib.Task,
+            to_provision: Optional[resources_lib.Resources],
             cluster_name: str) -> RetryingVmProvisioner.ToProvisionConfig:
-        handle = global_user_state.get_handle_from_cluster_name(cluster_name)
-        if handle is not None:
+        """Checks if the cluster exists and returns the provision config.
+
+        Raises:
+            exceptions.ResourcesMismatchError: If the resources in the task
+                does not match the existing cluster.
+            exceptions.InvalidClusterNameError: If the cluster name is invalid.
+            # TODO(zhwu): complete the list of exceptions.
+        """
+        record = global_user_state.get_cluster_from_name(cluster_name)
+        handle_before_refresh = None if record is None else record['handle']
+        status_before_refresh = None if record is None else record['status']
+
+        prev_cluster_status, handle = (
+            backend_utils.refresh_cluster_status_handle(
+                cluster_name,
+                # We force refresh for the init status to determine the actual
+                # state of a previous cluster in INIT state.
+                #
+                # This is important for the case, where an existing cluster is
+                # transitioned into INIT state due to key interruption during
+                # launching, with the following steps:
+                # (1) launch, after answering prompt immediately ctrl-c;
+                # (2) launch again.
+                # If we don't refresh the state of the cluster and reset it back
+                # to STOPPED, our failover logic will consider it as an abnormal
+                # cluster after hitting resources capacity limit on the cloud,
+                # and will start failover. This is not desired, because the user
+                # may want to keep the data on the disk of that cluster.
+                force_refresh_statuses={status_lib.ClusterStatus.INIT},
+                acquire_per_cluster_status_lock=False,
+            ))
+        if prev_cluster_status is not None:
+            assert handle is not None
             # Cluster already exists.
             self.check_resources_fit_cluster(handle, task)
             # Use the existing cluster.
             assert handle.launched_resources is not None, (cluster_name, handle)
             return RetryingVmProvisioner.ToProvisionConfig(
                 cluster_name,
                 handle.launched_resources,
                 handle.launched_nodes,
-                cluster_exists=True)
+                prev_cluster_status=prev_cluster_status)
         usage_lib.messages.usage.set_new_cluster()
         assert len(task.resources) == 1, task.resources
-        resources = list(task.resources)[0]
-        task_cloud = resources.cloud
         # Use the task_cloud, because the cloud in `to_provision` can be changed
         # later during the retry.
-        backend_utils.check_cluster_name_is_valid(cluster_name, task_cloud)
+        resources = list(task.resources)[0]
+        task_cloud = (resources.cloud
+                      if resources.cloud is not None else clouds.Cloud)
+        task_cloud.check_cluster_name_is_valid(cluster_name)
+
+        if to_provision is None:
+            # The cluster is recently terminated either by autostop or manually
+            # terminated on the cloud. We should use the previously terminated
+            # resources to provision the cluster.
+            assert isinstance(
+                handle_before_refresh, CloudVmRayResourceHandle), (
+                    f'Trying to launch cluster {cluster_name!r} recently '
+                    'terminated  on the cloud, but the handle is not a '
+                    f'CloudVmRayResourceHandle ({handle_before_refresh}).')
+            status_before_refresh_str = None
+            if status_before_refresh is not None:
+                status_before_refresh_str = status_before_refresh.value
+
+            logger.info(
+                f'The cluster {cluster_name!r} (status: '
+                f'{status_before_refresh_str}) was not found on the cloud: it '
+                'may be autodowned, manually terminated, or its launch never '
+                'succeeded. Provisioning a new cluster by using the same '
+                'resources as its original launch.')
+            to_provision = handle_before_refresh.launched_resources
+            self.check_resources_fit_cluster(handle_before_refresh, task)
 
         cloud = to_provision.cloud
         if isinstance(cloud, clouds.Local):
             # The field ssh_user is specified in the cluster config file.
             ssh_user = onprem_utils.get_local_cluster_config_or_error(
                 cluster_name)['auth']['ssh_user']
             logger.info(f'{colorama.Fore.CYAN}Connecting to local cluster: '
@@ -2853,22 +3858,24 @@
                 f'[{task.num_nodes}x {to_provision}].'
                 f'{colorama.Style.RESET_ALL}\n'
                 'Tip: to reuse an existing cluster, '
                 'specify --cluster (-c). '
                 'Run `sky status` to see existing clusters.')
         return RetryingVmProvisioner.ToProvisionConfig(cluster_name,
                                                        to_provision,
-                                                       task.num_nodes)
+                                                       task.num_nodes,
+                                                       prev_cluster_status=None)
 
-    def _set_tpu_name(self, cluster_config_file: str, num_nodes: int,
+    def _set_tpu_name(self, handle: CloudVmRayResourceHandle,
                       tpu_name: str) -> None:
         """Sets TPU_NAME on all nodes."""
-        ip_list = backend_utils.get_node_ips(cluster_config_file, num_nodes)
+        ip_list = handle.external_ips()
+        assert ip_list is not None, 'external_ips is not cached in handle'
         ssh_credentials = backend_utils.ssh_credential_from_yaml(
-            cluster_config_file)
+            handle.cluster_yaml)
 
         runners = command_runner.SSHCommandRunner.make_runner_list(
             ip_list, **ssh_credentials)
 
         def _setup_tpu_name_on_node(
                 runner: command_runner.SSHCommandRunner) -> None:
             cmd = (f'[[ -z $TPU_NAME ]] && echo "export TPU_NAME={tpu_name}" '
@@ -2877,31 +3884,32 @@
                                     log_path=os.path.join(
                                         self.log_dir, 'tpu_setup.log'))
             subprocess_utils.handle_returncode(
                 returncode, cmd, 'Failed to set TPU_NAME on node.')
 
         subprocess_utils.run_in_parallel(_setup_tpu_name_on_node, runners)
 
-    def _execute_file_mounts(self, handle: ResourceHandle,
+    def _execute_file_mounts(self, handle: CloudVmRayResourceHandle,
                              file_mounts: Dict[Path, Path]):
-        """Executes file mounts - rsyncing local files and
-        copying from remote stores."""
+        """Executes file mounts.
+
+        Rsyncing local files and copying from remote stores.
+        """
         # File mounts handling for remote paths possibly without write access:
         #  (1) in 'file_mounts' sections, add <prefix> to these target paths.
         #  (2) then, create symlinks from '/.../file' to '<prefix>/.../file'.
         if file_mounts is None or not file_mounts:
             return
         symlink_commands = []
         fore = colorama.Fore
         style = colorama.Style
         logger.info(f'{fore.CYAN}Processing file mounts.{style.RESET_ALL}')
         start = time.time()
-        ip_list = backend_utils.get_node_ips(handle.cluster_yaml,
-                                             handle.launched_nodes,
-                                             handle=handle)
+        ip_list = handle.external_ips()
+        assert ip_list is not None, 'external_ips is not cached in handle'
         ssh_credentials = backend_utils.ssh_credential_from_yaml(
             handle.cluster_yaml)
         runners = command_runner.SSHCommandRunner.make_runner_list(
             ip_list, **ssh_credentials)
         log_path = os.path.join(self.log_dir, 'file_mounts.log')
 
         # Check the files and warn
@@ -2918,14 +3926,16 @@
                         '.gitignore to exclude large files, as large sizes '
                         f'will slow down rsync. {style.RESET_ALL}')
                 if os.path.islink(full_src):
                     logger.warning(
                         f'{fore.YELLOW}Source path {src!r} is a symlink. '
                         f'Symlink contents are not uploaded.{style.RESET_ALL}')
 
+        os.makedirs(os.path.expanduser(self.log_dir), exist_ok=True)
+        os.system(f'touch {log_path}')
         tail_cmd = f'tail -n100 -f {log_path}'
         logger.info('To view detailed progress: '
                     f'{style.BRIGHT}{tail_cmd}{style.RESET_ALL}')
 
         for dst, src in file_mounts.items():
             # TODO: room for improvement.  Here there are many moving parts
             # (download gsutil on remote, run gsutil on remote).  Consider
@@ -2971,16 +3981,16 @@
                                                      destination=wrapped_dst)
                 # It is a directory so make sure it exists.
                 mkdir_for_wrapped_dst = f'mkdir -p {wrapped_dst}'
             else:
                 sync = storage.make_sync_file_command(source=src,
                                                       destination=wrapped_dst)
                 # It is a file so make sure *its parent dir* exists.
-                mkdir_for_wrapped_dst = \
-                    f'mkdir -p {os.path.dirname(wrapped_dst)}'
+                mkdir_for_wrapped_dst = (
+                    f'mkdir -p {os.path.dirname(wrapped_dst)}')
 
             download_target_commands = [
                 # Ensure sync can write to wrapped_dst (e.g., '/data/').
                 mkdir_for_wrapped_dst,
                 # Both the wrapped and the symlink dir exist; sync.
                 sync,
             ]
@@ -3007,15 +4017,15 @@
                     'Failed to create symlinks. The target destination '
                     f'may already exist. Log: {log_path}')
 
             subprocess_utils.run_in_parallel(_symlink_node, runners)
         end = time.time()
         logger.debug(f'File mount sync took {end - start} seconds.')
 
-    def _execute_storage_mounts(self, handle: ResourceHandle,
+    def _execute_storage_mounts(self, handle: CloudVmRayResourceHandle,
                                 storage_mounts: Dict[Path,
                                                      storage_lib.Storage]):
         """Executes storage mounts: installing mounting tools and mounting."""
         # Process only mount mode objects here. COPY mode objects have been
         # converted to regular copy file mounts and thus have been handled
         # in the '__execute_file_mounts' method.
         storage_mounts = {
@@ -3037,168 +4047,183 @@
 
         fore = colorama.Fore
         style = colorama.Style
         plural = 's' if len(storage_mounts) > 1 else ''
         logger.info(f'{fore.CYAN}Processing {len(storage_mounts)} '
                     f'storage mount{plural}.{style.RESET_ALL}')
         start = time.time()
-        ip_list = backend_utils.get_node_ips(handle.cluster_yaml,
-                                             handle.launched_nodes,
-                                             handle=handle)
+        ip_list = handle.external_ips()
+        assert ip_list is not None, 'external_ips is not cached in handle'
         ssh_credentials = backend_utils.ssh_credential_from_yaml(
             handle.cluster_yaml)
         runners = command_runner.SSHCommandRunner.make_runner_list(
             ip_list, **ssh_credentials)
         log_path = os.path.join(self.log_dir, 'storage_mounts.log')
 
         for dst, storage_obj in storage_mounts.items():
             if not os.path.isabs(dst) and not dst.startswith('~/'):
                 dst = f'{SKY_REMOTE_WORKDIR}/{dst}'
             # Get the first store and use it to mount
             store = list(storage_obj.stores.values())[0]
             mount_cmd = store.mount_command(dst)
             src_print = (storage_obj.source
                          if storage_obj.source else storage_obj.name)
-            backend_utils.parallel_data_transfer_to_nodes(
-                runners,
-                source=src_print,
-                target=dst,
-                cmd=mount_cmd,
-                run_rsync=False,
-                action_message='Mounting',
-                log_path=log_path,
-            )
+            if isinstance(src_print, list):
+                src_print = ', '.join(src_print)
+            try:
+                backend_utils.parallel_data_transfer_to_nodes(
+                    runners,
+                    source=src_print,
+                    target=dst,
+                    cmd=mount_cmd,
+                    run_rsync=False,
+                    action_message='Mounting',
+                    log_path=log_path,
+                )
+            except exceptions.CommandError as e:
+                if e.returncode == exceptions.MOUNT_PATH_NON_EMPTY_CODE:
+                    mount_path = (f'{colorama.Fore.RED}'
+                                  f'{colorama.Style.BRIGHT}{dst}'
+                                  f'{colorama.Style.RESET_ALL}')
+                    error_msg = (f'Mount path {mount_path} is non-empty.'
+                                 f' {mount_path} may be a standard unix '
+                                 f'path or may contain files from a previous'
+                                 f' task. To fix, change the mount path'
+                                 f' to an empty or non-existent path.')
+                    raise RuntimeError(error_msg) from None
+                else:
+                    # Strip the command (a big heredoc) from the exception
+                    raise exceptions.CommandError(
+                        e.returncode, command='to mount',
+                        error_msg=e.error_msg) from None
+
         end = time.time()
         logger.debug(f'Storage mount sync took {end - start} seconds.')
 
-    def _execute_task_one_node(self, handle: ResourceHandle,
+    def _execute_task_one_node(self, handle: CloudVmRayResourceHandle,
                                task: task_lib.Task, job_id: int,
                                detach_run: bool) -> None:
         # Launch the command as a Ray task.
         log_dir = os.path.join(self.log_dir, 'tasks')
-        log_path = os.path.join(log_dir, 'run.log')
 
         accelerator_dict = backend_utils.get_task_demands_dict(task)
+        internal_ips = handle.internal_ips()
+        assert internal_ips is not None, 'internal_ips is not cached in handle'
 
         codegen = RayCodeGen()
         is_local = isinstance(handle.launched_resources.cloud, clouds.Local)
-        codegen.add_prologue(job_id,
-                             spot_task=task.spot_task,
-                             setup_cmd=self._setup_cmd,
-                             envs=task.envs,
-                             setup_log_path=os.path.join(log_dir, 'setup.log'),
-                             is_local=is_local)
-        codegen.add_gang_scheduling_placement_group(1, accelerator_dict)
+        codegen.add_prologue(job_id, is_local=is_local)
+        codegen.add_gang_scheduling_placement_group_and_setup(
+            1,
+            accelerator_dict,
+            stable_cluster_internal_ips=internal_ips,
+            setup_cmd=self._setup_cmd,
+            setup_log_path=os.path.join(log_dir, 'setup.log'),
+            envs=task.envs,
+        )
 
         if callable(task.run):
             run_fn_code = textwrap.dedent(inspect.getsource(task.run))
             run_fn_name = task.run.__name__
             codegen.register_run_fn(run_fn_code, run_fn_name)
 
-        # If it is a managed spot job, the JOB_ID_ENV_VAR will have been already
-        # set by the controller.
+        # If it is a managed spot job, the TASK_ID_ENV_VAR will have been
+        # already set by the controller.
         job_run_id = task.envs.get(
-            constants.JOB_ID_ENV_VAR,
+            constants.TASK_ID_ENV_VAR,
             common_utils.get_global_job_id(self.run_timestamp,
                                            cluster_name=handle.cluster_name,
-                                           job_id=job_id))
+                                           job_id=str(job_id)))
 
         command_for_node = task.run if isinstance(task.run, str) else None
         use_sudo = isinstance(handle.launched_resources.cloud, clouds.Local)
         codegen.add_ray_task(
             bash_script=command_for_node,
             env_vars=task.envs,
             task_name=task.name,
             job_run_id=job_run_id,
             ray_resources_dict=backend_utils.get_task_demands_dict(task),
-            log_path=log_path,
+            log_dir=log_dir,
             use_sudo=use_sudo)
 
         codegen.add_epilogue()
 
         self._exec_code_on_head(handle,
                                 codegen.build(),
                                 job_id,
                                 executable='python3',
-                                detach_run=detach_run)
+                                detach_run=detach_run,
+                                spot_dag=task.spot_dag)
 
-    def _execute_task_n_nodes(self, handle: ResourceHandle, task: task_lib.Task,
-                              job_id: int, detach_run: bool) -> None:
+    def _execute_task_n_nodes(self, handle: CloudVmRayResourceHandle,
+                              task: task_lib.Task, job_id: int,
+                              detach_run: bool) -> None:
         # Strategy:
         #   ray.init(...)
         #   for node:
         #     submit _run_cmd(cmd) with resource {node_i: 1}
         log_dir_base = self.log_dir
         log_dir = os.path.join(log_dir_base, 'tasks')
         accelerator_dict = backend_utils.get_task_demands_dict(task)
+        internal_ips = handle.internal_ips()
+        assert internal_ips is not None, 'internal_ips is not cached in handle'
 
         # If TPU VM Pods is used, #num_nodes should be #num_tpu_devices
         is_tpu_vm_pod = tpu_utils.is_tpu_vm_pod(handle.launched_resources)
         if is_tpu_vm_pod:
             num_actual_nodes = tpu_utils.get_num_tpu_devices(
                 handle.launched_resources)
         else:
             num_actual_nodes = task.num_nodes
-
-        cluster_internal_ips = backend_utils.get_node_ips(handle.cluster_yaml,
-                                                          handle.launched_nodes,
-                                                          handle=handle,
-                                                          get_internal_ips=True)
-        # Ensure head node is the first element, then sort the remaining IPs
-        # for stableness
-        cluster_ips_sorted = [cluster_internal_ips[0]] + sorted(
-            cluster_internal_ips[1:])
+        assert isinstance(num_actual_nodes, int), num_actual_nodes
 
         codegen = RayCodeGen()
         is_local = isinstance(handle.launched_resources.cloud, clouds.Local)
-        codegen.add_prologue(job_id,
-                             spot_task=task.spot_task,
-                             setup_cmd=self._setup_cmd,
-                             envs=task.envs,
-                             setup_log_path=os.path.join(log_dir, 'setup.log'),
-                             is_local=is_local)
-        codegen.add_gang_scheduling_placement_group(
+        codegen.add_prologue(job_id, is_local=is_local)
+        codegen.add_gang_scheduling_placement_group_and_setup(
             num_actual_nodes,
             accelerator_dict,
-            cluster_ips_sorted=cluster_ips_sorted)
+            stable_cluster_internal_ips=internal_ips,
+            setup_cmd=self._setup_cmd,
+            setup_log_path=os.path.join(log_dir, 'setup.log'),
+            envs=task.envs)
 
         if callable(task.run):
             run_fn_code = textwrap.dedent(inspect.getsource(task.run))
             run_fn_name = task.run.__name__
             codegen.register_run_fn(run_fn_code, run_fn_name)
 
-        # If it is a managed spot job, the JOB_ID_ENV_VAR will have been already
-        # set by the controller.
+        # If it is a managed spot job, the TASK_ID_ENV_VAR will have been
+        # already set by the controller.
         job_run_id = task.envs.get(
-            constants.JOB_ID_ENV_VAR,
+            constants.TASK_ID_ENV_VAR,
             common_utils.get_global_job_id(self.run_timestamp,
                                            cluster_name=handle.cluster_name,
-                                           job_id=job_id))
+                                           job_id=str(job_id)))
 
         # TODO(zhwu): The resources limitation for multi-node ray.tune and
         # horovod should be considered.
         for i in range(num_actual_nodes):
             command_for_node = task.run if isinstance(task.run, str) else None
 
             # Ray's per-node resources, to constrain scheduling each command to
             # the corresponding node, represented by private IPs.
-            name = f'node-{i}'
-            log_path = os.path.join(f'{log_dir}', f'{name}.log')
             use_sudo = isinstance(handle.launched_resources.cloud, clouds.Local)
             codegen.add_ray_task(
                 bash_script=command_for_node,
                 env_vars=task.envs,
-                task_name=name,
+                task_name=task.name,
                 job_run_id=job_run_id,
                 ray_resources_dict=accelerator_dict,
-                log_path=log_path,
+                log_dir=log_dir,
                 gang_scheduling_id=i,
                 use_sudo=use_sudo,
             )
 
         codegen.add_epilogue()
         # TODO(zhanghao): Add help info for downloading logs.
         self._exec_code_on_head(handle,
                                 codegen.build(),
                                 job_id,
                                 executable='python3',
-                                detach_run=detach_run)
+                                detach_run=detach_run,
+                                spot_dag=task.spot_dag)
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/backends/docker_utils.py` & `skypilot-nightly-1.0.0.dev20230713/sky/backends/docker_utils.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 """Utilities for docker image generation."""
 import os
 import shutil
 import subprocess
 import tempfile
 import textwrap
-from typing import Optional
+from typing import Dict, Optional, Tuple
 
 import colorama
 
 from sky.adaptors import docker
 from sky import sky_logging
 from sky import task as task_mod
 
@@ -33,19 +33,19 @@
 SKY_DOCKER_SETUP_SCRIPT = 'sky_setup.sh'
 SKY_DOCKER_RUN_SCRIPT = 'sky_run.sh'
 SKY_DOCKER_WORKDIR = 'sky_workdir'
 
 
 def create_dockerfile(
     base_image: str,
-    setup_command: str,
+    setup_command: Optional[str],
     copy_path: str,
     build_dir: str,
-    run_command: str = None,
-) -> str:
+    run_command: Optional[str] = None,
+) -> Tuple[str, Dict[str, str]]:
     """Writes a valid dockerfile to the specified path.
 
     performs three operations:
     1. load base_image
     2. run some setup commands
     3. copy a directory to the image.
     the run/entrypoint can be optionally specified in the dockerfile or at
@@ -74,15 +74,16 @@
     if copy_path:
         workdir_name = os.path.basename(os.path.dirname(copy_path))
         # NOTE: This relies on copy_path being copied to build context.
         copy_docker_cmd = f'{workdir_name} /{workdir_name}/'
         dockerfile_contents += '\n' + DOCKERFILE_COPYCMD.format(
             copy_command=copy_docker_cmd)
 
-    def add_script_to_dockerfile(dockerfile_contents: str, multiline_cmds: str,
+    def add_script_to_dockerfile(dockerfile_contents: str,
+                                 multiline_cmds: Optional[str],
                                  out_filename: str):
         # Converts multiline commands to a script and adds the script to the
         # dockerfile. You still need to add the docker command to run the
         # script (either as CMD or RUN).
         script_path = os.path.join(build_dir, out_filename)
         bash_codegen(workdir_name, multiline_cmds, script_path)
 
@@ -126,41 +127,44 @@
     assert tag is not None, 'Image tag cannot be None - have you specified a ' \
                             'task name? '
     docker_client = docker.from_env()
     try:
         unused_image, unused_build_logs = docker_client.images.build(
             path=context_path, tag=tag, rm=True, quiet=False)
     except docker.build_error() as e:
-        colorama.init()
         style = colorama.Style
         fore = colorama.Fore
         logger.error(f'{fore.RED}Image build for {tag} failed - are your setup '
                      f'commands correct? Logs below{style.RESET_ALL}')
         logger.error(
             f'{style.BRIGHT}Image context is available at {context_path}'
             f'{style.RESET_ALL}')
         for line in e.build_log:
             if 'stream' in line:
                 logger.error(line['stream'].strip())
         raise
 
 
-def build_dockerimage(task, tag):
+def build_dockerimage(task: task_mod.Task,
+                      tag: str) -> Tuple[str, Dict[str, str]]:
     """
     Builds a docker image for the given task.
 
     This method is responsible for:
     1. Create a temp directory to set the build context.
     2. Copy dockerfile to this directory and copy contents
     3. Run the dockerbuild
     """
     # Get tempdir
     temp_dir = tempfile.mkdtemp(prefix='sky_local_')
 
     # Create dockerfile
+    if callable(task.run):
+        raise ValueError(
+            'Cannot build docker image for a task.run with function.')
     _, img_metadata = create_dockerfile(base_image=task.docker_image,
                                         setup_command=task.setup,
                                         copy_path=f'{SKY_DOCKER_WORKDIR}/',
                                         run_command=task.run,
                                         build_dir=temp_dir)
 
     dst = os.path.join(temp_dir, SKY_DOCKER_WORKDIR)
@@ -178,16 +182,18 @@
 
     # Clean up temp dir
     subprocess.run(['rm', '-rf', temp_dir], check=False)
 
     return tag, img_metadata
 
 
-def build_dockerimage_from_task(task: task_mod.Task):
+def build_dockerimage_from_task(
+        task: task_mod.Task) -> Tuple[str, Dict[str, str]]:
     """ Builds a docker image from a Task"""
+    assert task.name is not None, task
     tag, img_metadata = build_dockerimage(task, tag=task.name)
     return tag, img_metadata
 
 
 def push_dockerimage(local_tag, remote_name):
     raise NotImplementedError('Pushing images is not yet implemented.')
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/backends/local_docker_backend.py` & `skypilot-nightly-1.0.0.dev20230713/sky/backends/local_docker_backend.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 """Local docker backend for sky"""
 import subprocess
 import tempfile
 import typing
-from typing import Dict, Optional, Union
+from typing import Any, Dict, Optional, Tuple, Union
 
 import colorama
 from rich import console as rich_console
 
 from sky import backends
 from sky.adaptors import docker
 from sky import global_user_state
@@ -26,15 +26,29 @@
 
 _DOCKER_RUN_FOREVER_CMD = 'tail -f /dev/null'
 _DOCKER_DEFAULT_LABELS = {'app': 'sky'}
 _DOCKER_LABEL_PREFIX = 'skymeta_'
 _DOCKER_HANDLE_PREFIX = 'skydocker-'
 
 
-class LocalDockerBackend(backends.Backend):
+class LocalDockerResourceHandle(str, backends.ResourceHandle):
+    """The name of the cluster/container prefixed with the handle prefix."""
+
+    def __new__(cls, s, **kw):
+        if s.startswith(_DOCKER_HANDLE_PREFIX):
+            prefixed_str = s
+        else:
+            prefixed_str = _DOCKER_HANDLE_PREFIX + s
+        return str.__new__(cls, prefixed_str, **kw)
+
+    def get_cluster_name(self):
+        return self.lstrip(_DOCKER_HANDLE_PREFIX)
+
+
+class LocalDockerBackend(backends.Backend['LocalDockerResourceHandle']):
     """Local docker backend for debugging.
 
     Ignores resource demands when allocating. Optionally uses GPU if required.
 
     Here's a correspondence map to help understand how sky concepts map to
     Docker concepts:
 
@@ -74,26 +88,16 @@
 
     * There's no notion of resources - this can be constrained if required,
       but not implemented currently.
     """
 
     NAME = 'localdocker'
 
-    class ResourceHandle(str):
-        """The name of the cluster/container prefixed with the handle prefix."""
-
-        def __new__(cls, s, **kw):
-            if s.startswith(_DOCKER_HANDLE_PREFIX):
-                prefixed_str = s
-            else:
-                prefixed_str = _DOCKER_HANDLE_PREFIX + s
-            return str.__new__(cls, prefixed_str, **kw)
-
-        def get_cluster_name(self):
-            return self.lstrip(_DOCKER_HANDLE_PREFIX)
+    # Backward compatibility, with the old name of the handle.
+    ResourceHandle = LocalDockerResourceHandle  # pylint: disable=invalid-name
 
     # Define the Docker-in-Docker mount
     _dind_mount = {
         '/var/run/docker.sock': {
             'bind': '/var/run/docker.sock',
             'mode': 'rw'
         }
@@ -106,74 +110,86 @@
               Sets container runtime to 'nvidia' if set to True, else uses the
               default runtime. If set to 'auto', it detects if GPUs are present
               and automatically sets the container runtime.
 
         """
         self._use_gpu = backend_utils.check_local_gpus() if use_gpu == 'auto' \
             else use_gpu
-        self.volume_mounts = {}  # Stores the ResourceHandle->volume mounts map
-        self.images = {}  # Stores the ResourceHandle->[image_tag, metadata] map
-        self.containers = {}
+        self.volume_mounts: Dict[LocalDockerResourceHandle, Dict[str, Any]] = {
+        }  # Stores the LocalDockerResourceHandle->volume mounts map
+        self.images: Dict[LocalDockerResourceHandle, Tuple[str, Dict[
+            str, str]]] = {
+            }  # Stores the LocalDockerResourceHandle->[image_tag, metadata] map
+        self.containers: Dict[LocalDockerResourceHandle, Any] = {}
         self.client = docker.from_env()
         self._update_state()
 
     # --- Implementation of Backend APIs ---
 
-    def check_resources_fit_cluster(self, handle: ResourceHandle,
+    def check_resources_fit_cluster(self, handle: 'LocalDockerResourceHandle',
                                     task: 'task_lib.Task') -> None:
         pass
 
-    def _provision(self,
-                   task: 'task_lib.Task',
-                   to_provision: Optional['resources.Resources'],
-                   dryrun: bool,
-                   stream_logs: bool,
-                   cluster_name: str,
-                   retry_until_up: bool = False) -> ResourceHandle:
+    def _provision(
+            self,
+            task: 'task_lib.Task',
+            to_provision: Optional['resources.Resources'],
+            dryrun: bool,
+            stream_logs: bool,
+            cluster_name: str,
+            retry_until_up: bool = False
+    ) -> Optional[LocalDockerResourceHandle]:
         """
         Builds docker image for the task and returns the cluster name as handle.
 
         Since resource demands are ignored, There's no provisioning in local
         docker.
         """
+        del to_provision  # Unused
         assert task.name is not None, ('Task name cannot be None - have you '
                                        'specified a task name?')
+        if dryrun:
+            return None
         if retry_until_up:
             logger.warning(
                 f'Retrying until up is not supported in backend: {self.NAME}. '
                 'Ignored the flag.')
         if stream_logs:
             logger.info(
                 'Streaming build logs is not supported in LocalDockerBackend. '
                 'Build logs will be shown on failure.')
-        handle = LocalDockerBackend.ResourceHandle(cluster_name)
+        handle = LocalDockerResourceHandle(cluster_name)
         logger.info(f'Building docker image for task {task.name}. '
                     'This might take some time.')
         with console.status('[bold cyan]Building Docker image[/]'):
             image_tag, metadata = docker_utils.build_dockerimage_from_task(task)
-        self.images[handle] = [image_tag, metadata]
+        self.images[handle] = (image_tag, metadata)
         logger.info(f'Image {image_tag} built.')
         logger.info('Provisioning complete.')
-        global_user_state.add_or_update_cluster(cluster_name,
-                                                cluster_handle=handle,
-                                                ready=False)
+        global_user_state.add_or_update_cluster(
+            cluster_name,
+            cluster_handle=handle,
+            requested_resources=task.resources,
+            ready=False)
         return handle
 
-    def _sync_workdir(self, handle: ResourceHandle, workdir: Path) -> None:
+    def _sync_workdir(self, handle: LocalDockerResourceHandle,
+                      workdir: Path) -> None:
         """Workdir is sync'd by adding to the docker image.
 
         This happens in the execute step.
         """
+        del handle, workdir  # Unused
         logger.info('Since the workdir is synced at build time, sync_workdir is'
                     ' a NoOp. If you are running sky exec, your workdir has not'
                     ' been updated.')
 
     def _sync_file_mounts(
         self,
-        handle: ResourceHandle,
+        handle: LocalDockerResourceHandle,
         all_file_mounts: Dict[Path, Path],
         storage_mounts: Dict[Path, storage_lib.Storage],
     ) -> None:
         """File mounts in Docker are implemented with volume mounts (-v)."""
         assert not storage_mounts, \
             'Only local file mounts are supported with LocalDockerBackend.'
         docker_mounts = {}
@@ -186,24 +202,23 @@
             for container_path, local_path in all_file_mounts.items():
                 docker_mounts[local_path] = {
                     'bind': container_path,
                     'mode': 'rw'
                 }
         self.volume_mounts[handle] = docker_mounts
 
-    def _setup(self, handle: ResourceHandle, task: 'task_lib.Task') -> None:
-        """
-        Launches a container and runs a sleep command on it.
+    def _setup(self, handle: LocalDockerResourceHandle, task: 'task_lib.Task',
+               detach_setup: bool) -> None:
+        """Launches a container and runs a sleep command on it.
 
         setup() in LocalDockerBackend runs the container with a sleep job
         so that the container is kept alive and we can issue docker exec cmds
         to it to handle sky exec commands.
         """
-        del task  # unused
-        colorama.init()
+        del detach_setup  # unused
         style = colorama.Style
         assert handle in self.images, \
             f'No image found for {handle}, have you run Backend.provision()?'
         image_tag, metadata = self.images[handle]
         volumes = self.volume_mounts[handle]
         runtime = 'nvidia' if self._use_gpu else None
         logger.info(f'Image {image_tag} found. Running container now. use_gpu '
@@ -240,19 +255,21 @@
         self.containers[handle] = container
         logger.info(
             f'Your container is now running with name: {container.name}.\n'
             f'To get a shell in your container, run: {style.BRIGHT}docker exec '
             f'-it {container.name} /bin/bash{style.RESET_ALL}.\n'
             f'You can debug the image by running: {style.BRIGHT}docker run -it '
             f'{image_tag} /bin/bash{style.RESET_ALL}.\n')
-        global_user_state.add_or_update_cluster(cluster_name,
-                                                cluster_handle=handle,
-                                                ready=True)
+        global_user_state.add_or_update_cluster(
+            cluster_name,
+            cluster_handle=handle,
+            requested_resources=task.resources,
+            ready=True)
 
-    def _execute(self, handle: ResourceHandle, task: 'task_lib.Task',
+    def _execute(self, handle: LocalDockerResourceHandle, task: 'task_lib.Task',
                  detach_run: bool) -> None:
         """ Launches the container."""
 
         if detach_run:
             raise NotImplementedError('detach_run=True is not supported in '
                                       'LocalDockerBackend.')
 
@@ -264,17 +281,17 @@
         # Handle a basic task
         if task.run is None:
             logger.info(f'Nothing to run; run command not specified:\n{task}')
             return
 
         self._execute_task_one_node(handle, task)
 
-    def _post_execute(self, handle: ResourceHandle, down: bool) -> None:
+    def _post_execute(self, handle: LocalDockerResourceHandle,
+                      down: bool) -> None:
         del down  # unused
-        colorama.init()
         style = colorama.Style
         container = self.containers[handle]
 
         # Fetch latest status from docker daemon
         container.reload()
 
         if container.status == 'running':
@@ -288,15 +305,15 @@
                         f'{style.BRIGHT}{container.name}{style.RESET_ALL}')
         logger.info(
             'To create a new container for debugging without running the '
             f'task run command, run {style.BRIGHT}docker run -it '
             f'{container.image.tags[0]} /bin/bash{style.RESET_ALL}')
 
     def _teardown(self,
-                  handle: ResourceHandle,
+                  handle: LocalDockerResourceHandle,
                   terminate: bool,
                   purge: bool = False):
         """Teardown kills the container."""
         del purge  # Unused.
         if not terminate:
             logger.warning(
                 'LocalDockerBackend.teardown() will terminate '
@@ -305,14 +322,15 @@
         # If handle is not found in the self.containers, it implies it has
         # already been removed externally in docker. No action is needed
         # except for removing it from global_user_state.
         if handle in self.containers:
             container = self.containers[handle]
             container.remove(force=True)
         cluster_name = handle.get_cluster_name()
+
         global_user_state.remove_cluster(cluster_name, terminate=True)
 
     # --- Utilities ---
 
     def _update_state(self):
         """
         Updates local state of the backend object.
@@ -334,16 +352,20 @@
             for k, v in c.labels.items():
                 if k.startswith(_DOCKER_LABEL_PREFIX):
                     # Remove 'skymeta_' from key
                     metadata[k[len(_DOCKER_LABEL_PREFIX):]] = v
             self.images[c.name] = [c.image, metadata]
             self.containers[c.name] = c
 
-    def _execute_task_one_node(self, handle: ResourceHandle,
+    def _execute_task_one_node(self, handle: LocalDockerResourceHandle,
                                task: 'task_lib.Task') -> None:
+        if callable(task.run):
+            raise NotImplementedError(
+                'Tasks with callable run commands are not supported in '
+                'LocalDockerBackend.')
         container = self.containers[handle]
         _, image_metadata = self.images[handle]
         with tempfile.NamedTemporaryFile(mode='w') as temp_file:
             script_contents = docker_utils.bash_codegen(
                 workdir_name=image_metadata['workdir_name'],
                 multiline_cmds=task.run)
             temp_file.write(script_contents)
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/backends/onprem_utils.py` & `skypilot-nightly-1.0.0.dev20230713/sky/backends/onprem_utils.py`

 * *Files 8% similar despite different names*

```diff
@@ -4,21 +4,23 @@
 import os
 import socket
 import tempfile
 import textwrap
 from typing import Any, Dict, List, Optional, Tuple
 
 import click
+from packaging import version
 import rich.console as rich_console
 import yaml
 
 from sky import global_user_state
 from sky import sky_logging
 from sky.backends import backend_utils
 from sky.skylet import constants
+from sky.skylet import log_lib
 from sky.utils import command_runner
 from sky.utils import common_utils
 from sky.utils import schemas
 from sky.utils import subprocess_utils
 from sky.utils import ux_utils
 
 logger = sky_logging.init_logger(__name__)
@@ -62,15 +64,15 @@
     # Filter out folders.
     local_cluster_paths = [
         path for path in local_cluster_paths
         if os.path.isfile(path) and path.endswith('.yml')
     ]
 
     local_cluster_names = []
-    name_to_path_dict = {}
+    name_to_path_dict: Dict[str, str] = {}
 
     for path in local_cluster_paths:
         with open(path, 'r') as f:
             yaml_config = yaml.safe_load(f)
             if not suppress_error:
                 backend_utils.validate_schema(yaml_config,
                                               schemas.get_cluster_schema(),
@@ -122,27 +124,27 @@
     config = get_local_cluster_config_or_error(cluster_name)
     ips = config['cluster']['ips']
     if isinstance(ips, str):
         ips = [ips]
     return ips
 
 
-def get_local_auth_config(cluster_name: str) -> List[str]:
+def get_local_auth_config(cluster_name: str) -> Dict[str, str]:
     """Returns IP addresses of the local cluster."""
     config = get_local_cluster_config_or_error(cluster_name)
     return config['auth']
 
 
 def get_python_executable(cluster_name: str) -> str:
     """Returns the Ray cluster's python path."""
     config = get_local_cluster_config_or_error(cluster_name)
     return config['python']
 
 
-def get_job_owner(cluster_yaml: dict) -> str:
+def get_job_owner(cluster_yaml: str) -> str:
     """Get the owner of the job."""
     cluster_config = common_utils.read_yaml(os.path.expanduser(cluster_yaml))
     # User name is guaranteed to exist (on all jinja files)
     return cluster_config['auth']['ssh_user']
 
 
 def get_local_cluster_config_or_error(cluster_name: str) -> Dict[str, Any]:
@@ -177,26 +179,33 @@
     runners = command_runner.SSHCommandRunner.make_runner_list(
         ips, *ssh_credentials)
     sky_ray_version = constants.SKY_REMOTE_RAY_VERSION
 
     def _install_and_check_dependencies(
             runner: command_runner.SSHCommandRunner) -> None:
         # Checks for python3 installation.
-        backend_utils.run_command_and_handle_ssh_failure(
+        python_version = backend_utils.run_command_and_handle_ssh_failure(
             runner, ('python3 --version'),
             failure_message=f'Python3 is not installed on {runner.ip}.')
+        python_version = python_version.split(' ')[-1].strip()
+        python_version = version.Version(python_version)
+        min_python_version = version.Version('3.6')
+        if python_version < min_python_version:
+            raise ValueError(
+                f'Python {python_version} on {runner.ip} is less than '
+                f'the minimum requirement: Python {min_python_version}.')
 
         # Checks for pip3 installation.
         backend_utils.run_command_and_handle_ssh_failure(
             runner, ('pip3 --version'),
             failure_message=f'Pip3 is not installed on {runner.ip}.')
 
         # If Ray does not exist, installs Ray.
         backend_utils.run_command_and_handle_ssh_failure(
-            runner, (f'ray --version || '
+            runner, ('ray --version || '
                      f'(pip3 install ray[default]=={sky_ray_version})'),
             failure_message=f'Ray is not installed on {runner.ip}.')
 
         # If Ray exists, check Ray version. If the version does not match
         # raise an error.
         backend_utils.run_command_and_handle_ssh_failure(
             runner,
@@ -265,15 +274,15 @@
                             'P100',
                             'T4',
                             'P4',
                             'K80',
                             'A100',
                             '1080',
                             '2080',
-                            'A5000'
+                            'A5000',
                             'A6000']
         accelerators_dict = {}
         for acc in all_accelerators:
             output_str = os.popen(f'lspci | grep \\'{acc}\\'').read()
             output_lst = output_str.split('\\n')
             count = 0
             for output in output_lst:
@@ -306,17 +315,16 @@
         return node_accs
 
     custom_resources = subprocess_utils.run_in_parallel(
         _gather_cluster_accelerators, runners)
     return custom_resources
 
 
-def launch_ray_on_local_cluster(
-        cluster_config: Dict[str, Dict[str, Any]],
-        custom_resources: List[Dict[str, int]] = None) -> None:
+def launch_ray_on_local_cluster(cluster_config: Dict[str, Dict[str, Any]],
+                                custom_resources: List[Dict[str, int]]) -> None:
     """Launches Ray on all nodes for local cluster.
 
     Launches Ray on the root user of all nodes and opens the Ray dashboard port
     on the non-head nodes. This ensures that Sky can coordinate and cancel jobs
     across nodes.
 
     Args:
@@ -359,41 +367,46 @@
 
         subprocess_utils.run_in_parallel(_stop_ray_workers,
                                          [head_runner] + worker_runners)
 
     # Launching Ray on the head node.
     head_resources = json.dumps(custom_resources[0], separators=(',', ':'))
     head_gpu_count = sum(list(custom_resources[0].values()))
-    head_cmd = ('ray start --head --port=6379 '
-                '--object-manager-port=8076 --dashboard-port 8265 '
-                f'--resources={head_resources!r} --num-gpus={head_gpu_count}')
+    head_cmd = (f'ray start --head --port={constants.SKY_REMOTE_RAY_PORT} '
+                '--object-manager-port=8076 '
+                f'--dashboard-port {constants.SKY_REMOTE_RAY_DASHBOARD_PORT} '
+                f'--resources={head_resources!r} --num-gpus={head_gpu_count} '
+                f'--temp-dir {constants.SKY_REMOTE_RAY_TEMPDIR}')
 
     with console.status('[bold cyan]Launching ray cluster on head'):
         backend_utils.run_command_and_handle_ssh_failure(
             head_runner,
             head_cmd,
             failure_message='Failed to launch ray on head node.')
 
     if not worker_runners:
         return
 
     # Launches Ray on the worker nodes and links Ray dashboard from the head
     # to worker node.
     remote_ssh_key = f'~/.ssh/{os.path.basename(ssh_key)}'
     dashboard_remote_path = '~/.sky/dashboard_portforward.sh'
-    worker_runners = [(runner, idx) for idx, runner in enumerate(worker_runners)
-                     ]
+    worker_runner_idxs = [
+        (runner, idx) for idx, runner in enumerate(worker_runners)
+    ]
     # Connect head node's Ray dashboard to worker nodes
     # Worker nodes need access to Ray dashboard to poll the
     # JobSubmissionClient (in subprocess_daemon.py) for completed,
     # failed, or cancelled jobs.
     ssh_options = command_runner.ssh_options_list(
         ssh_private_key=remote_ssh_key, ssh_control_name=None)
     ssh_options = ' '.join(ssh_options)
-    port_cmd = (f'ssh -tt -L 8265:localhost:8265 '
+    ray_dashboard_port = constants.SKY_REMOTE_RAY_DASHBOARD_PORT
+    port_cmd = ('ssh -tt -L '
+                f'{ray_dashboard_port}:localhost:{ray_dashboard_port} '
                 f'{ssh_options} {ssh_user}@{head_ip} '
                 '\'while true; do sleep 86400; done\'')
     with console.status('[bold cyan]Waiting for workers.'):
 
         def _start_ray_workers(
                 runner_tuple: Tuple[command_runner.SSHCommandRunner, int]):
             runner, idx = runner_tuple
@@ -401,18 +414,22 @@
                 runner,
                 'ray stop -f',
                 failure_message=f'Failed to stop ray on {runner.ip}.')
 
             worker_resources = json.dumps(custom_resources[idx + 1],
                                           separators=(',', ':'))
             worker_gpu_count = sum(list(custom_resources[idx + 1].values()))
-            worker_cmd = (f'ray start --address={head_ip}:6379 '
-                          '--object-manager-port=8076 --dashboard-port 8265 '
-                          f'--resources={worker_resources!r} '
-                          f'--num-gpus={worker_gpu_count}')
+            worker_cmd = (
+                'ray start '
+                f'--address={head_ip}:{constants.SKY_REMOTE_RAY_PORT} '
+                '--object-manager-port=8076 --dashboard-port '
+                f'{constants.SKY_REMOTE_RAY_DASHBOARD_PORT} '
+                f'--resources={worker_resources!r} '
+                f'--num-gpus={worker_gpu_count} '
+                f'--temp-dir {constants.SKY_REMOTE_RAY_TEMPDIR}')
             backend_utils.run_command_and_handle_ssh_failure(
                 runner,
                 worker_cmd,
                 failure_message=
                 f'Failed to launch ray on worker node {runner.ip}.')
 
             # Connecting ray dashboard with worker node.
@@ -431,18 +448,18 @@
             backend_utils.run_command_and_handle_ssh_failure(
                 runner, f'chmod a+rwx {dashboard_remote_path};'
                 'screen -S ray-dashboard -X quit;'
                 f'screen -S ray-dashboard -dm {dashboard_remote_path}',
                 failure_message=
                 f'Failed to connect ray dashboard to worker node {runner.ip}.')
 
-        subprocess_utils.run_in_parallel(_start_ray_workers, worker_runners)
+        subprocess_utils.run_in_parallel(_start_ray_workers, worker_runner_idxs)
 
 
-def save_distributable_yaml(cluster_config: Dict[str, Dict[str, Any]]) -> None:
+def save_distributable_yaml(cluster_config: Dict[str, Any]) -> None:
     """Generates a distributable yaml for the system admin to send to users.
 
     Args:
         cluster_config: Dictionary representing the cluster config.
           Contains cluster-specific hyperparameters and the authentication
           config.
     """
@@ -499,7 +516,67 @@
                     'See `sky status` for local cluster name(s).')
             else:
                 raise click.UsageError(
                     'Specify -c [local_cluster] to launch on a local cluster.\n'
                     'See `sky status` for local cluster name(s).')
 
         return False
+
+
+def do_filemounts_and_setup_on_local_workers(
+        cluster_config_file: str,
+        worker_ips: Optional[List[str]] = None,
+        extra_setup_cmds: Optional[List[str]] = None):
+    """Completes filemounting and setup on worker nodes.
+
+    Syncs filemounts and runs setup on worker nodes for a local cluster. This
+    is a workaround for a Ray Autoscaler bug where `ray up` does not perform
+    filemounting or setup for local cluster worker nodes.
+    """
+    config = common_utils.read_yaml(cluster_config_file)
+
+    ssh_credentials = backend_utils.ssh_credential_from_yaml(
+        cluster_config_file)
+    if worker_ips is None:
+        worker_ips = config['provider']['worker_ips']
+    file_mounts = config['file_mounts']
+
+    setup_cmds = config['setup_commands']
+    if extra_setup_cmds is not None:
+        setup_cmds += extra_setup_cmds
+    setup_script = log_lib.make_task_bash_script('\n'.join(setup_cmds))
+
+    worker_runners = command_runner.SSHCommandRunner.make_runner_list(
+        worker_ips, **ssh_credentials)
+
+    # Uploads setup script to the worker node
+    with tempfile.NamedTemporaryFile('w', prefix='sky_setup_') as f:
+        f.write(setup_script)
+        f.flush()
+        setup_sh_path = f.name
+        setup_file = os.path.basename(setup_sh_path)
+        file_mounts[f'/tmp/{setup_file}'] = setup_sh_path
+
+        # Ray Autoscaler Bug: Filemounting + Ray Setup
+        # does not happen on workers.
+        def _setup_local_worker(runner: command_runner.SSHCommandRunner):
+            for dst, src in file_mounts.items():
+                mkdir_dst = f'mkdir -p {os.path.dirname(dst)}'
+                backend_utils.run_command_and_handle_ssh_failure(
+                    runner,
+                    mkdir_dst,
+                    failure_message=f'Failed to run {mkdir_dst} on remote.')
+                if os.path.isdir(src):
+                    src = os.path.join(src, '')
+                runner.rsync(source=src, target=dst, up=True, stream_logs=False)
+
+            setup_cmd = f'/bin/bash -i /tmp/{setup_file} 2>&1'
+            rc, stdout, _ = runner.run(setup_cmd,
+                                       stream_logs=False,
+                                       require_outputs=True)
+            subprocess_utils.handle_returncode(
+                rc,
+                setup_cmd,
+                'Failed to setup Ray autoscaler commands on remote.',
+                stderr=stdout)
+
+        subprocess_utils.run_in_parallel(_setup_local_worker, worker_runners)
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/backends/wheel_utils.py` & `skypilot-nightly-1.0.0.dev20230713/sky/backends/wheel_utils.py`

 * *Files identical despite different names*

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/benchmark/benchmark_state.py` & `skypilot-nightly-1.0.0.dev20230713/sky/benchmark/benchmark_state.py`

 * *Files 1% similar despite different names*

```diff
@@ -129,17 +129,16 @@
         'INSERT INTO benchmark'
         '(name, task, bucket, launched_at) '
         'VALUES (?, ?, ?, ?)',
         (benchmark_name, task_name, bucket_name, launched_at))
     _BENCHMARK_DB.conn.commit()
 
 
-def add_benchmark_result(
-        benchmark_name: str,
-        cluster_handle: 'backend_lib.Backend.ResourceHandle') -> None:
+def add_benchmark_result(benchmark_name: str,
+                         cluster_handle: 'backend_lib.ResourceHandle') -> None:
     name = cluster_handle.cluster_name
     num_nodes = cluster_handle.launched_nodes
     resources = pickle.dumps(cluster_handle.launched_resources)
     _BENCHMARK_DB.cursor.execute(
         'INSERT INTO benchmark_results'
         '(cluster, status, num_nodes, resources, record, benchmark) '
         'VALUES (?, ?, ?, ?, NULL, ?)', (name, BenchmarkStatus.INIT.value,
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/benchmark/benchmark_utils.py` & `skypilot-nightly-1.0.0.dev20230713/sky/benchmark/benchmark_utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -20,14 +20,15 @@
 from rich import progress as rich_progress
 
 import sky
 from sky import backends
 from sky import data
 from sky import global_user_state
 from sky import sky_logging
+from sky import status_lib
 from sky.backends import backend_utils
 from sky.benchmark import benchmark_state
 from sky.skylet import constants
 from sky.skylet import log_lib
 from sky.skylet import job_lib
 from sky.utils import log_utils
 from sky.utils import common_utils
@@ -115,14 +116,15 @@
 
     columns = [
         'CLUSTER',
         'CLOUD',
         '# NODES',
         'INSTANCE',
         'vCPUs',
+        'Mem(GB)',
         'ACCELERATORS',
         'PRICE ($/hr)',
     ]
     table_kwargs = {
         'hrules': prettytable.FRAME,
         'vrules': prettytable.NONE,
         'border': True,
@@ -132,26 +134,32 @@
     for cluster, resources in zip(clusters, candidate_resources):
         if resources.accelerators is None:
             accelerators = '-'
         else:
             accelerator, count = list(resources.accelerators.items())[0]
             accelerators = f'{accelerator}:{count}'
         cloud = resources.cloud
-        vcpus = cloud.get_vcpus_from_instance_type(resources.instance_type)
-        if vcpus is None:
-            vcpus = '-'
-        elif vcpus.is_integer():
-            vcpus = str(int(vcpus))
-        else:
-            vcpus = f'{vcpus:.1f}'
+        vcpus, mem = cloud.get_vcpus_mem_from_instance_type(
+            resources.instance_type)
+
+        def format_number(x):
+            if x is None:
+                return '-'
+            elif x.is_integer():
+                return str(int(x))
+            else:
+                return f'{x:.1f}'
+
+        vcpus = format_number(vcpus)
+        mem = format_number(mem)
         cost = num_nodes * resources.get_cost(3600)
         spot = '[Spot]' if resources.use_spot else ''
         row = [
             cluster, cloud, num_nodes, resources.instance_type + spot, vcpus,
-            accelerators, f'{cost:.2f}'
+            mem, accelerators, f'{cost:.2f}'
         ]
         candidate_table.add_row(row)
     logger.info(f'{candidate_table}\n')
 
 
 def _create_benchmark_bucket() -> Tuple[str, str]:
     # Generate a bucket name.
@@ -302,29 +310,29 @@
     job_status = None
     if record is not None:
         cluster_status, handle = backend_utils.refresh_cluster_status_handle(
             cluster)
         backend = backend_utils.get_backend_from_handle(handle)
         assert isinstance(backend, backends.CloudVmRayBackend)
 
-        if cluster_status == global_user_state.ClusterStatus.UP:
+        if cluster_status == status_lib.ClusterStatus.UP:
             # NOTE: The id of the benchmarking job must be 1.
             # TODO(woosuk): Handle exceptions.
             job_status = backend.get_job_status(handle,
                                                 job_ids=['1'],
                                                 stream_logs=False)['1']
 
     # Update the benchmark status.
-    if (cluster_status == global_user_state.ClusterStatus.INIT or
+    if (cluster_status == status_lib.ClusterStatus.INIT or
             job_status < job_lib.JobStatus.RUNNING):
         benchmark_status = benchmark_state.BenchmarkStatus.INIT
     elif job_status == job_lib.JobStatus.RUNNING:
         benchmark_status = benchmark_state.BenchmarkStatus.RUNNING
     elif (cluster_status is None or
-          cluster_status == global_user_state.ClusterStatus.STOPPED or
+          cluster_status == status_lib.ClusterStatus.STOPPED or
           (job_status is not None and job_status.is_terminal())):
         # The cluster has terminated or stopped, or
         # the cluster is UP and the job has terminated.
         if end_time is not None:
             # The job has terminated with zero exit code.
             benchmark_status = benchmark_state.BenchmarkStatus.FINISHED
         elif job_status == job_lib.JobStatus.SUCCEEDED:
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/cli.py` & `skypilot-nightly-1.0.0.dev20230713/sky/cli.py`

 * *Files 16% similar despite different names*

```diff
@@ -15,58 +15,59 @@
 
   # Tear down a specific cluster.
   >> sky down cluster_name
 
   # Tear down all existing clusters.
   >> sky down -a
 
-TODO:
-- Add support for local Docker backend.  Currently this module is very coupled
-  with CloudVmRayBackend, as seen by the many use of ray commands.
-
 NOTE: the order of command definitions in this file corresponds to how they are
 listed in "sky --help".  Take care to put logically connected commands close to
 each other.
 """
 import copy
 import datetime
 import functools
-import getpass
+import multiprocessing
 import os
 import shlex
+import signal
 import subprocess
 import sys
 import textwrap
+import time
 import typing
-from typing import Any, Dict, List, Optional, Tuple
+from typing import Any, Dict, List, Optional, Tuple, Union
+import webbrowser
 
 import click
 import colorama
 from rich import progress as rich_progress
 import yaml
 
 import sky
 from sky import backends
 from sky import check as sky_check
 from sky import clouds
+from sky import core
 from sky import exceptions
 from sky import global_user_state
 from sky import sky_logging
 from sky import spot as spot_lib
-from sky import core
+from sky import status_lib
 from sky.backends import backend_utils
 from sky.backends import onprem_utils
 from sky.benchmark import benchmark_state
 from sky.benchmark import benchmark_utils
 from sky.clouds import service_catalog
 from sky.data import storage_utils
 from sky.skylet import constants
 from sky.skylet import job_lib
 from sky.utils import log_utils
 from sky.utils import common_utils
+from sky.utils import dag_utils
 from sky.utils import command_runner
 from sky.utils import schemas
 from sky.utils import subprocess_utils
 from sky.utils import timeline
 from sky.utils import ux_utils
 from sky.utils.cli_utils import status_utils
 from sky.usage import usage_lib
@@ -95,14 +96,18 @@
     'tpunode': sky.Resources(cloud=sky.GCP(),
                              instance_type=None,
                              accelerators={'tpu-v2-8': 1},
                              accelerator_args={'runtime_version': '2.5.0'},
                              use_spot=False),
 }
 
+# The maximum number of in-progress spot jobs to show in the status
+# command.
+_NUM_SPOT_JOBS_TO_SHOW_IN_STATUS = 5
+
 
 def _get_glob_clusters(clusters: List[str]) -> List[str]:
     """Returns a list of clusters that match the glob pattern."""
     glob_clusters = []
     for cluster in clusters:
         glob_cluster = global_user_state.get_glob_cluster_names(cluster)
         if len(glob_cluster) == 0:
@@ -171,14 +176,29 @@
                                 type=str,
                                 help='Cloud provider to use.')
     instance_type_option = click.option('--instance-type',
                                         '-t',
                                         default=None,
                                         type=str,
                                         help='Instance type to use.')
+    cpus = click.option(
+        '--cpus',
+        default=None,
+        type=str,
+        help=('Number of vCPUs each instance must have '
+              '(e.g., ``--cpus=4`` (exactly 4) or ``--cpus=4+`` (at least 4)). '
+              'This is used to automatically select the instance type.'))
+    memory = click.option(
+        '--memory',
+        default=None,
+        type=str,
+        required=False,
+        help=('Amount of memory each instance must have in GB (e.g., '
+              '``--memory=16`` (exactly 16GB), ``--memory=16+`` (at least '
+              '16GB))'))
     gpus = click.option('--gpus',
                         default=None,
                         type=str,
                         help=('Type and number of GPUs to use '
                               '(e.g., ``--gpus=V100:8`` or ``--gpus=V100``).'))
     tpus = click.option(
         '--tpus',
@@ -198,14 +218,21 @@
                                 help='If true, use TPU VMs.')
 
     disk_size = click.option('--disk-size',
                              default=None,
                              type=int,
                              required=False,
                              help=('OS disk size in GBs.'))
+    disk_tier = click.option('--disk-tier',
+                             default=None,
+                             type=click.Choice(['low', 'medium', 'high'],
+                                               case_sensitive=False),
+                             required=False,
+                             help=('OS disk tier. Could be one of "low", '
+                                   '"medium", "high". Default: medium'))
     no_confirm = click.option('--yes',
                               '-y',
                               is_flag=True,
                               default=False,
                               required=False,
                               help='Skip confirmation prompt.')
     idle_autostop = click.option('--idle-minutes-to-autostop',
@@ -265,23 +292,26 @@
         retry_until_up,
 
         # Resource options
         *([cloud_option] if cli_func.__name__ != 'tpunode' else []),
         region_option,
         zone_option,
         instance_type_option,
+        cpus,
+        memory,
         *([gpus] if cli_func.__name__ == 'gpunode' else []),
         *([tpus] if cli_func.__name__ == 'tpunode' else []),
         spot_option,
         *([tpuvm_option] if cli_func.__name__ == 'tpunode' else []),
 
         # Attach options
         screen_option,
         tmux_option,
         disk_size,
+        disk_tier,
     ]
     decorator = functools.reduce(lambda res, f: f(res),
                                  reversed(click_decorators), cli_func)
 
     return decorator
 
 
@@ -289,15 +319,20 @@
     """Parse env vars into a (KEY, VAL) pair."""
     if '=' not in env_var:
         value = os.environ.get(env_var)
         if value is None:
             raise click.UsageError(
                 f'{env_var} is not set in local environment.')
         return (env_var, value)
-    return tuple(env_var.split('=', 1))
+    ret = tuple(env_var.split('=', 1))
+    if len(ret) != 2:
+        raise click.UsageError(
+            f'Invalid env var: {env_var}. Must be in the form of KEY=VAL '
+            'or KEY.')
+    return ret[0], ret[1]
 
 
 _TASK_OPTIONS = [
     click.option('--name',
                  '-n',
                  required=False,
                  type=str,
@@ -553,20 +588,23 @@
     return _add_options
 
 
 def _parse_override_params(cloud: Optional[str] = None,
                            region: Optional[str] = None,
                            zone: Optional[str] = None,
                            gpus: Optional[str] = None,
+                           cpus: Optional[str] = None,
+                           memory: Optional[str] = None,
                            instance_type: Optional[str] = None,
                            use_spot: Optional[bool] = None,
                            image_id: Optional[str] = None,
-                           disk_size: Optional[int] = None) -> Dict[str, Any]:
+                           disk_size: Optional[int] = None,
+                           disk_tier: Optional[str] = None) -> Dict[str, Any]:
     """Parses the override parameters into a dictionary."""
-    override_params = {}
+    override_params: Dict[str, Any] = {}
     if cloud is not None:
         if cloud.lower() == 'none':
             override_params['cloud'] = None
         else:
             override_params['cloud'] = clouds.CLOUD_REGISTRY.from_str(cloud)
     if region is not None:
         if region.lower() == 'none':
@@ -579,38 +617,50 @@
         else:
             override_params['zone'] = zone
     if gpus is not None:
         if gpus.lower() == 'none':
             override_params['accelerators'] = None
         else:
             override_params['accelerators'] = gpus
+    if cpus is not None:
+        if cpus.lower() == 'none':
+            override_params['cpus'] = None
+        else:
+            override_params['cpus'] = cpus
+    if memory is not None:
+        if memory.lower() == 'none':
+            override_params['memory'] = None
+        else:
+            override_params['memory'] = memory
     if instance_type is not None:
         if instance_type.lower() == 'none':
             override_params['instance_type'] = None
         else:
             override_params['instance_type'] = instance_type
     if use_spot is not None:
         override_params['use_spot'] = use_spot
     if image_id is not None:
         if image_id.lower() == 'none':
             override_params['image_id'] = None
         else:
             override_params['image_id'] = image_id
     if disk_size is not None:
         override_params['disk_size'] = disk_size
+    if disk_tier is not None:
+        override_params['disk_tier'] = disk_tier
     return override_params
 
 
 def _default_interactive_node_name(node_type: str):
     """Returns a deterministic name to refer to the same node."""
     # FIXME: this technically can collide in Azure/GCP with another
     # same-username user.  E.g., sky-gpunode-ubuntu.  Not a problem on AWS
     # which is the current cloud for interactive nodes.
     assert node_type in _INTERACTIVE_NODE_TYPES, node_type
-    return f'sky-{node_type}-{getpass.getuser()}'
+    return f'sky-{node_type}-{backend_utils.get_cleaned_username()}'
 
 
 def _infer_interactive_node_type(resources: sky.Resources):
     """Determine interactive node type from resources."""
     accelerators = resources.accelerators
     cloud = resources.cloud
     if accelerators:
@@ -639,14 +689,17 @@
         node_type: Only used for interactive node. Node type to attach to VM.
     """
     handle = global_user_state.get_handle_from_cluster_name(cluster_name)
     if handle is None:
         return
 
     if node_type is not None:
+        assert isinstance(handle,
+                          backends.CloudVmRayResourceHandle), (node_type,
+                                                               handle)
         inferred_node_type = _infer_interactive_node_type(
             handle.launched_resources)
         if node_type != inferred_node_type:
             name_arg = ''
             if cluster_name != _default_interactive_node_name(
                     inferred_node_type):
                 name_arg = f' -c {cluster_name}'
@@ -657,38 +710,46 @@
         return
     backend.check_resources_fit_cluster(handle, task)
 
 
 def _launch_with_confirm(
     task: sky.Task,
     backend: backends.Backend,
-    cluster: str,
+    cluster: Optional[str],
     *,
     dryrun: bool,
     detach_run: bool,
     detach_setup: bool = False,
     no_confirm: bool = False,
     idle_minutes_to_autostop: Optional[int] = None,
     down: bool = False,  # pylint: disable=redefined-outer-name
     retry_until_up: bool = False,
     no_setup: bool = False,
     node_type: Optional[str] = None,
-    is_local_cloud: Optional[bool] = False,
+    clone_disk_from: Optional[str] = None,
 ):
     """Launch a cluster with a Task."""
-    with sky.Dag() as dag:
-        dag.add(task)
     if cluster is None:
         cluster = backend_utils.generate_cluster_name()
+
+    clone_source_str = ''
+    if clone_disk_from is not None:
+        clone_source_str = f' from the disk of {clone_disk_from!r}'
+        task, _ = backend_utils.check_can_clone_disk_and_override_task(
+            clone_disk_from, cluster, task)
+
+    with sky.Dag() as dag:
+        dag.add(task)
+
     maybe_status, _ = backend_utils.refresh_cluster_status_handle(cluster)
     if maybe_status is None:
         # Show the optimize log before the prompt if the cluster does not exist.
         try:
             backend_utils.check_public_cloud_enabled()
-        except RuntimeError as e:
+        except exceptions.NoCloudAccessError as e:
             # Catch the exception where the public cloud is not enabled, and
             # only print the error message without the error type.
             click.secho(e, fg='yellow')
             sys.exit(1)
         dag = sky.optimize(dag)
     task = dag.tasks[0]
 
@@ -697,53 +758,56 @@
     confirm_shown = False
     if not no_confirm:
         # Prompt if (1) --cluster is None, or (2) cluster doesn't exist, or (3)
         # it exists but is STOPPED.
         prompt = None
         if maybe_status is None:
             cluster_str = '' if cluster is None else f' {cluster!r}'
-            if is_local_cloud:
+            if onprem_utils.check_if_local_cloud(cluster):
                 prompt = f'Initializing local cluster{cluster_str}. Proceed?'
             else:
-                prompt = f'Launching a new cluster{cluster_str}. Proceed?'
-        elif maybe_status == global_user_state.ClusterStatus.STOPPED:
+                prompt = (
+                    f'Launching a new cluster{cluster_str}{clone_source_str}. '
+                    'Proceed?')
+        elif maybe_status == status_lib.ClusterStatus.STOPPED:
             prompt = f'Restarting the stopped cluster {cluster!r}. Proceed?'
         if prompt is not None:
             confirm_shown = True
             click.confirm(prompt, default=True, abort=True, show_default=True)
 
     if node_type is not None:
-        if maybe_status != global_user_state.ClusterStatus.UP:
+        if maybe_status != status_lib.ClusterStatus.UP:
             click.secho(f'Setting up interactive node {cluster}...',
                         fg='yellow')
 
         # We do not sky.launch if interactive node is already up, so we need
         # to update idle timeout and autodown here.
         elif idle_minutes_to_autostop is not None:
             core.autostop(cluster, idle_minutes_to_autostop, down)
         elif down:
             core.autostop(cluster, 1, down)
 
     elif not confirm_shown:
         click.secho(f'Running task on cluster {cluster}...', fg='yellow')
 
-    if node_type is None or maybe_status != global_user_state.ClusterStatus.UP:
+    if node_type is None or maybe_status != status_lib.ClusterStatus.UP:
         # No need to sky.launch again when interactive node is already up.
         sky.launch(
             dag,
             dryrun=dryrun,
             stream_logs=True,
             cluster_name=cluster,
             detach_setup=detach_setup,
             detach_run=detach_run,
             backend=backend,
             idle_minutes_to_autostop=idle_minutes_to_autostop,
             down=down,
             retry_until_up=retry_until_up,
             no_setup=no_setup,
+            clone_disk_from=clone_disk_from,
         )
 
 
 # TODO: skip installing ray to speed up provisioning.
 def _create_and_ssh_into_node(
     node_type: str,
     resources: sky.Resources,
@@ -782,50 +846,68 @@
     assert node_type in _INTERACTIVE_NODE_TYPES, node_type
     assert session_manager in (None, 'screen', 'tmux'), session_manager
     if onprem_utils.check_if_local_cloud(cluster_name):
         raise click.BadParameter(
             f'Name {cluster_name!r} taken by a local cluster and cannot '
             f'be used for a {node_type}.')
 
+    backend = backend if backend is not None else backends.CloudVmRayBackend()
+    if not isinstance(backend, backends.CloudVmRayBackend):
+        raise click.UsageError('Interactive nodes are only supported for '
+                               f'{backends.CloudVmRayBackend.__name__} '
+                               f'backend. Got {type(backend).__name__}.')
+
+    maybe_status, handle = backend_utils.refresh_cluster_status_handle(
+        cluster_name)
+    if maybe_status is not None:
+        if user_requested_resources:
+            if not resources.less_demanding_than(handle.launched_resources):
+                name_arg = ''
+                if cluster_name != _default_interactive_node_name(node_type):
+                    name_arg = f' -c {cluster_name}'
+                raise click.UsageError(
+                    f'Relaunching interactive node {cluster_name!r} with '
+                    'mismatched resources.\n    '
+                    f'Requested resources: {resources}\n    '
+                    f'Launched resources: {handle.launched_resources}\n'
+                    'To login to existing cluster, use '
+                    f'{colorama.Style.BRIGHT}sky {node_type}{name_arg}'
+                    f'{colorama.Style.RESET_ALL}. To launch a new cluster, '
+                    f'use {colorama.Style.BRIGHT}sky {node_type} -c NEW_NAME '
+                    f'{colorama.Style.RESET_ALL}')
+        else:
+            # Use existing interactive node if it exists and no user
+            # resources were specified.
+            resources = handle.launched_resources
+
     # TODO: Add conda environment replication
     # should be setup =
     # 'conda env export | grep -v "^prefix: " > environment.yml'
     # && conda env create -f environment.yml
     task = sky.Task(
         node_type,
         workdir=None,
         setup=None,
     )
     task.set_resources(resources)
 
-    backend = backend if backend is not None else backends.CloudVmRayBackend()
-    maybe_status, _ = backend_utils.refresh_cluster_status_handle(cluster_name)
-    if maybe_status is not None and user_requested_resources:
-        name_arg = ''
-        if cluster_name != _default_interactive_node_name(node_type):
-            name_arg = f' -c {cluster_name}'
-        raise click.UsageError(
-            'Resources cannot be specified for an existing interactive node '
-            f'{cluster_name!r}. To login to the cluster, use: '
-            f'{colorama.Style.BRIGHT}'
-            f'sky {node_type}{name_arg}{colorama.Style.RESET_ALL}')
-
     _launch_with_confirm(
         task,
         backend,
         cluster_name,
         dryrun=False,
         detach_run=True,
         no_confirm=no_confirm,
         idle_minutes_to_autostop=idle_minutes_to_autostop,
         down=down,
         retry_until_up=retry_until_up,
         node_type=node_type,
     )
     handle = global_user_state.get_handle_from_cluster_name(cluster_name)
+    assert isinstance(handle, backends.CloudVmRayResourceHandle), handle
 
     # Use ssh rather than 'ray attach' to suppress ray messages, speed up
     # connection, and for allowing adding 'cd workdir' in the future.
     # Disable check, since the returncode could be non-zero if the user Ctrl-D.
     commands = []
     if session_manager == 'screen':
         commands += ['screen', '-D', '-R']
@@ -849,30 +931,38 @@
     click.secho(f'sky down {cluster_name}', bold=True)
     click.echo('To upload a folder:\t', nl=False)
     click.secho(f'rsync -rP /local/path {cluster_name}:/remote/path', bold=True)
     click.echo('To download a folder:\t', nl=False)
     click.secho(f'rsync -rP {cluster_name}:/remote/path /local/path', bold=True)
 
 
-def _check_yaml(entrypoint: str) -> Tuple[bool, dict]:
+def _check_yaml(entrypoint: str) -> Tuple[bool, Optional[Dict[str, Any]]]:
     """Checks if entrypoint is a readable YAML file.
 
     Args:
         entrypoint: Path to a YAML file.
     """
     is_yaml = True
-    config = None
+    config: Optional[List[Dict[str, Any]]] = None
+    result = None
     shell_splits = shlex.split(entrypoint)
-    yaml_file_provided = len(shell_splits) == 1 and \
-        (shell_splits[0].endswith('yaml') or shell_splits[0].endswith('.yml'))
+    yaml_file_provided = (len(shell_splits) == 1 and
+                          (shell_splits[0].endswith('yaml') or
+                           shell_splits[0].endswith('.yml')))
     try:
         with open(entrypoint, 'r') as f:
             try:
-                config = yaml.safe_load(f)
-                if isinstance(config, str):
+                config = list(yaml.safe_load_all(f))
+                if config:
+                    # FIXME(zongheng): in a chain DAG YAML it only returns the
+                    # first section. OK for downstream but is weird.
+                    result = config[0]
+                else:
+                    result = {}
+                if isinstance(result, str):
                     # 'sky exec cluster ./my_script.sh'
                     is_yaml = False
             except yaml.YAMLError as e:
                 if yaml_file_provided:
                     logger.debug(e)
                     invalid_reason = ('contains an invalid configuration. '
                                       ' Please check syntax.')
@@ -892,38 +982,48 @@
         is_yaml = False
     if not is_yaml:
         if yaml_file_provided:
             click.confirm(
                 f'{entrypoint!r} looks like a yaml path but {invalid_reason}\n'
                 'It will be treated as a command to be run remotely. Continue?',
                 abort=True)
-    return is_yaml, config
+    return is_yaml, result
 
 
-def _make_task_from_entrypoint_with_overrides(
+def _make_task_or_dag_from_entrypoint_with_overrides(
     entrypoint: List[str],
     *,
     name: Optional[str] = None,
     cluster: Optional[str] = None,
     workdir: Optional[str] = None,
     cloud: Optional[str] = None,
     region: Optional[str] = None,
     zone: Optional[str] = None,
     gpus: Optional[str] = None,
+    cpus: Optional[str] = None,
+    memory: Optional[str] = None,
     instance_type: Optional[str] = None,
     num_nodes: Optional[int] = None,
     use_spot: Optional[bool] = None,
     image_id: Optional[str] = None,
     disk_size: Optional[int] = None,
-    env: List[Dict[str, str]] = None,
+    disk_tier: Optional[str] = None,
+    env: Optional[List[Tuple[str, str]]] = None,
     # spot launch specific
     spot_recovery: Optional[str] = None,
-) -> sky.Task:
+) -> Union[sky.Task, sky.Dag]:
+    """Creates a task or a dag from an entrypoint with overrides.
+
+    Returns:
+        A dag iff the entrypoint is YAML and contains more than 1 task.
+        Otherwise, a task.
+    """
     entrypoint = ' '.join(entrypoint)
     is_yaml, yaml_config = _check_yaml(entrypoint)
+    entrypoint: Optional[str]
     if is_yaml:
         # Treat entrypoint as a yaml.
         click.secho('Task from YAML spec: ', fg='yellow', nl=False)
         click.secho(entrypoint, bold=True)
     else:
         if not entrypoint:
             entrypoint = None
@@ -931,51 +1031,66 @@
             # Treat entrypoint as a bash command.
             click.secho('Task from command: ', fg='yellow', nl=False)
             click.secho(entrypoint, bold=True)
 
     if onprem_utils.check_local_cloud_args(cloud, cluster, yaml_config):
         cloud = 'local'
 
+    override_params = _parse_override_params(cloud=cloud,
+                                             region=region,
+                                             zone=zone,
+                                             gpus=gpus,
+                                             cpus=cpus,
+                                             memory=memory,
+                                             instance_type=instance_type,
+                                             use_spot=use_spot,
+                                             image_id=image_id,
+                                             disk_size=disk_size,
+                                             disk_tier=disk_tier)
+
     if is_yaml:
+        assert entrypoint is not None
         usage_lib.messages.usage.update_user_task_yaml(entrypoint)
-        task = sky.Task.from_yaml(entrypoint)
+        dag = dag_utils.load_chain_dag_from_yaml(entrypoint, env_overrides=env)
+        if len(dag.tasks) > 1:
+            # When the dag has more than 1 task. It is unclear how to
+            # override the params for the dag. So we just ignore the
+            # override params.
+            if override_params:
+                click.secho(
+                    f'WARNING: override params {override_params} are ignored, '
+                    'since the yaml file contains multiple tasks.',
+                    fg='yellow')
+            return dag
+        assert len(dag.tasks) == 1, (
+            f'If you see this, please file an issue; tasks: {dag.tasks}')
+        task = dag.tasks[0]
     else:
         task = sky.Task(name='sky-cmd', run=entrypoint)
         task.set_resources({sky.Resources()})
 
     # Override.
     if workdir is not None:
         task.workdir = workdir
 
-    override_params = _parse_override_params(cloud=cloud,
-                                             region=region,
-                                             zone=zone,
-                                             gpus=gpus,
-                                             instance_type=instance_type,
-                                             use_spot=use_spot,
-                                             image_id=image_id,
-                                             disk_size=disk_size)
     # Spot launch specific.
     if spot_recovery is not None:
-        if spot_recovery.lower() == 'none':
-            override_params['spot_recovery'] = None
-        else:
-            override_params['spot_recovery'] = spot_recovery
+        override_params['spot_recovery'] = spot_recovery
 
     assert len(task.resources) == 1
     old_resources = list(task.resources)[0]
     new_resources = old_resources.copy(**override_params)
 
     task.set_resources({new_resources})
 
     if num_nodes is not None:
         task.num_nodes = num_nodes
     if name is not None:
         task.name = name
-    task.set_envs(env)
+    task.update_envs(env)
     # TODO(wei-lin): move this validation into Python API.
     if new_resources.accelerators is not None:
         acc, _ = list(new_resources.accelerators.items())[0]
         if acc.startswith('tpu-') and task.num_nodes > 1:
             raise ValueError('Multi-node TPU cluster is not supported. '
                              f'Got num_nodes={task.num_nodes}.')
     return task
@@ -1021,16 +1136,19 @@
 
 
 def _add_command_alias_to_group(group, command, name, hidden):
     """Add a alias of a command to a group."""
     new_command = copy.deepcopy(command)
     new_command.hidden = hidden
     new_command.name = name
-    new_command.invoke = _with_deprecation_warning(new_command.invoke,
-                                                   command.name, name)
+
+    orig = f'sky {group.name} {command.name}'
+    alias = f'sky {group.name} {name}'
+    new_command.invoke = _with_deprecation_warning(new_command.invoke, orig,
+                                                   alias)
     group.add_command(new_command, name=name)
 
 
 @click.group(cls=_NaturalOrderGroup, context_settings=_CONTEXT_SETTINGS)
 @click.option('--install-shell-completion',
               type=click.Choice(['bash', 'zsh', 'fish', 'auto']),
               callback=_install_shell_completion,
@@ -1084,20 +1202,42 @@
           'and do not stream execution logs.'))
 @click.option('--docker',
               'backend_name',
               flag_value=backends.LocalDockerBackend.NAME,
               default=False,
               help='If used, runs locally inside a docker container.')
 @_add_click_options(_TASK_OPTIONS + _EXTRA_RESOURCES_OPTIONS)
+@click.option('--cpus',
+              default=None,
+              type=str,
+              required=False,
+              help=('Number of vCPUs each instance must have (e.g., '
+                    '``--cpus=4`` (exactly 4) or ``--cpus=4+`` (at least 4)). '
+                    'This is used to automatically select the instance type.'))
+@click.option(
+    '--memory',
+    default=None,
+    type=str,
+    required=False,
+    help=('Amount of memory each instance must have in GB (e.g., '
+          '``--memory=16`` (exactly 16GB), ``--memory=16+`` (at least 16GB))'))
 @click.option('--disk-size',
               default=None,
               type=int,
               required=False,
               help=('OS disk size in GBs.'))
 @click.option(
+    '--disk-tier',
+    default=None,
+    type=click.Choice(['low', 'medium', 'high'], case_sensitive=False),
+    required=False,
+    help=(
+        'OS disk tier. Could be one of "low", "medium", "high". Default: medium'
+    ))
+@click.option(
     '--idle-minutes-to-autostop',
     '-i',
     default=None,
     type=int,
     required=False,
     help=('Automatically stop the cluster after this many minutes '
           'of idleness, i.e., no running or pending jobs in the cluster\'s job '
@@ -1134,92 +1274,123 @@
               required=False,
               help='Skip confirmation prompt.')
 @click.option('--no-setup',
               is_flag=True,
               default=False,
               required=False,
               help='Skip setup phase when (re-)launching cluster.')
+@click.option(
+    '--clone-disk-from',
+    '--clone',
+    default=None,
+    type=str,
+    **_get_shell_complete_args(_complete_cluster_name),
+    help=('[Experimental] Clone disk from an existing cluster to launch '
+          'a new one. This is useful when the new cluster needs to have '
+          'the same data on the boot disk as an existing cluster.'))
 @usage_lib.entrypoint
 def launch(
-    entrypoint: str,
+    entrypoint: List[str],
     cluster: Optional[str],
     dryrun: bool,
     detach_setup: bool,
     detach_run: bool,
     backend_name: Optional[str],
     name: Optional[str],
     workdir: Optional[str],
     cloud: Optional[str],
     region: Optional[str],
     zone: Optional[str],
     gpus: Optional[str],
+    cpus: Optional[str],
+    memory: Optional[str],
     instance_type: Optional[str],
     num_nodes: Optional[int],
     use_spot: Optional[bool],
     image_id: Optional[str],
-    env: List[Dict[str, str]],
+    env: List[Tuple[str, str]],
     disk_size: Optional[int],
+    disk_tier: Optional[str],
     idle_minutes_to_autostop: Optional[int],
     down: bool,  # pylint: disable=redefined-outer-name
     retry_until_up: bool,
     yes: bool,
     no_setup: bool,
+    clone_disk_from: Optional[str],
 ):
     # NOTE(dev): Keep the docstring consistent between the Python API and CLI.
     """Launch a task from a YAML or a command (rerun setup if cluster exists).
 
     If ENTRYPOINT points to a valid YAML file, it is read in as the task
     specification. Otherwise, it is interpreted as a bash command.
 
     In both cases, the commands are run under the task's workdir (if specified)
     and they undergo job queue scheduling.
     """
     backend_utils.check_cluster_name_not_reserved(
-        cluster, operation_str='Launching task on it')
+        cluster, operation_str='Launching tasks on it')
     if backend_name is None:
         backend_name = backends.CloudVmRayBackend.NAME
 
-    task = _make_task_from_entrypoint_with_overrides(
+    # A basic check. Programmatic calls will have a proper (but less
+    # informative) error from optimizer.
+    if (cloud is not None and cloud.lower() == 'azure' and
+            use_spot is not None and use_spot):
+        raise click.UsageError(
+            'SkyPilot currently has not implemented '
+            'support for spot instances on Azure. Please file '
+            'an issue if you need this feature.')
+
+    task_or_dag = _make_task_or_dag_from_entrypoint_with_overrides(
         entrypoint=entrypoint,
         name=name,
         cluster=cluster,
         workdir=workdir,
         cloud=cloud,
         region=region,
         zone=zone,
         gpus=gpus,
+        cpus=cpus,
+        memory=memory,
         instance_type=instance_type,
         num_nodes=num_nodes,
         use_spot=use_spot,
         image_id=image_id,
         env=env,
         disk_size=disk_size,
+        disk_tier=disk_tier,
     )
+    if isinstance(task_or_dag, sky.Dag):
+        raise click.UsageError(
+            'YAML specifies a DAG which is only supported by '
+            '`sky spot launch`. `sky launch` supports a '
+            'single task only.')
+    task = task_or_dag
 
+    backend: backends.Backend
     if backend_name == backends.LocalDockerBackend.NAME:
         backend = backends.LocalDockerBackend()
     elif backend_name == backends.CloudVmRayBackend.NAME:
         backend = backends.CloudVmRayBackend()
     else:
         with ux_utils.print_exception_no_traceback():
             raise ValueError(f'{backend_name} backend is not supported.')
 
-    _launch_with_confirm(
-        task,
-        backend,
-        cluster,
-        dryrun=dryrun,
-        detach_setup=detach_setup,
-        detach_run=detach_run,
-        no_confirm=yes,
-        idle_minutes_to_autostop=idle_minutes_to_autostop,
-        down=down,
-        retry_until_up=retry_until_up,
-        no_setup=no_setup,
-        is_local_cloud=onprem_utils.check_if_local_cloud(cluster))
+    _launch_with_confirm(task,
+                         backend,
+                         cluster,
+                         dryrun=dryrun,
+                         detach_setup=detach_setup,
+                         detach_run=detach_run,
+                         no_confirm=yes,
+                         idle_minutes_to_autostop=idle_minutes_to_autostop,
+                         down=down,
+                         retry_until_up=retry_until_up,
+                         no_setup=no_setup,
+                         clone_disk_from=clone_disk_from)
 
 
 @cli.command(cls=_DocumentedCodeCommand)
 @click.argument('cluster',
                 required=True,
                 type=str,
                 **_get_shell_complete_args(_complete_cluster_name))
@@ -1236,62 +1407,63 @@
     help=('If True, as soon as a job is submitted, return from this call '
           'and do not stream execution logs.'))
 @_add_click_options(_TASK_OPTIONS + _EXTRA_RESOURCES_OPTIONS)
 @usage_lib.entrypoint
 # pylint: disable=redefined-builtin
 def exec(
     cluster: str,
-    entrypoint: str,
+    entrypoint: List[str],
     detach_run: bool,
     name: Optional[str],
     cloud: Optional[str],
     region: Optional[str],
     zone: Optional[str],
     workdir: Optional[str],
     gpus: Optional[str],
     instance_type: Optional[str],
     num_nodes: Optional[int],
     use_spot: Optional[bool],
     image_id: Optional[str],
-    env: List[Dict[str, str]],
+    env: List[Tuple[str, str]],
 ):
     # NOTE(dev): Keep the docstring consistent between the Python API and CLI.
     """Execute a task or a command on a cluster (skip setup).
 
     If ENTRYPOINT points to a valid YAML file, it is read in as the task
     specification. Otherwise, it is interpreted as a bash command.
 
-    \b
     Actions performed by ``sky exec``:
 
-    \b
-    (1) workdir syncing, if:
-      - ENTRYPOINT is a YAML and ``workdir`` is specified inside; or
-      - ENTRYPOINT is a command and flag ``--workdir=<local_path>`` is set.
-    (2) executing the specified task's ``run`` commands / the bash command.
+    1. Workdir syncing, if:
+
+       - ENTRYPOINT is a YAML with the ``workdir`` field specified; or
+
+       - Flag ``--workdir=<local_dir>`` is set.
+
+    2. Executing the specified task's ``run`` commands / the bash command.
 
     ``sky exec`` is thus typically faster than ``sky launch``, provided a
     cluster already exists.
 
     All setup steps (provisioning, setup commands, file mounts syncing) are
     skipped.  If any of those specifications changed, this command will not
     reflect those changes.  To ensure a cluster's setup is up to date, use ``sky
     launch`` instead.
 
-    \b
     Execution and scheduling behavior:
 
-    \b
     - The task/command will undergo job queue scheduling, respecting any
       specified resource requirement. It can be executed on any node of the
       cluster with enough resources.
+
     - The task/command is run under the workdir (if specified).
+
     - The task/command is run non-interactively (without a pseudo-terminal or
-      pty), so interactive commands such as ``htop`` do not work.
-      Use ``ssh my_cluster`` instead.
+      pty), so interactive commands such as ``htop`` do not work. Use ``ssh
+      my_cluster`` instead.
 
     Typical workflow:
 
     .. code-block:: bash
 
       # First command: set up the cluster once.
       sky launch -c mycluster app.yaml
@@ -1313,113 +1485,349 @@
     """
     backend_utils.check_cluster_name_not_reserved(
         cluster, operation_str='Executing task on it')
     handle = global_user_state.get_handle_from_cluster_name(cluster)
     if handle is None:
         if onprem_utils.check_if_local_cloud(cluster):
             raise click.BadParameter(
-                constants.UNINITIALIZED_CLUSTER_MESSAGE.format(cluster=cluster))
+                constants.UNINITIALIZED_ONPREM_CLUSTER_MESSAGE.format(
+                    cluster=cluster))
         raise click.BadParameter(f'Cluster {cluster!r} not found. '
                                  'Use `sky launch` to provision first.')
     backend = backend_utils.get_backend_from_handle(handle)
 
-    task = _make_task_from_entrypoint_with_overrides(
+    task_or_dag = _make_task_or_dag_from_entrypoint_with_overrides(
         entrypoint=entrypoint,
         name=name,
         cluster=cluster,
         workdir=workdir,
         cloud=cloud,
         region=region,
         zone=zone,
         gpus=gpus,
+        cpus=None,
+        memory=None,
         instance_type=instance_type,
         use_spot=use_spot,
         image_id=image_id,
         num_nodes=num_nodes,
         env=env,
     )
 
+    if isinstance(task_or_dag, sky.Dag):
+        raise click.UsageError('YAML specifies a DAG, while `sky exec` '
+                               'supports a single task only.')
+    task = task_or_dag
+
     click.secho(f'Executing task on cluster {cluster}...', fg='yellow')
     sky.exec(task, backend=backend, cluster_name=cluster, detach_run=detach_run)
 
 
+def _get_spot_jobs(
+        refresh: bool,
+        skip_finished: bool,
+        show_all: bool,
+        limit_num_jobs_to_show: bool = False,
+        is_called_by_user: bool = False) -> Tuple[Optional[int], str]:
+    """Get the in-progress spot jobs.
+
+    Args:
+        refresh: Query the latest statuses, restarting the spot controller if
+            stopped.
+        skip_finished: Show only in-progress jobs.
+        show_all: Show all information of each spot job (e.g., region, price).
+        limit_num_jobs_to_show: If True, limit the number of jobs to show to
+            _NUM_SPOT_JOBS_TO_SHOW_IN_STATUS, which is mainly used by
+            `sky status`.
+        is_called_by_user: If this function is called by user directly, or an
+            internal call.
+
+    Returns:
+        A tuple of (num_in_progress_jobs, msg). If num_in_progress_jobs is None,
+        it means there is an error when querying the spot jobs. In this case,
+        msg contains the error message. Otherwise, msg contains the formatted
+        spot job table.
+    """
+    num_in_progress_jobs = None
+    try:
+        if not is_called_by_user:
+            usage_lib.messages.usage.set_internal()
+        with sky_logging.silent():
+            # Make the call silent
+            spot_jobs = core.spot_queue(refresh=refresh,
+                                        skip_finished=skip_finished)
+        num_in_progress_jobs = len(spot_jobs)
+    except exceptions.ClusterNotUpError as e:
+        controller_status = e.cluster_status
+        if controller_status == status_lib.ClusterStatus.INIT:
+            msg = ('Controller\'s latest status is INIT; jobs '
+                   'will not be shown until it becomes UP.')
+        else:
+            assert controller_status in [None, status_lib.ClusterStatus.STOPPED]
+            msg = 'No in progress jobs.'
+            if controller_status is None:
+                msg += (f' (See: {colorama.Style.BRIGHT}sky spot -h'
+                        f'{colorama.Style.RESET_ALL})')
+    except RuntimeError as e:
+        msg = ('Failed to query spot jobs due to connection '
+               'issues. Try again later. '
+               f'Details: {common_utils.format_exception(e, use_bracket=True)}')
+    except Exception as e:  # pylint: disable=broad-except
+        msg = ('Failed to query spot jobs: '
+               f'{common_utils.format_exception(e, use_bracket=True)}')
+    else:
+        max_jobs_to_show = (_NUM_SPOT_JOBS_TO_SHOW_IN_STATUS
+                            if limit_num_jobs_to_show else None)
+        msg = spot_lib.format_job_table(spot_jobs,
+                                        show_all=show_all,
+                                        max_jobs=max_jobs_to_show)
+    return num_in_progress_jobs, msg
+
+
 @cli.command()
 @click.option('--all',
               '-a',
               default=False,
               is_flag=True,
               required=False,
               help='Show all information in full.')
 @click.option(
     '--refresh',
     '-r',
     default=False,
     is_flag=True,
     required=False,
     help='Query the latest cluster statuses from the cloud provider(s).')
+@click.option('--show-spot-jobs/--no-show-spot-jobs',
+              default=True,
+              is_flag=True,
+              required=False,
+              help='Also show recent in-progress spot jobs, if any.')
+@click.argument('clusters',
+                required=False,
+                type=str,
+                nargs=-1,
+                **_get_shell_complete_args(_complete_cluster_name))
 @usage_lib.entrypoint
-def status(all: bool, refresh: bool):  # pylint: disable=redefined-builtin
+# pylint: disable=redefined-builtin
+def status(all: bool, refresh: bool, show_spot_jobs: bool, clusters: List[str]):
     # NOTE(dev): Keep the docstring consistent between the Python API and CLI.
     """Show clusters.
 
+    If CLUSTERS is given, show those clusters. Otherwise, show all clusters.
+
     The following fields for each cluster are recorded: cluster name, time
     since last launch, resources, region, zone, hourly price, status, autostop,
     command.
 
     Display all fields using ``sky status -a``.
 
-    \b
     Each cluster can have one of the following statuses:
 
-    \b
-    - INIT: The cluster may be live or down. It can happen in following cases:
-      (1) undergoing provisioning or runtime setup. (In other words, a
-      ``sky launch`` has started but has not completed.)
-      (2) Or, the cluster is in an abnormal state, e.g., some cluster nodes are
-      down, or the sky runtime has crashed.
-    - UP: Provisioning and runtime setup have succeeded and the cluster is
+    - ``INIT``: The cluster may be live or down. It can happen in the following
+      cases:
+
+      - Ongoing provisioning or runtime setup. (A ``sky launch`` has started
+        but has not completed.)
+
+      - Or, the cluster is in an abnormal state, e.g., some cluster nodes are
+        down, or the SkyPilot runtime is unhealthy. (To recover the cluster,
+        try ``sky launch`` again on it.)
+
+    - ``UP``: Provisioning and runtime setup have succeeded and the cluster is
       live.  (The most recent ``sky launch`` has completed successfully.)
-    - STOPPED: The cluster is stopped and the storage is persisted. Use
+
+    - ``STOPPED``: The cluster is stopped and the storage is persisted. Use
       ``sky start`` to restart the cluster.
 
-    The autostop column indicates how long the cluster will be autostopped
-    after minutes of idling (no jobs running). If the time is followed by
-    '(down)', e.g. '1m (down)', the cluster will be autodowned, rather than
-    autostopped.
+    Autostop column:
+
+    - Indicates after how many minutes of idleness (no in-progress jobs) the
+      cluster will be autostopped. '-' means disabled.
+
+    - If the time is followed by '(down)', e.g., '1m (down)', the cluster will
+      be autodowned, rather than autostopped.
+
+    Getting up-to-date cluster statuses:
+
+    - In normal cases where clusters are entirely managed by SkyPilot (i.e., no
+      manual operations in cloud consoles) and no autostopping is used, the
+      table returned by this command will accurately reflect the cluster
+      statuses.
+
+    - In cases where clusters are changed outside of SkyPilot (e.g., manual
+      operations in cloud consoles; unmanaged spot clusters getting preempted)
+      or for autostop-enabled clusters, use ``--refresh`` to query the latest
+      cluster statuses from the cloud providers.
     """
-    cluster_records = core.status(refresh=refresh)
+    # Using a pool with 1 worker to run the spot job query in parallel to speed
+    # up. The pool provides a AsyncResult object that can be used as a future.
+    with multiprocessing.Pool(1) as pool:
+        # Do not show spot queue if user specifies clusters.
+        show_spot_jobs = show_spot_jobs and not clusters
+        if show_spot_jobs:
+            # Run the spot job query in parallel to speed up the status query.
+            spot_jobs_future = pool.apply_async(
+                _get_spot_jobs,
+                kwds=dict(refresh=False,
+                          skip_finished=True,
+                          show_all=False,
+                          limit_num_jobs_to_show=not all,
+                          is_called_by_user=False))
+        click.echo(f'{colorama.Fore.CYAN}{colorama.Style.BRIGHT}Clusters'
+                   f'{colorama.Style.RESET_ALL}')
+        query_clusters: Optional[List[str]] = None
+        if clusters:
+            query_clusters = _get_glob_clusters(clusters)
+        cluster_records = core.status(cluster_names=query_clusters,
+                                      refresh=refresh)
+        nonreserved_cluster_records = []
+        reserved_clusters = []
+        for cluster_record in cluster_records:
+            cluster_name = cluster_record['name']
+            if cluster_name in backend_utils.SKY_RESERVED_CLUSTER_NAMES:
+                reserved_clusters.append(cluster_record)
+            else:
+                nonreserved_cluster_records.append(cluster_record)
+        local_clusters = onprem_utils.check_and_get_local_clusters(
+            suppress_error=True)
+
+        num_pending_autostop = 0
+        num_pending_autostop += status_utils.show_status_table(
+            nonreserved_cluster_records + reserved_clusters, all)
+        status_utils.show_local_status_table(local_clusters)
+
+        hints = []
+        if show_spot_jobs:
+            click.echo(f'\n{colorama.Fore.CYAN}{colorama.Style.BRIGHT}'
+                       f'Managed spot jobs{colorama.Style.RESET_ALL}')
+            with log_utils.safe_rich_status('[cyan]Checking spot jobs[/]'):
+                try:
+                    num_in_progress_jobs, msg = spot_jobs_future.get()
+                except KeyboardInterrupt:
+                    pool.terminate()
+                    # Set to -1, so that the controller is not considered
+                    # down, and the hint for showing sky spot queue
+                    # will still be shown.
+                    num_in_progress_jobs = -1
+                    msg = 'KeyboardInterrupt'
+
+                try:
+                    pool.close()
+                    pool.join()
+                except SystemExit as e:
+                    # This is to avoid a "Exception ignored" problem caused by
+                    # ray worker setting the sigterm handler to sys.exit(15)
+                    # (see ray/_private/worker.py).
+                    # TODO (zhwu): Remove any importing of ray in SkyPilot.
+                    if e.code != 15:
+                        raise
+
+            click.echo(msg)
+            if num_in_progress_jobs is not None:
+                # spot controller is UP.
+                job_info = ''
+                if num_in_progress_jobs > 0:
+                    plural_and_verb = ' is'
+                    if num_in_progress_jobs > 1:
+                        plural_and_verb = 's are'
+                    job_info = (
+                        f'{num_in_progress_jobs} spot job{plural_and_verb} '
+                        'in progress')
+                    if num_in_progress_jobs > _NUM_SPOT_JOBS_TO_SHOW_IN_STATUS:
+                        job_info += (
+                            f' ({_NUM_SPOT_JOBS_TO_SHOW_IN_STATUS} latest ones '
+                            'shown)')
+                    job_info += '. '
+                hints.append(
+                    f'* {job_info}To see all spot jobs: {colorama.Style.BRIGHT}'
+                    f'sky spot queue{colorama.Style.RESET_ALL}')
+
+        if num_pending_autostop > 0 and not refresh:
+            # Don't print this hint if there's no pending autostop or user has
+            # already passed --refresh.
+            plural_and_verb = ' has'
+            if num_pending_autostop > 1:
+                plural_and_verb = 's have'
+            hints.append(f'* {num_pending_autostop} cluster{plural_and_verb} '
+                         'auto{stop,down} scheduled. Refresh statuses with: '
+                         f'{colorama.Style.BRIGHT}sky status --refresh'
+                         f'{colorama.Style.RESET_ALL}')
+        if hints:
+            click.echo('\n' + '\n'.join(hints))
+
+
+@cli.command()
+@click.option('--all',
+              '-a',
+              default=False,
+              is_flag=True,
+              required=False,
+              help='Show all information in full.')
+@usage_lib.entrypoint
+def cost_report(all: bool):  # pylint: disable=redefined-builtin
+    # NOTE(dev): Keep the docstring consistent between the Python API and CLI.
+    """Show estimated costs for launched clusters.
+
+    For each cluster, this shows: cluster name, resources, launched time,
+    duration that cluster was up, and total estimated cost.
+
+    The estimated cost column indicates the price for the cluster based on the
+    type of resources being used and the duration of use up until now. This
+    means if the cluster is UP, successive calls to cost-report will show
+    increasing price.
+
+    This CLI is experimental. The estimated cost is calculated based on the
+    local cache of the cluster status, and may not be accurate for:
+
+    - Clusters with autostop/use_spot set; or
+
+    - Clusters that were terminated/stopped on the cloud console.
+    """
+    cluster_records = core.cost_report()
+
     nonreserved_cluster_records = []
     reserved_clusters = dict()
     for cluster_record in cluster_records:
         cluster_name = cluster_record['name']
         if cluster_name in backend_utils.SKY_RESERVED_CLUSTER_NAMES:
             cluster_group_name = backend_utils.SKY_RESERVED_CLUSTER_NAMES[
                 cluster_name]
-            reserved_clusters[cluster_group_name] = cluster_record
+            # to display most recent entry for each reserved cluster
+            # TODO(sgurram): fix assumption of sorted order of clusters
+            if cluster_group_name not in reserved_clusters:
+                reserved_clusters[cluster_group_name] = cluster_record
         else:
             nonreserved_cluster_records.append(cluster_record)
-    local_clusters = onprem_utils.check_and_get_local_clusters(
-        suppress_error=True)
 
-    num_pending_autostop = 0
-    num_pending_autostop += status_utils.show_status_table(
+    total_cost = status_utils.get_total_cost_of_displayed_records(
         nonreserved_cluster_records, all)
+
+    status_utils.show_cost_report_table(nonreserved_cluster_records, all)
     for cluster_group_name, cluster_record in reserved_clusters.items():
-        num_pending_autostop += status_utils.show_status_table(
+        status_utils.show_cost_report_table(
             [cluster_record], all, reserved_group_name=cluster_group_name)
-    if num_pending_autostop > 0:
-        plural = ' has'
-        if num_pending_autostop > 1:
-            plural = 's have'
-        click.echo('\n'
-                   f'{num_pending_autostop} cluster{plural} '
-                   'auto{stop,down} scheduled. Refresh statuses with: '
-                   f'{colorama.Style.BRIGHT}sky status --refresh'
-                   f'{colorama.Style.RESET_ALL}')
-    status_utils.show_local_status_table(local_clusters)
+        total_cost += cluster_record['total_cost']
+
+    click.echo(f'\n{colorama.Style.BRIGHT}'
+               f'Total Cost: ${total_cost:.2f}{colorama.Style.RESET_ALL}')
+
+    if not all:
+        click.secho(
+            f'Showing up to {status_utils.NUM_COST_REPORT_LINES} '
+            'most recent clusters. '
+            'To see all clusters in history, '
+            'pass the --all flag.',
+            fg='yellow')
+
+    click.secho(
+        'This feature is experimental. '
+        'Costs for clusters with auto{stop,down} '
+        'scheduled may not be accurate.',
+        fg='yellow')
 
 
 @cli.command()
 @click.option('--all-users',
               '-a',
               default=False,
               is_flag=True,
@@ -1433,15 +1841,15 @@
               help='Show only pending/running jobs\' information.')
 @click.argument('clusters',
                 required=False,
                 type=str,
                 nargs=-1,
                 **_get_shell_complete_args(_complete_cluster_name))
 @usage_lib.entrypoint
-def queue(clusters: Tuple[str], skip_finished: bool, all_users: bool):
+def queue(clusters: List[str], skip_finished: bool, all_users: bool):
     # NOTE(dev): Keep the docstring consistent between the Python API and CLI.
     """Show the job queue for cluster(s)."""
     click.secho('Fetching and parsing job queue...', fg='yellow')
     show_local_clusters = False
     if clusters:
         clusters = _get_glob_clusters(clusters)
     else:
@@ -1449,20 +1857,23 @@
         cluster_infos = global_user_state.get_clusters()
         clusters = [c['name'] for c in cluster_infos]
 
     unsupported_clusters = []
     for cluster in clusters:
         try:
             job_table = core.queue(cluster, skip_finished, all_users)
-        except exceptions.NotSupportedError as e:
-            unsupported_clusters.append(cluster)
-            click.echo(str(e))
-            continue
-        except (RuntimeError, exceptions.ClusterNotUpError) as e:
-            click.echo(str(e))
+        except (RuntimeError, exceptions.NotSupportedError,
+                exceptions.ClusterNotUpError, exceptions.CloudUserIdentityError,
+                exceptions.ClusterOwnerIdentityMismatchError) as e:
+            if isinstance(e, exceptions.NotSupportedError):
+                unsupported_clusters.append(cluster)
+            click.echo(f'{colorama.Fore.YELLOW}Failed to get the job queue for '
+                       f'cluster {cluster!r}.{colorama.Style.RESET_ALL}\n'
+                       f'  {common_utils.class_fullname(e.__class__)}: '
+                       f'{common_utils.remove_color(str(e))}')
             continue
         job_table = job_lib.format_job_queue(job_table)
         click.echo(f'\nJob queue of cluster {cluster}\n{job_table}')
 
     local_clusters = onprem_utils.check_and_get_local_clusters()
     for local_cluster in local_clusters:
         if local_cluster not in clusters and show_local_clusters:
@@ -1480,28 +1891,29 @@
 
 @cli.command()
 @click.option(
     '--sync-down',
     '-s',
     is_flag=True,
     default=False,
-    help='Sync down the logs of the job (this is useful for distributed jobs to'
-    'download a separate log for each job from all the workers).')
+    help='Sync down the logs of a job to the local machine. For a distributed'
+    ' job, a separate log file from each worker will be downloaded.')
 @click.option(
     '--status',
     is_flag=True,
     default=False,
     help=('If specified, do not show logs but exit with a status code for the '
           'job\'s status: 0 for succeeded, or 1 for all other statuses.'))
 @click.option(
     '--follow/--no-follow',
     is_flag=True,
     default=True,
-    help=('Follow the logs of the job. [default: --follow] '
-          'If --no-follow is specified, print the log so far and exit.'))
+    help=('Follow the logs of a job. '
+          'If --no-follow is specified, print the log so far and exit. '
+          '[default: --follow]'))
 @click.argument('cluster',
                 required=True,
                 type=str,
                 **_get_shell_complete_args(_complete_cluster_name))
 @click.argument('job_ids', type=str, nargs=-1)
 # TODO(zhwu): support logs by job name
 @usage_lib.entrypoint
@@ -1515,30 +1927,32 @@
     # NOTE(dev): Keep the docstring consistent between the Python API and CLI.
     """Tail the log of a job.
 
     If JOB_ID is not provided, the latest job on the cluster will be used.
 
     1. If no flags are provided, tail the logs of the job_id specified. At most
     one job_id can be provided.
-    2. If --status is specified, print the status of the job and exit with
-    returncode 0 if the job is succeeded, or 1 otherwise. At most one job_id can
+
+    2. If ``--status`` is specified, print the status of the job and exit with
+    returncode 0 if the job succeeded, or 1 otherwise. At most one job_id can
     be specified.
-    3. If --sync-down is specified, the logs of the job will be downloaded from
-    the cluster and saved to the local machine under `~/sky_logs`. Mulitple
-    job_ids can be specified.
+
+    3. If ``--sync-down`` is specified, the logs of the job will be downloaded
+    from the cluster and saved to the local machine under
+    ``~/sky_logs``. Mulitple job_ids can be specified.
     """
     if sync_down and status:
         raise click.UsageError(
             'Both --sync_down and --status are specified '
             '(ambiguous). To fix: specify at most one of them.')
 
     if len(job_ids) > 1 and not sync_down:
         raise click.UsageError(
-            f'Cannot stream logs of multiple jobs {job_ids}. '
-            'Set --sync-down to download them.')
+            f'Cannot stream logs of multiple jobs (IDs: {", ".join(job_ids)}).'
+            '\nPass -s/--sync-down to download the logs instead.')
 
     job_ids = None if not job_ids else job_ids
 
     if sync_down:
         core.download_logs(cluster, job_ids)
         return
 
@@ -1573,24 +1987,67 @@
                 **_get_shell_complete_args(_complete_cluster_name))
 @click.option('--all',
               '-a',
               default=False,
               is_flag=True,
               required=False,
               help='Cancel all jobs on the specified cluster.')
+@click.option('--yes',
+              '-y',
+              is_flag=True,
+              default=False,
+              required=False,
+              help='Skip confirmation prompt.')
 @click.argument('jobs', required=False, type=int, nargs=-1)
 @usage_lib.entrypoint
-def cancel(cluster: str, all: bool, jobs: List[int]):  # pylint: disable=redefined-builtin
+def cancel(cluster: str, all: bool, jobs: List[int], yes: bool):  # pylint: disable=redefined-builtin
     # NOTE(dev): Keep the docstring consistent between the Python API and CLI.
     """Cancel job(s)."""
+    bold = colorama.Style.BRIGHT
+    reset = colorama.Style.RESET_ALL
+    if not jobs and not all:
+        # Friendly message for usage like 'sky cancel 1' / 'sky cancel myclus'.
+        message = textwrap.dedent(f"""\
+          Use:
+            {bold}sky cancel <cluster_name> <job IDs>{reset}   -- cancel one or more jobs on a cluster
+            {bold}sky cancel <cluster_name> -a / --all{reset}  -- cancel all jobs on a cluster
+
+          Job IDs can be looked up by {bold}sky queue{reset}.""")
+        raise click.UsageError(message)
+
+    if not yes:
+        job_ids = ' '.join(map(str, jobs))
+        plural = 's' if len(job_ids) > 1 else ''
+        job_identity_str = f'job{plural} {job_ids}'
+        if all:
+            job_identity_str = 'all jobs'
+        job_identity_str += f' on cluster {cluster!r}'
+        click.confirm(f'Cancelling {job_identity_str}. Proceed?',
+                      default=True,
+                      abort=True,
+                      show_default=True)
+
     try:
         core.cancel(cluster, all, jobs)
+    except exceptions.NotSupportedError:
+        # Friendly message for usage like 'sky cancel <spot controller> -a/<job
+        # id>'.
+        if all:
+            arg_str = '--all'
+        else:
+            arg_str = ' '.join(map(str, jobs))
+        error_str = ('Cancelling the spot controller\'s jobs is not allowed.'
+                     f'\nTo cancel spot jobs, use: sky spot cancel <spot '
+                     f'job IDs> [--all]'
+                     f'\nDo you mean: {bold}sky spot cancel {arg_str}{reset}')
+        click.echo(error_str)
+        sys.exit(1)
     except ValueError as e:
         raise click.UsageError(str(e))
-    except (exceptions.NotSupportedError, exceptions.ClusterNotUpError) as e:
+    except exceptions.ClusterNotUpError as e:
         click.echo(str(e))
         sys.exit(1)
 
 
 @cli.command(cls=_DocumentedCodeCommand)
 @click.argument('clusters',
                 nargs=-1,
@@ -1605,15 +2062,15 @@
               '-y',
               is_flag=True,
               default=False,
               required=False,
               help='Skip confirmation prompt.')
 @usage_lib.entrypoint
 def stop(
-    clusters: Tuple[str],
+    clusters: List[str],
     all: Optional[bool],  # pylint: disable=redefined-builtin
     yes: bool,
 ):
     # NOTE(dev): Keep the docstring consistent between the Python API and CLI.
     """Stop cluster(s).
 
     CLUSTER is the name (or glob pattern) of the cluster to stop.  If both
@@ -1659,22 +2116,23 @@
               is_flag=True,
               help='Apply this command to all existing clusters.')
 @click.option('--idle-minutes',
               '-i',
               type=int,
               default=None,
               required=False,
-              help='Set the idle minutes before autostopping the cluster.')
+              help=('Set the idle minutes before autostopping the cluster. '
+                    'See the doc above for detailed semantics.'))
 @click.option(
     '--cancel',
     default=False,
     is_flag=True,
     required=False,
-    help='Cancel the currently active auto{stop,down} setting for the '
-    'cluster.')
+    help='Cancel any currently active auto{stop,down} setting for the '
+    'cluster. No-op if there is no active setting.')
 @click.option(
     '--down',
     default=False,
     is_flag=True,
     required=False,
     help='Use autodown (tear down the cluster; non-restartable), instead '
     'of autostop (restartable).')
@@ -1682,54 +2140,62 @@
               '-y',
               is_flag=True,
               default=False,
               required=False,
               help='Skip confirmation prompt.')
 @usage_lib.entrypoint
 def autostop(
-    clusters: Tuple[str],
+    clusters: List[str],
     all: Optional[bool],  # pylint: disable=redefined-builtin
     idle_minutes: Optional[int],
     cancel: bool,  # pylint: disable=redefined-outer-name
     down: bool,  # pylint: disable=redefined-outer-name
     yes: bool,
 ):
     # NOTE(dev): Keep the docstring consistent between the Python API and CLI.
-    """Schedule or cancel an autostop or autodown for cluster(s).
+    """Schedule an autostop or autodown for cluster(s).
+
+    Autostop/autodown will automatically stop or teardown a cluster when it
+    becomes idle for a specified duration.  Idleness means there are no
+    in-progress (pending/running) jobs in a cluster's job queue.
 
-    CLUSTERS are the name (or glob pattern) of the clusters to stop.  If both
+    CLUSTERS are the names (or glob patterns) of the clusters to stop. If both
     CLUSTERS and ``--all`` are supplied, the latter takes precedence.
 
-    If --down is passed, autodown (tear down the cluster; non-restartable) is
-    used, rather than autostop (restartable).
+    Idleness time of a cluster is reset to zero, when any of these happens:
 
-    ``--idle-minutes`` is the number of minutes of idleness (no pending/running
-    jobs) after which the cluster will be stopped automatically.
-    Scheduling autostop twice on the same cluster will overwrite the previous
-    autostop schedule.
+    - A job is submitted (``sky launch`` or ``sky exec``).
 
-    ``--cancel`` will cancel the autostopping. If the cluster was not scheduled
-    autostop, this will do nothing to autostop.
+    - The cluster has restarted.
 
-    If ``--idle-minutes`` and ``--cancel`` are not specified, default to 5
-    minutes.
+    - An autostop is set when there is no active setting. (Namely, either
+      there's never any autostop setting set, or the previous autostop setting
+      was canceled.) This is useful for restarting the autostop timer.
 
-    When multiple configurations are specified for the same cluster, e.g. using
-    ``sky autostop`` or ``sky launch -i``, the last one takes precedence.
+    Example: say a cluster without any autostop set has been idle for 1 hour,
+    then an autostop of 30 minutes is set. The cluster will not be immediately
+    autostopped. Instead, the idleness timer only starts counting after the
+    autostop setting was set.
 
-    Examples:
+    When multiple autostop settings are specified for the same cluster, the
+    last setting takes precedence.
+
+    Typical usage:
 
     .. code-block:: bash
 
-        # Set auto stopping for a specific cluster.
+        # Autostop this cluster after 60 minutes of idleness.
         sky autostop cluster_name -i 60
         \b
-        # Cancel auto stopping for a specific cluster.
+        # Cancel autostop for a specific cluster.
         sky autostop cluster_name --cancel
-
+        \b
+        # Since autostop was canceled in the last command, idleness will
+        # restart counting after this command.
+        sky autostop cluster_name -i 60
     """
     if cancel and idle_minutes is not None:
         raise click.UsageError(
             'Only one of --idle-minutes and --cancel should be specified. '
             f'cancel: {cancel}, idle_minutes: {idle_minutes}')
     if cancel:
         idle_minutes = -1
@@ -1776,15 +2242,15 @@
     '--down',
     default=False,
     is_flag=True,
     required=False,
     help=
     ('Autodown the cluster: tear down the cluster after specified minutes of '
      'idle time after all jobs finish (successfully or abnormally). Requires '
-     ' --idle-minutes-to-autostop to be set.'),
+     '--idle-minutes-to-autostop to be set.'),
 )
 @click.option(
     '--retry-until-up',
     '-r',
     default=False,
     is_flag=True,
     required=False,
@@ -1797,15 +2263,15 @@
     is_flag=True,
     required=False,
     help=('Force start the cluster even if it is already UP. Useful for '
           'upgrading the SkyPilot runtime on the cluster.'))
 @usage_lib.entrypoint
 # pylint: disable=redefined-builtin
 def start(
-        clusters: Tuple[str],
+        clusters: List[str],
         all: bool,
         yes: bool,
         idle_minutes_to_autostop: Optional[int],
         down: bool,  # pylint: disable=redefined-outer-name
         retry_until_up: bool,
         force: bool):
     # NOTE(dev): Keep the docstring consistent between the Python API and CLI.
@@ -1891,36 +2357,59 @@
             #
             #    2. It can be an up cluster that failed one of the setup steps.
             #      This way 'sky start' can change its status to UP, enabling
             #      'sky ssh' to debug things (otherwise `sky ssh` will fail an
             #      INIT state cluster due to head_ip not being cached).
             #
             #      This can be replicated by adding `exit 1` to Task.setup.
-            if (not force and
-                    cluster_status == global_user_state.ClusterStatus.UP):
+            if (not force and cluster_status == status_lib.ClusterStatus.UP):
                 # An UP cluster; skipping 'sky start' because:
                 #  1. For a really up cluster, this has no effects (ray up -y
                 #    --no-restart) anyway.
                 #  2. A cluster may show as UP but is manually stopped in the
                 #    UI.  If Azure/GCP: ray autoscaler doesn't support reusing,
                 #    so 'sky start existing' will actually launch a new
                 #    cluster with this name, leaving the original cluster
                 #    zombied (remains as stopped in the cloud's UI).
                 #
                 #    This is dangerous and unwanted behavior!
                 click.echo(f'Cluster {name} already has status UP.')
                 continue
 
             assert force or cluster_status in (
-                global_user_state.ClusterStatus.INIT,
-                global_user_state.ClusterStatus.STOPPED), cluster_status
+                status_lib.ClusterStatus.INIT,
+                status_lib.ClusterStatus.STOPPED), cluster_status
             to_start.append(name)
     if not to_start:
         return
 
+    # Checks for reserved clusters (spot controller).
+    reserved, non_reserved = [], []
+    for name in to_start:
+        if name in backend_utils.SKY_RESERVED_CLUSTER_NAMES:
+            reserved.append(name)
+        else:
+            non_reserved.append(name)
+    if reserved and non_reserved:
+        assert len(reserved) == 1, reserved
+        # Keep this behavior the same as _down_or_stop_clusters().
+        raise click.UsageError(
+            'Starting the spot controller with other cluster(s) '
+            'is currently not supported.\n'
+            'Please start the former independently.')
+    if reserved:
+        assert len(reserved) == 1, reserved
+        bold = backend_utils.BOLD
+        reset_bold = backend_utils.RESET_BOLD
+        if idle_minutes_to_autostop is not None:
+            raise click.UsageError(
+                'Autostop options are currently not allowed when starting the '
+                'spot controller. Use the default autostop settings by directly'
+                f' calling: {bold}sky start {reserved[0]}{reset_bold}')
+
     if not yes:
         cluster_str = 'clusters' if len(to_start) > 1 else 'cluster'
         cluster_list = ', '.join(to_start)
         click.confirm(
             f'Restarting {len(to_start)} {cluster_str}: '
             f'{cluster_list}. Proceed?',
             default=True,
@@ -1930,17 +2419,19 @@
     for name in to_start:
         try:
             core.start(name,
                        idle_minutes_to_autostop,
                        retry_until_up,
                        down=down,
                        force=force)
-        except exceptions.NotSupportedError as e:
+        except (exceptions.NotSupportedError,
+                exceptions.ClusterOwnerIdentityMismatchError) as e:
             click.echo(str(e))
-        click.secho(f'Cluster {name} started.', fg='green')
+        else:
+            click.secho(f'Cluster {name} started.', fg='green')
 
 
 @cli.command(cls=_DocumentedCodeCommand)
 @click.argument('clusters',
                 nargs=-1,
                 required=False,
                 **_get_shell_complete_args(_complete_cluster_name))
@@ -1960,15 +2451,15 @@
               is_flag=True,
               default=False,
               required=False,
               help='Ignore cloud provider errors (if any). '
               'Useful for cleaning up manually deleted cluster(s).')
 @usage_lib.entrypoint
 def down(
-    clusters: Tuple[str],
+    clusters: List[str],
     all: Optional[bool],  # pylint: disable=redefined-builtin
     yes: bool,
     purge: bool,
 ):
     # NOTE(dev): Keep the docstring consistent between the Python API and CLI.
     """Tear down cluster(s).
 
@@ -2003,76 +2494,98 @@
     _down_or_stop_clusters(clusters,
                            apply_to_all=all,
                            down=True,
                            no_confirm=yes,
                            purge=purge)
 
 
-def _hint_for_down_spot_controller(controller_name: str):
+def _hint_or_raise_for_down_spot_controller(controller_name: str):
     # spot_jobs will be empty when the spot cluster is not running.
     cluster_status, _ = backend_utils.refresh_cluster_status_handle(
         controller_name)
     if cluster_status is None:
         click.echo('Managed spot controller has already been torn down.')
         return
 
+    if cluster_status == status_lib.ClusterStatus.INIT:
+        with ux_utils.print_exception_no_traceback():
+            raise exceptions.NotSupportedError(
+                f'{colorama.Fore.RED}Tearing down the spot controller while '
+                'it is in INIT state is not supported (this means a spot '
+                'launch is in progress or the previous launch failed), as we '
+                'cannot '
+                'guarantee that all the spot jobs are finished. Please wait '
+                'until the spot controller is UP or fix it with '
+                f'{colorama.Style.BRIGHT}sky start '
+                f'{spot_lib.SPOT_CONTROLLER_NAME}{colorama.Style.RESET_ALL}.')
     msg = (f'{colorama.Fore.YELLOW}WARNING: Tearing down the managed '
            f'spot controller ({cluster_status.value}). Please be '
            f'aware of the following:{colorama.Style.RESET_ALL}'
            '\n * All logs and status information of the spot '
            'jobs (output of `sky spot queue`) will be lost.')
-    if cluster_status == global_user_state.ClusterStatus.UP:
-        try:
-            spot_jobs = core.spot_queue(refresh=False)
-        except exceptions.ClusterNotUpError:
-            # The spot controller cluster status changed during querying
-            # the spot jobs, use the latest cluster status, so that the
-            # message for INIT and STOPPED states will be correctly
-            # added to the message.
-            cluster_status = backend_utils.refresh_cluster_status_handle(
-                controller_name)
-            spot_jobs = []
+    click.echo(msg)
+    if cluster_status == status_lib.ClusterStatus.UP:
+        with log_utils.safe_rich_status(
+                '[bold cyan]Checking for in-progress spot jobs[/]'):
+            try:
+                spot_jobs = core.spot_queue(refresh=False)
+            except exceptions.ClusterNotUpError:
+                # The spot controller cluster status changed during querying
+                # the spot jobs, use the latest cluster status, so that the
+                # message for INIT and STOPPED states will be correctly
+                # added to the message.
+                cluster_status = backend_utils.refresh_cluster_status_handle(
+                    controller_name)
+                spot_jobs = []
 
         # Find in-progress spot jobs, and hint users to cancel them.
         non_terminal_jobs = [
             job for job in spot_jobs if not job['status'].is_terminal()
         ]
-        if (cluster_status == global_user_state.ClusterStatus.UP and
+        if (cluster_status == status_lib.ClusterStatus.UP and
                 non_terminal_jobs):
-            msg += ('\n * In-progress spot jobs found, their resources '
-                    'will not be terminated and require manual cleanup:\n')
             job_table = spot_lib.format_job_table(non_terminal_jobs,
                                                   show_all=False)
+            msg = (f'{colorama.Fore.RED}In-progress spot jobs found. '
+                   'To avoid resource leakage, cancel all jobs first: '
+                   f'{colorama.Style.BRIGHT}sky spot cancel -a'
+                   f'{colorama.Style.RESET_ALL}\n')
             # Add prefix to each line to align with the bullet point.
             msg += '\n'.join(
                 ['   ' + line for line in job_table.split('\n') if line != ''])
-    if cluster_status == global_user_state.ClusterStatus.INIT:
-        msg += ('\n * If there are pending/in-progress spot jobs, those '
-                'resources will not be terminated and require manual cleanup.')
-    click.echo(msg)
+            with ux_utils.print_exception_no_traceback():
+                raise exceptions.NotSupportedError(msg)
+        else:
+            click.echo(' * No in-progress spot jobs found. It should be safe '
+                       'to terminate (see caveats above).')
 
 
 def _down_or_stop_clusters(
-        names: Tuple[str],
+        names: List[str],
         apply_to_all: Optional[bool],
         down: bool,  # pylint: disable=redefined-outer-name
         no_confirm: bool,
         purge: bool = False,
         idle_minutes_to_autostop: Optional[int] = None) -> None:
     """Tears down or (auto-)stops a cluster (or all clusters).
 
     Reserved clusters (spot controller) can only be terminated if the cluster
     name is explicitly and uniquely specified (not via glob) and purge is set
     to True.
     """
-    command = 'down' if down else 'stop'
+    if down:
+        command = 'down'
+    elif idle_minutes_to_autostop is not None:
+        command = 'autostop'
+    else:
+        command = 'stop'
     if not names and apply_to_all is None:
         raise click.UsageError(
-            f'sky {command} requires either a cluster name (see `sky status`) '
-            'or --all.')
+            f'`sky {command}` requires either a cluster name (see `sky status`)'
+            ' or --all.')
 
     operation = 'Terminating' if down else 'Stopping'
     if idle_minutes_to_autostop is not None:
         is_cancel = idle_minutes_to_autostop < 0
         verb = 'Cancelling' if is_cancel else 'Scheduling'
         option_str = 'down' if down else 'stop'
         if is_cancel:
@@ -2103,39 +2616,42 @@
         # Make sure the reserved clusters are explicitly specified without other
         # normal clusters.
         if len(reserved_clusters) > 0:
             if len(names) != 0:
                 names_str = ', '.join(map(repr, names))
                 raise click.UsageError(
                     f'{operation} reserved cluster(s) '
-                    f'{reserved_clusters_str} with multiple other cluster(s) '
-                    f'{names_str} is not supported.\n'
+                    f'{reserved_clusters_str} with other cluster(s) '
+                    f'{names_str} is currently not supported.\n'
                     f'Please omit the reserved cluster(s) {reserved_clusters}.')
             if not down:
                 raise click.UsageError(
                     f'{operation} reserved cluster(s) '
-                    f'{reserved_clusters_str} is not supported.')
+                    f'{reserved_clusters_str} is currently not supported.')
             else:
                 # TODO(zhwu): We can only have one reserved cluster (spot
                 # controller).
                 assert len(reserved_clusters) == 1, reserved_clusters
-                _hint_for_down_spot_controller(reserved_clusters[0])
-
-                click.confirm('Proceed?',
-                              default=False,
-                              abort=True,
-                              show_default=True)
+                _hint_or_raise_for_down_spot_controller(reserved_clusters[0])
+                confirm_str = 'delete'
+                user_input = click.prompt(
+                    f'To proceed, please check the warning above and type '
+                    f'{colorama.Style.BRIGHT}{confirm_str!r}'
+                    f'{colorama.Style.RESET_ALL}',
+                    type=str)
+                if user_input != confirm_str:
+                    raise click.Abort()
                 no_confirm = True
         names += reserved_clusters
 
     if apply_to_all:
         all_clusters = global_user_state.get_clusters()
         if len(names) > 0:
             click.echo(
-                f'Both --all and cluster(s) specified for sky {command}. '
+                f'Both --all and cluster(s) specified for `sky {command}`. '
                 'Letting --all take effect.')
         # We should not remove reserved clusters when --all is specified.
         # Otherwise, it would be very easy to accidentally delete a reserved
         # cluster.
         names = [
             record['name']
             for record in all_clusters
@@ -2201,30 +2717,32 @@
                         f'{colorama.Style.RESET_ALL}')
         else:
             try:
                 if down:
                     core.down(name, purge=purge)
                 else:
                     core.stop(name, purge=purge)
-            except RuntimeError:
+            except RuntimeError as e:
                 message = (
                     f'{colorama.Fore.RED}{operation} cluster {name}...failed. '
-                    'Please check the logs and try again.'
-                    f'{colorama.Style.RESET_ALL}')
-            except exceptions.NotSupportedError as e:
+                    f'{colorama.Style.RESET_ALL}'
+                    f'\nReason: {common_utils.format_exception(e)}.')
+            except (exceptions.NotSupportedError,
+                    exceptions.ClusterOwnerIdentityMismatchError) as e:
                 message = str(e)
             else:  # no exception raised
                 message = (
                     f'{colorama.Fore.GREEN}{operation} cluster {name}...done.'
                     f'{colorama.Style.RESET_ALL}')
                 if not down:
                     message += ('\n  To restart the cluster, run: '
                                 f'{colorama.Style.BRIGHT}sky start {name}'
                                 f'{colorama.Style.RESET_ALL}')
                 success_progress = True
+
         progress.stop()
         click.echo(message)
         if success_progress:
             progress.update(task, advance=1)
         progress.start()
 
     with progress:
@@ -2235,19 +2753,20 @@
 
 
 @_interactive_node_cli_command
 @usage_lib.entrypoint
 # pylint: disable=redefined-outer-name
 def gpunode(cluster: str, yes: bool, port_forward: Optional[List[int]],
             cloud: Optional[str], region: Optional[str], zone: Optional[str],
-            instance_type: Optional[str], gpus: Optional[str],
+            instance_type: Optional[str], cpus: Optional[str],
+            memory: Optional[str], gpus: Optional[str],
             use_spot: Optional[bool], screen: Optional[bool],
             tmux: Optional[bool], disk_size: Optional[int],
-            idle_minutes_to_autostop: Optional[int], down: bool,
-            retry_until_up: bool):
+            disk_tier: Optional[str], idle_minutes_to_autostop: Optional[int],
+            down: bool, retry_until_up: bool):
     """Launch or attach to an interactive GPU node.
 
     Examples:
 
     .. code-block:: bash
 
         # Launch a default gpunode.
@@ -2278,30 +2797,34 @@
         session_manager = 'tmux' if tmux else 'screen'
     name = cluster
     if name is None:
         name = _default_interactive_node_name('gpunode')
 
     user_requested_resources = not (cloud is None and region is None and
                                     zone is None and instance_type is None and
+                                    cpus is None and memory is None and
                                     gpus is None and use_spot is None)
     default_resources = _INTERACTIVE_NODE_DEFAULT_RESOURCES['gpunode']
     cloud_provider = clouds.CLOUD_REGISTRY.from_str(cloud)
     if gpus is None and instance_type is None:
         # Use this request if both gpus and instance_type are not specified.
         gpus = default_resources.accelerators
         instance_type = default_resources.instance_type
     if use_spot is None:
         use_spot = default_resources.use_spot
     resources = sky.Resources(cloud=cloud_provider,
                               region=region,
                               zone=zone,
                               instance_type=instance_type,
+                              cpus=cpus,
+                              memory=memory,
                               accelerators=gpus,
                               use_spot=use_spot,
-                              disk_size=disk_size)
+                              disk_size=disk_size,
+                              disk_tier=disk_tier)
 
     _create_and_ssh_into_node(
         'gpunode',
         resources,
         cluster_name=name,
         port_forward=port_forward,
         session_manager=session_manager,
@@ -2314,18 +2837,20 @@
 
 
 @_interactive_node_cli_command
 @usage_lib.entrypoint
 # pylint: disable=redefined-outer-name
 def cpunode(cluster: str, yes: bool, port_forward: Optional[List[int]],
             cloud: Optional[str], region: Optional[str], zone: Optional[str],
-            instance_type: Optional[str], use_spot: Optional[bool],
+            instance_type: Optional[str], cpus: Optional[str],
+            memory: Optional[str], use_spot: Optional[bool],
             screen: Optional[bool], tmux: Optional[bool],
-            disk_size: Optional[int], idle_minutes_to_autostop: Optional[int],
-            down: bool, retry_until_up: bool):
+            disk_size: Optional[int], disk_tier: Optional[str],
+            idle_minutes_to_autostop: Optional[int], down: bool,
+            retry_until_up: bool):
     """Launch or attach to an interactive CPU node.
 
     Examples:
 
     .. code-block:: bash
 
         # Launch a default cpunode.
@@ -2355,27 +2880,31 @@
         session_manager = 'tmux' if tmux else 'screen'
     name = cluster
     if name is None:
         name = _default_interactive_node_name('cpunode')
 
     user_requested_resources = not (cloud is None and region is None and
                                     zone is None and instance_type is None and
+                                    cpus is None and memory is None and
                                     use_spot is None)
     default_resources = _INTERACTIVE_NODE_DEFAULT_RESOURCES['cpunode']
     cloud_provider = clouds.CLOUD_REGISTRY.from_str(cloud)
     if instance_type is None:
         instance_type = default_resources.instance_type
     if use_spot is None:
         use_spot = default_resources.use_spot
     resources = sky.Resources(cloud=cloud_provider,
                               region=region,
                               zone=zone,
                               instance_type=instance_type,
+                              cpus=cpus,
+                              memory=memory,
                               use_spot=use_spot,
-                              disk_size=disk_size)
+                              disk_size=disk_size,
+                              disk_tier=disk_tier)
 
     _create_and_ssh_into_node(
         'cpunode',
         resources,
         cluster_name=name,
         port_forward=port_forward,
         session_manager=session_manager,
@@ -2388,19 +2917,21 @@
 
 
 @_interactive_node_cli_command
 @usage_lib.entrypoint
 # pylint: disable=redefined-outer-name
 def tpunode(cluster: str, yes: bool, port_forward: Optional[List[int]],
             region: Optional[str], zone: Optional[str],
-            instance_type: Optional[str], tpus: Optional[str],
+            instance_type: Optional[str], cpus: Optional[str],
+            memory: Optional[str], tpus: Optional[str],
             use_spot: Optional[bool], tpu_vm: Optional[bool],
             screen: Optional[bool], tmux: Optional[bool],
-            disk_size: Optional[int], idle_minutes_to_autostop: Optional[int],
-            down: bool, retry_until_up: bool):
+            disk_size: Optional[int], disk_tier: Optional[str],
+            idle_minutes_to_autostop: Optional[int], down: bool,
+            retry_until_up: bool):
     """Launch or attach to an interactive TPU node.
 
     Examples:
 
     .. code-block:: bash
 
         # Launch a default tpunode.
@@ -2429,15 +2960,16 @@
     if screen or tmux:
         session_manager = 'tmux' if tmux else 'screen'
     name = cluster
     if name is None:
         name = _default_interactive_node_name('tpunode')
 
     user_requested_resources = not (region is None and zone is None and
-                                    instance_type is None and tpus is None and
+                                    instance_type is None and cpus is None and
+                                    memory is None and tpus is None and
                                     use_spot is None)
     default_resources = _INTERACTIVE_NODE_DEFAULT_RESOURCES['tpunode']
     accelerator_args = default_resources.accelerator_args
     if tpu_vm:
         accelerator_args['tpu_vm'] = True
         accelerator_args['runtime_version'] = 'tpu-vm-base'
     if instance_type is None:
@@ -2446,18 +2978,21 @@
         tpus = default_resources.accelerators
     if use_spot is None:
         use_spot = default_resources.use_spot
     resources = sky.Resources(cloud=sky.GCP(),
                               region=region,
                               zone=zone,
                               instance_type=instance_type,
+                              cpus=cpus,
+                              memory=memory,
                               accelerators=tpus,
                               accelerator_args=accelerator_args,
                               use_spot=use_spot,
-                              disk_size=disk_size)
+                              disk_size=disk_size,
+                              disk_tier=disk_tier)
 
     _create_and_ssh_into_node(
         'tpunode',
         resources,
         cluster_name=name,
         port_forward=port_forward,
         session_manager=session_manager,
@@ -2466,73 +3001,114 @@
         idle_minutes_to_autostop=idle_minutes_to_autostop,
         down=down,
         retry_until_up=retry_until_up,
     )
 
 
 @cli.command()
+@click.option('--verbose',
+              '-v',
+              is_flag=True,
+              default=False,
+              help='Show the activated account for each cloud.')
 @usage_lib.entrypoint
-def check():
-    """Determine the set of clouds available to use.
+def check(verbose: bool):
+    """Check which clouds are available to use.
+
+    This checks access credentials for all clouds supported by SkyPilot. If a
+    cloud is detected to be inaccessible, the reason and correction steps will
+    be shown.
 
-    This checks access credentials for AWS, Azure and GCP; on failure, it shows
-    the reason and suggests correction steps. Tasks will only run on clouds
-    that you have access to.
+    The enabled clouds are cached and form the "search space" to be considered
+    for each task.
     """
-    sky_check.check()
+    sky_check.check(verbose=verbose)
 
 
 @cli.command()
-@click.argument('gpu_name', required=False)
+@click.argument('accelerator_str', required=False)
 @click.option('--all',
               '-a',
               is_flag=True,
               default=False,
               help='Show details of all GPU/TPU/accelerator offerings.')
 @click.option('--cloud',
               default=None,
               type=str,
               help='Cloud provider to query.')
+@click.option(
+    '--region',
+    required=False,
+    type=str,
+    help=
+    ('The region to use. If not specified, shows accelerators from all regions.'
+    ),
+)
+@service_catalog.use_default_catalog
 @usage_lib.entrypoint
-def show_gpus(gpu_name: Optional[str], all: bool, cloud: Optional[str]):  # pylint: disable=redefined-builtin
-    """Show supported GPU/TPU/accelerators.
+def show_gpus(
+        accelerator_str: Optional[str],
+        all: bool,  # pylint: disable=redefined-builtin
+        cloud: Optional[str],
+        region: Optional[str]):
+    """Show supported GPU/TPU/accelerators and their prices.
 
     The names and counts shown can be set in the ``accelerators`` field in task
     YAMLs, or in the ``--gpus`` flag in CLI commands. For example, if this
     table shows 8x V100s are supported, then the string ``V100:8`` will be
     accepted by the above.
 
-    To show the detailed information of a GPU/TPU type (which clouds offer it,
-    the quantity in each VM type, etc.), use ``sky show-gpus <gpu>``.
+    To show the detailed information of a GPU/TPU type (its price, which clouds
+    offer it, the quantity in each VM type, etc.), use ``sky show-gpus <gpu>``.
 
     To show all accelerators, including less common ones and their detailed
     information, use ``sky show-gpus --all``.
 
-    NOTE: The price displayed for each instance type is the lowest across all
-    regions for both on-demand and spot instances.
+    Definitions of certain fields:
+
+    * ``DEVICE_MEM``: Memory of a single device; does not depend on the device
+      count of the instance (VM).
+
+    * ``HOST_MEM``: Memory of the host instance (VM).
+
+    If ``--region`` is not specified, the price displayed for each instance
+    type is the lowest across all regions for both on-demand and spot
+    instances. There may be multiple regions with the same lowest price.
     """
+    # validation for the --region flag
+    if region is not None and cloud is None:
+        raise click.UsageError(
+            'The --region flag is only valid when the --cloud flag is set.')
+    # This will validate 'cloud' and raise if not found.
+    clouds.CLOUD_REGISTRY.from_str(cloud)
+    service_catalog.validate_region_zone(region, None, clouds=cloud)
     show_all = all
-    if show_all and gpu_name is not None:
+    if show_all and accelerator_str is not None:
         raise click.UsageError('--all is only allowed without a GPU name.')
 
     def _list_to_str(lst):
         return ', '.join([str(e) for e in lst])
 
     def _output():
         gpu_table = log_utils.create_table(
-            ['NVIDIA_GPU', 'AVAILABLE_QUANTITIES'])
+            ['COMMON_GPU', 'AVAILABLE_QUANTITIES'])
         tpu_table = log_utils.create_table(
             ['GOOGLE_TPU', 'AVAILABLE_QUANTITIES'])
         other_table = log_utils.create_table(
             ['OTHER_GPU', 'AVAILABLE_QUANTITIES'])
 
-        if gpu_name is None:
-            result = service_catalog.list_accelerator_counts(gpus_only=True,
-                                                             clouds=cloud)
-            # NVIDIA GPUs
+        name, quantity = None, None
+
+        if accelerator_str is None:
+            result = service_catalog.list_accelerator_counts(
+                gpus_only=True,
+                clouds=cloud,
+                region_filter=region,
+            )
+            # "Common" GPUs
             for gpu in service_catalog.get_common_gpus():
                 if gpu in result:
                     gpu_table.add_row([gpu, _list_to_str(result.pop(gpu))])
             yield from gpu_table.get_string()
 
             # Google TPUs
             for tpu in service_catalog.get_tpus():
@@ -2546,63 +3122,108 @@
             if show_all:
                 yield '\n\n'
                 for gpu, qty in sorted(result.items()):
                     other_table.add_row([gpu, _list_to_str(qty)])
                 yield from other_table.get_string()
                 yield '\n\n'
             else:
+                yield ('\n\nHint: use -a/--all to see all accelerators '
+                       '(including non-common ones) and pricing.')
                 return
+        else:
+            # Parse accelerator string
+            accelerator_split = accelerator_str.split(':')
+            if len(accelerator_split) > 2:
+                raise click.UsageError(
+                    f'Invalid accelerator string {accelerator_str}. '
+                    'Expected format: <accelerator_name>[:<quantity>].')
+            if len(accelerator_split) == 2:
+                name = accelerator_split[0]
+                # Check if quantity is valid
+                try:
+                    quantity = int(accelerator_split[1])
+                    if quantity <= 0:
+                        raise ValueError(
+                            'Quantity cannot be non-positive integer.')
+                except ValueError as invalid_quantity:
+                    raise click.UsageError(
+                        f'Invalid accelerator quantity {accelerator_split[1]}. '
+                        'Expected a positive integer.') from invalid_quantity
+            else:
+                name, quantity = accelerator_str, None
 
-        # Show detailed accelerator information
         result = service_catalog.list_accelerators(gpus_only=True,
-                                                   name_filter=gpu_name,
+                                                   name_filter=name,
+                                                   quantity_filter=quantity,
+                                                   region_filter=region,
                                                    clouds=cloud)
         if len(result) == 0:
-            yield f'Resources \'{gpu_name}\' not found. '
+            quantity_str = (f' with requested quantity {quantity}'
+                            if quantity else '')
+            yield f'Resources \'{name}\'{quantity_str} not found. '
             yield 'Try \'sky show-gpus --all\' '
             yield 'to show available accelerators.'
             return
 
-        yield '*NOTE*: for most GCP accelerators, '
-        yield 'INSTANCE_TYPE == (attachable) means '
-        yield 'the host VM\'s cost is not included.\n\n'
+        if cloud is None or cloud.lower() == 'gcp':
+            yield '*NOTE*: for most GCP accelerators, '
+            yield 'INSTANCE_TYPE == (attachable) means '
+            yield 'the host VM\'s cost is not included.\n\n'
+
         import pandas as pd  # pylint: disable=import-outside-toplevel
         for i, (gpu, items) in enumerate(result.items()):
-            accelerator_table = log_utils.create_table([
+            accelerator_table_headers = [
                 'GPU',
                 'QTY',
                 'CLOUD',
                 'INSTANCE_TYPE',
+                'DEVICE_MEM',
                 'vCPUs',
-                'HOST_MEMORY',
+                'HOST_MEM',
                 'HOURLY_PRICE',
                 'HOURLY_SPOT_PRICE',
-            ])
+            ]
+            if not show_all:
+                accelerator_table_headers.append('REGION')
+            accelerator_table = log_utils.create_table(
+                accelerator_table_headers)
             for item in items:
                 instance_type_str = item.instance_type if not pd.isna(
                     item.instance_type) else '(attachable)'
                 cpu_count = item.cpu_count
                 if pd.isna(cpu_count):
                     cpu_str = '-'
-                elif isinstance(cpu_count, float):
-                    if cpu_count.is_integer():
+                elif isinstance(cpu_count, (float, int)):
+                    if int(cpu_count) == cpu_count:
                         cpu_str = str(int(cpu_count))
                     else:
                         cpu_str = f'{cpu_count:.1f}'
-                mem_str = f'{item.memory:.0f}GB' if not pd.isna(
+                device_memory_str = (f'{item.device_memory:.0f}GB' if
+                                     not pd.isna(item.device_memory) else '-')
+                host_memory_str = f'{item.memory:.0f}GB' if not pd.isna(
                     item.memory) else '-'
                 price_str = f'$ {item.price:.3f}' if not pd.isna(
                     item.price) else '-'
                 spot_price_str = f'$ {item.spot_price:.3f}' if not pd.isna(
                     item.spot_price) else '-'
-                accelerator_table.add_row([
-                    item.accelerator_name, item.accelerator_count, item.cloud,
-                    instance_type_str, cpu_str, mem_str, price_str,
-                    spot_price_str
-                ])
+                region_str = item.region if not pd.isna(item.region) else '-'
+                accelerator_table_vals = [
+                    item.accelerator_name,
+                    item.accelerator_count,
+                    item.cloud,
+                    instance_type_str,
+                    device_memory_str,
+                    cpu_str,
+                    host_memory_str,
+                    price_str,
+                    spot_price_str,
+                ]
+                if not show_all:
+                    accelerator_table_vals.append(region_str)
+                accelerator_table.add_row(accelerator_table_vals)
 
             if i != 0:
                 yield '\n\n'
             yield from accelerator_table.get_string()
 
     if show_all:
         click.echo_via_pager(_output())
@@ -2610,24 +3231,31 @@
         for out in _output():
             click.echo(out, nl=False)
         click.echo()
 
 
 @cli.group(cls=_NaturalOrderGroup)
 def storage():
-    """Storage related commands."""
+    """SkyPilot Storage CLI."""
     pass
 
 
 @storage.command('ls', cls=_DocumentedCodeCommand)
+@click.option('--all',
+              '-a',
+              default=False,
+              is_flag=True,
+              required=False,
+              help='Show all information in full.')
 @usage_lib.entrypoint
-def storage_ls():
-    """List storage objects created."""
+# pylint: disable=redefined-builtin
+def storage_ls(all: bool):
+    """List storage objects managed by SkyPilot."""
     storages = sky.storage_ls()
-    storage_table = storage_utils.format_storage_table(storages)
+    storage_table = storage_utils.format_storage_table(storages, show_all=all)
     click.echo(storage_table)
 
 
 @storage.command('delete', cls=_DocumentedCodeCommand)
 @click.argument('names',
                 required=False,
                 type=str,
@@ -2636,15 +3264,15 @@
 @click.option('--all',
               '-a',
               default=False,
               is_flag=True,
               required=False,
               help='Delete all storage objects.')
 @usage_lib.entrypoint
-def storage_delete(names: Tuple[str], all: bool):  # pylint: disable=redefined-builtin
+def storage_delete(names: List[str], all: bool):  # pylint: disable=redefined-builtin
     """Delete storage objects.
 
     Examples:
 
     .. code-block:: bash
 
         # Delete two storage objects.
@@ -2661,21 +3289,20 @@
     if all:
         click.echo('Deleting all storage objects.')
         storages = sky.storage_ls()
         names = [s['name'] for s in storages]
     else:
         names = _get_glob_storages(names)
 
-    for name in names:
-        sky.storage_delete(name)
+    subprocess_utils.run_in_parallel(sky.storage_delete, names)
 
 
 @cli.group(cls=_NaturalOrderGroup)
 def admin():
-    """Sky administrator commands for local clusters."""
+    """SkyPilot On-prem administrator CLI."""
     pass
 
 
 @admin.command('deploy', cls=_DocumentedCodeCommand)
 @click.argument('clusterspec_yaml', required=True, type=str, nargs=-1)
 @usage_lib.entrypoint
 def admin_deploy(clusterspec_yaml: str):
@@ -2698,14 +3325,15 @@
     clusterspec_yaml = ' '.join(clusterspec_yaml)
     assert clusterspec_yaml
     is_yaml, yaml_config = _check_yaml(clusterspec_yaml)
     backend_utils.validate_schema(yaml_config, schemas.get_cluster_schema(),
                                   'Invalid cluster YAML: ')
     if not is_yaml:
         raise ValueError('Must specify cluster config')
+    assert yaml_config is not None, (is_yaml, yaml_config)
 
     auth_config = yaml_config['auth']
     ips = yaml_config['cluster']['ips']
     if not isinstance(ips, list):
         ips = [ips]
     local_cluster_name = yaml_config['cluster']['name']
     usage_lib.record_cluster_name_for_current_operation(local_cluster_name)
@@ -2740,98 +3368,103 @@
         local_cluster_name)
     onprem_utils.save_distributable_yaml(yaml_config)
     click.secho(f'Saved in {sanitized_yaml_path} \n', fg='yellow', nl=False)
     click.secho(f'Successfully deployed local cluster {local_cluster_name!r}\n',
                 fg='green')
 
 
-# Managed Spot CLIs
-def _is_spot_controller_up(
-    stopped_message: str,
-) -> Tuple[Optional[global_user_state.ClusterStatus],
-           Optional[backends.Backend.ResourceHandle]]:
-    controller_status, handle = backend_utils.refresh_cluster_status_handle(
-        spot_lib.SPOT_CONTROLLER_NAME, force_refresh=True)
-    if controller_status is None:
-        click.echo('No managed spot job has been run.')
-    elif controller_status != global_user_state.ClusterStatus.UP:
-        msg = (f'Spot controller {spot_lib.SPOT_CONTROLLER_NAME} '
-               f'is {controller_status.value}.')
-        if controller_status == global_user_state.ClusterStatus.STOPPED:
-            msg += f'\n{stopped_message}'
-        if controller_status == global_user_state.ClusterStatus.INIT:
-            msg += '\nPlease wait for the controller to be ready.'
-        click.echo(msg)
-        handle = None
-    return controller_status, handle
-
-
 @cli.group(cls=_NaturalOrderGroup)
 def spot():
-    """Managed spot instances related commands."""
+    """Managed Spot commands (spot instances with auto-recovery)."""
     pass
 
 
 @spot.command('launch', cls=_DocumentedCodeCommand)
 @click.argument('entrypoint',
                 required=True,
                 type=str,
                 nargs=-1,
                 **_get_shell_complete_args(_complete_file_name))
 # TODO(zhwu): Add --dryrun option to test the launch command.
 @_add_click_options(_TASK_OPTIONS + _EXTRA_RESOURCES_OPTIONS)
+@click.option('--cpus',
+              default=None,
+              type=str,
+              required=False,
+              help=('Number of vCPUs each instance must have (e.g., '
+                    '``--cpus=4`` (exactly 4) or ``--cpus=4+`` (at least 4)). '
+                    'This is used to automatically select the instance type.'))
+@click.option(
+    '--memory',
+    default=None,
+    type=str,
+    required=False,
+    help=('Amount of memory each instance must have in GB (e.g., '
+          '``--memory=16`` (exactly 16GB), ``--memory=16+`` (at least 16GB))'))
 @click.option('--spot-recovery',
               default=None,
               type=str,
               help='Spot recovery strategy to use for the managed spot task.')
 @click.option('--disk-size',
               default=None,
               type=int,
               required=False,
               help=('OS disk size in GBs.'))
 @click.option(
+    '--disk-tier',
+    default=None,
+    type=click.Choice(['low', 'medium', 'high'], case_sensitive=False),
+    required=False,
+    help=(
+        'OS disk tier. Could be one of "low", "medium", "high". Default: medium'
+    ))
+@click.option(
     '--detach-run',
     '-d',
     default=False,
     is_flag=True,
     help=('If True, as soon as a job is submitted, return from this call '
           'and do not stream execution logs.'))
 @click.option(
-    '--retry-until-up',
-    '-r',
-    default=False,
+    '--retry-until-up/--no-retry-until-up',
+    '-r/-no-r',
+    default=None,
     is_flag=True,
     required=False,
-    help=('Whether to retry provisioning infinitely until the cluster is up, '
-          'if we fail to launch the cluster on any possible region/cloud due '
-          'to unavailability errors. This applies to launching the the spot '
-          'clusters (both initial and recovery attempts).'))
+    help=('(Default: True; this flag is deprecated and will be removed in a '
+          'future release.) Whether to retry provisioning infinitely until the '
+          'cluster is up, if unavailability errors are encountered. This '
+          'applies to launching the spot clusters (both the initial and any '
+          'recovery attempts), not the spot controller.'))
 @click.option('--yes',
               '-y',
               is_flag=True,
               default=False,
               required=False,
               help='Skip confirmation prompt.')
 @timeline.event
 @usage_lib.entrypoint
 def spot_launch(
-    entrypoint: str,
+    entrypoint: List[str],
     name: Optional[str],
     workdir: Optional[str],
     cloud: Optional[str],
     region: Optional[str],
     zone: Optional[str],
     gpus: Optional[str],
+    cpus: Optional[str],
+    memory: Optional[str],
     instance_type: Optional[str],
     num_nodes: Optional[int],
     use_spot: Optional[bool],
     image_id: Optional[str],
     spot_recovery: Optional[str],
-    env: List[Dict[str, str]],
+    env: List[Tuple[str, str]],
     disk_size: Optional[int],
+    disk_tier: Optional[str],
     detach_run: bool,
     retry_until_up: bool,
     yes: bool,
 ):
     """Launch a managed spot job from a YAML or a command.
 
     If ENTRYPOINT points to a valid YAML file, it is read in as the task
@@ -2842,42 +3475,76 @@
     .. code-block:: bash
 
       # You can use normal task YAMLs.
       sky spot launch task.yaml
 
       sky spot launch 'echo hello!'
     """
-    if name is None:
-        name = backend_utils.generate_cluster_name()
-    else:
-        backend_utils.check_cluster_name_is_valid(name)
-
-    task = _make_task_from_entrypoint_with_overrides(
+    task_or_dag = _make_task_or_dag_from_entrypoint_with_overrides(
         entrypoint,
         name=name,
         workdir=workdir,
         cloud=cloud,
         region=region,
         zone=zone,
         gpus=gpus,
+        cpus=cpus,
+        memory=memory,
         instance_type=instance_type,
         num_nodes=num_nodes,
         use_spot=use_spot,
         image_id=image_id,
         env=env,
         disk_size=disk_size,
+        disk_tier=disk_tier,
         spot_recovery=spot_recovery,
     )
+    # Deprecation.
+    if retry_until_up is not None:
+        flag_str = '--retry-until-up'
+        if not retry_until_up:
+            flag_str = '--no-retry-until-up'
+        click.secho(
+            f'Flag {flag_str} is deprecated and will be removed in a '
+            'future release (managed spot jobs will always be retried). '
+            'Please file an issue if this does not work for you.',
+            fg='yellow')
+    else:
+        retry_until_up = True
+
+    if not isinstance(task_or_dag, sky.Dag):
+        assert isinstance(task_or_dag, sky.Task), task_or_dag
+        with sky.Dag() as dag:
+            dag.add(task_or_dag)
+            dag.name = task_or_dag.name
+    else:
+        dag = task_or_dag
+
+    if name is not None:
+        dag.name = name
+
+    dag_utils.maybe_infer_and_fill_dag_and_task_names(dag)
 
     if not yes:
-        prompt = f'Launching a new spot task {name!r}. Proceed?'
+        prompt = f'Launching a new spot job {dag.name!r}. Proceed?'
         if prompt is not None:
             click.confirm(prompt, default=True, abort=True, show_default=True)
 
-    sky.spot_launch(task,
+    for task in dag.tasks:
+        # We try our best to validate the cluster name before we launch the
+        # task. If the cloud is not specified, this will only validate the
+        # cluster name against the regex, and the cloud-specific validation will
+        # be done by the spot controller when actually launching the spot
+        # cluster.
+        resources = list(task.resources)[0]
+        task_cloud = (resources.cloud
+                      if resources.cloud is not None else clouds.Cloud)
+        task_cloud.check_cluster_name_is_valid(name)
+
+    sky.spot_launch(dag,
                     name,
                     detach_run=detach_run,
                     retry_until_up=retry_until_up)
 
 
 @spot.command('queue', cls=_DocumentedCodeCommand)
 @click.option('--all',
@@ -2890,65 +3557,89 @@
     '--refresh',
     '-r',
     default=False,
     is_flag=True,
     required=False,
     help='Query the latest statuses, restarting the spot controller if stopped.'
 )
+@click.option('--skip-finished',
+              '-s',
+              default=False,
+              is_flag=True,
+              required=False,
+              help='Show only pending/running jobs\' information.')
 @usage_lib.entrypoint
 # pylint: disable=redefined-builtin
-def spot_queue(all: bool, refresh: bool):
+def spot_queue(all: bool, refresh: bool, skip_finished: bool):
     """Show statuses of managed spot jobs.
 
-    \b
     Each spot job can have one of the following statuses:
 
-    \b
-    - SUBMITTED: The job is submitted to the spot controller.
-    - STARTING: The job is starting (starting a spot cluster).
-    - RUNNING: The job is running.
-    - RECOVERING: The spot cluster is recovering from a preemption.
-    - SUCCEEDED: The job succeeded.
-    - FAILED: The job failed due to an error from the job itself.
-    - FAILED_NO_RESOURCES: The job failed due to resources being unavailable
-        after a maximum number of retry attempts.
-    - FAILED_CONTROLLER: The job failed due to an unexpected error in the spot
-        controller.
-    - CANCELLED: The job was cancelled by the user.
-
-    If the job failed, either due to user code or spot unavailability, the error
-    log can be found with ``sky logs sky-spot-controller-<user_hash> job_id``.
-    Please find your exact spot controller name with ``sky status``.
+    - ``PENDING``: Job is waiting for a free slot on the spot controller to be
+      accepted.
+
+    - ``SUBMITTED``: Job is submitted to and accepted by the spot controller.
+
+    - ``STARTING``: Job is starting (provisioning a spot cluster).
+
+    - ``RUNNING``: Job is running.
+
+    - ``RECOVERING``: The spot cluster is recovering from a preemption.
+
+    - ``SUCCEEDED``: Job succeeded.
+
+    - ``CANCELLING``: Job was requested to be cancelled by the user, and the
+      cancellation is in progress.
+
+    - ``CANCELLED``: Job was cancelled by the user.
+
+    - ``FAILED``: Job failed due to an error from the job itself.
+
+    - ``FAILED_SETUP``: Job failed due to an error from the job's ``setup``
+      commands.
+
+    - ``FAILED_PRECHECKS``: Job failed due to an error from our prechecks such
+      as invalid cluster names or an infeasible resource is specified.
+
+    - ``FAILED_NO_RESOURCE``: Job failed due to resources being unavailable
+      after a maximum number of retries.
+
+    - ``FAILED_CONTROLLER``: Job failed due to an unexpected error in the spot
+      controller.
+
+    If the job failed, either due to user code or spot unavailability, the
+    error log can be found with ``sky spot logs --controller``, e.g.:
+
+    .. code-block:: bash
+
+      sky spot logs --controller job_id
+
+    This also shows the logs for provisioning and any preemption and recovery
+    attempts.
 
     (Tip) To fetch job statuses every 60 seconds, use ``watch``:
 
     .. code-block:: bash
 
       watch -n60 sky spot queue
+
     """
     click.secho('Fetching managed spot job statuses...', fg='yellow')
-    try:
-        job_table = core.spot_queue(refresh=refresh)
-    except exceptions.ClusterNotUpError:
-        cache = spot_lib.load_job_table_cache()
-        if cache is not None:
-            readable_time = log_utils.readable_time_duration(cache[0])
-            table_message = (
-                f'\n{colorama.Fore.YELLOW}Cached job status table '
-                f'[last updated: {readable_time}]:{colorama.Style.RESET_ALL}\n'
-                f'{cache[1]}\n')
-        else:
-            table_message = 'No cached job status table found.'
-        click.echo(table_message)
-        return
-    job_table = spot_lib.format_job_table(job_table, all)
-
-    # Dump cache
-    spot_lib.dump_job_table_cache(job_table)
-    click.echo(f'Managed spot jobs:\n{job_table}')
+    with log_utils.safe_rich_status('[cyan]Checking spot jobs[/]'):
+        _, msg = _get_spot_jobs(refresh=refresh,
+                                skip_finished=skip_finished,
+                                show_all=all,
+                                is_called_by_user=True)
+    if not skip_finished:
+        in_progress_only_hint = ''
+    else:
+        in_progress_only_hint = ' (showing in-progress jobs only)'
+    click.echo(f'{colorama.Fore.CYAN}{colorama.Style.BRIGHT}'
+               f'Managed spot jobs{colorama.Style.RESET_ALL}'
+               f'{in_progress_only_hint}\n{msg}')
 
 
 _add_command_alias_to_group(spot, spot_queue, 'status', hidden=True)
 
 
 @spot.command('cancel', cls=_DocumentedCodeCommand)
 @click.option('--name',
@@ -2970,32 +3661,32 @@
               required=False,
               help='Skip confirmation prompt.')
 @usage_lib.entrypoint
 # pylint: disable=redefined-builtin
 def spot_cancel(name: Optional[str], job_ids: Tuple[int], all: bool, yes: bool):
     """Cancel managed spot jobs.
 
-    You can provide either a job name or a list of job ids to be cancelled.
+    You can provide either a job name or a list of job IDs to be cancelled.
     They are exclusive options.
+
     Examples:
 
     .. code-block:: bash
 
-        # Cancel managed spot job with name 'my-job'
-        $ sky spot cancel -n my-job
-
-        # Cancel managed spot jobs with IDs 1, 2, 3
-        $ sky spot cancel 1 2 3
-
+      # Cancel managed spot job with name 'my-job'
+      $ sky spot cancel -n my-job
+      \b
+      # Cancel managed spot jobs with IDs 1, 2, 3
+      $ sky spot cancel 1 2 3
     """
-
-    _, handle = _is_spot_controller_up(
+    _, handle = spot_lib.is_spot_controller_up(
         'All managed spot jobs should have finished.')
     if handle is None:
-        return
+        # Hint messages already printed by the call above.
+        sys.exit(1)
 
     job_id_str = ','.join(map(str, job_ids))
     if sum([len(job_ids) > 0, name is not None, all]) != 1:
         argument_str = f'--job-ids {job_id_str}' if len(job_ids) > 0 else ''
         argument_str += f' --name {name}' if name is not None else ''
         argument_str += ' --all' if all else ''
         raise click.UsageError(
@@ -3023,19 +3714,92 @@
               help='Managed spot job name.')
 @click.option(
     '--follow/--no-follow',
     is_flag=True,
     default=True,
     help=('Follow the logs of the job. [default: --follow] '
           'If --no-follow is specified, print the log so far and exit.'))
+@click.option(
+    '--controller',
+    is_flag=True,
+    default=False,
+    help=('Show the controller logs of this job; useful for debugging '
+          'launching/recoveries, etc.'))
 @click.argument('job_id', required=False, type=int)
 @usage_lib.entrypoint
-def spot_logs(name: Optional[str], job_id: Optional[int], follow: bool):
+def spot_logs(name: Optional[str], job_id: Optional[int], follow: bool,
+              controller: bool):
     """Tail the log of a managed spot job."""
-    core.spot_tail_logs(name=name, job_id=job_id, follow=follow)
+    try:
+        if controller:
+            core.tail_logs(spot_lib.SPOT_CONTROLLER_NAME,
+                           job_id=job_id,
+                           follow=follow)
+        else:
+            core.spot_tail_logs(name=name, job_id=job_id, follow=follow)
+    except exceptions.ClusterNotUpError:
+        # Hint messages already printed by the call above.
+        sys.exit(1)
+
+
+@spot.command('dashboard', cls=_DocumentedCodeCommand)
+@click.option(
+    '--port',
+    '-p',
+    default=None,
+    type=int,
+    required=False,
+    help=('Local port to use for the dashboard. If None, a free port is '
+          'automatically chosen.'))
+@usage_lib.entrypoint
+def spot_dashboard(port: Optional[int]):
+    """Opens a dashboard for spot jobs (needs controller to be UP)."""
+    # TODO(zongheng): ideally, the controller/dashboard server should expose the
+    # API perhaps via REST. Then here we would (1) not have to use SSH to try to
+    # see if the controller is UP first, which is slow; (2) not have to run SSH
+    # port forwarding first (we'd just launch a local dashboard which would make
+    # REST API calls to the controller dashboard server).
+    click.secho('Checking if spot controller is up...', fg='yellow')
+    hint = (
+        'Dashboard is not available if spot controller is not up. Run a spot '
+        'job first.')
+    _, handle = spot_lib.is_spot_controller_up(stopped_message=hint,
+                                               non_existent_message=hint)
+    if handle is None:
+        sys.exit(1)
+    # SSH forward a free local port to remote's dashboard port.
+    remote_port = constants.SPOT_DASHBOARD_REMOTE_PORT
+    if port is None:
+        free_port = common_utils.find_free_port(remote_port)
+    else:
+        free_port = port
+    ssh_command = (f'ssh -qNL {free_port}:localhost:{remote_port} '
+                   f'{spot_lib.SPOT_CONTROLLER_NAME}')
+    click.echo('Forwarding port: ', nl=False)
+    click.secho(f'{ssh_command}', dim=True)
+
+    with subprocess.Popen(ssh_command, shell=True,
+                          start_new_session=True) as ssh_process:
+        time.sleep(3)  # Added delay for ssh_command to initialize.
+        webbrowser.open(f'http://localhost:{free_port}')
+        click.secho(
+            f'Dashboard is now available at: http://127.0.0.1:{free_port}',
+            fg='green')
+        try:
+            ssh_process.wait()
+        except KeyboardInterrupt:
+            # When user presses Ctrl-C in terminal, exits the previous ssh
+            # command so that <free local port> is freed up.
+            try:
+                os.killpg(os.getpgid(ssh_process.pid), signal.SIGTERM)
+            except ProcessLookupError:
+                # This happens if spot controller is auto-stopped.
+                pass
+        finally:
+            click.echo('Exiting.')
 
 
 # ==============================
 # Sky Benchmark CLIs
 # ==============================
 
 
@@ -3075,15 +3839,15 @@
         if not isinstance(candidate, dict):
             raise ValueError('Each resource candidate must be a dictionary.')
     return candidates
 
 
 @cli.group(cls=_NaturalOrderGroup)
 def bench():
-    """Sky Benchmark related commands."""
+    """SkyPilot Benchmark CLI."""
     pass
 
 
 @bench.command('launch', cls=_DocumentedCodeCommand)
 @click.argument('entrypoint',
                 required=True,
                 type=str,
@@ -3102,14 +3866,22 @@
                     'Example values: "T4:4,V100:8" (without blank spaces).'))
 @click.option('--disk-size',
               default=None,
               type=int,
               required=False,
               help=('OS disk size in GBs.'))
 @click.option(
+    '--disk-tier',
+    default=None,
+    type=click.Choice(['low', 'medium', 'high'], case_sensitive=False),
+    required=False,
+    help=(
+        'OS disk tier. Could be one of "low", "medium", "high". Default: medium'
+    ))
+@click.option(
     '--idle-minutes-to-autostop',
     '-i',
     default=None,
     type=int,
     required=False,
     help=('Automatically stop the cluster after this many minutes '
           'of idleness after setup/file_mounts. This is equivalent to '
@@ -3130,16 +3902,17 @@
     cloud: Optional[str],
     region: Optional[str],
     zone: Optional[str],
     gpus: Optional[str],
     num_nodes: Optional[int],
     use_spot: Optional[bool],
     image_id: Optional[str],
-    env: List[Dict[str, str]],
+    env: List[Tuple[str, str]],
     disk_size: Optional[int],
+    disk_tier: Optional[str],
     idle_minutes_to_autostop: Optional[int],
     yes: bool,
 ) -> None:
     """Benchmark a task on different resources.
 
     Example usage: `sky bench launch mytask.yaml -b mytask --gpus V100,T4`
     will benchmark your task on a V100 cluster and a T4 cluster simultaneously.
@@ -3157,14 +3930,15 @@
         raise click.BadParameter('Please specify a task yaml to benchmark.')
 
     is_yaml, config = _check_yaml(entrypoint)
     if not is_yaml:
         raise click.BadParameter(
             'Sky Benchmark does not support command line tasks. '
             'Please provide a YAML file.')
+    assert config is not None, (is_yaml, config)
 
     click.secho('Benchmarking a task from YAML spec: ', fg='yellow', nl=False)
     click.secho(entrypoint, bold=True)
 
     candidates = _get_candidate_configs(entrypoint)
     # Check if the candidate configs are specified in both CLI and YAML.
     if candidates is not None:
@@ -3187,32 +3961,36 @@
                 raise click.BadParameter(f'use_spot {message}')
         if image_id is not None:
             if any('image_id' in candidate for candidate in candidates):
                 raise click.BadParameter(f'image_id {message}')
         if disk_size is not None:
             if any('disk_size' in candidate for candidate in candidates):
                 raise click.BadParameter(f'disk_size {message}')
+        if disk_tier is not None:
+            if any('disk_tier' in candidate for candidate in candidates):
+                raise click.BadParameter(f'disk_tier {message}')
 
     # The user can specify the benchmark candidates in either of the two ways:
     # 1. By specifying resources.candidates in the YAML.
     # 2. By specifying gpu types as a command line argument (--gpus).
+    override_gpu = None
     if gpus is not None:
-        gpus = gpus.split(',')
-        gpus = [gpu.strip() for gpu in gpus]
+        gpu_list = gpus.split(',')
+        gpu_list = [gpu.strip() for gpu in gpu_list]
         if '' in gpus:
             raise click.BadParameter('Remove blanks in --gpus.')
 
-        if len(gpus) == 1:
-            gpus = gpus[0]
+        if len(gpu_list) == 1:
+            override_gpu = gpu_list[0]
         else:
             # If len(gpus) > 1, gpus is intrepreted
             # as a list of benchmark candidates.
             if candidates is None:
-                candidates = [{'accelerators': gpu} for gpu in gpus]
-                gpus = None
+                candidates = [{'accelerators': gpu} for gpu in gpu_list]
+                override_gpu = None
             else:
                 raise ValueError('Provide benchmark candidates in either '
                                  '--gpus or resources.candidates in the YAML.')
     if candidates is None:
         candidates = [{}]
 
     if 'resources' not in config:
@@ -3225,18 +4003,19 @@
     if workdir is not None:
         config['workdir'] = workdir
     if num_nodes is not None:
         config['num_nodes'] = num_nodes
     override_params = _parse_override_params(cloud=cloud,
                                              region=region,
                                              zone=zone,
-                                             gpus=gpus,
+                                             gpus=override_gpu,
                                              use_spot=use_spot,
                                              image_id=image_id,
-                                             disk_size=disk_size)
+                                             disk_size=disk_size,
+                                             disk_tier=disk_tier)
     resources_config.update(override_params)
     if 'cloud' in resources_config:
         cloud = resources_config.pop('cloud')
         if cloud is not None:
             resources_config['cloud'] = str(cloud)
     if 'region' in resources_config:
         if resources_config['region'] is None:
@@ -3260,22 +4039,22 @@
                                              candidate_configs)
     if not yes:
         plural = 's' if len(candidates) > 1 else ''
         prompt = f'Launching {len(candidates)} cluster{plural}. Proceed?'
         click.confirm(prompt, default=True, abort=True, show_default=True)
 
     # Configs that are only accepted by the CLI.
-    commandline_args = {}
+    commandline_args: Dict[str, Any] = {}
     # Set the default idle minutes to autostop as 5, mimicking
     # the serverless execution.
     if idle_minutes_to_autostop is None:
         idle_minutes_to_autostop = 5
     commandline_args['idle-minutes-to-autostop'] = idle_minutes_to_autostop
     if len(env) > 0:
-        commandline_args['env'] = [f'{k}={v}' for k, v in env.items()]
+        commandline_args['env'] = [f'{k}={v}' for k, v in env]
 
     # Launch the benchmarking clusters in detach mode in parallel.
     benchmark_created = benchmark_utils.launch_benchmark_clusters(
         benchmark, clusters, candidate_configs, commandline_args)
 
     # If at least one cluster is created, print the following messages.
     if benchmark_created:
@@ -3509,15 +4288,15 @@
 ) -> None:
     """Tear down all clusters belonging to a benchmark."""
     record = benchmark_state.get_benchmark_from_name(benchmark)
     if record is None:
         raise click.BadParameter(f'Benchmark {benchmark} does not exist.')
 
     clusters = benchmark_state.get_benchmark_clusters(benchmark)
-    to_stop = []
+    to_stop: List[str] = []
     for cluster in clusters:
         if cluster in clusters_to_exclude:
             continue
         if global_user_state.get_cluster_from_name(cluster) is None:
             continue
         to_stop.append(cluster)
 
@@ -3599,14 +4378,15 @@
                        f'{backend_utils.RESET_BOLD} '
                        'before deleting the benchmark report.')
             success = False
         else:
             bucket_name = benchmark_state.get_benchmark_from_name(
                 benchmark)['bucket']
             handle = global_user_state.get_handle_from_storage_name(bucket_name)
+            assert handle is not None, bucket_name
             bucket_type = list(handle.sky_stores.keys())[0]
             benchmark_utils.remove_benchmark_logs(benchmark, bucket_name,
                                                   bucket_type)
             benchmark_state.delete_benchmark(benchmark)
             message = (f'{colorama.Fore.GREEN}Benchmark report for '
                        f'{benchmark} deleted.{colorama.Style.RESET_ALL}')
             success = True
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/cloud_stores.py` & `skypilot-nightly-1.0.0.dev20230713/sky/cloud_stores.py`

 * *Files 18% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 * Better implementation (e.g., fsspec, smart_open, using each cloud's SDK).
 """
 import subprocess
 import urllib.parse
 
 from sky.clouds import gcp
 from sky.data import data_utils
-from sky.adaptors import aws
+from sky.adaptors import aws, cloudflare
 
 
 class CloudStorage:
     """Interface for a cloud object store."""
 
     def is_directory(self, url: str) -> bool:
         """Returns whether 'url' is a directory.
@@ -66,41 +66,39 @@
         return True
 
     def make_sync_dir_command(self, source: str, destination: str) -> str:
         """Downloads using AWS CLI."""
         # AWS Sync by default uses 10 threads to upload files to the bucket.
         # To increase parallelism, modify max_concurrent_requests in your
         # aws config file (Default path: ~/.aws/config).
-        download_via_awscli = f'mkdir -p {destination} && \
-                                aws s3 sync --no-follow-symlinks ' \
-                              f'{source} {destination}'
+        download_via_awscli = ('aws s3 sync --no-follow-symlinks '
+                               f'{source} {destination}')
 
         all_commands = list(self._GET_AWSCLI)
         all_commands.append(download_via_awscli)
         return ' && '.join(all_commands)
 
     def make_sync_file_command(self, source: str, destination: str) -> str:
         """Downloads a file using AWS CLI."""
-        download_via_awscli = (f'mkdir -p {destination} &&'
-                               f'aws s3 cp {source} {destination}')
+        download_via_awscli = f'aws s3 cp {source} {destination}'
 
         all_commands = list(self._GET_AWSCLI)
         all_commands.append(download_via_awscli)
         return ' && '.join(all_commands)
 
 
 class GcsCloudStorage(CloudStorage):
     """Google Cloud Storage."""
 
     # We use gsutil as a basic implementation.  One pro is that its -m
     # multi-threaded download is nice, which frees us from implementing
     # parellel workers on our end.
     # The gsutil command is part of the Google Cloud SDK, and we reuse
     # the installation logic here.
-    _GET_GSUTIL = gcp.GCLOUD_INSTALLATION_COMMAND
+    _GET_GSUTIL = gcp.GOOGLE_SDK_INSTALLATION_COMMAND
 
     _GSUTIL = ('GOOGLE_APPLICATION_CREDENTIALS='
                f'{gcp.DEFAULT_GCP_APPLICATION_CREDENTIAL_PATH} gsutil')
 
     def is_directory(self, url: str) -> bool:
         """Returns whether 'url' is a directory.
         In cloud object stores, a "directory" refers to a regular object whose
@@ -131,34 +129,98 @@
         url = url if url.endswith('/') else (url + '/')
         assert out == url, (out, url)
         return True
 
     def make_sync_dir_command(self, source: str, destination: str) -> str:
         """Downloads a directory using gsutil."""
         download_via_gsutil = (
-            f'{self._GSUTIL} -m rsync -r {source} {destination}')
+            f'{self._GSUTIL} -m rsync -e -r {source} {destination}')
         all_commands = [self._GET_GSUTIL]
         all_commands.append(download_via_gsutil)
         return ' && '.join(all_commands)
 
     def make_sync_file_command(self, source: str, destination: str) -> str:
         """Downloads a file using gsutil."""
         download_via_gsutil = f'{self._GSUTIL} -m cp {source} {destination}'
         all_commands = [self._GET_GSUTIL]
         all_commands.append(download_via_gsutil)
         return ' && '.join(all_commands)
 
 
+class R2CloudStorage(CloudStorage):
+    """Cloudflare Cloud Storage."""
+
+    # List of commands to install AWS CLI
+    _GET_AWSCLI = [
+        'aws --version >/dev/null 2>&1 || pip3 install awscli',
+    ]
+
+    def is_directory(self, url: str) -> bool:
+        """Returns whether R2 'url' is a directory.
+
+        In cloud object stores, a "directory" refers to a regular object whose
+        name is a prefix of other objects.
+        """
+        r2 = cloudflare.resource('s3')
+        bucket_name, path = data_utils.split_r2_path(url)
+        bucket = r2.Bucket(bucket_name)
+
+        num_objects = 0
+        for obj in bucket.objects.filter(Prefix=path):
+            num_objects += 1
+            if obj.key == path:
+                return False
+            # If there are more than 1 object in filter, then it is a directory
+            if num_objects == 3:
+                return True
+
+        # A directory with few or no items
+        return True
+
+    def make_sync_dir_command(self, source: str, destination: str) -> str:
+        """Downloads using AWS CLI."""
+        # AWS Sync by default uses 10 threads to upload files to the bucket.
+        # To increase parallelism, modify max_concurrent_requests in your
+        # aws config file (Default path: ~/.aws/config).
+        endpoint_url = cloudflare.create_endpoint()
+        if 'r2://' in source:
+            source = source.replace('r2://', 's3://')
+        download_via_awscli = ('AWS_SHARED_CREDENTIALS_FILE='
+                               f'{cloudflare.R2_CREDENTIALS_PATH} '
+                               'aws s3 sync --no-follow-symlinks '
+                               f'{source} {destination} '
+                               f'--endpoint {endpoint_url} '
+                               f'--profile={cloudflare.R2_PROFILE_NAME}')
+
+        all_commands = list(self._GET_AWSCLI)
+        all_commands.append(download_via_awscli)
+        return ' && '.join(all_commands)
+
+    def make_sync_file_command(self, source: str, destination: str) -> str:
+        """Downloads a file using AWS CLI."""
+        endpoint_url = cloudflare.create_endpoint()
+        download_via_awscli = ('AWS_SHARED_CREDENTIALS_FILE='
+                               f'{cloudflare.R2_CREDENTIALS_PATH} '
+                               f'aws s3 cp s3://{source} {destination} '
+                               f'--endpoint {endpoint_url} '
+                               f'--profile={cloudflare.R2_PROFILE_NAME}')
+
+        all_commands = list(self._GET_AWSCLI)
+        all_commands.append(download_via_awscli)
+        return ' && '.join(all_commands)
+
+
 def get_storage_from_path(url: str) -> CloudStorage:
     """Returns a CloudStorage by identifying the scheme:// in a URL."""
     result = urllib.parse.urlsplit(url)
 
     if result.scheme not in _REGISTRY:
         assert False, (f'Scheme {result.scheme} not found in'
                        f' supported storage ({_REGISTRY.keys()}); path {url}')
     return _REGISTRY[result.scheme]
 
 
 _REGISTRY = {
     'gs': GcsCloudStorage(),
     's3': S3CloudStorage(),
+    'r2': R2CloudStorage()
 }
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/clouds/aws.py` & `skypilot-nightly-1.0.0.dev20230713/sky/clouds/ibm.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,350 +1,485 @@
-"""Amazon Web Services."""
-
-# pylint: disable=import-outside-toplevel
-
-import json
+"""IBM Web Services."""
 import os
-import subprocess
+import yaml
+import json
 import typing
-from typing import Dict, Iterator, List, Optional, Tuple
+from typing import Any, Dict, Iterator, List, Optional, Tuple
 
 from sky import clouds
-from sky import exceptions
+from sky import sky_logging
+from sky import status_lib
+from sky.adaptors import ibm
+from sky.adaptors.ibm import CREDENTIAL_FILE
 from sky.clouds import service_catalog
+from sky.utils import ux_utils
 
 if typing.TYPE_CHECKING:
     # renaming to avoid shadowing variables
     from sky import resources as resources_lib
 
-# Minimum set of files under ~/.aws that grant AWS access.
-_CREDENTIAL_FILES = [
-    'credentials',
-]
-
-
-def _run_output(cmd):
-    proc = subprocess.run(cmd,
-                          shell=True,
-                          check=True,
-                          stderr=subprocess.PIPE,
-                          stdout=subprocess.PIPE)
-    return proc.stdout.decode('ascii')
+logger = sky_logging.init_logger(__name__)
 
 
 @clouds.CLOUD_REGISTRY.register
-class AWS(clouds.Cloud):
-    """Amazon Web Services."""
+class IBM(clouds.Cloud):
+    """IBM Web Services."""
 
-    _REPR = 'AWS'
+    _REPR = 'IBM'
+    # IBM's VPC instance name length is limited to 63 chars.
+    # since Ray names the instances using the following
+    # format: ray-{cluster_name}-{node_type}-{8-char-uuid}
+    # it leaves 63-3-5-8=47 characters for the cluster name.
+    _MAX_CLUSTER_NAME_LEN_LIMIT = 47
     _regions: List[clouds.Region] = []
 
-    #### Regions/Zones ####
+    @classmethod
+    def _cloud_unsupported_features(
+            cls) -> Dict[clouds.CloudImplementationFeatures, str]:
+        return {
+            clouds.CloudImplementationFeatures.CLONE_DISK_FROM_CLUSTER:
+                (f'Migrating disk is not supported in {cls._REPR}.'),
+        }
 
     @classmethod
-    def regions(cls):
-        if not cls._regions:
-            # https://aws.amazon.com/premiumsupport/knowledge-center/vpc-find-availability-zone-options/
-            cls._regions = [
-                clouds.Region('us-west-1').set_zones([
-                    clouds.Zone('us-west-1a'),
-                    clouds.Zone('us-west-1b'),
-                ]),
-                clouds.Region('us-west-2').set_zones([
-                    clouds.Zone('us-west-2a'),
-                    clouds.Zone('us-west-2b'),
-                    clouds.Zone('us-west-2c'),
-                    clouds.Zone('us-west-2d'),
-                ]),
-                clouds.Region('us-east-2').set_zones([
-                    clouds.Zone('us-east-2a'),
-                    clouds.Zone('us-east-2b'),
-                    clouds.Zone('us-east-2c'),
-                ]),
-                clouds.Region('us-east-1').set_zones([
-                    clouds.Zone('us-east-1a'),
-                    clouds.Zone('us-east-1b'),
-                    clouds.Zone('us-east-1c'),
-                    clouds.Zone('us-east-1d'),
-                    clouds.Zone('us-east-1e'),
-                    clouds.Zone('us-east-1f'),
-                ]),
-            ]
-        return cls._regions
+    def _max_cluster_name_length(cls) -> Optional[int]:
+        return cls._MAX_CLUSTER_NAME_LEN_LIMIT
 
     @classmethod
-    def region_zones_provision_loop(
+    def regions_with_offering(cls, instance_type: str,
+                              accelerators: Optional[Dict[str, int]],
+                              use_spot: bool, region: Optional[str],
+                              zone: Optional[str]) -> List[clouds.Region]:
+        del accelerators  # unused
+        if use_spot:
+            return []
+        regions = service_catalog.get_region_zones_for_instance_type(
+            instance_type, use_spot, 'ibm')
+
+        if region is not None:
+            regions = [r for r in regions if r.name == region]
+        if zone is not None:
+            for r in regions:
+                assert r.zones is not None, r
+                r.set_zones([z for z in r.zones if z.name == zone])
+            regions = [r for r in regions if r.zones]
+        return regions
+
+    @classmethod
+    def zones_provision_loop(
         cls,
         *,
-        instance_type: Optional[str] = None,
+        region: str,
+        num_nodes: int,
+        instance_type: str,
         accelerators: Optional[Dict[str, int]] = None,
-        use_spot: bool,
-    ) -> Iterator[Tuple[clouds.Region, List[clouds.Zone]]]:
-        # AWS provisioner can handle batched requests, so yield all zones under
-        # each region.
-        del accelerators  # unused
-
-        if instance_type is None:
-            # fallback to manually specified region/zones
-            regions = cls.regions()
-        else:
-            regions = service_catalog.get_region_zones_for_instance_type(
-                instance_type, use_spot, 'aws')
-        for region in regions:
-            yield region, region.zones
-
-    @classmethod
-    def get_default_ami(cls, region_name: str, instance_type: str) -> str:
-        acc = cls.get_accelerators_from_instance_type(instance_type)
-        image_id = service_catalog.get_image_id_from_tag(
-            'skypilot:gpu-ubuntu-2004', region_name, clouds='aws')
-        if acc is not None:
-            assert len(acc) == 1, acc
-            acc_name = list(acc.keys())[0]
-            if acc_name == 'K80':
-                image_id = service_catalog.get_image_id_from_tag(
-                    'skypilot:k80-ubuntu-2004', region_name, clouds='aws')
-        if image_id is not None:
-            return image_id
-        # Raise ResourcesUnavailableError to make sure the failover in
-        # CloudVMRayBackend will be correctly triggered.
-        # TODO(zhwu): This is a information leakage to the cloud implementor,
-        # we need to find a better way to handle this.
-        raise exceptions.ResourcesUnavailableError(
-            'No image found in catalog for region '
-            f'{region_name}. Try setting a valid image_id.')
-
-    @classmethod
-    def _get_image_id(cls, region_name: str, instance_type: str,
-                      image_id: Optional[Dict[str, str]]) -> str:
-        if image_id is not None:
-            if None in image_id:
-                image_id = image_id[None]
-            else:
-                assert region_name in image_id, image_id
-                image_id = image_id[region_name]
-            if image_id.startswith('skypilot:'):
-                image_id = service_catalog.get_image_id_from_tag(image_id,
-                                                                 region_name,
-                                                                 clouds='aws')
-                if image_id is None:
-                    # Raise ResourcesUnavailableError to make sure the failover
-                    # in CloudVMRayBackend will be correctly triggered.
-                    # TODO(zhwu): This is a information leakage to the cloud
-                    # implementor, we need to find a better way to handle this.
-                    raise exceptions.ResourcesUnavailableError(
-                        f'No image found for region {region_name}')
-            return image_id
-        return cls.get_default_ami(region_name, instance_type)
+        use_spot: bool = False,
+    ) -> Iterator[Optional[List[clouds.Zone]]]:
+        """Loops over (region, zones) to retry for provisioning.
+
+        returning a single zone list with its region,
+        since ibm cloud currently doesn't
+        support retries for list of zones.
+
+        Args:
+            instance_type: The instance type to provision.
+            accelerators: The accelerators to provision.
+            use_spot: Whether to use spot instances.
+        """
+        # del accelerators  # unused
+        del num_nodes  # unused
+        assert use_spot is False, (
+            'current IBM implementation doesn\'t support spot instances')
+
+        regions = cls.regions_with_offering(instance_type,
+                                            accelerators,
+                                            use_spot,
+                                            region=region,
+                                            zone=None)
+        # IBM provisioner currently takes 1 zone per request.
+        for r in regions:
+            assert r.zones is not None, r
+            for zone in r.zones:
+                yield [zone]
 
     @classmethod
     def get_zone_shell_cmd(cls) -> Optional[str]:
-        # The command for getting the current zone is from:
-        # https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-identity-documents.html  # pylint: disable=line-too-long
-        command_str = (
-            'curl -s http://169.254.169.254/latest/dynamic/instance-identity/document'  # pylint: disable=line-too-long
-            ' | python3 -u -c "import sys, json; '
-            'print(json.load(sys.stdin)[\'availabilityZone\'])"')
-        return command_str
+        return None
 
-    #### Normal methods ####
-
-    def instance_type_to_hourly_cost(self, instance_type: str, use_spot: bool):
+    def instance_type_to_hourly_cost(self,
+                                     instance_type: str,
+                                     use_spot: bool,
+                                     region: Optional[str] = None,
+                                     zone: Optional[str] = None) -> float:
+        # Currently doesn't support spot instances, hence use_spot set to False.
+        del use_spot
         return service_catalog.get_hourly_cost(instance_type,
-                                               region=None,
-                                               use_spot=use_spot,
-                                               clouds='aws')
-
-    def accelerators_to_hourly_cost(self, accelerators,
-                                    use_spot: bool) -> float:
-        # AWS includes accelerators as part of the instance type.  Implementing
-        # this is also necessary for e.g., the instance may have 4 GPUs, while
-        # the task specifies to use 1 GPU.
+                                               use_spot=False,
+                                               region=region,
+                                               zone=zone,
+                                               clouds='ibm')
+
+    def accelerators_to_hourly_cost(self,
+                                    accelerators: Dict[str, int],
+                                    use_spot: bool,
+                                    region: Optional[str] = None,
+                                    zone: Optional[str] = None) -> float:
+        del accelerators, use_spot, region, zone  # unused
+        # Currently Isn't implemented in the same manner by aws and azure.
         return 0
 
-    def get_egress_cost(self, num_gigabytes: float):
-        # In general, query this from the cloud:
-        #   https://aws.amazon.com/s3/pricing/
-        # NOTE: egress from US East (Ohio).
-        # NOTE: Not accurate as the pricing tier is based on cumulative monthly
-        # usage.
-        if num_gigabytes > 150 * 1024:
-            return 0.05 * num_gigabytes
-        cost = 0.0
-        if num_gigabytes >= 50 * 1024:
-            cost += (num_gigabytes - 50 * 1024) * 0.07
-            num_gigabytes -= 50 * 1024
-
-        if num_gigabytes >= 10 * 1024:
-            cost += (num_gigabytes - 10 * 1024) * 0.085
-            num_gigabytes -= 10 * 1024
-
-        if num_gigabytes > 1:
-            cost += (num_gigabytes - 1) * 0.09
-
-        cost += 0.0
+    def get_egress_cost(self, num_gigabytes):
+        """Returns the egress cost. Currently true for us-south, i.e. Dallas.
+        based on https://cloud.ibm.com/objectstorage/create#pricing. """
+        cost = 0
+        price_thresholds = [{
+            'threshold': 150,
+            'price_per_gb': 0.05
+        }, {
+            'threshold': 50,
+            'price_per_gb': 0.07
+        }, {
+            'threshold': 0,
+            'price_per_gb': 0.09
+        }]
+        for price_threshold in price_thresholds:
+            # pylint: disable=line-too-long
+            cost += (num_gigabytes - price_threshold['threshold']
+                    ) * price_threshold['price_per_gb']
+            num_gigabytes -= (num_gigabytes - price_threshold['threshold'])
         return cost
 
-    def is_same_cloud(self, other: clouds.Cloud):
-        return isinstance(other, AWS)
-
-    @classmethod
-    def get_default_instance_type(cls) -> str:
-        # 8 vCpus, 32 GB RAM. 3rd generation Intel Xeon. General Purpose.
-        return 'm6i.2xlarge'
-
-    # TODO: factor the following three methods, as they are the same logic
-    # between Azure and AWS.
-    @classmethod
-    def get_accelerators_from_instance_type(
-        cls,
-        instance_type: str,
-    ) -> Optional[Dict[str, int]]:
-        return service_catalog.get_accelerators_from_instance_type(
-            instance_type, clouds='aws')
-
-    @classmethod
-    def get_vcpus_from_instance_type(
-        cls,
-        instance_type: str,
-    ) -> float:
-        return service_catalog.get_vcpus_from_instance_type(instance_type,
-                                                            clouds='aws')
+    def is_same_cloud(self, other):
+        return isinstance(other, IBM)
 
     def make_deploy_resources_variables(
-            self, resources: 'resources_lib.Resources',
-            region: Optional['clouds.Region'],
-            zones: Optional[List['clouds.Zone']]) -> Dict[str, str]:
-        if region is None:
-            assert zones is None, (
-                'Set either both or neither for: region, zones.')
-            region = self._get_default_region()
-            zones = region.zones
-        else:
-            assert zones is not None, (
-                'Set either both or neither for: region, zones.')
+        self,
+        resources: 'resources_lib.Resources',
+        region: 'clouds.Region',
+        zones: Optional[List['clouds.Zone']],
+    ) -> Dict[str, Optional[str]]:
+        """Converts planned sky.Resources to cloud-specific resource variables.
+
+        These variables are used to fill the node type section (instance type,
+        any accelerators, etc.) in the cloud's deployment YAML template.
+
+        Cloud-agnostic sections (e.g., commands to run) need not be returned by
+        this function.
+
+        Returns:
+          A dictionary of cloud-specific node type variables.
+        """
+
+        def _get_profile_resources(instance_profile):
+            """returns a dict representing the
+             cpu, memory and gpu of specified instance profile"""
+
+            cpu_num, memory_num = self.get_vcpus_mem_from_instance_type(
+                instance_profile)
+            gpu_num = self.get_accelerators_from_instance_type(instance_profile)
+            gpu_num = list(gpu_num.values())[0] if gpu_num else 0
+            return {'CPU': cpu_num, 'memory': memory_num, 'GPU': gpu_num}
 
         region_name = region.name
-        zones = [zone.name for zone in zones]
+        # zones is guaranteed to be initialized for
+        # clouds implementing 'zones_provision_loop()'
+        zone_names = [zone.name for zone in zones]  # type: ignore[union-attr]
 
         r = resources
-        # r.accelerators is cleared but .instance_type encodes the info.
+        assert not r.use_spot, \
+            'IBM does not currently support spot instances in this framework'
+
         acc_dict = self.get_accelerators_from_instance_type(r.instance_type)
         if acc_dict is not None:
             custom_resources = json.dumps(acc_dict, separators=(',', ':'))
         else:
             custom_resources = None
 
-        image_id = self._get_image_id(region_name, r.instance_type, r.image_id)
+        instance_resources = _get_profile_resources(r.instance_type)
+
+        worker_instance_type = get_cred_file_field('worker_instance_type',
+                                                   r.instance_type)
+        worker_instance_resources = _get_profile_resources(worker_instance_type)
+        # r.image_id: {clouds.Region:image_id} - property of Resources class
+        image_id = r.image_id[
+            region.name] if r.image_id else self.get_default_image(region_name)
 
         return {
             'instance_type': r.instance_type,
+            'instance_resources': instance_resources,
+            'worker_instance_type': worker_instance_type,
+            'worker_instance_resources': worker_instance_resources,
             'custom_resources': custom_resources,
             'use_spot': r.use_spot,
             'region': region_name,
-            'zones': ','.join(zones),
+            'zones': ','.join(zone_names),
             'image_id': image_id,
+            'iam_api_key': get_cred_file_field('iam_api_key'),
+            'resource_group_id': get_cred_file_field('resource_group_id'),
+            'disk_capacity': get_cred_file_field('disk_capacity', 100)
         }
 
+    @classmethod
+    def get_vcpus_mem_from_instance_type(
+        cls,
+        instance_type: str,
+    ) -> Tuple[Optional[float], Optional[float]]:
+        return service_catalog.get_vcpus_mem_from_instance_type(instance_type,
+                                                                clouds='ibm')
+
+    @classmethod
+    def get_accelerators_from_instance_type(
+        cls,
+        instance_type: str,
+    ) -> Optional[Dict[str, int]]:
+        """Returns {acc: acc_count} held by 'instance_type', if any."""
+        return service_catalog.get_accelerators_from_instance_type(
+            instance_type, clouds='ibm')
+
+    @classmethod
+    def get_default_instance_type(
+            cls,
+            cpus: Optional[str] = None,
+            memory: Optional[str] = None,
+            disk_tier: Optional[str] = None) -> Optional[str]:
+        return service_catalog.get_default_instance_type(cpus=cpus,
+                                                         memory=memory,
+                                                         disk_tier=disk_tier,
+                                                         clouds='ibm')
+
     def get_feasible_launchable_resources(self,
                                           resources: 'resources_lib.Resources'):
-        fuzzy_candidate_list = []
+        """Returns a list of feasible and launchable resources.
+
+        Feasible resources refer to an offering respecting the resource
+        requirements.  Currently, this function implements "filtering" the
+        cloud's offerings only w.r.t. accelerators constraints.
+
+        Launchable resources require a cloud and an instance type be assigned.
+        """
+        fuzzy_candidate_list: Optional[List[str]] = []
         if resources.instance_type is not None:
             assert resources.is_launchable(), resources
-            # Treat Resources(AWS, p3.2x, V100) as Resources(AWS, p3.2x).
             resources = resources.copy(accelerators=None)
             return ([resources], fuzzy_candidate_list)
 
         def _make(instance_list):
             resource_list = []
             for instance_type in instance_list:
                 r = resources.copy(
-                    cloud=AWS(),
+                    cloud=IBM(),
                     instance_type=instance_type,
-                    # Setting this to None as AWS doesn't separately bill /
+                    # Setting this to None as IBM doesn't separately bill /
                     # attach the accelerators.  Billed as part of the VM type.
                     accelerators=None,
-                )
+                    cpus=None,
+                    memory=None)
                 resource_list.append(r)
             return resource_list
 
         # Currently, handle a filter on accelerators only.
         accelerators = resources.accelerators
         if accelerators is None:
-            # No requirements to filter, so just return a default VM type.
-            return (_make([AWS.get_default_instance_type()]),
-                    fuzzy_candidate_list)
+            # Return a default instance type with the given number of vCPUs.
+            default_instance_type = IBM.get_default_instance_type(
+                cpus=resources.cpus,
+                memory=resources.memory,
+                disk_tier=resources.disk_tier)
+            if default_instance_type is None:
+                return ([], [])
+            else:
+                return (_make([default_instance_type]), [])
 
         assert len(accelerators) == 1, resources
         acc, acc_count = list(accelerators.items())[0]
         (instance_list, fuzzy_candidate_list
-        ) = service_catalog.get_instance_type_for_accelerator(acc,
-                                                              acc_count,
-                                                              clouds='aws')
+        ) = service_catalog.get_instance_type_for_accelerator(
+            acc,
+            acc_count,
+            cpus=resources.cpus,
+            memory=resources.memory,
+            region=resources.region,
+            zone=resources.zone,
+            clouds='ibm')
         if instance_list is None:
             return ([], fuzzy_candidate_list)
         return (_make(instance_list), fuzzy_candidate_list)
 
-    def check_credentials(self) -> Tuple[bool, Optional[str]]:
-        """Checks if the user has access credentials to this cloud."""
+    @classmethod
+    def get_default_image(cls, region) -> str:
+        """
+        Returns default image id, currently stock ubuntu 22-04.
+        if user specified 'image_id' in ~/.ibm/credentials.yaml
+            matching this 'region', returns it instead.
+        """
+
+        def _get_image_objects():
+            # pylint: disable=E1136
+            images = []
+            res = client.list_images().get_result()
+            images.extend(res['images'])
+
+            while res.get('next'):
+                link_to_next = res['next']['href'].split('start=')[1].split(
+                    '&limit')[0]
+                res = client.list_images(start=link_to_next).get_result()
+                images.extend(res['images'])
+            return images
+
+        # if user specified an image id for the region, return it.
+        # IBM-TODO take into account user input when --image-id
+        # is implemented
+        user_image = get_cred_file_field('image_id')
+        if user_image and region in user_image:
+            logger.debug(f'using user image: {user_image[region]} '
+                         f'in region: {region}.')
+            return user_image[region]
+
+        client = ibm.client(region=region)
+        # returns default image: "ibm-ubuntu-22-04" with amd architecture
+        return next((img for img in _get_image_objects() if
+         img['name'].startswith('ibm-ubuntu-22-04') \
+            and img['operating_system']['architecture'].startswith(
+                'amd')))['id']
+
+    def get_image_size(self, image_id: str, region: Optional[str]) -> float:
+        assert region is not None, (image_id, region)
+        client = ibm.client(region=region)
         try:
-            import boto3
-            import botocore
-        except ImportError:
-            raise ImportError('Fail to import dependencies for AWS.'
-                              'Try pip install "skypilot[aws]"') from None
-        help_str = (
-            ' Run the following commands:'
-            '\n      $ pip install boto3'
-            '\n      $ aws configure'
-            '\n    For more info: '
-            'https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html'  # pylint: disable=line-too-long
-        )
-        # This file is required because it will be synced to remote VMs for
-        # `aws` to access private storage buckets.
-        # `aws configure list` does not guarantee this file exists.
-        if not os.path.isfile(os.path.expanduser('~/.aws/credentials')):
-            return (False, '~/.aws/credentials does not exist.' + help_str)
-
-        # Checks if the AWS CLI is installed properly
+            image_data = client.get_image(image_id).get_result()
+        # pylint: disable=line-too-long
+        except ibm.ibm_cloud_sdk_core.ApiException as e:  # type: ignore[union-attr]
+            logger.error(e.message)
+            with ux_utils.print_exception_no_traceback():
+                raise ValueError(f'Image {image_id!r} not found in '
+                                 f'IBM region "{region}"') from None
         try:
-            _run_output('aws configure list')
-        except subprocess.CalledProcessError:
-            return False, (
-                'AWS CLI is not installed properly.'
-                ' Run the following commands under sky folder:'
-                # TODO(zhwu): after we publish sky to PyPI,
-                # change this to `pip install sky[aws]`
-                '\n     $ pip install .[aws]'
-                '\n   Credentials may also need to be set.' + help_str)
-
-        # Checks if AWS credentials 1) exist and 2) are valid.
-        # https://stackoverflow.com/questions/53548737/verify-aws-credentials-with-boto3
-        sts = boto3.client('sts')
+            # image_size['file']['size'] is not relevant, since
+            # the minimum size of a volume onto which this image
+            # may be provisioned is stored in minimum_provisioned_size
+            # pylint: disable=unsubscriptable-object
+            image_size = image_data['minimum_provisioned_size']
+        except KeyError:
+            logger.error('Image missing metadata:"minimum_provisioned_size". '
+                         'Image may be in status: Failed/Pending.')
+            with ux_utils.print_exception_no_traceback():
+                raise ValueError(
+                    f'IBM image {image_id!r} in '
+                    f'region "{region}", is missing'
+                    'metadata:"minimum_provisioned_size". '
+                    'Image may be in status: Failed/Pending') from None
+        return image_size
+
+    @classmethod
+    def check_credentials(cls) -> Tuple[bool, Optional[str]]:
+        """Checks if the user has access credentials to this cloud."""
+        # IBM-TODO - create a configuration script.
+        required_fields = ['iam_api_key', 'resource_group_id']
+        help_str = ('    Store your API key and Resource Group id '
+                    f'in {CREDENTIAL_FILE} in the following format:\n'
+                    '      iam_api_key: <IAM_API_KEY>\n'
+                    '      resource_group_id: <RESOURCE_GROUP_ID>')
+
+        base_config = _read_credential_file()
+        if not base_config:
+            return (False, 'Missing credential file at '
+                    f'{os.path.expanduser(CREDENTIAL_FILE)}.\n' + help_str)
+
+        for field in required_fields:
+            if field not in base_config:
+                return (False, f'Missing field "{field}" in '
+                        f'{os.path.expanduser(CREDENTIAL_FILE)}.\n' + help_str)
+
+        # verifies ability of user to create a client,
+        # e.g. bad API KEY.
         try:
-            sts.get_caller_identity()
-        except botocore.exceptions.NoCredentialsError:
-            return False, 'AWS credentials are not set.' + help_str
-        except botocore.exceptions.ClientError:
-            return False, (
-                'Failed to access AWS services with credentials stored in ~/.aws/credentials.'
-                ' Make sure that the access and secret keys are correct.'
-                ' To reconfigure the credentials, ' + help_str[1].lower() +
-                help_str[2:])
-        return True, None
+            ibm.client()
+            return True, None
+        # pylint: disable=W0703
+        except Exception as e:
+            return (False, f'{str(e)}' + help_str)
+
+    @classmethod
+    def check_disk_tier_enabled(cls, instance_type: str,
+                                disk_tier: str) -> None:
+        del instance_type, disk_tier  # unused
 
     def get_credential_file_mounts(self) -> Dict[str, str]:
-        return {
-            f'~/.aws/{filename}': f'~/.aws/{filename}'
-            for filename in _CREDENTIAL_FILES
-        }
+        """Returns a {remote:local} credential path mapping
+         written to the cluster's file_mounts segment
+         of its yaml file (e.g., ibm-ray.yml.j2)
+        """
+        return {CREDENTIAL_FILE: CREDENTIAL_FILE}
 
     def instance_type_exists(self, instance_type):
-        return service_catalog.instance_type_exists(instance_type, clouds='aws')
+        """Returns whether the instance type exists for this cloud."""
+        return service_catalog.instance_type_exists(instance_type, clouds='ibm')
+
+    def validate_region_zone(self, region: Optional[str], zone: Optional[str]):
+        """Validates the region and zone."""
+        return service_catalog.validate_region_zone(region, zone, clouds='ibm')
 
     def accelerator_in_region_or_zone(self,
                                       accelerator: str,
                                       acc_count: int,
                                       region: Optional[str] = None,
                                       zone: Optional[str] = None) -> bool:
+        """Returns whether the accelerator is valid in the region or zone."""
         return service_catalog.accelerator_in_region_or_zone(
-            accelerator, acc_count, region, zone, 'aws')
+            accelerator, acc_count, region, zone, 'ibm')
+
+    @classmethod
+    def query_status(cls, name: str, tag_filters: Dict[str, str],
+                     region: Optional[str], zone: Optional[str],
+                     **kwargs) -> List['status_lib.ClusterStatus']:
+        del tag_filters, zone, kwargs  # unused
+
+        status_map: Dict[str, Any] = {
+            'pending': status_lib.ClusterStatus.INIT,
+            'starting': status_lib.ClusterStatus.INIT,
+            'restarting': status_lib.ClusterStatus.INIT,
+            'running': status_lib.ClusterStatus.UP,
+            'stopping': status_lib.ClusterStatus.STOPPED,
+            'stopped': status_lib.ClusterStatus.STOPPED,
+            'deleting': None,
+            'failed': status_lib.ClusterStatus.INIT,
+            'cluster_deleted': []
+        }
+
+        client = ibm.client(region=region)
+        search_client = ibm.search_client()
+        # pylint: disable=E1136
+        vpcs_filtered_by_tags_and_region = search_client.search(
+            query=f'type:vpc AND tags:{name} AND region:{region}',
+            fields=['tags', 'region', 'type'],
+            limit=1000).get_result()['items']
+        if not vpcs_filtered_by_tags_and_region:
+            # a vpc could have been removed unkownlingly to skypilot, such as
+            # via `sky autostop --down`, or simply manually (e.g. via console).
+            logger.warning('No vpc exists in '
+                           f'{region} '
+                           f'with tag: {name}')
+            return status_map['cluster_deleted']
+        vpc_id = vpcs_filtered_by_tags_and_region[0]['crn'].rsplit(':', 1)[-1]
+        instances = client.list_instances(
+            vpc_id=vpc_id).get_result()['instances']
+
+        return [status_map[instance['status']] for instance in instances]
+
+
+def _read_credential_file():
+    try:
+        with open(os.path.expanduser(CREDENTIAL_FILE), 'r',
+                  encoding='utf-8') as f:
+            return yaml.safe_load(f)
+    except FileNotFoundError:
+        return False
+
+
+def get_cred_file_field(field, default_val=None) -> str:
+    """returns a the value of a field from the user's
+     credentials file if exists, else default_val"""
+    base_config = _read_credential_file()
+    if not base_config:
+        raise FileNotFoundError('Missing '
+                                f'credential file at {CREDENTIAL_FILE}')
+    return base_config.get(field, default_val)
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/clouds/cloud.py` & `skypilot-nightly-1.0.0.dev20230713/sky/clouds/service_catalog/oci_catalog.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,218 +1,203 @@
-"""Interfaces: clouds, regions, and zones."""
-import collections
-import typing
-from typing import Dict, Iterator, List, Optional, Tuple
+"""OCI Offerings Catalog.
+
+This module loads the service catalog file and can be used to query
+instance types and pricing information for OCI.
 
-from sky.clouds import service_catalog
-from sky.utils import ux_utils
+History:
+ - Hysun He (hysun.he@oracle.com) @ Apr, 2023: Initial implementation
+ - Hysun He (hysun.he@oracle.com) @ Jun, 2023: Reduce retry times by
+   excluding those unsubscribed regions.
+"""
+
+import typing
+import logging
+import threading
+from typing import Dict, List, Optional, Tuple
+from sky.clouds.service_catalog import common
+from sky.skylet.providers.oci.config import oci_conf
+from sky.adaptors import oci as oci_adaptor
 
 if typing.TYPE_CHECKING:
-    from sky import resources
+    from sky.clouds import cloud
+    import pandas as pd
 
+logger = logging.getLogger(__name__)
 
-class Region(collections.namedtuple('Region', ['name'])):
-    """A region."""
-    name: str
-    zones: List['Zone'] = []
-
-    def set_zones(self, zones: List['Zone']):
-        self.zones = zones
-        for zone in self.zones:
-            zone.region = self
-        return self
-
-
-class Zone(collections.namedtuple('Zone', ['name'])):
-    """A zone, typically grouped under a region."""
-    name: str
-    region: Region
-
-
-class _CloudRegistry(dict):
-    """Registry of clouds."""
-
-    def from_str(self, name: Optional[str]) -> Optional['Cloud']:
-        if name is None:
-            return None
-        if name.lower() not in self:
-            with ux_utils.print_exception_no_traceback():
-                raise ValueError(f'Cloud {name} is not a valid cloud among '
-                                 f'{list(self.keys())}')
-        return self.get(name.lower())
-
-    def register(self, cloud_cls: 'Cloud') -> None:
-        name = cloud_cls.__name__.lower()
-        assert name not in self, f'{name} already registered'
-        self[name] = cloud_cls()
-        return cloud_cls
-
-
-CLOUD_REGISTRY = _CloudRegistry()
-
-
-class Cloud:
-    """A cloud provider."""
-
-    _REPR = '<Cloud>'
-
-    #### Regions/Zones ####
-
-    @classmethod
-    def regions(cls) -> List[Region]:
-        raise NotImplementedError
-
-    @classmethod
-    def region_zones_provision_loop(
-        cls,
-        *,
-        instance_type: Optional[str] = None,
-        accelerators: Optional[Dict[str, int]] = None,
-        use_spot: Optional[bool] = False,
-    ) -> Iterator[Tuple[Region, List[Zone]]]:
-        """Loops over (region, zones) to retry for provisioning.
-
-        Certain clouds' provisioners may handle batched requests, retrying for
-        itself a list of zones under a region.  Others may need a specific zone
-        per provision request (in that case, yields (region, a one-element list
-        for each zone)).
-        Optionally, caller can filter the yielded region/zones by specifying the
-        instance_type, accelerators, and use_spot.
-
-        Args:
-            instance_type: The instance type to provision.
-            accelerators: The accelerators to provision.
-            use_spot: Whether to use spot instances.
-
-        Typical usage:
-
-            for region, zones in cloud.region_zones_provision_loop(
-                instance_type,
-                accelerators,
-                use_spot
-            ):
-                success = try_provision(region, zones, resources)
-                if success:
-                    break
-        """
-        raise NotImplementedError
-
-    @classmethod
-    def get_zone_shell_cmd(cls) -> Optional[str]:
-        """Returns the shell command to obtain the zone of instance."""
-        raise NotImplementedError
-
-    #### Normal methods ####
-
-    # TODO: incorporate region/zone into the API.
-    def instance_type_to_hourly_cost(self, instance_type, use_spot):
-        """Returns the hourly on-demand/spot price for an instance type."""
-        raise NotImplementedError
-
-    def accelerators_to_hourly_cost(self, accelerators, use_spot):
-        """Returns the hourly on-demand price for accelerators."""
-        raise NotImplementedError
-
-    def get_egress_cost(self, num_gigabytes):
-        """Returns the egress cost.
-
-        TODO: takes into account "per month" accumulation per account.
-        """
-        raise NotImplementedError
-
-    def is_same_cloud(self, other):
-        raise NotImplementedError
-
-    def make_deploy_resources_variables(
-        self,
-        resources: 'resources.Resources',
-        region: Optional['Region'],
-        zones: Optional[List['Zone']],
-    ) -> Dict[str, str]:
-        """Converts planned sky.Resources to cloud-specific resource variables.
-
-        These variables are used to fill the node type section (instance type,
-        any accelerators, etc.) in the cloud's deployment YAML template.
-
-        Cloud-agnostic sections (e.g., commands to run) need not be returned by
-        this function.
-
-        Returns:
-          A dictionary of cloud-specific node type variables.
-        """
-        raise NotImplementedError
-
-    @classmethod
-    def get_vcpus_from_instance_type(cls,
-                                     instance_type: str) -> Optional[float]:
-        """Returns the number of virtual CPUs that the instance type offers."""
-        raise NotImplementedError
-
-    @classmethod
-    def get_accelerators_from_instance_type(
-        cls,
-        instance_type: str,
-    ) -> Optional[Dict[str, int]]:
-        """Returns {acc: acc_count} held by 'instance_type', if any."""
-        raise NotImplementedError
-
-    @classmethod
-    def get_default_instance_type(cls) -> str:
-        raise NotImplementedError
-
-    @classmethod
-    def _get_default_region(cls) -> Region:
-        raise NotImplementedError
-
-    @classmethod
-    def is_image_tag_valid(cls, image_tag: str, region: Optional[str]) -> bool:
-        """Validates that the image tag is valid for this cloud."""
-        return service_catalog.is_image_tag_valid(image_tag,
-                                                  region,
-                                                  clouds=cls._REPR.lower())
-
-    def get_feasible_launchable_resources(self, resources):
-        """Returns a list of feasible and launchable resources.
-
-        Feasible resources refer to an offering respecting the resource
-        requirements.  Currently, this function implements "filtering" the
-        cloud's offerings only w.r.t. accelerators constraints.
-
-        Launchable resources require a cloud and an instance type be assigned.
-        """
-        raise NotImplementedError
-
-    def check_credentials(self) -> Tuple[bool, Optional[str]]:
-        """Checks if the user has access credentials to this cloud.
-
-        Returns a boolean of whether the user can access this cloud, and a
-        string describing the reason if the user cannot access.
-        """
-        raise NotImplementedError
-
-    def get_credential_file_mounts(self) -> Dict[str, str]:
-        """Returns the files necessary to access this cloud.
-
-        Returns a dictionary that will be added to a task's file mounts.
-        """
-        raise NotImplementedError
-
-    def instance_type_exists(self, instance_type):
-        """Returns whether the instance type exists for this cloud."""
-        raise NotImplementedError
-
-    def validate_region_zone(self, region: Optional[str], zone: Optional[str]):
-        """Validates the region and zone."""
-        return service_catalog.validate_region_zone(region,
-                                                    zone,
-                                                    clouds=self._REPR.lower())
-
-    def accelerator_in_region_or_zone(self,
-                                      accelerator: str,
-                                      acc_count: int,
-                                      region: Optional[str] = None,
-                                      zone: Optional[str] = None) -> bool:
-        """Returns whether the accelerator is valid in the region or zone."""
-        raise NotImplementedError
+_df = None
+_image_df = common.read_catalog('oci/images.csv')
 
-    def __repr__(self):
-        return self._REPR
+_lock = threading.RLock()
+
+
+def _get_df() -> 'pd.DataFrame':
+    with _lock:
+        global _df
+        if _df is not None:
+            return _df
+
+        df = common.read_catalog('oci/vms.csv')
+        try:
+            oci_adaptor.get_oci()
+        except ImportError:
+            _df = df
+            return _df
+
+        try:
+            config_profile = oci_conf.get_profile()
+            client = oci_adaptor.get_identity_client(profile=config_profile)
+
+            subscriptions = client.list_region_subscriptions(
+                tenancy_id=oci_adaptor.get_oci_config(
+                    profile=config_profile)['tenancy']).data
+
+            subscribed_regions = [r.region_name for r in subscriptions]
+
+        except (oci_adaptor.get_oci().exceptions.ConfigFileNotFound,
+                oci_adaptor.get_oci().exceptions.InvalidConfig) as e:
+            # This should only happen in testing where oci config is
+            # missing, because it means the 'sky check' will fail if
+            # enter here (meaning OCI disabled).
+            logger.debug(f'It is OK goes here when testing: {str(e)}')
+            subscribed_regions = []
+
+        except oci_adaptor.service_exception() as e:
+            # Should never expect going here. However, we still catch
+            # it so that if any OCI call failed, the program can still
+            # proceed with try-and-error way.
+            logger.warning(
+                f'Unexpected exception when handle catalog: {str(e)}')
+            subscribed_regions = []
+
+        if subscribed_regions:
+            _df = df[df['Region'].isin(subscribed_regions)]
+        else:
+            _df = df
+
+        return _df
+
+
+def instance_type_exists(instance_type: str) -> bool:
+    return common.instance_type_exists_impl(_get_df(), instance_type)
+
+
+def validate_region_zone(
+        region: Optional[str],
+        zone: Optional[str]) -> Tuple[Optional[str], Optional[str]]:
+    return common.validate_region_zone_impl('oci', _get_df(), region, zone)
+
+
+def accelerator_in_region_or_zone(acc_name: str,
+                                  acc_count: int,
+                                  region: Optional[str] = None,
+                                  zone: Optional[str] = None) -> bool:
+    return common.accelerator_in_region_or_zone_impl(_get_df(), acc_name,
+                                                     acc_count, region, zone)
+
+
+def get_hourly_cost(instance_type: str,
+                    use_spot: bool = False,
+                    region: Optional[str] = None,
+                    zone: Optional[str] = None) -> float:
+    """Returns the cost, or the cheapest cost among all zones for spot."""
+    return common.get_hourly_cost_impl(_get_df(), instance_type, use_spot,
+                                       region, zone)
+
+
+def get_default_instance_type(cpus: Optional[str] = None,
+                              memory: Optional[str] = None,
+                              disk_tier: Optional[str] = None) -> Optional[str]:
+    del disk_tier  # unused
+    if cpus is None:
+        cpus = f'{oci_conf.DEFAULT_NUM_VCPUS}+'
+
+    if memory is None:
+        memory_gb_or_ratio = f'{oci_conf.DEFAULT_MEMORY_CPU_RATIO}x'
+    else:
+        memory_gb_or_ratio = memory
+
+    instance_type_prefix = tuple(
+        f'{family}' for family in oci_conf.DEFAULT_INSTANCE_FAMILY)
+
+    df = _get_df()
+    df = df[df['InstanceType'].notna()]
+    df = df[df['InstanceType'].str.startswith(instance_type_prefix)]
+
+    logger.debug(f'# get_default_instance_type: {df}')
+    return common.get_instance_type_for_cpus_mem_impl(df, cpus,
+                                                      memory_gb_or_ratio)
+
+
+def get_accelerators_from_instance_type(
+        instance_type: str) -> Optional[Dict[str, int]]:
+    return common.get_accelerators_from_instance_type_impl(
+        _get_df(), instance_type)
+
+
+def get_instance_type_for_accelerator(
+    acc_name: str,
+    acc_count: int,
+    cpus: Optional[str] = None,
+    memory: Optional[str] = None,
+    use_spot: bool = False,
+    region: Optional[str] = None,
+    zone: Optional[str] = None,
+) -> Tuple[Optional[List[str]], List[str]]:
+    """
+    Returns a list of instance types satisfying the required count of
+    accelerators with sorted prices and a list of candidates with fuzzy search.
+    """
+    return common.get_instance_type_for_accelerator_impl(df=_get_df(),
+                                                         acc_name=acc_name,
+                                                         acc_count=acc_count,
+                                                         cpus=cpus,
+                                                         memory=memory,
+                                                         use_spot=use_spot,
+                                                         region=region,
+                                                         zone=zone)
+
+
+def get_region_zones_for_instance_type(instance_type: str,
+                                       use_spot: bool) -> List['cloud.Region']:
+    df = _get_df()
+    df = df[df['InstanceType'] == instance_type]
+    return common.get_region_zones(df, use_spot)
+
+
+def list_accelerators(
+        gpus_only: bool,
+        name_filter: Optional[str],
+        region_filter: Optional[str],
+        quantity_filter: Optional[int],
+        case_sensitive: bool = True
+) -> Dict[str, List[common.InstanceTypeInfo]]:
+    """Returns all instance types in OCI offering GPUs."""
+    return common.list_accelerators_impl('OCI', _get_df(), gpus_only,
+                                         name_filter, region_filter,
+                                         quantity_filter, case_sensitive)
+
+
+def get_vcpus_mem_from_instance_type(
+        instance_type: str) -> Tuple[Optional[float], Optional[float]]:
+    return common.get_vcpus_mem_from_instance_type_impl(_get_df(),
+                                                        instance_type)
+
+
+def get_image_id_from_tag(tag: str, region: Optional[str]) -> Optional[str]:
+    """Returns the image id from the tag."""
+    # Always try get region-specific imageid first (for backward compatible)
+    image_str = common.get_image_id_from_tag_impl(_image_df, tag, region)
+    if image_str is None:
+        # Support cross-region (general) imageid
+        image_str = common.get_image_id_from_tag_impl(_image_df, tag, None)
+
+    df = _image_df[_image_df['Tag'].str.fullmatch(tag)]
+    app_catalog_listing_id = df['AppCatalogListingId'].iloc[0]
+    resource_version = df['ResourceVersion'].iloc[0]
+
+    return (f'{image_str}{oci_conf.IMAGE_TAG_SPERATOR}{app_catalog_listing_id}'
+            f'{oci_conf.IMAGE_TAG_SPERATOR}{resource_version}')
+
+
+def is_image_tag_valid(tag: str, region: Optional[str]) -> bool:
+    """Returns whether the image tag is valid."""
+    return common.is_image_tag_valid_impl(_image_df, tag, region)
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/clouds/gcp.py` & `skypilot-nightly-1.0.0.dev20230713/sky/clouds/oci.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,508 +1,579 @@
-"""Google Cloud Platform."""
-import json
+"""
+Oracle Cloud Infrastructure (OCI)
+
+History:
+ - Hysun He (hysun.he@oracle.com) @ Apr, 2023: Initial implementation
+ - Hysun He (hysun.he@oracle.com) @ May 4, 2023: Support use the default
+   image_id (configurable) if no image_id specified in the task yaml.
+"""
 import os
-import subprocess
-import time
+import json
 import typing
+import logging
 from typing import Dict, Iterator, List, Optional, Tuple
 
-from google import auth
-
 from sky import clouds
+from sky import exceptions
+from sky import status_lib
 from sky.clouds import service_catalog
+from sky.utils import common_utils
 from sky.utils import ux_utils
+from sky.adaptors import oci as oci_adaptor
+from sky.skylet.providers.oci.config import oci_conf
 
 if typing.TYPE_CHECKING:
-    from sky import resources
+    # Renaming to avoid shadowing variables.
+    from sky import resources as resources_lib
+
+logger = logging.getLogger(__name__)
 
-DEFAULT_GCP_APPLICATION_CREDENTIAL_PATH = os.path.expanduser(
-    '~/.config/gcloud/'
-    'application_default_credentials.json')
-
-GCP_CONFIG_PATH = '~/.config/gcloud/configurations/config_default'
-# Do not place the backup under the gcloud config directory, as ray
-# autoscaler can overwrite that directory on the remote nodes.
-GCP_CONFIG_SKY_BACKUP_PATH = '~/.sky/.sky_gcp_config_default'
-
-# Minimum set of files under ~/.config/gcloud that grant GCP access.
-_CREDENTIAL_FILES = [
-    'credentials.db',
-    'application_default_credentials.json',
-    'access_tokens.db',
-    'configurations',
-    'legacy_credentials',
-    'active_config',
-]
-
-_GCLOUD_INSTALLATION_LOG = '~/.sky/logs/gcloud_installation.log'
-# Need to be run with /bin/bash
-# We factor out the installation logic to keep it align in both spot
-# controller and cloud stores.
-GCLOUD_INSTALLATION_COMMAND = f'pushd /tmp &>/dev/null && \
-    gcloud --help > /dev/null 2>&1 || \
-    {{ mkdir -p {os.path.dirname(_GCLOUD_INSTALLATION_LOG)} && \
-    wget --quiet https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-sdk-382.0.0-linux-x86_64.tar.gz > {_GCLOUD_INSTALLATION_LOG} && \
-    tar xzf google-cloud-sdk-382.0.0-linux-x86_64.tar.gz >> {_GCLOUD_INSTALLATION_LOG} && \
-    rm -rf ~/google-cloud-sdk >> {_GCLOUD_INSTALLATION_LOG}  && \
-    mv google-cloud-sdk ~/ && \
-    ~/google-cloud-sdk/install.sh -q >> {_GCLOUD_INSTALLATION_LOG} 2>&1 && \
-    echo "source ~/google-cloud-sdk/path.bash.inc > /dev/null 2>&1" >> ~/.bashrc && \
-    source ~/google-cloud-sdk/path.bash.inc >> {_GCLOUD_INSTALLATION_LOG} 2>&1; }} && \
-    {{ cp {GCP_CONFIG_SKY_BACKUP_PATH} {GCP_CONFIG_PATH} > /dev/null 2>&1 || true; }} && \
-    popd &>/dev/null'
-
-
-def _run_output(cmd):
-    proc = subprocess.run(cmd,
-                          shell=True,
-                          check=True,
-                          stderr=subprocess.PIPE,
-                          stdout=subprocess.PIPE)
-    return proc.stdout.decode('ascii')
-
-
-def is_api_disabled(endpoint: str, project_id: str) -> bool:
-    proc = subprocess.run((f'gcloud services list --project {project_id} '
-                           f' | grep {endpoint}.googleapis.com'),
-                          check=False,
-                          shell=True,
-                          stderr=subprocess.PIPE,
-                          stdout=subprocess.PIPE)
-    return proc.returncode != 0
+_tenancy_prefix = None
 
 
 @clouds.CLOUD_REGISTRY.register
-class GCP(clouds.Cloud):
-    """Google Cloud Platform."""
+class OCI(clouds.Cloud):
+    """ OCI: Oracle Cloud Infrastructure """
+
+    _REPR = 'OCI'
+
+    _MAX_CLUSTER_NAME_LEN_LIMIT = 200
 
-    _REPR = 'GCP'
     _regions: List[clouds.Region] = []
-    _zones: List[clouds.Zone] = []
 
-    #### Regions/Zones ####
+    _INDENT_PREFIX = '    '
 
     @classmethod
-    def regions(cls) -> List[clouds.Region]:
-        if not cls._regions:
-            # https://cloud.google.com/compute/docs/regions-zones
-            cls._regions = [
-                clouds.Region('us-west1').set_zones([
-                    clouds.Zone('us-west1-a'),
-                    clouds.Zone('us-west1-b'),
-                    # clouds.Zone('us-west1-c'),  # No GPUs.
-                ]),
-                clouds.Region('us-central1').set_zones([
-                    clouds.Zone('us-central1-a'),
-                    clouds.Zone('us-central1-b'),
-                    clouds.Zone('us-central1-c'),
-                    clouds.Zone('us-central1-f'),
-                ]),
-                clouds.Region('us-east1').set_zones([
-                    clouds.Zone('us-east1-b'),
-                    clouds.Zone('us-east1-c'),
-                    clouds.Zone('us-east1-d'),
-                ]),
-                clouds.Region('us-east4').set_zones([
-                    clouds.Zone('us-east4-a'),
-                    clouds.Zone('us-east4-b'),
-                    clouds.Zone('us-east4-c'),
-                ]),
-                clouds.Region('us-west2').set_zones([
-                    # clouds.Zone('us-west2-a'),  # No GPUs.
-                    clouds.Zone('us-west2-b'),
-                    clouds.Zone('us-west2-c'),
-                ]),
-                # Ignoring us-west3 as it doesn't have GPUs.
-                clouds.Region('us-west4').set_zones([
-                    clouds.Zone('us-west4-a'),
-                    clouds.Zone('us-west4-b'),
-                    # clouds.Zone('us-west4-c'),  # No GPUs.
-                ]),
-            ]
-        return cls._regions
+    def _cloud_unsupported_features(
+            cls) -> Dict[clouds.CloudImplementationFeatures, str]:
+        return {
+            clouds.CloudImplementationFeatures.CLONE_DISK_FROM_CLUSTER:
+                (f'Migrating disk is not supported in {cls._REPR}.'),
+        }
 
     @classmethod
-    def region_zones_provision_loop(
-        cls,
-        *,
-        instance_type: Optional[str] = None,
-        accelerators: Optional[Dict[str, int]] = None,
-        use_spot: Optional[bool] = False,
-    ) -> Iterator[Tuple[clouds.Region, List[clouds.Zone]]]:
-        # GCP provisioner currently takes 1 zone per request.
-        if accelerators is None:
-            if instance_type is None:
-                # fallback to manually specified region/zones
-                regions = cls.regions()
-            else:
-                regions = service_catalog.get_region_zones_for_instance_type(
-                    instance_type, use_spot, clouds='gcp')
-        else:
-            assert len(accelerators) == 1, accelerators
-            acc = list(accelerators.keys())[0]
-            acc_count = list(accelerators.values())[0]
-            regions = service_catalog.get_region_zones_for_accelerators(
-                acc, acc_count, use_spot, clouds='gcp')
-
-        for region in regions:
-            for zone in region.zones:
-                yield (region, [zone])
+    def _max_cluster_name_length(cls) -> Optional[int]:
+        return cls._MAX_CLUSTER_NAME_LEN_LIMIT
 
     @classmethod
-    def get_zone_shell_cmd(cls) -> Optional[str]:
-        # The command for getting the current zone is from:
-        # https://cloud.google.com/compute/docs/metadata/querying-metadata
-        command_str = (
-            'curl -s http://metadata.google.internal/computeMetadata/v1/instance/zone'  # pylint: disable=line-too-long
-            ' -H "Metadata-Flavor: Google" | awk -F/ \'{print $4}\'')
-        return command_str
+    def regions_with_offering(cls, instance_type: str,
+                              accelerators: Optional[Dict[str, int]],
+                              use_spot: bool, region: Optional[str],
+                              zone: Optional[str]) -> List[clouds.Region]:
+        del accelerators  # unused
+
+        regions = service_catalog.get_region_zones_for_instance_type(
+            instance_type, use_spot, 'oci')
+
+        if region is not None:
+            regions = [r for r in regions if r.name == region]
+        if zone is not None:
+            for r in regions:
+                assert r.zones is not None, r
+                r.set_zones([z for z in r.zones if z.name == zone])
+            regions = [r for r in regions if r.zones]
+        return regions
 
-    #### Normal methods ####
+    @classmethod
+    def get_vcpus_mem_from_instance_type(
+        cls,
+        instance_type: str,
+    ) -> Tuple[Optional[float], Optional[float]]:
+        return service_catalog.get_vcpus_mem_from_instance_type(instance_type,
+                                                                clouds='oci')
 
-    def instance_type_to_hourly_cost(self, instance_type, use_spot):
+    @classmethod
+    def zones_provision_loop(
+        cls,
+        *,
+        region: str,
+        num_nodes: int,
+        instance_type: str,
+        accelerators: Optional[Dict[str, int]] = None,
+        use_spot: bool = False,
+    ) -> Iterator[List[clouds.Zone]]:
+        del num_nodes  # unused
+        regions = cls.regions_with_offering(instance_type,
+                                            accelerators,
+                                            use_spot,
+                                            region=region,
+                                            zone=None)
+        for r in regions:
+            assert r.zones is not None, r
+            for zone in r.zones:
+                yield [zone]
+
+    def instance_type_to_hourly_cost(self,
+                                     instance_type: str,
+                                     use_spot: bool,
+                                     region: Optional[str] = None,
+                                     zone: Optional[str] = None) -> float:
         return service_catalog.get_hourly_cost(instance_type,
-                                               region=None,
                                                use_spot=use_spot,
-                                               clouds='gcp')
-
-    def accelerators_to_hourly_cost(self, accelerators, use_spot: bool):
-        assert len(accelerators) == 1, accelerators
-        acc, acc_count = list(accelerators.items())[0]
-        return service_catalog.get_accelerator_hourly_cost(acc,
-                                                           acc_count,
-                                                           use_spot,
-                                                           clouds='gcp')
-
-    def get_egress_cost(self, num_gigabytes):
-        # In general, query this from the cloud:
-        #   https://cloud.google.com/storage/pricing#network-pricing
-        # NOTE: egress to worldwide (excl. China, Australia).
-        if num_gigabytes <= 1024:
-            return 0.12 * num_gigabytes
-        elif num_gigabytes <= 1024 * 10:
-            return 0.11 * num_gigabytes
-        else:
-            return 0.08 * num_gigabytes
+                                               region=region,
+                                               zone=zone,
+                                               clouds='oci')
+
+    def accelerators_to_hourly_cost(self,
+                                    accelerators: Dict[str, int],
+                                    use_spot: bool,
+                                    region: Optional[str] = None,
+                                    zone: Optional[str] = None) -> float:
+        del accelerators, use_spot, region, zone  # unused
+        return 0.0
+
+    def get_egress_cost(self, num_gigabytes: float) -> float:
+        """
+        https://www.oracle.com/cis/cloud/networking/pricing/
+        """
+        # Free for first 10T (per month)
+        if num_gigabytes <= 10 * 1024:
+            return 0.0
+
+        # We need to calculate the egress cost by region.
+        # Fortunately, most of time, this cost looks not a
+        # big deal comparing to the price of GPU instances.
+        # # Calculate cost for over 10T (per month)
+        # logger.debug(f"* get_egress_cost. region {region}")
+        # if region in REGIONS_NorthAmerica_Europe_UK:
+        #     return (num_gigabytes - 10 * 1024) * 0.0085
+        # elif region in REGIONS_JAPAC_SouthAmerica:
+        #     return (num_gigabytes - 10 * 1024) * 0.025
+        # elif region in REGIONS_MidEast_Africa:
+        #     return (num_gigabytes - 10 * 1024) * 0.05
+        # else:
+        #     logger.debug(f"* ! Region {region} is not listed for cost calc!")
+        #     return 0.0
+        return (num_gigabytes - 10 * 1024) * 0.0085
+
+    def is_same_cloud(self, other: clouds.Cloud) -> bool:
+        # Returns true if the two clouds are the same cloud type.
+        return isinstance(other, OCI)
 
-    def is_same_cloud(self, other):
-        return isinstance(other, GCP)
+    @classmethod
+    def get_default_instance_type(
+            cls,
+            cpus: Optional[str] = None,
+            memory: Optional[str] = None,
+            disk_tier: Optional[str] = None) -> Optional[str]:
+        return service_catalog.get_default_instance_type(cpus=cpus,
+                                                         memory=memory,
+                                                         disk_tier=disk_tier,
+                                                         clouds='oci')
 
     @classmethod
-    def get_default_instance_type(cls) -> str:
-        # 8 vCpus, 52 GB RAM.  First-gen general purpose.
-        return 'n1-highmem-8'
+    def get_accelerators_from_instance_type(
+        cls,
+        instance_type: str,
+    ) -> Optional[Dict[str, int]]:
+        return service_catalog.get_accelerators_from_instance_type(
+            instance_type, clouds='oci')
 
     @classmethod
-    def _get_default_region(cls) -> clouds.Region:
-        return cls.regions()[-1]
+    def get_zone_shell_cmd(cls) -> Optional[str]:
+        return None
 
     def make_deploy_resources_variables(
-            self, resources: 'resources.Resources',
+            self, resources: 'resources_lib.Resources',
             region: Optional['clouds.Region'],
-            zones: Optional[List['clouds.Zone']]) -> Dict[str, str]:
-        if region is None:
-            assert zones is None, (
-                'Set either both or neither for: region, zones.')
-            region = self._get_default_region()
-            zones = region.zones
-        else:
-            assert zones is not None, (
-                'Set either both or neither for: region, zones.')
+            zones: Optional[List['clouds.Zone']]) -> Dict[str, Optional[str]]:
+        assert region is not None, resources
 
-        region_name = region.name
-        zones = [zones[0].name]
+        acc_dict = self.get_accelerators_from_instance_type(
+            resources.instance_type)
+        if acc_dict is not None:
+            custom_resources = json.dumps(acc_dict, separators=(',', ':'))
+        else:
+            custom_resources = None
 
-        # gcloud compute images list \
-        # --project deeplearning-platform-release \
-        # --no-standard-images
-        # We use the debian image, as the ubuntu image has some connectivity
-        # issue when first booted.
-        image_id = service_catalog.get_image_id_from_tag(
-            'skypilot:cpu-debian-10', clouds='gcp')
-
-        r = resources
-        # Find GPU spec, if any.
-        resources_vars = {
-            'instance_type': r.instance_type,
-            'region': region_name,
-            'zones': ','.join(zones),
-            'gpu': None,
-            'gpu_count': None,
-            'tpu': None,
-            'tpu_vm': False,
-            'custom_resources': None,
-            'use_spot': r.use_spot,
+        image_str = self._get_image_id(resources.image_id, region.name,
+                                       resources.instance_type)
+        image_cols = image_str.split(oci_conf.IMAGE_TAG_SPERATOR)
+        if len(image_cols) == 3:
+            image_id = image_cols[0]
+            listing_id = image_cols[1]
+            res_ver = image_cols[2]
+        else:
+            image_id = resources.image_id
+            listing_id = None
+            res_ver = None
+
+        cpus = resources.cpus
+        instance_type_arr = resources.instance_type.split(
+            oci_conf.INSTANCE_TYPE_RES_SPERATOR)
+        instance_type = instance_type_arr[0]
+
+        # Improvement:
+        # Fault-tolerant to the catalog file: special shapes does
+        # not need cpu/memory configuration, so ignore these info
+        # from the catalog file to avoid inconsistence (mainly due
+        # to the shape changed in future.)
+        if len(instance_type_arr) < 2:
+            cpus = None
+        else:
+            if cpus is None:
+                cpus, mems = OCI.get_vcpus_mem_from_instance_type(
+                    resources.instance_type)
+                resources = resources.copy(
+                    cpus=cpus,
+                    memory=mems,
+                )
+            if cpus is None and resources.instance_type.startswith(
+                    oci_conf.VM_PREFIX):
+                cpus = f'{oci_conf.DEFAULT_NUM_VCPUS}'
+
+        zone = resources.zone
+        if zone is None:
+            # If zone is not specified, try to get the first zone.
+            if zones is None:
+                regions = service_catalog.get_region_zones_for_instance_type(
+                    instance_type=resources.instance_type,
+                    use_spot=resources.use_spot,
+                    clouds='oci')
+                zones = [r for r in iter(regions) if r.name == region.name
+                        ][0].zones
+
+            if zones is not None:
+                zone = zones[0].name
+
+        global _tenancy_prefix
+        if _tenancy_prefix is None:
+            try:
+                identity_client = oci_adaptor.get_identity_client(
+                    region=region.name, profile=oci_conf.get_profile())
+
+                ad_list = identity_client.list_availability_domains(
+                    compartment_id=oci_adaptor.get_oci_config(
+                        profile=oci_conf.get_profile())['tenancy']).data
+
+                first_ad = ad_list[0]
+                _tenancy_prefix = str(first_ad.name).split(':')[0]
+            except (oci_adaptor.get_oci().exceptions.ConfigFileNotFound,
+                    oci_adaptor.get_oci().exceptions.InvalidConfig) as e:
+                # This should only happen in testing where oci config is
+                # monkeypatched. In real use, if the OCI config is not
+                # valid, the 'sky check' would fail (OCI disabled).
+                logger.debug(f'It is OK goes here when testing: {str(e)}')
+                pass
+
+        # Disk performane: Volume Performance Units.
+        vpu = self.get_vpu_from_disktier(
+            cpus=None if cpus is None else float(cpus),
+            disk_tier=resources.disk_tier)
+
+        return {
+            'instance_type': instance_type,
+            'custom_resources': custom_resources,
+            'region': region.name,
+            'cpus': str(cpus),
+            'memory': resources.memory,
+            'disk_size': resources.disk_size,
+            'vpu': str(vpu),
+            'zone': f'{_tenancy_prefix}:{zone}',
+            'image': image_id,
+            'app_catalog_listing_id': listing_id,
+            'resource_version': res_ver,
+            'use_spot': resources.use_spot
         }
-        accelerators = r.accelerators
-        if accelerators is not None:
-            assert len(accelerators) == 1, r
-            acc, acc_count = list(accelerators.items())[0]
-            resources_vars['custom_resources'] = json.dumps(accelerators,
-                                                            separators=(',',
-                                                                        ':'))
-            if 'tpu' in acc:
-                resources_vars['tpu_type'] = acc.replace('tpu-', '')
-                assert r.accelerator_args is not None, r
-
-                resources_vars['tpu_vm'] = r.accelerator_args.get('tpu_vm')
-                resources_vars['runtime_version'] = r.accelerator_args[
-                    'runtime_version']
-                resources_vars['tpu_name'] = r.accelerator_args.get('tpu_name')
-            else:
-                # Convert to GCP names:
-                # https://cloud.google.com/compute/docs/gpus
-                if acc == 'A100-80GB':
-                    # A100-80GB has a different name pattern.
-                    resources_vars['gpu'] = 'nvidia-{}'.format(acc.lower())
-                else:
-                    resources_vars['gpu'] = 'nvidia-tesla-{}'.format(
-                        acc.lower())
-                resources_vars['gpu_count'] = acc_count
-                if acc == 'K80':
-                    # Though the image is called cu113, it actually has later
-                    # versions of CUDA as noted below.
-                    # CUDA driver version 470.57.02, CUDA Library 11.4
-                    image_id = service_catalog.get_image_id_from_tag(
-                        'skypilot:k80-debian-10', clouds='gcp')
-                else:
-                    # Though the image is called cu113, it actually has later
-                    # versions of CUDA as noted below.
-                    # CUDA driver version 510.47.03, CUDA Library 11.6
-                    # Does not support torch==1.13.0 with cu117
-                    image_id = service_catalog.get_image_id_from_tag(
-                        'skypilot:gpu-debian-10', clouds='gcp')
-
-        if resources.image_id is not None:
-            if None in resources.image_id:
-                image_id = resources.image_id[None]
-            else:
-                assert region_name in resources.image_id, resources.image_id
-                image_id = resources.image_id[region_name]
-
-        assert image_id is not None, (image_id, r)
-        resources_vars['image_id'] = image_id
 
-        return resources_vars
-
-    def get_feasible_launchable_resources(self, resources):
-        fuzzy_candidate_list = []
+    def get_feasible_launchable_resources(self,
+                                          resources: 'resources_lib.Resources'):
         if resources.instance_type is not None:
             assert resources.is_launchable(), resources
-            return ([resources], fuzzy_candidate_list)
+            resources = resources.copy(accelerators=None)
+            return ([resources], [])
 
-        # No other resources (cpu/mem) to filter for now, so just return a
-        # default VM type.
-        host_vm_type = GCP.get_default_instance_type()
-        acc_dict = None
-        # Find instance candidates to meet user's requirements
-        if resources.accelerators is not None:
-            assert len(resources.accelerators.items(
-            )) == 1, 'cannot handle more than one accelerator candidates.'
-            acc, acc_count = list(resources.accelerators.items())[0]
-            (instance_list, fuzzy_candidate_list
-            ) = service_catalog.get_instance_type_for_accelerator(acc,
-                                                                  acc_count,
-                                                                  clouds='gcp')
-
-            if instance_list is None:
-                return ([], fuzzy_candidate_list)
-            assert len(
-                instance_list
-            ) == 1, f'More than one instance type matched, {instance_list}'
-
-            host_vm_type = instance_list[0]
-            acc_dict = {acc: acc_count}
-            if resources.accelerator_args is not None:
-                use_tpu_vm = resources.accelerator_args.get('tpu_vm', False)
-                if use_tpu_vm:
-                    host_vm_type = 'TPU-VM'
-        r = resources.copy(
-            cloud=GCP(),
-            instance_type=host_vm_type,
-            accelerators=acc_dict,
-        )
-        return ([r], fuzzy_candidate_list)
+        def _make(instance_list):
+            resource_list = []
+            for instance_type in instance_list:
+                r = resources.copy(
+                    cloud=OCI(),
+                    instance_type=instance_type,
+                    # Setting this to None as OCI doesn't separately bill /
+                    # attach the accelerators.  Billed as part of the VM type.
+                    accelerators=None,
+                    cpus=None,
+                    memory=None,
+                )
+                resource_list.append(r)
+            return resource_list
 
-    @classmethod
-    def get_accelerators_from_instance_type(
-        cls,
-        instance_type: str,
-    ) -> Optional[Dict[str, int]]:
-        # GCP handles accelerators separately from regular instance types,
-        # hence return none here.
-        return None
+        # Currently, handle a filter on accelerators only.
+        accelerators = resources.accelerators
+        if accelerators is None:
+            # Return a default instance type with the given number of vCPUs.
+            default_instance_type = OCI.get_default_instance_type(
+                cpus=resources.cpus,
+                memory=resources.memory,
+                disk_tier=resources.disk_tier)
 
-    @classmethod
-    def get_vcpus_from_instance_type(
-        cls,
-        instance_type: str,
-    ) -> float:
-        return service_catalog.get_vcpus_from_instance_type(instance_type,
-                                                            clouds='gcp')
+            if default_instance_type is None:
+                return ([], [])
+            else:
+                return (_make([default_instance_type]), [])
+
+        assert len(accelerators) == 1, resources
+
+        acc, acc_count = list(accelerators.items())[0]
+        (instance_list, fuzzy_candidate_list
+        ) = service_catalog.get_instance_type_for_accelerator(
+            acc,
+            acc_count,
+            use_spot=resources.use_spot,
+            cpus=resources.cpus,
+            memory=resources.memory,
+            region=resources.region,
+            zone=resources.zone,
+            clouds='oci')
+        if instance_list is None:
+            return ([], fuzzy_candidate_list)
 
-    def check_credentials(self) -> Tuple[bool, Optional[str]]:
+        return (_make(instance_list), fuzzy_candidate_list)
+
+    @classmethod
+    def check_credentials(cls) -> Tuple[bool, Optional[str]]:
         """Checks if the user has access credentials to this cloud."""
+
+        short_credential_help_str = (
+            'For more details, refer to: '
+            # pylint: disable=line-too-long
+            'https://skypilot.readthedocs.io/en/latest/getting-started/installation.html#oracle-cloud-infrastructure-oci'
+        )
+        credential_help_str = (
+            'To configure credentials, go to: '
+            'https://docs.oracle.com/en-us/iaas/Content/API/Concepts/'
+            'apisigningkey.htm\n'
+            f'{cls._INDENT_PREFIX}Please make sure the API keys and the config '
+            'files are placed under ~/.oci:\n'
+            f'{cls._INDENT_PREFIX}  ~/.oci/config\n'
+            f'{cls._INDENT_PREFIX}  ~/.oci/oci_api_key.pem\n'
+            f'{cls._INDENT_PREFIX}The ~/.oci/config file should have the '
+            'following format:\n'
+            f'{cls._INDENT_PREFIX}  [DEFAULT]\n'
+            f'{cls._INDENT_PREFIX}  user=ocid1.user.oc1..aaaaaaaa\n'
+            f'{cls._INDENT_PREFIX}  '
+            'fingerprint=aa:bb:cc:dd:ee:ff:gg:hh:ii:jj:kk:ll:mm:nn:oo:pp\n'
+            f'{cls._INDENT_PREFIX}  tenancy=ocid1.tenancy.oc1..aaaaaaaa\n'
+            f'{cls._INDENT_PREFIX}  region=us-sanjose-1\n'
+            f'{cls._INDENT_PREFIX}  key_file=~/.oci/oci_api_key.pem')
+
         try:
-            # These files are required because they will be synced to remote
-            # VMs for `gsutil` to access private storage buckets.
-            # `auth.default()` does not guarantee these files exist.
-            for file in [
-                    '~/.config/gcloud/access_tokens.db',
-                    '~/.config/gcloud/credentials.db'
-            ]:
-                assert os.path.isfile(os.path.expanduser(file))
-            # Check if application default credentials are set.
-            self.get_project_id()
-            # Calling `auth.default()` ensures the GCP client library works,
-            # which is used by Ray Autoscaler to launch VMs.
-            auth.default()
-            # Check google-api-python-client installation.
             # pylint: disable=import-outside-toplevel,unused-import
-            import googleapiclient
+            import oci
+        except ImportError:
+            return False, ('`oci` is not installed. Install it with: '
+                           'pip install oci\n'
+                           f'{cls._INDENT_PREFIX}{short_credential_help_str}')
+
+        conf_file = oci_adaptor.get_config_file()
+
+        help_str = (f'Missing credential file at {conf_file}. '
+                    f'{short_credential_help_str}')
+        if not os.path.isfile(os.path.expanduser(conf_file)):
+            return (False, help_str)
 
-            # Check the installation of google-cloud-sdk.
-            _run_output('gcloud --version')
-        except (AssertionError, auth.exceptions.DefaultCredentialsError,
-                subprocess.CalledProcessError, FileNotFoundError, KeyError,
-                ImportError):
-            # See also: https://stackoverflow.com/a/53307505/1165051
+        try:
+            user = oci_adaptor.get_identity_client(
+                region=None, profile=oci_conf.get_profile()).get_user(
+                    oci_adaptor.get_oci_config(
+                        profile=oci_conf.get_profile())['user']).data
+            del user
+            # TODO[Hysun]: More privilege check can be added
+            return True, None
+        except (oci_adaptor.get_oci().exceptions.ConfigFileNotFound,
+                oci_adaptor.get_oci().exceptions.InvalidConfig,
+                oci_adaptor.service_exception()) as e:
             return False, (
-                'GCP tools are not installed or credentials are not set. '
-                'Run the following commands:\n    '
-                # Install the Google Cloud SDK:
-                '  $ pip install google-api-python-client\n    '
-                '  $ conda install -c conda-forge google-cloud-sdk -y\n    '
-                # This authenticates the CLI to make `gsutil` work:
-                '  $ gcloud init\n    '
-                # This will generate
-                # ~/.config/gcloud/application_default_credentials.json.
-                '  $ gcloud auth application-default login\n    '
-                'For more info: '
-                'https://skypilot.readthedocs.io/en/latest/getting-started/installation.html'  # pylint: disable=line-too-long
-            )
-
-        # Check APIs.
-        project_id = self.get_project_id()
-        apis = (
-            ('compute', 'Compute Engine'),
-            ('cloudresourcemanager', 'Cloud Resource Manager'),
-            ('iam', 'Identity and Access Management (IAM)'),
-            ('tpu', 'Cloud TPU'),  # Keep as final element.
-        )
-        enabled_api = False
-        for endpoint, display_name in apis:
-            if is_api_disabled(endpoint, project_id):
-                # For 'compute': ~55-60 seconds for the first run. If already
-                # enabled, ~1s. Other API endpoints take ~1-5s to enable.
-                if endpoint == 'compute':
-                    suffix = ' (free of charge; this may take a minute)'
-                else:
-                    suffix = ' (free of charge)'
-                print(f'\nEnabling {display_name} API{suffix}...')
-                t1 = time.time()
-                proc = subprocess.run(
-                    f'gcloud services enable {endpoint}.googleapis.com '
-                    f'--project {project_id}',
-                    check=False,
-                    shell=True,
-                    stdout=subprocess.PIPE,
-                    stderr=subprocess.STDOUT)
-                if proc.returncode == 0:
-                    enabled_api = True
-                    print(f'Done. Took {time.time() - t1:.1f} secs.')
-                elif endpoint != 'tpu':
-                    print('Failed. Detailed output:')
-                    print(proc.stdout.decode())
-                    return False, (
-                        f'{display_name} API is disabled. Please retry '
-                        '`sky check` in a few minutes, or manually enable it.')
-                else:
-                    # TPU API failed. Should still enable GCP.
-                    print('Failed to enable Cloud TPU API. '
-                          'This can be ignored if you do not use TPUs. '
-                          'Otherwise, please enable it manually.\n'
-                          'Detailed output:')
-                    print(proc.stdout.decode())
-
-        if enabled_api:
-            print('\nHint: Enabled GCP API(s) may take a few minutes to take '
-                  'effect. If any SkyPilot commands/calls failed, retry after '
-                  'some time.')
-
-        return True, None
+                f'OCI credential is not correctly set. '
+                f'Check the credential file at {conf_file}\n'
+                f'{cls._INDENT_PREFIX}{credential_help_str}\n'
+                f'{cls._INDENT_PREFIX}Error details: '
+                f'{common_utils.format_exception(e, use_bracket=True)}')
 
     def get_credential_file_mounts(self) -> Dict[str, str]:
-        # Create a backup of the config_default file, as the original file can
-        # be modified on the remote cluster by ray causing authentication
-        # problems. The backup file will be updated to the remote cluster
-        # whenever the original file is not empty and will be applied
-        # appropriately on the remote cluster when neccessary.
-        if (os.path.exists(os.path.expanduser(GCP_CONFIG_PATH)) and
-                os.path.getsize(os.path.expanduser(GCP_CONFIG_PATH)) > 0):
-            subprocess.run(f'cp {GCP_CONFIG_PATH} {GCP_CONFIG_SKY_BACKUP_PATH}',
-                           shell=True,
-                           check=True)
-        elif not os.path.exists(os.path.expanduser(GCP_CONFIG_SKY_BACKUP_PATH)):
-            raise RuntimeError(
-                'GCP credential file is empty. Please make sure you '
-                'have run: gcloud init')
-
-        # Excluding the symlink to the python executable created by the gcp
-        # credential, which causes problem for ray up multiple nodes, tracked
-        # in #494, #496, #483.
-        credentials = {
-            f'~/.config/gcloud/{filename}': f'~/.config/gcloud/{filename}'
-            for filename in _CREDENTIAL_FILES
+        """Returns a dict of credential file paths to mount paths."""
+        oci_cfg_file = oci_adaptor.get_config_file()
+        # Pass-in a profile parameter so that multiple profile in oci
+        # config file is supported (2023/06/09).
+        oci_cfg = oci_adaptor.get_oci_config(profile=oci_conf.get_profile())
+        api_key_file = oci_cfg[
+            'key_file'] if 'key_file' in oci_cfg else 'BadConf'
+        sky_cfg_file = oci_conf.get_sky_user_config_file()
+
+        # OCI config and API key file are mandatory
+        credential_files = [oci_cfg_file, api_key_file]
+
+        # Sky config file is optional
+        if os.path.exists(sky_cfg_file):
+            credential_files.append(sky_cfg_file)
+
+        file_mounts = {
+            f'{filename}': f'{filename}' for filename in credential_files
         }
-        credentials[GCP_CONFIG_SKY_BACKUP_PATH] = GCP_CONFIG_SKY_BACKUP_PATH
-        return credentials
 
-    def instance_type_exists(self, instance_type):
-        return service_catalog.instance_type_exists(instance_type, 'gcp')
+        logger.debug(f'OCI credential file mounts: {file_mounts}')
+        return file_mounts
+
+    @classmethod
+    def get_current_user_identity(cls) -> Optional[List[str]]:
+        # NOTE: used for very advanced SkyPilot functionality
+        # Can implement later if desired
+        # If the user switches the compartment_ocid, the existing clusters
+        # might be leaked, as `sky status -r` will fail to find the original
+        # clusters in the new compartment and the identity check is missing.
+        return None
+
+    def instance_type_exists(self, instance_type: str) -> bool:
+        return service_catalog.instance_type_exists(instance_type, 'oci')
+
+    def validate_region_zone(self, region: Optional[str], zone: Optional[str]):
+        return service_catalog.validate_region_zone(region, zone, clouds='oci')
 
     def accelerator_in_region_or_zone(self,
                                       accelerator: str,
                                       acc_count: int,
                                       region: Optional[str] = None,
                                       zone: Optional[str] = None) -> bool:
         return service_catalog.accelerator_in_region_or_zone(
-            accelerator, acc_count, region, zone, 'gcp')
+            accelerator, acc_count, region, zone, 'oci')
 
-    @classmethod
-    def get_project_id(cls, dryrun: bool = False) -> str:
-        # TODO(zhwu): change the project id fetching with the following command
-        # `gcloud info --format='value(config.project)'`
-        if dryrun:
-            return 'dryrun-project-id'
-        if 'GOOGLE_APPLICATION_CREDENTIALS' in os.environ:
-            gcp_credential_path = os.environ['GOOGLE_APPLICATION_CREDENTIALS']
+    def get_image_size(self, image_id: str, region: Optional[str]) -> float:
+        # We ignore checking the image size because most of situations the
+        # boot volume size is larger than the image size. For specific rare
+        # situations, the configuration/setup commands should make sure the
+        # correct size of the disk.
+        return 0
+
+    def _get_image_id(
+        self,
+        image_id: Optional[Dict[Optional[str], str]],
+        region_name: str,
+        instance_type: str,
+    ) -> str:
+        if image_id is None:
+            return self._get_default_image(region_name=region_name,
+                                           instance_type=instance_type)
+        if None in image_id:
+            image_id_str = image_id[None]
         else:
-            gcp_credential_path = DEFAULT_GCP_APPLICATION_CREDENTIAL_PATH
-        if not os.path.exists(gcp_credential_path):
+            assert region_name in image_id, image_id
+            image_id_str = image_id[region_name]
+        if image_id_str.startswith('skypilot:'):
+            image_id_str = service_catalog.get_image_id_from_tag(image_id_str,
+                                                                 region_name,
+                                                                 clouds='oci')
+            if image_id_str is None:
+                logger.critical(
+                    '! Real image_id not found! - {region_name}:{image_id}')
+                # Raise ResourcesUnavailableError to make sure the failover
+                # in CloudVMRayBackend will be correctly triggered.
+                # TODO(zhwu): This is a information leakage to the cloud
+                # implementor, we need to find a better way to handle this.
+                raise exceptions.ResourcesUnavailableError(
+                    '! ERR: No image found in catalog for region '
+                    f'{region_name}. Try setting a valid image_id.')
+
+        logger.debug(f'Got real image_id {image_id_str}')
+        return image_id_str
+
+    def _get_default_image(self, region_name: str, instance_type: str) -> str:
+        acc = self.get_accelerators_from_instance_type(instance_type)
+
+        if acc is None:
+            image_tag = oci_conf.get_default_image_tag()
+            image_id_str = service_catalog.get_image_id_from_tag(image_tag,
+                                                                 region_name,
+                                                                 clouds='oci')
+        else:
+            assert len(acc) == 1, acc
+            image_tag = oci_conf.get_default_gpu_image_tag()
+            image_id_str = service_catalog.get_image_id_from_tag(image_tag,
+                                                                 region_name,
+                                                                 clouds='oci')
+
+        if image_id_str is not None:
+            logger.debug(
+                f'Got default image_id {image_id_str} from tag {image_tag}')
+            return image_id_str
+
+        # Raise ResourcesUnavailableError to make sure the failover in
+        # CloudVMRayBackend will be correctly triggered.
+        # TODO(zhwu): This is a information leakage to the cloud implementor,
+        # we need to find a better way to handle this.
+        raise exceptions.ResourcesUnavailableError(
+            'ERR: No image found in catalog for region '
+            f'{region_name}. Try update your default image_id settings.')
+
+    @classmethod
+    def check_disk_tier_enabled(cls, instance_type: str,
+                                disk_tier: str) -> None:
+        # All the disk_tier are supported for any instance_type
+        del instance_type, disk_tier  # unused
+
+    def get_vpu_from_disktier(self, cpus: Optional[float],
+                              disk_tier: Optional[str]) -> int:
+        vpu = oci_conf.BOOT_VOLUME_VPU[disk_tier]
+        if cpus is None:
+            return vpu
+
+        if cpus <= 2:
+            vpu = oci_conf.DISK_TIER_LOW if disk_tier is None else vpu
+            if vpu > oci_conf.DISK_TIER_LOW:
+                # If only 1 OCPU is configured, best to use the OCI default
+                # VPU (10) for the boot volume. Even if the VPU is configured
+                # to higher value (no error to launch the instance), we cannot
+                # fully achieve its IOPS/throughput performance.
+                logger.warning(
+                    f'Automatically set the VPU to {oci_conf.DISK_TIER_LOW}'
+                    f' as only 2x vCPU is configured.')
+                vpu = oci_conf.DISK_TIER_LOW
+        elif cpus < 8:
+            # If less than 4 OCPU is configured, best not to set the disk_tier
+            # to 'high' (vpu=100). Even if the disk_tier is configured to high
+            # (no error to launch the instance), we cannot fully achieve its
+            # IOPS/throughput performance.
+            if vpu > oci_conf.DISK_TIER_MEDIUM:
+                logger.warning(
+                    f'Automatically set the VPU to {oci_conf.DISK_TIER_MEDIUM}'
+                    f' as less than 8x vCPU is configured.')
+                vpu = oci_conf.DISK_TIER_MEDIUM
+        return vpu
+
+    @classmethod
+    def query_status(cls, name: str, tag_filters: Dict[str, str],
+                     region: Optional[str], zone: Optional[str],
+                     **kwargs) -> List[status_lib.ClusterStatus]:
+        del zone, kwargs  # Unused.
+        # Check the lifecycleState definition from the page
+        # https://docs.oracle.com/en-us/iaas/api/#/en/iaas/latest/Instance/
+        status_map = {
+            'PROVISIONING': status_lib.ClusterStatus.INIT,
+            'STARTING': status_lib.ClusterStatus.INIT,
+            'RUNNING': status_lib.ClusterStatus.UP,
+            'STOPPING': status_lib.ClusterStatus.STOPPED,
+            'STOPPED': status_lib.ClusterStatus.STOPPED,
+            'TERMINATED': None,
+            'TERMINATING': None,
+        }
+
+        # pylint: disable=import-outside-toplevel
+        from sky.skylet.providers.oci.query_helper import oci_query_helper
+
+        status_list = []
+        try:
+            vms = oci_query_helper.query_instances_by_tags(
+                tag_filters=tag_filters, region=region)
+        except Exception as e:  # pylint: disable=broad-except
             with ux_utils.print_exception_no_traceback():
-                raise FileNotFoundError(
-                    f'No GCP credentials found at '
-                    f'{gcp_credential_path}. Please set the '
-                    f'GOOGLE_APPLICATION_CREDENTIALS '
-                    f'environment variable to point to '
-                    f'the path of your credentials file.')
-
-        with open(gcp_credential_path, 'r') as fp:
-            gcp_credentials = json.load(fp)
-        project_id = gcp_credentials.get('quota_project_id',
-                                         None) or gcp_credentials['project_id']
-        return project_id
-
-    @staticmethod
-    def check_host_accelerator_compatibility(
-            instance_type: str, accelerators: Optional[Dict[str, int]]) -> None:
-        service_catalog.check_host_accelerator_compatibility(
-            instance_type, accelerators, 'gcp')
-
-    @staticmethod
-    def check_accelerator_attachable_to_host(
-            instance_type: str,
-            accelerators: Optional[Dict[str, int]],
-            zone: Optional[str] = None) -> None:
-        service_catalog.check_accelerator_attachable_to_host(
-            instance_type, accelerators, zone, 'gcp')
+                raise exceptions.ClusterStatusFetchingError(
+                    f'Failed to query OCI cluster {name!r} status. '
+                    'Details: '
+                    f'{common_utils.format_exception(e, use_bracket=True)}')
+
+        for node in vms:
+            vm_status = node.lifecycle_state
+            if vm_status in status_map:
+                sky_status = status_map[vm_status]
+                if sky_status is not None:
+                    status_list.append(sky_status)
+
+        return status_list
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/clouds/local.py` & `skypilot-nightly-1.0.0.dev20230713/sky/clouds/local.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,28 +1,19 @@
 """Local/On-premise."""
-import subprocess
 import typing
 from typing import Dict, Iterator, List, Optional, Tuple
 
 from sky import clouds
+from sky import exceptions
 
 if typing.TYPE_CHECKING:
     # Renaming to avoid shadowing variables.
     from sky import resources as resources_lib
 
 
-def _run_output(cmd):
-    proc = subprocess.run(cmd,
-                          shell=True,
-                          check=True,
-                          stderr=subprocess.PIPE,
-                          stdout=subprocess.PIPE)
-    return proc.stdout.decode('ascii')
-
-
 @clouds.CLOUD_REGISTRY.register
 class Local(clouds.Cloud):
     """Local/on-premise cloud.
 
     This Cloud has the following special treatment of Cloud concepts:
 
     - Catalog: Does not have service catalog.
@@ -32,44 +23,73 @@
     - Cluster: All local clusters are part of the local cloud.
     - Credentials: No checking is done (in `sky check`) and users must
         provide their own credentials instead of Sky autogenerating
         cluster credentials.
     """
     _DEFAULT_INSTANCE_TYPE = 'on-prem'
     LOCAL_REGION = clouds.Region('Local')
-    _regions: List[clouds.Region] = [LOCAL_REGION]
+    _CLOUD_UNSUPPORTED_FEATURES = {
+        clouds.CloudImplementationFeatures.STOP:
+            ('Local cloud does not support stopping instances.'),
+        clouds.CloudImplementationFeatures.AUTOSTOP:
+            ('Local cloud does not support stopping instances.'),
+        clouds.CloudImplementationFeatures.CLONE_DISK_FROM_CLUSTER:
+            ('Migrating disk is not supported for Local.'),
+    }
+
+    @classmethod
+    def _cloud_unsupported_features(
+            cls) -> Dict[clouds.CloudImplementationFeatures, str]:
+        return cls._CLOUD_UNSUPPORTED_FEATURES
+
+    @classmethod
+    def _max_cluster_name_length(cls) -> Optional[int]:
+        return None
 
     @classmethod
-    def regions(cls):
-        return cls._regions
+    def regions_with_offering(cls, instance_type: str,
+                              accelerators: Optional[Dict[str, int]],
+                              use_spot: bool, region: Optional[str],
+                              zone: Optional[str]) -> List[clouds.Region]:
+        """Local cloud resources are placed in only one region."""
+        del instance_type, accelerators, use_spot, region, zone
+        return [cls.LOCAL_REGION]
 
     @classmethod
-    def region_zones_provision_loop(
+    def zones_provision_loop(
         cls,
         *,
-        instance_type: Optional[str] = None,
+        region: str,
+        num_nodes: int,
+        instance_type: str,
         accelerators: Optional[Dict[str, int]] = None,
-        use_spot: bool,
-    ) -> Iterator[Tuple[clouds.Region, List[clouds.Zone]]]:
-        del instance_type
-        del use_spot
-        del accelerators  # unused
-        for region in cls.regions():
-            yield region, region.zones
+        use_spot: bool = False,
+    ) -> Iterator[None]:
+        del num_nodes  # Unused.
+        regions = cls.regions_with_offering(instance_type,
+                                            accelerators,
+                                            use_spot=use_spot,
+                                            region=region,
+                                            zone=None)
+        for r in regions:
+            assert r.zones is None, r
+            yield r.zones
 
     #### Normal methods ####
 
-    def instance_type_to_hourly_cost(self, instance_type: str,
-                                     use_spot: bool) -> float:
+    def instance_type_to_hourly_cost(self, instance_type: str, use_spot: bool,
+                                     region: Optional[str],
+                                     zone: Optional[str]) -> float:
         # On-prem machines on Sky are assumed free
         # (minus electricity/utility bills).
         return 0.0
 
-    def accelerators_to_hourly_cost(self, accelerators,
-                                    use_spot: bool) -> float:
+    def accelerators_to_hourly_cost(self, accelerators, use_spot: bool,
+                                    region: Optional[str],
+                                    zone: Optional[str]) -> float:
         # Hourly cost of accelerators is 0 for local cloud.
         return 0.0
 
     def get_egress_cost(self, num_gigabytes: float) -> float:
         # Egress cost from a local cluster is assumed to be 0.
         return 0.0
 
@@ -77,41 +97,51 @@
         return 'Local'
 
     def is_same_cloud(self, other: clouds.Cloud) -> bool:
         # Returns true if the two clouds are the same cloud type.
         return isinstance(other, Local)
 
     @classmethod
-    def get_default_instance_type(cls) -> str:
+    def get_default_instance_type(cls,
+                                  cpus: Optional[str] = None,
+                                  memory: Optional[str] = None,
+                                  disk_tier: Optional[str] = None) -> str:
         # There is only "1" instance type for local cloud: on-prem
+        del cpus, memory, disk_tier  # Unused.
         return Local._DEFAULT_INSTANCE_TYPE
 
     @classmethod
-    def get_vcpus_from_instance_type(cls,
-                                     instance_type: str) -> Optional[float]:
-        return None
+    def get_vcpus_mem_from_instance_type(
+            cls, instance_type: str) -> Tuple[Optional[float], Optional[float]]:
+        return None, None
 
     @classmethod
     def get_accelerators_from_instance_type(
         cls,
         instance_type: str,
     ) -> Optional[Dict[str, int]]:
         # This function is called, as the instance_type is `on-prem`.
         # Local cloud will return no accelerators. This is deferred to
         # the ResourceHandle, which calculates the accelerators in the cluster.
         return None
 
+    @classmethod
+    def regions(cls) -> List[clouds.Region]:
+        return [Local.LOCAL_REGION]
+
     def make_deploy_resources_variables(
             self, resources: 'resources_lib.Resources',
             region: Optional['clouds.Region'],
-            zones: Optional[List['clouds.Zone']]) -> Dict[str, str]:
+            zones: Optional[List['clouds.Zone']]) -> Dict[str, Optional[str]]:
         return {}
 
     def get_feasible_launchable_resources(self,
                                           resources: 'resources_lib.Resources'):
+        if resources.disk_tier is not None:
+            return ([], [])
         # The entire local cluster's resources is considered launchable, as the
         # check for task resources is deferred later.
         # The check for task resources meeting cluster resources is run in
         # cloud_vm_ray_backend._check_task_resources_smaller_than_cluster.
         resources = resources.copy(
             instance_type=Local.get_default_instance_type(),
             # Setting this to None as AWS doesn't separately bill /
@@ -131,15 +161,16 @@
         # accelerators are guaranteed to be found within the region.
         return True
 
     @classmethod
     def get_zone_shell_cmd(cls) -> Optional[str]:
         return None
 
-    def check_credentials(self) -> Tuple[bool, Optional[str]]:
+    @classmethod
+    def check_credentials(cls) -> Tuple[bool, Optional[str]]:
         # This method is called in `sky check`.
         # As credentials are not generated by Sky (supplied by user instead),
         # this method will always return True.
         return True, None
 
     def get_credential_file_mounts(self) -> Dict[str, str]:
         # There are no credentials to upload to remote in Local mode.
@@ -153,7 +184,14 @@
     def validate_region_zone(self, region: Optional[str], zone: Optional[str]):
         # Returns true if the region name is same as Local cloud's
         # one and only region: 'Local'.
         assert zone is None
         if region is None or region != Local.LOCAL_REGION.name:
             raise ValueError(f'Region {region!r} does not match the Local'
                              ' cloud region {Local.LOCAL_REGION.name!r}.')
+        return region, zone
+
+    @classmethod
+    def check_disk_tier_enabled(cls, instance_type: str,
+                                disk_tier: str) -> None:
+        raise exceptions.NotSupportedError(
+            'Local cloud does not support disk tiers.')
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/clouds/service_catalog/__init__.py` & `skypilot-nightly-1.0.0.dev20230713/sky/clouds/service_catalog/__init__.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,33 +1,34 @@
 """Service catalog."""
 import collections
 import importlib
 import typing
-from typing import Dict, List, Optional, Tuple, Union
+from typing import Dict, List, Optional, Set, Tuple, Union
 
 from sky.clouds.service_catalog.constants import (
     HOSTED_CATALOG_DIR_URL,
     CATALOG_SCHEMA_VERSION,
     LOCAL_CATALOG_DIR,
 )
+from sky.clouds.service_catalog.config import use_default_catalog
 
 if typing.TYPE_CHECKING:
     from sky.clouds import cloud
     from sky.clouds.service_catalog import common
 
 CloudFilter = Optional[Union[List[str], str]]
-_ALL_CLOUDS = ('aws', 'azure', 'gcp')
+_ALL_CLOUDS = ('aws', 'azure', 'gcp', 'ibm', 'lambda', 'scp', 'oci')
 
 
-def _map_clouds_catalog(clouds: CloudFilter, method_name, *args, **kwargs):
+def _map_clouds_catalog(clouds: CloudFilter, method_name: str, *args, **kwargs):
     if clouds is None:
         clouds = list(_ALL_CLOUDS)
     single = isinstance(clouds, str)
     if single:
-        clouds = [clouds]
+        clouds = [clouds]  # type: ignore
 
     results = []
     for cloud in clouds:
         try:
             cloud_module = importlib.import_module(
                 f'sky.clouds.service_catalog.{cloud}_catalog')
         except ModuleNotFoundError:
@@ -42,56 +43,68 @@
                 f'implement the "{method_name}" method') from None
         results.append(method(*args, **kwargs))
     if single:
         return results[0]
     return results
 
 
+@use_default_catalog
 def list_accelerators(
     gpus_only: bool = True,
     name_filter: Optional[str] = None,
+    region_filter: Optional[str] = None,
+    quantity_filter: Optional[int] = None,
     clouds: CloudFilter = None,
     case_sensitive: bool = True,
 ) -> 'Dict[str, List[common.InstanceTypeInfo]]':
     """List the names of all accelerators offered by Sky.
 
+    This will include all accelerators offered by Sky, including those
+    that may not be available in the user's account.
+
     Returns: A dictionary of canonical accelerator names mapped to a list
     of instance type offerings. See usage in cli.py.
     """
     results = _map_clouds_catalog(clouds, 'list_accelerators', gpus_only,
-                                  name_filter, case_sensitive)
+                                  name_filter, region_filter, quantity_filter,
+                                  case_sensitive)
     if not isinstance(results, list):
         results = [results]
-    ret = collections.defaultdict(list)
+    ret: Dict[str,
+              List['common.InstanceTypeInfo']] = collections.defaultdict(list)
     for result in results:
         for gpu, items in result.items():
             ret[gpu] += items
     return dict(ret)
 
 
 def list_accelerator_counts(
     gpus_only: bool = True,
     name_filter: Optional[str] = None,
+    region_filter: Optional[str] = None,
+    quantity_filter: Optional[int] = None,
     clouds: CloudFilter = None,
 ) -> Dict[str, List[int]]:
     """List all accelerators offered by Sky and available counts.
 
     Returns: A dictionary of canonical accelerator names mapped to a list
     of available counts. See usage in cli.py.
     """
     results = _map_clouds_catalog(clouds, 'list_accelerators', gpus_only,
-                                  name_filter)
+                                  name_filter, region_filter, quantity_filter,
+                                  False)
     if not isinstance(results, list):
         results = [results]
-    ret = collections.defaultdict(set)
+    accelerator_counts: Dict[str, Set[int]] = collections.defaultdict(set)
     for result in results:
         for gpu, items in result.items():
             for item in items:
-                ret[gpu].add(item.accelerator_count)
-    for gpu, counts in ret.items():
+                accelerator_counts[gpu].add(item.accelerator_count)
+    ret: Dict[str, List[int]] = {}
+    for gpu, counts in accelerator_counts.items():
         ret[gpu] = sorted(counts)
     return ret
 
 
 def instance_type_exists(instance_type: str,
                          clouds: CloudFilter = None) -> bool:
     """Check the existence of a instance type."""
@@ -115,72 +128,130 @@
     clouds: CloudFilter = None,
 ) -> bool:
     """Returns True if the accelerator is in the region or zone."""
     return _map_clouds_catalog(clouds, 'accelerator_in_region_or_zone',
                                acc_name, acc_count, region, zone)
 
 
+def regions(clouds: CloudFilter = None) -> 'List[cloud.Region]':
+    """Returns the list of regions in a Cloud's catalog.
+    Each Region object contains a list of Zones, if available.
+    """
+    return _map_clouds_catalog(clouds, 'regions')
+
+
 def get_region_zones_for_instance_type(
         instance_type: str,
         use_spot: bool,
         clouds: CloudFilter = None) -> 'List[cloud.Region]':
     """Returns a list of regions for a given instance type."""
     return _map_clouds_catalog(clouds, 'get_region_zones_for_instance_type',
                                instance_type, use_spot)
 
 
 def get_hourly_cost(instance_type: str,
-                    region: Optional[str],
                     use_spot: bool,
-                    clouds: CloudFilter = None):
-    """Returns the cost, or the cheapest cost among all zones for spot."""
-    return _map_clouds_catalog(clouds, 'get_hourly_cost', instance_type, region,
-                               use_spot)
+                    region: Optional[str],
+                    zone: Optional[str],
+                    clouds: CloudFilter = None) -> float:
+    """Returns the hourly price of a VM instance in the given region and zone.
+
+    * If (region, zone) == (None, None), return the cheapest hourly price among
+        all regions and zones.
+    * If (region, zone) == (str, None), return the cheapest hourly price among
+        all the zones in the given region.
+    * If (region, zone) == (None, str), return the hourly price of the instance
+        type in the zone.
+    * If (region, zone) == (str, str), zone must be in the region, and the
+        function returns the hourly price of the instance type in the zone.
+    """
+    return _map_clouds_catalog(clouds, 'get_hourly_cost', instance_type,
+                               use_spot, region, zone)
 
 
-def get_vcpus_from_instance_type(instance_type: str,
-                                 clouds: CloudFilter = None) -> Optional[float]:
+def get_vcpus_mem_from_instance_type(
+        instance_type: str,
+        clouds: CloudFilter = None) -> Tuple[Optional[float], Optional[float]]:
     """Returns the number of virtual CPUs from a instance type."""
-    return _map_clouds_catalog(clouds, 'get_vcpus_from_instance_type',
+    return _map_clouds_catalog(clouds, 'get_vcpus_mem_from_instance_type',
                                instance_type)
 
 
+def get_default_instance_type(cpus: Optional[str] = None,
+                              memory: Optional[str] = None,
+                              disk_tier: Optional[str] = None,
+                              clouds: CloudFilter = None) -> Optional[str]:
+    """Returns the cloud's default instance type for given #vCPUs and memory.
+
+    For example, if cpus='4', this method returns the default instance type
+    with 4 vCPUs.  If cpus='4+', this method returns the default instance
+    type with 4 or more vCPUs.
+
+    If memory_gb_or_ratio is not specified, this method returns the General
+    Purpose instance type with the given number of vCPUs. If memory_gb_or_ratio
+    is specified, this method returns the cheapest instance type that meets
+    the given CPU and memory requirement.
+    """
+    return _map_clouds_catalog(clouds, 'get_default_instance_type', cpus,
+                               memory, disk_tier)
+
+
 def get_accelerators_from_instance_type(
         instance_type: str,
         clouds: CloudFilter = None) -> Optional[Dict[str, int]]:
     """Returns the accelerators from a instance type."""
     return _map_clouds_catalog(clouds, 'get_accelerators_from_instance_type',
                                instance_type)
 
 
 def get_instance_type_for_accelerator(
     acc_name: str,
     acc_count: int,
+    cpus: Optional[str] = None,
+    memory: Optional[str] = None,
+    use_spot: bool = False,
+    region: Optional[str] = None,
+    zone: Optional[str] = None,
     clouds: CloudFilter = None,
 ) -> Tuple[Optional[List[str]], List[str]]:
     """
     Returns a list of instance types satisfying the required count of
     accelerators with sorted prices and a list of candidates with fuzzy search.
     """
     return _map_clouds_catalog(clouds, 'get_instance_type_for_accelerator',
-                               acc_name, acc_count)
+                               acc_name, acc_count, cpus, memory, use_spot,
+                               region, zone)
 
 
 def get_accelerator_hourly_cost(
     acc_name: str,
     acc_count: int,
     use_spot: bool,
+    region: Optional[str] = None,
+    zone: Optional[str] = None,
     clouds: CloudFilter = None,
 ) -> float:
-    """Returns the hourly cost with the accelerators."""
+    """Returns the hourly price of the accelerator in the given region and zone.
+
+    * If (region, zone) == (None, None), return the cheapest hourly price among
+        all regions and zones.
+    * If (region, zone) == (str, None), return the cheapest hourly price among
+        all the zones in the given region.
+    * If (region, zone) == (None, str), return the hourly price of the
+        accelerator in the zone.
+    * If (region, zone) == (str, str), zone must be in the region, and the
+        function returns the hourly price of the accelerator in the zone.
+    """
     return _map_clouds_catalog(clouds,
                                'get_accelerator_hourly_cost',
                                acc_name,
                                acc_count,
-                               use_spot=use_spot)
+                               use_spot=use_spot,
+                               region=region,
+                               zone=zone)
 
 
 def get_region_zones_for_accelerators(
         acc_name: str,
         acc_count: int,
         use_spot: bool,
         clouds: CloudFilter = None) -> 'List[cloud.Region]':
@@ -218,15 +289,24 @@
     _map_clouds_catalog(clouds, 'check_accelerator_attachable_to_host',
                         instance_type, accelerators, zone)
 
 
 def get_common_gpus() -> List[str]:
     """Returns a list of commonly used GPU names."""
     return [
-        'V100', 'V100-32GB', 'A100', 'A100-80GB', 'P100', 'K80', 'T4', 'M60'
+        'A10',
+        'A10G',
+        'A100',
+        'A100-80GB',
+        'K80',
+        'M60',
+        'P100',
+        'T4',
+        'V100',
+        'V100-32GB',
     ]
 
 
 def get_tpus() -> List[str]:
     """Returns a list of TPU names."""
     # TODO(wei-lin): refactor below hard-coded list.
     return [
@@ -241,15 +321,15 @@
                           clouds: CloudFilter = None) -> str:
     """Returns the image ID from the tag."""
     return _map_clouds_catalog(clouds, 'get_image_id_from_tag', tag, region)
 
 
 def is_image_tag_valid(tag: str,
                        region: Optional[str],
-                       clouds: CloudFilter = None) -> None:
+                       clouds: CloudFilter = None) -> bool:
     """Validates the image tag."""
     return _map_clouds_catalog(clouds, 'is_image_tag_valid', tag, region)
 
 
 __all__ = [
     'list_accelerators',
     'list_accelerator_counts',
@@ -260,12 +340,14 @@
     'get_accelerator_hourly_cost',
     'get_region_zones_for_accelerators',
     'get_common_gpus',
     'get_tpus',
     # Images
     'get_image_id_from_tag',
     'is_image_tag_valid',
+    # Configuration
+    'use_default_catalog',
     # Constants
     'HOSTED_CATALOG_DIR_URL',
     'CATALOG_SCHEMA_VERSION',
     'LOCAL_CATALOG_DIR',
 ]
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/clouds/service_catalog/aws_catalog.py` & `skypilot-nightly-1.0.0.dev20230713/sky/clouds/service_catalog/ibm_catalog.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,97 +1,124 @@
-"""AWS Offerings Catalog.
+"""
+IBM Offerings Catalog.
 
 This module loads the service catalog file and can be used to query
-instance types and pricing information for AWS.
+instance types and pricing information for IBM.
 """
-import typing
-from typing import Dict, List, Optional, Tuple
 
+from sky import sky_logging
+from sky.clouds import cloud
 from sky.clouds.service_catalog import common
+from sky.adaptors import ibm
+from typing import Dict, List, Optional, Tuple
 
-if typing.TYPE_CHECKING:
-    from sky.clouds import cloud
+logger = sky_logging.init_logger(__name__)
 
-_df = common.read_catalog('aws/vms.csv')
-_image_df = common.read_catalog('aws/images.csv')
+_DEFAULT_INSTANCE_FAMILY = 'bx2'
+_DEFAULT_NUM_VCPUS = '8'
+_DEFAULT_MEMORY = 32
+
+_df = common.read_catalog('ibm/vms.csv')
 
 
 def instance_type_exists(instance_type: str) -> bool:
     return common.instance_type_exists_impl(_df, instance_type)
 
 
-def validate_region_zone(
-        region: Optional[str],
-        zone: Optional[str]) -> Tuple[Optional[str], Optional[str]]:
-    return common.validate_region_zone_impl(_df, region, zone)
+def validate_region_zone(region: Optional[str], zone: Optional[str]):
+    return common.validate_region_zone_impl('IBM', _df, region, zone)
 
 
 def accelerator_in_region_or_zone(acc_name: str,
                                   acc_count: int,
                                   region: Optional[str] = None,
                                   zone: Optional[str] = None) -> bool:
     return common.accelerator_in_region_or_zone_impl(_df, acc_name, acc_count,
                                                      region, zone)
 
 
 def get_hourly_cost(instance_type: str,
+                    use_spot: bool = False,
                     region: Optional[str] = None,
-                    use_spot: bool = False) -> float:
-    """Returns the cost, or the cheapest cost among all zones for spot."""
-    return common.get_hourly_cost_impl(_df, instance_type, region, use_spot)
+                    zone: Optional[str] = None) -> float:
+    return common.get_hourly_cost_impl(_df, instance_type, use_spot, region,
+                                       zone)
 
 
-def get_vcpus_from_instance_type(instance_type: str) -> Optional[float]:
-    return common.get_vcpus_from_instance_type_impl(_df, instance_type)
+def get_vcpus_mem_from_instance_type(
+        instance_type: str) -> Tuple[Optional[float], Optional[float]]:
+    return common.get_vcpus_mem_from_instance_type_impl(_df, instance_type)
 
 
 def get_accelerators_from_instance_type(
         instance_type: str) -> Optional[Dict[str, int]]:
     return common.get_accelerators_from_instance_type_impl(_df, instance_type)
 
 
 def get_instance_type_for_accelerator(
     acc_name: str,
     acc_count: int,
+    cpus: Optional[str] = None,
+    memory: Optional[str] = None,
+    use_spot: bool = False,
+    region: Optional[str] = None,
+    zone: Optional[str] = None,
 ) -> Tuple[Optional[List[str]], List[str]]:
     """
     Returns a list of instance types satisfying the required count of
     accelerators with sorted prices and a list of candidates with fuzzy search.
     """
     return common.get_instance_type_for_accelerator_impl(df=_df,
                                                          acc_name=acc_name,
-                                                         acc_count=acc_count)
+                                                         acc_count=acc_count,
+                                                         cpus=cpus,
+                                                         memory=memory,
+                                                         use_spot=use_spot,
+                                                         region=region,
+                                                         zone=zone)
 
 
 def get_region_zones_for_instance_type(instance_type: str,
-                                       use_spot: bool) -> List['cloud.Region']:
+                                       use_spot: bool) -> List[cloud.Region]:
     df = _df[_df['InstanceType'] == instance_type]
-    region_list = common.get_region_zones(df, use_spot)
-    # Hack: Enforce US regions are always tried first:
-    #   [US regions sorted by price] + [non-US regions sorted by price]
-    us_region_list = []
-    other_region_list = []
-    for region in region_list:
-        if region.name.startswith('us-'):
-            us_region_list.append(region)
-        else:
-            other_region_list.append(region)
-    return us_region_list + other_region_list
-
-
-def list_accelerators(gpus_only: bool,
-                      name_filter: Optional[str],
-                      case_sensitive: bool = True
-                     ) -> Dict[str, List[common.InstanceTypeInfo]]:
-    """Returns all instance types in AWS offering accelerators."""
-    return common.list_accelerators_impl('AWS', _df, gpus_only, name_filter,
+    return common.get_region_zones(df, use_spot)
+
+
+def list_accelerators(
+        gpus_only: bool,
+        name_filter: Optional[str],
+        region_filter: Optional[str],
+        quantity_filter: Optional[int],
+        case_sensitive: bool = True
+) -> Dict[str, List[common.InstanceTypeInfo]]:
+    """Returns all instance types in IBM offering accelerators."""
+    return common.list_accelerators_impl('IBM', _df, gpus_only, name_filter,
+                                         region_filter, quantity_filter,
                                          case_sensitive)
 
 
-def get_image_id_from_tag(tag: str, region: Optional[str]) -> Optional[str]:
-    """Returns the image id from the tag."""
-    return common.get_image_id_from_tag_impl(_image_df, tag, region)
+def get_default_instance_type(cpus: Optional[str] = None,
+                              memory: Optional[str] = None,
+                              disk_tier: Optional[str] = None) -> Optional[str]:
+    del disk_tier  # unused
+    if cpus is None and memory is None:
+        cpus = f'{_DEFAULT_NUM_VCPUS}+'
+
+    if memory is None:
+        memory_gb_or_ratio = f'{_DEFAULT_MEMORY}+'
+    else:
+        memory_gb_or_ratio = memory
+    instance_type_prefix = f'{_DEFAULT_INSTANCE_FAMILY}-'
+    df = _df[_df['InstanceType'].str.startswith(instance_type_prefix)]
+    return common.get_instance_type_for_cpus_mem_impl(df, cpus,
+                                                      memory_gb_or_ratio)
 
 
 def is_image_tag_valid(tag: str, region: Optional[str]) -> bool:
     """Returns whether the image tag is valid."""
-    return common.is_image_tag_valid_impl(_image_df, tag, region)
+    vpc_client = ibm.client(region=region)
+    try:
+        vpc_client.get_image(tag)
+    except ibm.ibm_cloud_sdk_core.ApiException as e:  # type: ignore[union-attr]
+        logger.error(e.message)
+        return False
+    return True
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/clouds/service_catalog/common.py` & `skypilot-nightly-1.0.0.dev20230713/sky/clouds/service_catalog/common.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,19 +1,23 @@
 """Common utilities for service catalog."""
+import ast
+import hashlib
 import os
+import time
 from typing import Dict, List, NamedTuple, Optional, Tuple
 
 import difflib
+import filelock
 import requests
 import pandas as pd
 
 from sky import sky_logging
-from sky.backends import backend_utils
 from sky.clouds import cloud as cloud_lib
 from sky.clouds.service_catalog import constants
+from sky.utils import log_utils
 from sky.utils import ux_utils
 
 logger = sky_logging.init_logger(__name__)
 
 _CATALOG_DIR = os.path.join(constants.LOCAL_CATALOG_DIR,
                             constants.CATALOG_SCHEMA_VERSION)
 os.makedirs(_CATALOG_DIR, exist_ok=True)
@@ -24,58 +28,115 @@
 
     - cloud: Cloud name.
     - instance_type: String that can be used in YAML to specify this instance
       type. E.g. `p3.2xlarge`.
     - accelerator_name: Canonical name of the accelerator. E.g. `V100`.
     - accelerator_count: Number of accelerators offered by this instance type.
     - cpu_count: Number of vCPUs offered by this instance type.
+    - device_memory: Device memory in GiB.
     - memory: Instance memory in GiB.
     - price: Regular instance price per hour (cheapest across all regions).
     - spot_price: Spot instance price per hour (cheapest across all regions).
+    - region: Region where this instance type belongs to.
     """
     cloud: str
     instance_type: Optional[str]
     accelerator_name: str
     accelerator_count: int
     cpu_count: Optional[float]
+    device_memory: Optional[float]
     memory: Optional[float]
     price: float
     spot_price: float
+    region: str
 
 
 def get_catalog_path(filename: str) -> str:
     return os.path.join(_CATALOG_DIR, filename)
 
 
-def read_catalog(filename: str) -> pd.DataFrame:
+def read_catalog(filename: str,
+                 pull_frequency_hours: Optional[int] = None) -> pd.DataFrame:
     """Reads the catalog from a local CSV file.
 
     If the file does not exist, download the up-to-date catalog that matches
     the schema version.
+    If `pull_frequency_hours` is not None: pull the latest catalog with
+    possibly updated prices, if the local catalog file is older than
+    `pull_frequency_hours` and no changes to the local catalog file are
+    made after the last pull.
     """
     assert filename.endswith('.csv'), 'The catalog file must be a CSV file.'
+    assert (pull_frequency_hours is None or
+            pull_frequency_hours > 0), pull_frequency_hours
     catalog_path = get_catalog_path(filename)
     cloud = cloud_lib.CLOUD_REGISTRY.from_str(os.path.dirname(filename))
-    if not os.path.exists(catalog_path):
-        url = f'{constants.HOSTED_CATALOG_DIR_URL}/{constants.CATALOG_SCHEMA_VERSION}/{filename}'  # pylint: disable=line-too-long
-        with backend_utils.safe_console_status(
-                f'Downloading {cloud} catalog...'):
-            try:
-                r = requests.get(url)
-                r.raise_for_status()
-            except requests.exceptions.RequestException as e:
-                logger.error(f'Failed to download {cloud} catalog:')
-                with ux_utils.print_exception_no_traceback():
-                    raise e
-        # Save the catalog to a local file.
-        os.makedirs(os.path.dirname(catalog_path), exist_ok=True)
-        with open(catalog_path, 'w') as f:
-            f.write(r.text)
-        logger.info(f'A new {cloud} catalog has been downloaded to '
-                    f'{catalog_path}')
+
+    meta_path = os.path.join(_CATALOG_DIR, '.meta', filename)
+    os.makedirs(os.path.dirname(meta_path), exist_ok=True)
+
+    # Atomic check, to avoid conflicts with other processes.
+    # TODO(mraheja): remove pylint disabling when filelock version updated
+    # pylint: disable=abstract-class-instantiated
+    with filelock.FileLock(meta_path + '.lock'):
+
+        def _need_update() -> bool:
+            if not os.path.exists(catalog_path):
+                return True
+            if pull_frequency_hours is None:
+                return False
+            # Check the md5 of the file to see if it has changed.
+            with open(catalog_path, 'rb') as f:
+                file_md5 = hashlib.md5(f.read()).hexdigest()
+            md5_filepath = meta_path + '.md5'
+            if os.path.exists(md5_filepath):
+                with open(md5_filepath, 'r') as f:
+                    last_md5 = f.read()
+                if file_md5 != last_md5:
+                    # Do not update the file if the user modified it.
+                    return False
+
+            last_update = os.path.getmtime(catalog_path)
+            return last_update + pull_frequency_hours * 3600 < time.time()
+
+        if _need_update():
+            url = f'{constants.HOSTED_CATALOG_DIR_URL}/{constants.CATALOG_SCHEMA_VERSION}/{filename}'  # pylint: disable=line-too-long
+            update_frequency_str = ''
+            if pull_frequency_hours is not None:
+                update_frequency_str = f' (every {pull_frequency_hours} hours)'
+            with log_utils.safe_rich_status(
+                (f'Updating {cloud} catalog: '
+                 f'{filename}'
+                 f'{update_frequency_str}')) as status:
+                try:
+                    r = requests.get(url)
+                    r.raise_for_status()
+                except requests.exceptions.RequestException as e:
+                    ux_utils.console_newline()
+                    status.stop()
+                    error_str = (f'Failed to fetch {cloud} catalog '
+                                 f'{filename}. ')
+                    if os.path.exists(catalog_path):
+                        logger.warning(
+                            f'{error_str}Using cached catalog files.')
+                        # Update catalog file modification time.
+                        os.utime(catalog_path, None)  # Sets to current time
+                    else:
+                        logger.error(
+                            f'{error_str}Please check your internet connection.'
+                        )
+                        with ux_utils.print_exception_no_traceback():
+                            raise e
+                else:
+                    # Download successful, save the catalog to a local file.
+                    os.makedirs(os.path.dirname(catalog_path), exist_ok=True)
+                    with open(catalog_path, 'w') as f:
+                        f.write(r.text)
+                    with open(meta_path + '.md5', 'w') as f:
+                        f.write(hashlib.md5(r.text.encode()).hexdigest())
 
     try:
         df = pd.read_csv(catalog_path)
     except Exception as e:  # pylint: disable=broad-except
         # As users can manually modify the catalog, read_csv can fail.
         logger.error(f'Failed to read {catalog_path}. '
                      'To fix: delete the csv file and try again.')
@@ -84,50 +145,66 @@
     return df
 
 
 def _get_instance_type(
     df: pd.DataFrame,
     instance_type: str,
     region: Optional[str],
+    zone: Optional[str] = None,
 ) -> pd.DataFrame:
     idx = df['InstanceType'] == instance_type
     if region is not None:
-        idx &= df['Region'] == region
+        idx &= df['Region'].str.lower() == region.lower()
+    if zone is not None:
+        # NOTE: For Azure instances, zone must be None.
+        idx &= df['AvailabilityZone'] == zone
     return df[idx]
 
 
 def instance_type_exists_impl(df: pd.DataFrame, instance_type: str) -> bool:
     """Returns True if the instance type is valid."""
     return instance_type in df['InstanceType'].unique()
 
 
 def validate_region_zone_impl(
-        df: pd.DataFrame, region: Optional[str],
+        cloud_name: str, df: pd.DataFrame, region: Optional[str],
         zone: Optional[str]) -> Tuple[Optional[str], Optional[str]]:
     """Validates whether region and zone exist in the catalog."""
 
-    def _get_candidate_str(loc: str, all_loc: List[str]) -> List[str]:
+    def _get_candidate_str(loc: str, all_loc: List[str]) -> str:
         candidate_loc = difflib.get_close_matches(loc, all_loc, n=5, cutoff=0.9)
         candidate_loc = sorted(candidate_loc)
         candidate_strs = ''
         if len(candidate_loc) > 0:
             candidate_strs = ', '.join(candidate_loc)
             candidate_strs = f'\nDid you mean one of these: {candidate_strs!r}?'
         return candidate_strs
 
+    def _get_all_supported_regions_str() -> str:
+        all_regions: List[str] = sorted(
+            df['Region'].str.lower().unique().tolist())
+        return (f'\nList of supported {cloud_name} regions: '
+                f'{", ".join(all_regions)!r}')
+
     validated_region, validated_zone = region, zone
 
     filter_df = df
     if region is not None:
-        filter_df = filter_df[filter_df['Region'] == region]
+        filter_df = filter_df[filter_df['Region'].str.lower() == region.lower()]
         if len(filter_df) == 0:
             with ux_utils.print_exception_no_traceback():
                 error_msg = (f'Invalid region {region!r}')
-                error_msg += _get_candidate_str(region, df['Region'].unique())
+                candidate_strs = _get_candidate_str(
+                    region.lower(), df['Region'].str.lower().unique())
+                if not candidate_strs:
+                    error_msg += _get_all_supported_regions_str()
+                    raise ValueError(error_msg)
+                error_msg += candidate_strs
                 raise ValueError(error_msg)
+        validated_region = filter_df['Region'].unique()[0]
 
     if zone is not None:
         maybe_region_df = filter_df
         filter_df = filter_df[filter_df['AvailabilityZone'] == zone]
         if len(filter_df) == 0:
             region_str = f' for region {region!r}' if region else ''
             df = maybe_region_df if region else df
@@ -141,47 +218,150 @@
         validated_region = region_df[0]
     return validated_region, validated_zone
 
 
 def get_hourly_cost_impl(
     df: pd.DataFrame,
     instance_type: str,
+    use_spot: bool,
     region: Optional[str],
-    use_spot: bool = False,
+    zone: Optional[str],
 ) -> float:
-    """Returns the cost, or the cheapest cost among all zones for spot."""
-    df = _get_instance_type(df, instance_type, region)
-    assert pd.isnull(
-        df['Price'].iloc[0]) is False, (f'Missing price for "{instance_type}, '
-                                        f'Spot: {use_spot}" in the catalog.')
-    # TODO(zhwu): We should handle the price difference among different regions.
-    price_str = 'SpotPrice' if use_spot else 'Price'
-    assert region is None or len(set(df[price_str])) == 1, df
+    """Returns the hourly price of a VM instance in the given region and zone.
 
-    cheapest_idx = df[price_str].idxmin()
+    Refer to get_hourly_cost in service_catalog/__init__.py for the docstring.
+    """
+    df = _get_instance_type(df, instance_type, region, zone)
+    if df.empty:
+        if zone is None:
+            if region is None:
+                region_or_zone = 'all regions'
+            else:
+                region_or_zone = f'region {region!r}'
+        else:
+            region_or_zone = f'zone {zone!r}'
+        with ux_utils.print_exception_no_traceback():
+            raise ValueError(f'Instance type {instance_type!r} not found '
+                             f'in {region_or_zone}.')
+
+    # If the zone is specified, only one row should be found by the query.
+    assert zone is None or len(df) == 1, df
+    if use_spot:
+        price_str = 'SpotPrice'
+    else:
+        price_str = 'Price'
+        # For AWS/Azure/GCP on-demand instances, the price is the same across
+        # all the zones in the same region.
+        assert region is None or len(set(df[price_str])) == 1, df
 
+    cheapest_idx = df[price_str].idxmin()
     cheapest = df.loc[cheapest_idx]
     return cheapest[price_str]
 
 
-def get_vcpus_from_instance_type_impl(
+def _get_value(value):
+    if pd.isna(value):
+        return None
+    return float(value)
+
+
+def get_vcpus_mem_from_instance_type_impl(
     df: pd.DataFrame,
     instance_type: str,
-) -> Optional[float]:
+) -> Tuple[Optional[float], Optional[float]]:
     df = _get_instance_type(df, instance_type, None)
     if len(df) == 0:
         with ux_utils.print_exception_no_traceback():
             raise ValueError(f'No instance type {instance_type} found.')
     assert len(set(df['vCPUs'])) == 1, ('Cannot determine the number of vCPUs '
                                         f'of the instance type {instance_type}.'
                                         f'\n{df}')
+    assert len(set(
+        df['MemoryGiB'])) == 1, ('Cannot determine the memory size '
+                                 f'of the instance type {instance_type}.'
+                                 f'\n{df}')
+
     vcpus = df['vCPUs'].iloc[0]
-    if pd.isna(vcpus):
+    mem = df['MemoryGiB'].iloc[0]
+
+    return _get_value(vcpus), _get_value(mem)
+
+
+def _filter_with_cpus(df: pd.DataFrame, cpus: Optional[str]) -> pd.DataFrame:
+    if cpus is None:
+        return df
+
+    # The following code is redundant with the code in resources.py::_set_cpus()
+    # but we add it here for safety.
+    if cpus.endswith('+'):
+        num_cpus_str = cpus[:-1]
+    else:
+        num_cpus_str = cpus
+    try:
+        num_cpus = float(num_cpus_str)
+    except ValueError:
+        with ux_utils.print_exception_no_traceback():
+            raise ValueError(f'The "cpus" field should be either a number or '
+                             f'a string "<number>+". Found: {cpus!r}') from None
+
+    if cpus.endswith('+'):
+        return df[df['vCPUs'] >= num_cpus]
+    else:
+        return df[df['vCPUs'] == num_cpus]
+
+
+def _filter_with_mem(df: pd.DataFrame,
+                     memory_gb_or_ratio: Optional[str]) -> pd.DataFrame:
+    if memory_gb_or_ratio is None:
+        return df
+
+    # The following code is partially redundant with the code in
+    # resources.py::_set_memory() but we add it here for safety.
+    if memory_gb_or_ratio.endswith(('+', 'x')):
+        memory_gb_str = memory_gb_or_ratio[:-1]
+    else:
+        memory_gb_str = memory_gb_or_ratio
+    try:
+        memory = float(memory_gb_str)
+    except ValueError:
+        with ux_utils.print_exception_no_traceback():
+            raise ValueError(f'The "memory" field should be either a number or '
+                             'a string "<number>+" or "<number>x". Found: '
+                             f'{memory_gb_or_ratio!r}') from None
+    if memory_gb_or_ratio.endswith('+'):
+        return df[df['MemoryGiB'] >= memory]
+    elif memory_gb_or_ratio.endswith('x'):
+        return df[df['MemoryGiB'] >= df['vCPUs'] * memory]
+    else:
+        return df[df['MemoryGiB'] == memory]
+
+
+def get_instance_type_for_cpus_mem_impl(
+        df: pd.DataFrame, cpus: Optional[str],
+        memory_gb_or_ratio: Optional[str]) -> Optional[str]:
+    """Returns the cheapest instance type that satisfies the requirements.
+
+    Args:
+        df: The catalog cloud catalog data frame.
+        cpus: The number of vCPUs. Can be a number or a string "<number>+". If
+            the string ends with "+", then the returned instance type should
+            have at least the given number of vCPUs.
+        memory_gb_or_ratio: The memory size in GB. Can be a number or a string
+            "<number>+" or "<number>x". If the string ends with "+", then the
+            returned instance type should have at least the given memory size.
+            If the string ends with "x", then the returned instance type should
+            have at least the given number of vCPUs times the given ratio.
+    """
+    df = _filter_with_cpus(df, cpus)
+    df = _filter_with_mem(df, memory_gb_or_ratio)
+    if df.empty:
         return None
-    return float(vcpus)
+    # Sort by the price.
+    df = df.sort_values(by=['Price'], ascending=True)
+    return df['InstanceType'].iloc[0]
 
 
 def get_accelerators_from_instance_type_impl(
     df: pd.DataFrame,
     instance_type: str,
 ) -> Optional[Dict[str, int]]:
     df = _get_instance_type(df, instance_type, None)
@@ -195,14 +375,19 @@
     return {acc_name: int(acc_count)}
 
 
 def get_instance_type_for_accelerator_impl(
     df: pd.DataFrame,
     acc_name: str,
     acc_count: int,
+    cpus: Optional[str] = None,
+    memory: Optional[str] = None,
+    use_spot: bool = False,
+    region: Optional[str] = None,
+    zone: Optional[str] = None,
 ) -> Tuple[Optional[List[str]], List[str]]:
     """
     Returns a list of instance types satisfying the required count of
     accelerators with sorted prices and a list of candidates with fuzzy search.
     """
     result = df[(df['AcceleratorName'].str.fullmatch(acc_name, case=False)) &
                 (df['AcceleratorCount'] == acc_count)]
@@ -215,51 +400,85 @@
                                      'AcceleratorCount']].drop_duplicates()
         fuzzy_candidate_list = []
         if len(fuzzy_result) > 0:
             for _, row in fuzzy_result.iterrows():
                 fuzzy_candidate_list.append(f'{row["AcceleratorName"]}:'
                                             f'{int(row["AcceleratorCount"])}')
         return (None, fuzzy_candidate_list)
+
+    result = _filter_with_cpus(result, cpus)
+    result = _filter_with_mem(result, memory)
+    if region is not None:
+        result = result[result['Region'].str.lower() == region]
+    if zone is not None:
+        # NOTE: For Azure regions, zone must be None.
+        result = result[result['AvailabilityZone'] == zone]
+    if len(result) == 0:
+        return ([], [])
+
     # Current strategy: choose the cheapest instance
-    result = result.sort_values('Price', ascending=True)
+    price_str = 'SpotPrice' if use_spot else 'Price'
+    result = result.sort_values(price_str, ascending=True)
     instance_types = list(result['InstanceType'].drop_duplicates())
     return (instance_types, [])
 
 
 def list_accelerators_impl(
     cloud: str,
     df: pd.DataFrame,
     gpus_only: bool,
     name_filter: Optional[str],
+    region_filter: Optional[str],
+    quantity_filter: Optional[int],
     case_sensitive: bool = True,
 ) -> Dict[str, List[InstanceTypeInfo]]:
     """Lists accelerators offered in a cloud service catalog.
 
     `name_filter` is a regular expression used to filter accelerator names
     using pandas.Series.str.contains.
 
     Returns a mapping from the canonical names of accelerators to a list of
     instance types offered by this cloud.
     """
     if gpus_only:
         df = df[~df['GpuInfo'].isna()]
+    df = df.copy()  # avoid column assignment warning
+
+    try:
+        gpu_info_df = df['GpuInfo'].apply(ast.literal_eval)
+        df['DeviceMemoryGiB'] = gpu_info_df.apply(
+            lambda row: row['Gpus'][0]['MemoryInfo']['SizeInMiB']) / 1024.0
+    except ValueError:
+        # TODO(zongheng,woosuk): GCP/Azure catalogs do not have well-formed
+        # GpuInfo fields. So the above will throw:
+        #  ValueError: malformed node or string: <_ast.Name object at ..>
+        df['DeviceMemoryGiB'] = None
+
     df = df[[
         'InstanceType',
         'AcceleratorName',
         'AcceleratorCount',
         'vCPUs',
-        'MemoryGiB',
+        'DeviceMemoryGiB',  # device memory
+        'MemoryGiB',  # host memory
         'Price',
         'SpotPrice',
+        'Region',
     ]].dropna(subset=['AcceleratorName']).drop_duplicates()
     if name_filter is not None:
         df = df[df['AcceleratorName'].str.contains(name_filter,
                                                    case=case_sensitive,
                                                    regex=True)]
+    if region_filter is not None:
+        df = df[df['Region'].str.contains(region_filter,
+                                          case=case_sensitive,
+                                          regex=True)]
     df['AcceleratorCount'] = df['AcceleratorCount'].astype(int)
+    if quantity_filter is not None:
+        df = df[df['AcceleratorCount'] == quantity_filter]
     grouped = df.groupby('AcceleratorName')
 
     def make_list_from_df(rows):
         # Only keep the lowest prices across regions.
         rows = rows.groupby([
             'InstanceType', 'AcceleratorName', 'AcceleratorCount', 'vCPUs',
             'MemoryGiB'
@@ -268,47 +487,54 @@
         ret = rows.apply(
             lambda row: InstanceTypeInfo(
                 cloud,
                 row['InstanceType'],
                 row['AcceleratorName'],
                 row['AcceleratorCount'],
                 row['vCPUs'],
+                row['DeviceMemoryGiB'],
                 row['MemoryGiB'],
                 row['Price'],
                 row['SpotPrice'],
+                row['Region'],
             ),
             axis='columns',
         ).tolist()
         ret.sort(key=lambda info: (info.accelerator_count, info.cpu_count
                                    if info.cpu_count is not None else 0))
         return ret
 
     return {k: make_list_from_df(v) for k, v in grouped}
 
 
 def get_region_zones(df: pd.DataFrame,
                      use_spot: bool) -> List[cloud_lib.Region]:
     """Returns a list of regions/zones from a dataframe."""
     price_str = 'SpotPrice' if use_spot else 'Price'
-    df = df.dropna(subset=[price_str]).sort_values(price_str)
+    sort_keys = [price_str, 'Region']
+    if 'AvailabilityZone' in df.columns:
+        sort_keys.append('AvailabilityZone')
+    # If NaN appears in any of the sort keys, drop the row, as that means
+    # errors in the data.
+    df = df.dropna(subset=sort_keys).sort_values(sort_keys)
     regions = [cloud_lib.Region(region) for region in df['Region'].unique()]
     if 'AvailabilityZone' in df.columns:
         zones_in_region = df.groupby('Region')['AvailabilityZone'].apply(
             lambda x: [cloud_lib.Zone(zone) for zone in x])
         for region in regions:
             region.set_zones(zones_in_region[region.name])
     return regions
 
 
 def _accelerator_in_region(df: pd.DataFrame, acc_name: str, acc_count: int,
                            region: str) -> bool:
     """Returns True if the accelerator is in the region."""
     return len(df[(df['AcceleratorName'] == acc_name) &
                   (df['AcceleratorCount'] == acc_count) &
-                  (df['Region'] == region)]) > 0
+                  (df['Region'].str.lower() == region.lower())]) > 0
 
 
 def _accelerator_in_zone(df: pd.DataFrame, acc_name: str, acc_count: int,
                          zone: str) -> bool:
     """Returns True if the accelerator is in the zone."""
     return len(df[(df['AcceleratorName'] == acc_name) &
                   (df['AcceleratorCount'] == acc_count) &
@@ -322,14 +548,15 @@
     region: Optional[str] = None,
     zone: Optional[str] = None,
 ) -> bool:
     """Returns True if the accelerator is in the region or zone."""
     assert region is not None or zone is not None, (
         'Both region and zone are None.')
     if zone is None:
+        assert region is not None
         return _accelerator_in_region(df, accelerator_name, acc_count, region)
     else:
         return _accelerator_in_zone(df, accelerator_name, acc_count, zone)
 
 
 # Images
 def get_image_id_from_tag_impl(df: pd.DataFrame, tag: str,
@@ -339,15 +566,15 @@
     If region is None, there must be only one image with the given tag.
 
     Returns None if a region (or globally if region is None) does not have
     an image that matches the tag.
     """
     df = df[df['Tag'] == tag]
     if region is not None:
-        df = df[df['Region'] == region]
+        df = df[df['Region'].str.lower() == region.lower()]
     assert len(df) <= 1, ('Multiple images found for tag '
                           f'{tag} in region {region}')
     if len(df) == 0:
         return None
     image_id = df['ImageId'].iloc[0]
     if pd.isna(image_id):
         return None
@@ -355,10 +582,10 @@
 
 
 def is_image_tag_valid_impl(df: pd.DataFrame, tag: str,
                             region: Optional[str]) -> bool:
     """Returns True if the image tag is valid."""
     df = df[df['Tag'] == tag]
     if region is not None:
-        df = df[df['Region'] == region]
+        df = df[df['Region'].str.lower() == region.lower()]
     df = df.dropna(subset=['ImageId'])
     return len(df) > 0
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/clouds/service_catalog/data_fetchers/fetch_azure.py` & `skypilot-nightly-1.0.0.dev20230713/sky/clouds/service_catalog/data_fetchers/fetch_azure.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,52 +1,58 @@
 """A script that queries Azure API to get instance types and pricing info.
 
 This script takes about 1 minute to finish.
 """
 import argparse
 import json
+from multiprocessing import pool as mp_pool
 import os
 import subprocess
-from typing import Optional, Set, Tuple
+from typing import List, Optional, Set
 import urllib
 
 import numpy as np
 import pandas as pd
-import ray
 import requests
 
 US_REGIONS = [
     'centralus',
     'eastus',
     'eastus2',
     'northcentralus',
     'southcentralus',
     'westcentralus',
     'westus',
     'westus2',
-    # 'WestUS3',   # WestUS3 pricing table is broken as of 2021/11.
+    'westus3',
 ]
 
+# Exclude the following regions as they do not have ProductName in the
+# pricing table. Reference: #1768
+EXCLUDED_REGIONS = {
+    'eastus2euap',
+    'centraluseuap',
+}
 
-# To enable all the regions, uncomment the following line.
-def get_regions() -> Tuple[str]:
+
+def get_regions() -> List[str]:
     """Get all available regions."""
     proc = subprocess.run(
         'az account list-locations  --query "[?not_null(metadata.latitude)] '
         '.{RegionName:name , RegionDisplayName:regionalDisplayName}" -o json',
         shell=True,
         check=True,
         stdout=subprocess.PIPE)
     items = json.loads(proc.stdout.decode('utf-8'))
     regions = [
         item['RegionName']
         for item in items
         if not item['RegionName'].endswith('stg')
     ]
-    return tuple(regions)
+    return regions
 
 
 # Azure secretly deprecated the M60 family which is still returned by its API.
 # We have to manually remove it.
 DEPRECATED_FAMILIES = ['standardNVSv2Family']
 
 USEFUL_COLUMNS = [
@@ -62,73 +68,64 @@
     ]
     if region is not None:
         filters.append(f'armRegionName eq \'{region}\'')
     filters_str = urllib.parse.quote(' and '.join(filters))
     return f'https://prices.azure.com/api/retail/prices?$filter={filters_str}'
 
 
-@ray.remote
 def get_pricing_df(region: Optional[str] = None) -> pd.DataFrame:
     all_items = []
     url = get_pricing_url(region)
     print(f'Getting pricing for {region}')
     page = 0
     while url is not None:
         page += 1
         if page % 10 == 0:
             print(f'Fetched pricing pages {page}')
         r = requests.get(url)
         r.raise_for_status()
-        content = r.content.decode('ascii')
-        content = json.loads(content)
+        content_str = r.content.decode('ascii')
+        content = json.loads(content_str)
         items = content.get('Items', [])
         if len(items) == 0:
             break
         all_items += items
         url = content.get('NextPageLink')
     print(f'Done fetching pricing {region}')
     df = pd.DataFrame(all_items)
     assert 'productName' in df.columns, (region, df.columns)
     return df[(~df['productName'].str.contains(' Windows')) &
               (df['unitPrice'] > 0)]
 
 
-@ray.remote
-def get_all_regions_pricing_df(regions: Set[str]) -> pd.DataFrame:
-    dfs = ray.get([get_pricing_df.remote(region) for region in regions])
-    return pd.concat(dfs)
-
-
-@ray.remote
 def get_sku_df(region_set: Set[str]) -> pd.DataFrame:
     print('Fetching SKU list')
     # To get a complete list, --all option is necessary.
     proc = subprocess.run(
-        'az vm list-skus --all',
+        'az vm list-skus --all --resource-type virtualMachines -o json',
         shell=True,
         check=True,
         stdout=subprocess.PIPE,
     )
     print('Done fetching SKUs')
     items = json.loads(proc.stdout.decode('ascii'))
     filtered_items = []
     for item in items:
         # zones = item['locationInfo'][0]['zones']
         region = item['locations'][0]
-        if region not in region_set:
+        if region.lower() not in region_set:
             continue
         item['Region'] = region
         filtered_items.append(item)
 
     df = pd.DataFrame(filtered_items)
-    df = df[(df['resourceType'] == 'virtualMachines')]
     return df
 
 
-def get_gpu_name(family: str) -> str:
+def get_gpu_name(family: str) -> Optional[str]:
     gpu_data = {
         'standardNCFamily': 'K80',
         'standardNCSv2Family': 'P100',
         'standardNCSv3Family': 'V100',
         'standardNCPromoFamily': 'K80',
         'StandardNCASv3_T4Family': 'T4',
         'standardNDSv2Family': 'V100-32GB',
@@ -147,96 +144,105 @@
     # so we do not include them here.
     # https://docs.microsoft.com/en-us/azure/virtual-machines/np-series
     family = family.replace(' ', '')
     return gpu_data.get(family)
 
 
 def get_all_regions_instance_types_df(region_set: Set[str]):
-    df, df_sku = ray.get([
-        get_all_regions_pricing_df.remote(region_set),
-        get_sku_df.remote(region_set),
-    ])
+    with mp_pool.Pool() as pool:
+        dfs = pool.map_async(get_pricing_df, region_set)
+        df_sku = pool.apply_async(get_sku_df, (region_set,))
+        dfs = dfs.get()
+        df = pd.concat(dfs)
+        df_sku = df_sku.get()
+
     print('Processing dataframes')
     df.drop_duplicates(inplace=True)
 
     df = df[df['unitPrice'] > 0]
 
     print('Getting price df')
     df['merge_name'] = df['armSkuName']
+    # Use lower case for the Region, as for westus3, the SKU API returns
+    # WestUS3.
+    # This is inconsistent with the region name used in the pricing API, and
+    # the case does not matter for launching instances, so we can safely
+    # discard the case.
+    df['Region'] = df['armRegionName'].str.lower()
     df['is_promo'] = df['skuName'].str.endswith(' Low Priority')
     df.rename(columns={
         'armSkuName': 'InstanceType',
-        'armRegionName': 'Region',
-    },
-              inplace=True)
+    }, inplace=True)
     demand_df = df[~df['skuName'].str.contains(' Spot')][[
         'is_promo', 'InstanceType', 'Region', 'unitPrice'
     ]]
     spot_df = df[df['skuName'].str.contains(' Spot')][[
         'is_promo', 'InstanceType', 'Region', 'unitPrice'
     ]]
+
     demand_df.set_index(['InstanceType', 'Region', 'is_promo'], inplace=True)
     spot_df.set_index(['InstanceType', 'Region', 'is_promo'], inplace=True)
 
     demand_df = demand_df.rename(columns={'unitPrice': 'Price'})
     spot_df = spot_df.rename(columns={'unitPrice': 'SpotPrice'})
 
     print('Getting sku df')
     df_sku['is_promo'] = df_sku['name'].str.endswith('_Promo')
     df_sku.rename(columns={'name': 'InstanceType'}, inplace=True)
+
     df_sku['merge_name'] = df_sku['InstanceType'].str.replace('_Promo', '')
+    df_sku['Region'] = df_sku['Region'].str.lower()
 
     print('Joining')
     df = df_sku.join(demand_df,
                      on=['merge_name', 'Region', 'is_promo'],
                      how='left')
     df = df.join(spot_df, on=['merge_name', 'Region', 'is_promo'], how='left')
 
     def get_capabilities(row):
         gpu_name = None
         gpu_count = np.nan
         vcpus = np.nan
-        memory_gb = np.nan
+        memory = np.nan
         gen_version = None
         caps = row['capabilities']
         for item in caps:
             assert isinstance(item, dict), (item, caps)
             if item['name'] == 'GPUs':
                 gpu_name = get_gpu_name(row['family'])
                 if gpu_name is not None:
                     gpu_count = item['value']
             elif item['name'] == 'vCPUs':
                 vcpus = float(item['value'])
             elif item['name'] == 'MemoryGB':
-                memory_gb = item['value']
+                memory = item['value']
             elif item['name'] == 'HyperVGenerations':
                 gen_version = item['value']
-        return gpu_name, gpu_count, vcpus, memory_gb, gen_version
+        return gpu_name, gpu_count, vcpus, memory, gen_version
 
     def get_additional_columns(row):
-        gpu_name, gpu_count, vcpus, memory_gb, gen_version = get_capabilities(
-            row)
+        gpu_name, gpu_count, vcpus, memory, gen_version = get_capabilities(row)
         return pd.Series({
             'AcceleratorName': gpu_name,
             'AcceleratorCount': gpu_count,
             'vCPUs': vcpus,
-            'MemoryGiB': memory_gb,
+            'MemoryGiB': memory,
             'GpuInfo': gpu_name,
             'Generation': gen_version,
         })
 
     df_ret = pd.concat(
         [df, df.apply(get_additional_columns, axis='columns')],
         axis='columns',
     )
 
     before_drop_len = len(df_ret)
     df_ret.dropna(subset=['InstanceType'], inplace=True, how='all')
     after_drop_len = len(df_ret)
-    print('Dropped {} duplicated rows'.format(before_drop_len - after_drop_len))
+    print(f'Dropped {before_drop_len - after_drop_len} duplicated rows')
 
     # Filter out deprecated families
     df_ret = df_ret.loc[~df_ret['family'].isin(DEPRECATED_FAMILIES)]
     df_ret = df_ret[USEFUL_COLUMNS]
     return df_ret
 
 
@@ -244,15 +250,14 @@
     parser = argparse.ArgumentParser()
     parser.add_argument(
         '--all-regions',
         action='store_true',
         help='Fetch all global regions, not just the U.S. ones.')
     args = parser.parse_args()
 
-    ray.init()
     region_filter = get_regions() if args.all_regions else US_REGIONS
-    region_filter = set(region_filter)
+    region_filter = set(region_filter) - EXCLUDED_REGIONS
 
     instance_df = get_all_regions_instance_types_df(region_filter)
     os.makedirs('azure', exist_ok=True)
     instance_df.to_csv('azure/vms.csv', index=False)
     print('Azure Service Catalog saved to azure/vms.csv')
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/clouds/service_catalog/gcp_catalog.py` & `skypilot-nightly-1.0.0.dev20230713/sky/clouds/service_catalog/gcp_catalog.py`

 * *Files 20% similar despite different names*

```diff
@@ -12,26 +12,53 @@
 from sky import exceptions
 from sky.clouds.service_catalog import common
 from sky.utils import ux_utils
 
 if typing.TYPE_CHECKING:
     from sky.clouds import cloud
 
-_df = common.read_catalog('gcp/vms.csv')
-_image_df = common.read_catalog('gcp/images.csv')
+# Pull the latest catalog every week.
+# GCP guarantees that the catalog is updated at most once per 30 days, but
+# different VMs can be updated at different time. Thus, we pull the catalog
+# every 7 hours to make sure we have the latest information.
+_PULL_FREQUENCY_HOURS = 7
+
+_df = common.read_catalog('gcp/vms.csv',
+                          pull_frequency_hours=_PULL_FREQUENCY_HOURS)
+_image_df = common.read_catalog('gcp/images.csv',
+                                pull_frequency_hours=_PULL_FREQUENCY_HOURS)
 
 _TPU_REGIONS = [
     'us-central1',
     'europe-west4',
     'asia-east1',
 ]
 
+# We will select from the following three CPU instance families:
+_DEFAULT_INSTANCE_FAMILY = [
+    # This is the latest general-purpose instance family as of Mar 2023.
+    # CPU: Intel Ice Lake 8373C or Cascade Lake 6268CL.
+    # Memory: 4 GiB RAM per 1 vCPU;
+    'n2-standard',
+    # This is the latest memory-optimized instance family as of Mar 2023.
+    # CPU: Intel Ice Lake 8373C or Cascade Lake 6268CL.
+    # Memory: 8 GiB RAM per 1 vCPU;
+    'n2-highmem',
+    # This is the latest compute-optimized instance family as of Mar 2023.
+    # CPU: Intel Ice Lake 8373C or Cascade Lake 6268CL.
+    # Memory: 1 GiB RAM per 1 vCPU;
+    'n2-highcpu',
+]
+_DEFAULT_NUM_VCPUS = 8
+_DEFAULT_MEMORY_CPU_RATIO = 4
+
 # This can be switched between n1 and n2.
 # n2 is not allowed for launching GPUs.
 _DEFAULT_HOST_VM_FAMILY = 'n1'
+_DEFAULT_GPU_MEMORY_CPU_RATIO = 4
 
 # TODO(zongheng): fix A100 info directly in catalog.
 # https://cloud.google.com/blog/products/compute/a2-vms-with-nvidia-a100-gpus-are-ga
 # count -> vm type
 _A100_INSTANCE_TYPE_DICTS = {
     'A100': {
         1: 'a2-highgpu-1g',
@@ -123,15 +150,15 @@
     }
 }
 
 
 def _is_power_of_two(x: int) -> bool:
     """Returns true if x is a power of two."""
     # https://stackoverflow.com/questions/600293/how-to-check-if-a-number-is-a-power-of-2
-    return x and not x & (x - 1)
+    return bool(x and not x & (x - 1))
 
 
 def _closest_power_of_two(x: int) -> int:
     """Returns the closest power of 2 less than or equal to x."""
     if _is_power_of_two(x):
         return x
     return 1 << ((x - 1).bit_length() - 1)
@@ -142,74 +169,110 @@
     if instance_type == 'TPU-VM':
         return True
     return common.instance_type_exists_impl(_df, instance_type)
 
 
 def get_hourly_cost(
     instance_type: str,
-    region: Optional[str] = None,
     use_spot: bool = False,
+    region: Optional[str] = None,
+    zone: Optional[str] = None,
 ) -> float:
-    """Returns the hourly price for a given instance type and region."""
     if instance_type == 'TPU-VM':
         # Currently the host VM of TPU does not cost extra.
         return 0
-    return common.get_hourly_cost_impl(_df, instance_type, region, use_spot)
+    return common.get_hourly_cost_impl(_df, instance_type, use_spot, region,
+                                       zone)
 
 
-def get_vcpus_from_instance_type(instance_type: str) -> Optional[float]:
-    # The number of vCPUs provided with a TPU VM is not officially documented.
+def get_vcpus_mem_from_instance_type(
+        instance_type: str) -> Tuple[Optional[float], Optional[float]]:
+    # The number of vCPUs and memory size provided with a TPU VM is not
+    # officially documented.
     if instance_type == 'TPU-VM':
-        return None
-    return common.get_vcpus_from_instance_type_impl(_df, instance_type)
+        return None, None
+    return common.get_vcpus_mem_from_instance_type_impl(_df, instance_type)
+
+
+def get_default_instance_type(cpus: Optional[str] = None,
+                              memory: Optional[str] = None,
+                              disk_tier: Optional[str] = None) -> Optional[str]:
+    del disk_tier  # unused
+    if cpus is None and memory is None:
+        cpus = f'{_DEFAULT_NUM_VCPUS}+'
+    if memory is None:
+        memory_gb_or_ratio = f'{_DEFAULT_MEMORY_CPU_RATIO}x'
+    else:
+        memory_gb_or_ratio = memory
+    instance_type_prefix = tuple(
+        f'{family}-' for family in _DEFAULT_INSTANCE_FAMILY)
+    df = _df[_df['InstanceType'].notna()]
+    df = df[df['InstanceType'].str.startswith(instance_type_prefix)]
+    return common.get_instance_type_for_cpus_mem_impl(df, cpus,
+                                                      memory_gb_or_ratio)
 
 
 def get_instance_type_for_accelerator(
-        acc_name: str, acc_count: int) -> Tuple[Optional[List[str]], List[str]]:
+        acc_name: str,
+        acc_count: int,
+        cpus: Optional[str] = None,
+        memory: Optional[str] = None,
+        use_spot: bool = False,
+        region: Optional[str] = None,
+        zone: Optional[str] = None) -> Tuple[Optional[List[str]], List[str]]:
     """Fetch instance types with similar CPU count for given accelerator.
 
     Return: a list with a single matched instance type and a list of candidates
     with fuzzy search (should be empty as it must have already been generated in
     caller).
     """
     (instance_list,
      fuzzy_candidate_list) = common.get_instance_type_for_accelerator_impl(
-         df=_df, acc_name=acc_name, acc_count=acc_count)
+         _df, acc_name, acc_count, cpus, memory, use_spot, region, zone)
     if instance_list is None:
         return None, fuzzy_candidate_list
 
     if acc_name in _A100_INSTANCE_TYPE_DICTS:
         # If A100 is used, host VM type must be A2.
         # https://cloud.google.com/compute/docs/gpus#a100-gpus
-        return [_A100_INSTANCE_TYPE_DICTS[acc_name][acc_count]], []
+
+        df = _df[_df['InstanceType'].notna()]
+        instance_type = _A100_INSTANCE_TYPE_DICTS[acc_name][acc_count]
+        df = df[df['InstanceType'] == instance_type]
+
+        # Check the cpus and memory specified by the user.
+        instance_type = common.get_instance_type_for_cpus_mem_impl(
+            df, cpus, memory)
+        if instance_type is None:
+            return None, []
+        return [instance_type], []
+
     if acc_name not in _NUM_ACC_TO_NUM_CPU:
         acc_name = 'DEFAULT'
 
-    num_cpus = _NUM_ACC_TO_NUM_CPU[acc_name].get(acc_count, None)
-    # The (acc_name, acc_count) should be validated in the caller.
-    assert num_cpus is not None, (acc_name, acc_count)
-    mem_type = 'highmem'
-    # patches for the number of cores per GPU, as some of the combinations
-    # are not supported by GCP.
-    if _DEFAULT_HOST_VM_FAMILY == 'n1':
-        if num_cpus < 96:
-            num_cpus = _closest_power_of_two(num_cpus)
-        else:
-            num_cpus = 96
-    else:
-        if num_cpus > 80:
-            mem_type = 'standard'
+    assert _DEFAULT_HOST_VM_FAMILY == 'n1'
+
+    if cpus is None and memory is None:
+        cpus = f'{_NUM_ACC_TO_NUM_CPU[acc_name].get(acc_count, None)}+'
+    if memory is None:
+        memory = f'{_DEFAULT_GPU_MEMORY_CPU_RATIO}x'
+    df = _df[_df['InstanceType'].notna()]
+    df = df[df['InstanceType'].str.startswith(f'{_DEFAULT_HOST_VM_FAMILY}-')]
+
+    instance_type = common.get_instance_type_for_cpus_mem_impl(df, cpus, memory)
     # The fuzzy candidate should have already been fetched in the caller.
-    return [f'{_DEFAULT_HOST_VM_FAMILY}-{mem_type}-{num_cpus}'], []
+    if instance_type is None:
+        return None, []
+    return [instance_type], []
 
 
 def validate_region_zone(
         region: Optional[str],
         zone: Optional[str]) -> Tuple[Optional[str], Optional[str]]:
-    return common.validate_region_zone_impl(_df, region, zone)
+    return common.validate_region_zone_impl('gcp', _df, region, zone)
 
 
 def accelerator_in_region_or_zone(acc_name: str,
                                   acc_count: int,
                                   region: Optional[str] = None,
                                   zone: Optional[str] = None) -> bool:
     return common.accelerator_in_region_or_zone_impl(_df, acc_name, acc_count,
@@ -223,36 +286,40 @@
 
 
 def _get_accelerator(
     df: pd.DataFrame,
     accelerator: str,
     count: int,
     region: Optional[str],
+    zone: Optional[str] = None,
 ) -> pd.DataFrame:
     idx = (df['AcceleratorName'].str.fullmatch(
         accelerator, case=False)) & (df['AcceleratorCount'] == count)
     if region is not None:
         idx &= df['Region'] == region
+    if zone is not None:
+        idx &= df['AvailabilityZone'] == zone
     return df[idx]
 
 
 def get_accelerator_hourly_cost(accelerator: str,
                                 count: int,
+                                use_spot: bool = False,
                                 region: Optional[str] = None,
-                                use_spot: bool = False) -> float:
-    """Returns the cost, or the cheapest cost among all zones for spot."""
+                                zone: Optional[str] = None) -> float:
     # NOTE: As of 2022/4/13, Prices of TPU v3-64 to v3-2048 are not available on
     # https://cloud.google.com/tpu/pricing. We put estimates in gcp catalog.
     if region is None:
         for tpu_region in _TPU_REGIONS:
             df = _get_accelerator(_df, accelerator, count, tpu_region)
             if len(set(df['Price'])) == 1:
                 region = tpu_region
                 break
-    df = _get_accelerator(_df, accelerator, count, region)
+
+    df = _get_accelerator(_df, accelerator, count, region, zone)
     assert len(set(df['Price'])) == 1, df
     if not use_spot:
         return df['Price'].iloc[0]
 
     cheapest_idx = df['SpotPrice'].idxmin()
     if pd.isnull(cheapest_idx):
         return df['Price'].iloc[0]
@@ -260,20 +327,33 @@
     cheapest = df.loc[cheapest_idx]
     return cheapest['SpotPrice']
 
 
 def list_accelerators(
     gpus_only: bool,
     name_filter: Optional[str] = None,
+    region_filter: Optional[str] = None,
+    quantity_filter: Optional[int] = None,
     case_sensitive: bool = True,
 ) -> Dict[str, List[common.InstanceTypeInfo]]:
     """Returns all instance types in GCP offering GPUs."""
     results = common.list_accelerators_impl('GCP', _df, gpus_only, name_filter,
+                                            region_filter, quantity_filter,
                                             case_sensitive)
 
+    # Remove GPUs that are unsupported by SkyPilot.
+    new_results = {}
+    for acc_name, acc_info in results.items():
+        if (acc_name.startswith('tpu') or
+                acc_name in _NUM_ACC_TO_MAX_CPU_AND_MEMORY or
+                acc_name in _A100_INSTANCE_TYPE_DICTS):
+            new_results[acc_name] = acc_info
+            new_results[acc_name] = acc_info
+    results = new_results
+
     a100_infos = results.get('A100', []) + results.get('A100-80GB', [])
     if not a100_infos:
         return results
 
     # Unlike other GPUs that can be attached to different sizes of N1 VMs,
     # A100 GPUs can only be attached to fixed-size A2 VMs.
     # Thus, we can show their exact cost including the host VM prices.
@@ -283,20 +363,22 @@
         a100_host_vm_type = _A100_INSTANCE_TYPE_DICTS[info.accelerator_name][
             info.accelerator_count]
         df = _df[_df['InstanceType'] == a100_host_vm_type]
         cpu_count = df['vCPUs'].iloc[0]
         memory = df['MemoryGiB'].iloc[0]
         vm_price = common.get_hourly_cost_impl(_df,
                                                a100_host_vm_type,
-                                               None,
-                                               use_spot=False)
+                                               use_spot=False,
+                                               region=None,
+                                               zone=None)
         vm_spot_price = common.get_hourly_cost_impl(_df,
                                                     a100_host_vm_type,
-                                                    None,
-                                                    use_spot=True)
+                                                    use_spot=True,
+                                                    region=None,
+                                                    zone=None)
         new_infos[info.accelerator_name].append(
             info._replace(
                 instance_type=a100_host_vm_type,
                 cpu_count=cpu_count,
                 memory=memory,
                 # total cost = VM instance + GPU.
                 price=info.price + vm_price,
@@ -387,21 +469,22 @@
         return
 
     acc = list(accelerators.items())
     assert len(acc) == 1, acc
     acc_name, acc_count = acc[0]
 
     if acc_name.startswith('tpu-'):
-        # TODO(woosuk): Check max vcpus and memory for each TPU type.
+        # TODO(woosuk): Check max vCPUs and memory for each TPU type.
         assert instance_type == 'TPU-VM' or instance_type.startswith('n1-')
         return
 
     if acc_name in _A100_INSTANCE_TYPE_DICTS:
         valid_counts = list(_A100_INSTANCE_TYPE_DICTS[acc_name].keys())
     else:
+        assert acc_name in _NUM_ACC_TO_MAX_CPU_AND_MEMORY, acc_name
         valid_counts = list(_NUM_ACC_TO_MAX_CPU_AND_MEMORY[acc_name].keys())
     if acc_count not in valid_counts:
         with ux_utils.print_exception_no_traceback():
             raise exceptions.ResourcesMismatchError(
                 f'{acc_name}:{acc_count} is not launchable on GCP. '
                 f'The valid {acc_name} counts are {valid_counts}.')
 
@@ -445,10 +528,12 @@
 
 
 def get_image_id_from_tag(tag: str, region: Optional[str]) -> Optional[str]:
     """Returns the image id from the tag."""
     return common.get_image_id_from_tag_impl(_image_df, tag, region)
 
 
-def validate_image_tag(tag: str, region: Optional[str]) -> bool:
+def is_image_tag_valid(tag: str, region: Optional[str]) -> bool:
     """Returns whether the image tag is valid."""
-    return common.is_image_tag_valid_impl(_image_df, tag, region)
+    # GCP images are not region-specific.
+    del region  # Unused.
+    return common.is_image_tag_valid_impl(_image_df, tag, None)
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/core.py` & `skypilot-nightly-1.0.0.dev20230713/sky/core.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,43 +1,50 @@
 """SDK functions for cluster/job management."""
-import colorama
 import getpass
 import sys
-from typing import Any, Dict, List, Optional, Tuple
+from typing import Any, Dict, List, Optional, Union
+
+import colorama
 
+from sky import clouds
 from sky import dag
 from sky import task
 from sky import backends
 from sky import data
 from sky import exceptions
 from sky import global_user_state
 from sky import sky_logging
 from sky import spot
+from sky import status_lib
 from sky.backends import backend_utils
-from sky.backends import onprem_utils
 from sky.skylet import constants
 from sky.skylet import job_lib
 from sky.usage import usage_lib
+from sky.utils import log_utils
 from sky.utils import tpu_utils
 from sky.utils import ux_utils
 from sky.utils import subprocess_utils
 
 logger = sky_logging.init_logger(__name__)
 
 # ======================
 # = Cluster Management =
 # ======================
 
 # pylint: disable=redefined-builtin
 
 
 @usage_lib.entrypoint
-def status(refresh: bool = False) -> List[Dict[str, Any]]:
+def status(cluster_names: Optional[Union[str, List[str]]] = None,
+           refresh: bool = False) -> List[Dict[str, Any]]:
     # NOTE(dev): Keep the docstring consistent between the Python API and CLI.
-    """Get all cluster statuses.
+    """Get cluster statuses.
+
+    If cluster_names is given, return those clusters. Otherwise, return all
+    clusters.
 
     Each returned value has the following fields:
 
     .. code-block:: python
 
         {
             'name': (str) cluster name,
@@ -47,52 +54,151 @@
               cluster,
             'status': (sky.ClusterStatus) cluster status,
             'autostop': (int) idle time before autostop,
             'to_down': (bool) whether autodown is used instead of autostop,
             'metadata': (dict) metadata of the cluster,
         }
 
+    Each cluster can have one of the following statuses:
+
+    - ``INIT``: The cluster may be live or down. It can happen in the following
+      cases:
+
+      - Ongoing provisioning or runtime setup. (A ``sky.launch()`` has started
+        but has not completed.)
+      - Or, the cluster is in an abnormal state, e.g., some cluster nodes are
+        down, or the SkyPilot runtime is unhealthy. (To recover the cluster,
+        try ``sky launch`` again on it.)
+
+    - ``UP``: Provisioning and runtime setup have succeeded and the cluster is
+      live.  (The most recent ``sky.launch()`` has completed successfully.)
+
+    - ``STOPPED``: The cluster is stopped and the storage is persisted. Use
+      ``sky.start()`` to restart the cluster.
+
+    Autostop column:
+
+    - The autostop column indicates how long the cluster will be autostopped
+      after minutes of idling (no jobs running). If ``to_down`` is True, the
+      cluster will be autodowned, rather than autostopped.
+
+    Getting up-to-date cluster statuses:
+
+    - In normal cases where clusters are entirely managed by SkyPilot (i.e., no
+      manual operations in cloud consoles) and no autostopping is used, the
+      table returned by this command will accurately reflect the cluster
+      statuses.
+
+    - In cases where the clusters are changed outside of SkyPilot (e.g., manual
+      operations in cloud consoles; unmanaged spot clusters getting preempted)
+      or for autostop-enabled clusters, use ``refresh=True`` to query the
+      latest cluster statuses from the cloud providers.
+
     Args:
+        cluster_names: a list of cluster names to query. If not
+            provided, all clusters will be queried.
         refresh: whether to query the latest cluster statuses from the cloud
             provider(s).
 
     Returns:
         A list of dicts, with each dict containing the information of a
+        cluster. If a cluster is found to be terminated or not found, it will
+        be omitted from the returned list.
+    """
+    return backend_utils.get_clusters(include_reserved=True,
+                                      refresh=refresh,
+                                      cluster_names=cluster_names)
+
+
+@usage_lib.entrypoint
+def cost_report() -> List[Dict[str, Any]]:
+    # NOTE(dev): Keep the docstring consistent between the Python API and CLI.
+    """Get all cluster cost reports, including those that have been downed.
+
+    Each returned value has the following fields:
+
+    .. code-block:: python
+
+        {
+            'name': (str) cluster name,
+            'launched_at': (int) timestamp of last launch on this cluster,
+            'duration': (int) total seconds that cluster was up and running,
+            'last_use': (str) the last command/entrypoint that affected this
+            'num_nodes': (int) number of nodes launched for cluster,
+            'resources': (resources.Resources) type of resource launched,
+            'cluster_hash': (str) unique hash identifying cluster,
+            'usage_intervals': (List[Tuple[int, int]]) cluster usage times,
+            'total_cost': (float) cost given resources and usage intervals,
+        }
+
+    The estimated cost column indicates price for the cluster based on the type
+    of resources being used and the duration of use up until the call to
+    status. This means if the cluster is UP, successive calls to report will
+    show increasing price. The estimated cost is calculated based on the local
+    cache of the cluster status, and may not be accurate for the cluster with
+    autostop/use_spot set or terminated/stopped on the cloud console.
+
+    Returns:
+        A list of dicts, with each dict containing the cost information of a
         cluster.
     """
-    cluster_records = backend_utils.get_clusters(include_reserved=True,
-                                                 refresh=refresh)
-    return cluster_records
+    cluster_reports = global_user_state.get_clusters_from_history()
+
+    def get_total_cost(cluster_report: dict) -> float:
+        duration = cluster_report['duration']
+        launched_nodes = cluster_report['num_nodes']
+        launched_resources = cluster_report['resources']
+
+        cost = (launched_resources.get_cost(duration) * launched_nodes)
+        return cost
+
+    for cluster_report in cluster_reports:
+        cluster_report['total_cost'] = get_total_cost(cluster_report)
+
+    return cluster_reports
 
 
 def _start(
     cluster_name: str,
     idle_minutes_to_autostop: Optional[int] = None,
     retry_until_up: bool = False,
     down: bool = False,  # pylint: disable=redefined-outer-name
     force: bool = False,
-) -> backends.Backend.ResourceHandle:
+) -> backends.CloudVmRayResourceHandle:
 
     cluster_status, handle = backend_utils.refresh_cluster_status_handle(
         cluster_name)
     if handle is None:
         raise ValueError(f'Cluster {cluster_name!r} does not exist.')
-    if not force and cluster_status == global_user_state.ClusterStatus.UP:
-        print(f'Cluster {cluster_name!r} is already up.')
-        return
+    if not force and cluster_status == status_lib.ClusterStatus.UP:
+        sky_logging.print(f'Cluster {cluster_name!r} is already up.')
+        return handle
     assert force or cluster_status in (
-        global_user_state.ClusterStatus.INIT,
-        global_user_state.ClusterStatus.STOPPED), cluster_status
+        status_lib.ClusterStatus.INIT,
+        status_lib.ClusterStatus.STOPPED), cluster_status
 
     backend = backend_utils.get_backend_from_handle(handle)
     if not isinstance(backend, backends.CloudVmRayBackend):
         raise exceptions.NotSupportedError(
             f'Starting cluster {cluster_name!r} with backend {backend.NAME} '
             'is not supported.')
 
+    if cluster_name == spot.SPOT_CONTROLLER_NAME:
+        if down:
+            raise ValueError('Using autodown (rather than autostop) is not '
+                             'supported for the spot controller. Pass '
+                             '`down=False` or omit it instead.')
+        if idle_minutes_to_autostop is not None:
+            raise ValueError(
+                'Passing a custom autostop setting is currently not '
+                'supported when starting the spot controller. To '
+                'fix: omit the `idle_minutes_to_autostop` argument to use the '
+                f'default autostop settings (got: {idle_minutes_to_autostop}).')
+        idle_minutes_to_autostop = spot.SPOT_CONTROLLER_IDLE_MINUTES_TO_AUTOSTOP
+
     # NOTE: if spot_queue() calls _start() and hits here, that entrypoint
     # would have a cluster name (the controller) filled in.
     usage_lib.record_cluster_name_for_current_operation(cluster_name)
 
     with dag.Dag():
         dummy_task = task.Task().set_resources(handle.launched_resources)
         dummy_task.num_nodes = handle.launched_nodes
@@ -144,19 +250,25 @@
         down: Autodown the cluster: tear down the cluster after specified
             minutes of idle time after all jobs finish (successfully or
             abnormally). Requires ``idle_minutes_to_autostop`` to be set.
         force: whether to force start the cluster even if it is already up.
             Useful for upgrading SkyPilot runtime.
 
     Raises:
-        ValueError: the specified cluster does not exist; or if ``down`` is set
-          to True but ``idle_minutes_to_autostop`` is None.
+        ValueError: argument values are invalid: (1) the specified cluster does
+          not exist; (2) if ``down`` is set to True but
+          ``idle_minutes_to_autostop`` is None; (3) if the specified cluster is
+          the managed spot controller, and either ``idle_minutes_to_autostop``
+          is not None or ``down`` is True (omit them to use the default
+          autostop settings).
         sky.exceptions.NotSupportedError: if the cluster to restart was
           launched using a non-default backend that does not support this
           operation.
+        sky.exceptions.ClusterOwnerIdentitiesMismatchError: if the cluster to
+            restart was launched by a different user.
     """
     if down and idle_minutes_to_autostop is None:
         raise ValueError(
             '`idle_minutes_to_autostop` must be set if `down` is True.')
     _start(cluster_name,
            idle_minutes_to_autostop,
            retry_until_up,
@@ -188,33 +300,39 @@
     if cluster_name in backend_utils.SKY_RESERVED_CLUSTER_NAMES:
         raise exceptions.NotSupportedError(
             f'Stopping sky reserved cluster {cluster_name!r} '
             f'is not supported.')
     handle = global_user_state.get_handle_from_cluster_name(cluster_name)
     if handle is None:
         raise ValueError(f'Cluster {cluster_name!r} does not exist.')
-    if tpu_utils.is_tpu_vm_pod(handle.launched_resources):
-        # Reference:
-        # https://cloud.google.com/tpu/docs/managing-tpus-tpu-vm#stopping_a_with_gcloud  # pylint: disable=line-too-long
-        raise exceptions.NotSupportedError(
-            f'Stopping cluster {cluster_name!r} with TPU VM Pod '
-            'is not supported.')
 
     backend = backend_utils.get_backend_from_handle(handle)
-    if (isinstance(backend, backends.CloudVmRayBackend) and
-            handle.launched_resources.use_spot):
-        # Disable spot instances to be stopped.
-        # TODO(suquark): enable GCP+spot to be stopped in the future.
-        raise exceptions.NotSupportedError(
-            f'{colorama.Fore.YELLOW}Stopping cluster '
-            f'{cluster_name!r}... skipped.{colorama.Style.RESET_ALL}\n'
-            '  Stopping spot instances is not supported as the attached '
-            'disks will be lost.\n'
-            '  To terminate the cluster instead, run: '
-            f'{colorama.Style.BRIGHT}sky down {cluster_name}')
+
+    if isinstance(backend, backends.CloudVmRayBackend):
+        assert isinstance(handle, backends.CloudVmRayResourceHandle), handle
+        if tpu_utils.is_tpu_vm_pod(handle.launched_resources):
+            # Reference:
+            # https://cloud.google.com/tpu/docs/managing-tpus-tpu-vm#stopping_a_with_gcloud  # pylint: disable=line-too-long
+            raise exceptions.NotSupportedError(
+                f'Stopping cluster {cluster_name!r} with TPU VM Pod '
+                'is not supported.')
+        # Check cloud supports stopping instances
+        cloud = handle.launched_resources.cloud
+        cloud.check_features_are_supported(
+            {clouds.CloudImplementationFeatures.STOP})
+        if handle.launched_resources.use_spot:
+            # Disable spot instances to be stopped.
+            # TODO(suquark): enable GCP+spot to be stopped in the future.
+            raise exceptions.NotSupportedError(
+                f'{colorama.Fore.YELLOW}Stopping cluster '
+                f'{cluster_name!r}... skipped.{colorama.Style.RESET_ALL}\n'
+                '  Stopping spot instances is not supported as the attached '
+                'disks will be lost.\n'
+                '  To terminate the cluster instead, run: '
+                f'{colorama.Style.BRIGHT}sky down {cluster_name}')
     usage_lib.record_cluster_name_for_current_operation(cluster_name)
     backend.teardown(handle, terminate=False, purge=purge)
 
 
 @usage_lib.entrypoint
 def down(cluster_name: str, purge: bool = False) -> None:
     # NOTE(dev): Keep the docstring consistent between the Python API and CLI.
@@ -230,14 +348,15 @@
 
     Args:
         cluster_name: name of the cluster to down.
         purge: whether to ignore cloud provider errors (if any).
 
     Raises:
         ValueError: the specified cluster does not exist.
+        RuntimeError: failed to tear down the cluster.
         sky.exceptions.NotSupportedError: the specified cluster is the managed
           spot controller.
     """
     handle = global_user_state.get_handle_from_cluster_name(cluster_name)
     if handle is None:
         raise ValueError(f'Cluster {cluster_name!r} does not exist.')
 
@@ -249,113 +368,106 @@
 @usage_lib.entrypoint
 def autostop(
         cluster_name: str,
         idle_minutes: int,
         down: bool = False,  # pylint: disable=redefined-outer-name
 ) -> None:
     # NOTE(dev): Keep the docstring consistent between the Python API and CLI.
-    """Schedule or cancel an autostop/autodown for a cluster.
+    """Schedule an autostop/autodown for a cluster.
+
+    Autostop/autodown will automatically stop or teardown a cluster when it
+    becomes idle for a specified duration.  Idleness means there are no
+    in-progress (pending/running) jobs in a cluster's job queue.
+
+    Idleness time of a cluster is reset to zero, whenever:
 
-    When multiple configurations are specified for the same cluster (e.g., this
-    function is called multiple times), the last one takes precedence.
+    - A job is submitted (``sky.launch()`` or ``sky.exec()``).
+
+    - The cluster has restarted.
+
+    - An autostop is set when there is no active setting. (Namely, either
+      there's never any autostop setting set, or the previous autostop setting
+      was canceled.) This is useful for restarting the autostop timer.
+
+    Example: say a cluster without any autostop set has been idle for 1 hour,
+    then an autostop of 30 minutes is set. The cluster will not be immediately
+    autostopped. Instead, the idleness timer only starts counting after the
+    autostop setting was set.
+
+    When multiple autostop settings are specified for the same cluster, the
+    last setting takes precedence.
 
     Args:
         cluster_name: name of the cluster.
         idle_minutes: the number of minutes of idleness (no pending/running
           jobs) after which the cluster will be stopped automatically. Setting
-          to a negative number means cancel any autostop/autodown setting.
+          to a negative number cancels any autostop/autodown setting.
         down: if true, use autodown (tear down the cluster; non-restartable),
           rather than autostop (restartable).
 
     Raises:
-        ValueError: the specified cluster does not exist.
-        sky.exceptions.NotSupportedError: if the specified cluster is a TPU VM
-          Pod cluster, or the managed spot controller, or was launched using a
-          non-default backend.
-        sky.exceptions.ClusterNotUpError: the cluster is not UP.
+        ValueError: if the cluster does not exist.
+        sky.exceptions.ClusterNotUpError: if the cluster is not UP.
+        sky.exceptions.NotSupportedError: if the cluster is not based on
+          CloudVmRayBackend or the cluster is TPU VM Pod.
+        sky.exceptions.ClusterOwnerIdentityMismatchError: if the current user is
+          not the same as the user who created the cluster.
+        sky.exceptions.CloudUserIdentityError: if we fail to get the current
+          user identity.
     """
     is_cancel = idle_minutes < 0
     verb = 'Cancelling' if is_cancel else 'Scheduling'
     option_str = 'down' if down else 'stop'
     if is_cancel:
         option_str = '{stop,down}'
-    operation = f'{verb} auto{option_str} on'
+    operation = f'{verb} auto{option_str}'
     if cluster_name in backend_utils.SKY_RESERVED_CLUSTER_NAMES:
         raise exceptions.NotSupportedError(
             f'{operation} sky reserved cluster {cluster_name!r} '
             f'is not supported.')
-    (cluster_status,
-     handle) = backend_utils.refresh_cluster_status_handle(cluster_name)
-    if handle is None:
-        raise ValueError(f'Cluster {cluster_name!r} does not exist.')
+    handle = backend_utils.check_cluster_available(
+        cluster_name,
+        operation=operation,
+    )
+    backend = backend_utils.get_backend_from_handle(handle)
+    if not isinstance(backend, backends.CloudVmRayBackend):
+        raise exceptions.NotSupportedError(
+            f'{operation} cluster {cluster_name!r} with backend '
+            f'{backend.__class__.__name__!r} is not supported.')
+    elif handle.launched_resources.use_spot and not down and not is_cancel:
+        # Disable spot instances to be autostopped.
+        # TODO(ewzeng): allow autostop for spot when stopping is supported.
+        raise exceptions.NotSupportedError(
+            f'{colorama.Fore.YELLOW}Scheduling autostop on cluster '
+            f'{cluster_name!r}...skipped.{colorama.Style.RESET_ALL}\n'
+            '  Stopping spot instances is not supported as the attached '
+            'disks will be lost.')
+
     if tpu_utils.is_tpu_vm_pod(handle.launched_resources):
         # Reference:
         # https://cloud.google.com/tpu/docs/managing-tpus-tpu-vm#stopping_a_with_gcloud  # pylint: disable=line-too-long
         raise exceptions.NotSupportedError(
             f'{operation} cluster {cluster_name!r} with TPU VM Pod '
             'is not supported.')
 
-    backend = backend_utils.get_backend_from_handle(handle)
-    if not isinstance(backend, backends.CloudVmRayBackend):
-        with ux_utils.print_exception_no_traceback():
-            raise exceptions.NotSupportedError(
-                f'{colorama.Fore.YELLOW}{operation} cluster '
-                f'{cluster_name!r}... skipped{colorama.Style.RESET_ALL}'
-                f'\n  auto{option_str} is only supported by backend: '
-                f'{backends.CloudVmRayBackend.NAME}')
-    if cluster_status != global_user_state.ClusterStatus.UP:
-        with ux_utils.print_exception_no_traceback():
-            raise exceptions.ClusterNotUpError(
-                f'{colorama.Fore.YELLOW}{operation} cluster '
-                f'{cluster_name!r} (status: {cluster_status.value})... skipped'
-                f'{colorama.Style.RESET_ALL}'
-                f'\n  auto{option_str} can only be set/unset for '
-                f'{global_user_state.ClusterStatus.UP.value} clusters.')
+    # Check autostop is implemented for cloud
+    cloud = handle.launched_resources.cloud
+    if not down and idle_minutes >= 0:
+        cloud.check_features_are_supported(
+            {clouds.CloudImplementationFeatures.AUTOSTOP})
+
     usage_lib.record_cluster_name_for_current_operation(cluster_name)
     backend.set_autostop(handle, idle_minutes, down)
 
 
 # ==================
 # = Job Management =
 # ==================
 
 
-def _check_cluster_available(cluster_name: str,
-                             operation: str) -> backends.Backend.ResourceHandle:
-    """Check if the cluster is available."""
-    cluster_status, handle = backend_utils.refresh_cluster_status_handle(
-        cluster_name)
-    if handle is None:
-        with ux_utils.print_exception_no_traceback():
-            raise exceptions.ClusterNotUpError(
-                f'{colorama.Fore.YELLOW}Cluster {cluster_name!r} does not '
-                f'exist; skipped.{colorama.Style.RESET_ALL}')
-    backend = backend_utils.get_backend_from_handle(handle)
-    if isinstance(backend, backends.LocalDockerBackend):
-        # LocalDockerBackend does not support job queues
-        raise exceptions.NotSupportedError(
-            f'Cluster {cluster_name} with LocalDockerBackend does '
-            f'not support {operation}.')
-    if cluster_status != global_user_state.ClusterStatus.UP:
-        if onprem_utils.check_if_local_cloud(cluster_name):
-            raise exceptions.ClusterNotUpError(
-                constants.UNINITIALIZED_ONPREM_CLUSTER_MESSAGE.format(
-                    cluster_name))
-        raise exceptions.ClusterNotUpError(
-            f'{colorama.Fore.YELLOW}Cluster {cluster_name!r} is not up '
-            f'(status: {cluster_status.value}); skipped.'
-            f'{colorama.Style.RESET_ALL}')
-
-    if handle.head_ip is None:
-        raise exceptions.ClusterNotUpError(
-            f'Cluster {cluster_name!r} has been stopped or not properly set up.'
-            ' Please re-launch it with `sky start`.')
-    return handle
-
-
 @usage_lib.entrypoint
 def queue(cluster_name: str,
           skip_finished: bool = False,
           all_users: bool = False) -> List[dict]:
     # NOTE(dev): Keep the docstring consistent between the Python API and CLI.
     """Get the job queue of a cluster.
 
@@ -373,25 +485,34 @@
                 'end_at': (int) timestamp of ended,
                 'resources': (str) resources,
                 'status': (job_lib.JobStatus) job status,
                 'log_path': (str) log path,
             }
         ]
     raises:
-        RuntimeError: if failed to get the job queue.
-        sky.exceptions.ClusterNotUpError: the cluster is not up.
-        sky.exceptions.NotSupportedError: the feature is not supported.
+        ValueError: if the cluster does not exist.
+        sky.exceptions.ClusterNotUpError: if the cluster is not UP.
+        sky.exceptions.NotSupportedError: if the cluster is not based on
+          CloudVmRayBackend.
+        sky.exceptions.ClusterOwnerIdentityMismatchError: if the current user is
+          not the same as the user who created the cluster.
+        sky.exceptions.CloudUserIdentityError: if we fail to get the current
+          user identity.
+        RuntimeError: if failed to get the job queue with ssh.
     """
     all_jobs = not skip_finished
-    username = getpass.getuser()
+    username: Optional[str] = getpass.getuser()
     if all_users:
         username = None
     code = job_lib.JobLibCodeGen.get_job_queue(username, all_jobs)
 
-    handle = _check_cluster_available(cluster_name, 'getting the job queue')
+    handle = backend_utils.check_cluster_available(
+        cluster_name,
+        operation='getting the job queue',
+    )
     backend = backend_utils.get_backend_from_handle(handle)
 
     returncode, jobs_payload, stderr = backend.run_on_head(handle,
                                                            code,
                                                            require_outputs=True,
                                                            separate_stderr=True)
     if returncode != 0:
@@ -400,78 +521,124 @@
                            f'{colorama.Style.RESET_ALL}')
     jobs = job_lib.load_job_queue(jobs_payload)
     return jobs
 
 
 @usage_lib.entrypoint
 # pylint: disable=redefined-builtin
-def cancel(cluster_name: str,
-           all: bool = False,
-           job_ids: Optional[List[int]] = None) -> None:
+def cancel(
+    cluster_name: str,
+    all: bool = False,
+    job_ids: Optional[List[int]] = None,
+    # pylint: disable=invalid-name
+    _try_cancel_if_cluster_is_init: bool = False,
+) -> None:
     # NOTE(dev): Keep the docstring consistent between the Python API and CLI.
     """Cancel jobs on a cluster.
 
     Please refer to the sky.cli.cancel for the document.
+    Additional arguments:
+        _try_cancel_if_cluster_is_init: (bool) whether to try cancelling the job
+            even if the cluster is not UP, but the head node is still alive.
+            This is used by the spot controller to cancel the job when the
+            worker node is preempted in the spot cluster.
 
     Raises:
-        ValueError: arguments are invalid or the cluster is not supported.
-        sky.exceptions.ClusterNotUpError: the cluster is not up.
-        sky.exceptions.NotSupportedError: the feature is not supported.
+        ValueError: if arguments are invalid, or the cluster does not exist.
+        sky.exceptions.ClusterNotUpError: if the cluster is not UP.
+        sky.exceptions.NotSupportedError: if the specified cluster is a
+          reserved cluster that does not support this operation.
+        sky.exceptions.ClusterOwnerIdentityMismatchError: if the current user is
+          not the same as the user who created the cluster.
+        sky.exceptions.CloudUserIdentityError: if we fail to get the current
+          user identity.
     """
-    job_ids = [] if job_ids is None else job_ids
-    if len(job_ids) == 0 and not all:
+    if not job_ids and not all:
         raise ValueError(
             'sky cancel requires either a job id '
             f'(see `sky queue {cluster_name} -s`) or the --all flag.')
 
     backend_utils.check_cluster_name_not_reserved(
         cluster_name, operation_str='Cancelling jobs')
 
     # Check the status of the cluster.
-    handle = _check_cluster_available(cluster_name, 'cancelling jobs')
+    handle = None
+    try:
+        handle = backend_utils.check_cluster_available(
+            cluster_name,
+            operation='cancelling jobs',
+        )
+    except exceptions.ClusterNotUpError as e:
+        if not _try_cancel_if_cluster_is_init:
+            raise
+        assert (e.handle is None or
+                isinstance(e.handle, backends.CloudVmRayResourceHandle)), e
+        if (e.handle is None or e.handle.head_ip is None):
+            raise
+        handle = e.handle
+        # Even if the cluster is not UP, we can still try to cancel the job if
+        # the head node is still alive. This is useful when a spot cluster's
+        # worker node is preempted, but we can still cancel the job on the head
+        # node.
+
+    assert handle is not None, (
+        f'handle for cluster {cluster_name!r} should not be None')
+
     backend = backend_utils.get_backend_from_handle(handle)
 
     if all:
-        print(f'{colorama.Fore.YELLOW}'
-              f'Cancelling all jobs on cluster {cluster_name!r}...'
-              f'{colorama.Style.RESET_ALL}')
+        sky_logging.print(f'{colorama.Fore.YELLOW}'
+                          f'Cancelling all jobs on cluster {cluster_name!r}...'
+                          f'{colorama.Style.RESET_ALL}')
         job_ids = None
     else:
+        assert job_ids is not None, 'job_ids should not be None'
         jobs_str = ', '.join(map(str, job_ids))
-        print(f'{colorama.Fore.YELLOW}'
-              f'Cancelling jobs ({jobs_str}) on cluster {cluster_name!r}...'
-              f'{colorama.Style.RESET_ALL}')
+        sky_logging.print(
+            f'{colorama.Fore.YELLOW}'
+            f'Cancelling jobs ({jobs_str}) on cluster {cluster_name!r}...'
+            f'{colorama.Style.RESET_ALL}')
 
     backend.cancel_jobs(handle, job_ids)
 
 
 @usage_lib.entrypoint
 def tail_logs(cluster_name: str,
-              job_id: Optional[str],
+              job_id: Optional[int],
               follow: bool = True) -> None:
     # NOTE(dev): Keep the docstring consistent between the Python API and CLI.
     """Tail the logs of a job.
 
     Please refer to the sky.cli.tail_logs for the document.
 
     Raises:
-        ValueError: arguments are invalid or the cluster is not supported.
-        sky.exceptions.ClusterNotUpError: the cluster is not up.
-        sky.exceptions.NotSupportedError: the feature is not supported.
+        ValueError: arguments are invalid or the cluster is not supported or
+          the cluster does not exist.
+        sky.exceptions.ClusterNotUpError: if the cluster is not UP.
+        sky.exceptions.NotSupportedError: if the cluster is not based on
+          CloudVmRayBackend.
+        sky.exceptions.ClusterOwnerIdentityMismatchError: if the current user is
+          not the same as the user who created the cluster.
+        sky.exceptions.CloudUserIdentityError: if we fail to get the current
+          user identity.
     """
     # Check the status of the cluster.
-    handle = _check_cluster_available(cluster_name, 'tailing logs')
+    handle = backend_utils.check_cluster_available(
+        cluster_name,
+        operation='tailing logs',
+    )
     backend = backend_utils.get_backend_from_handle(handle)
 
     job_str = f'job {job_id}'
     if job_id is None:
         job_str = 'the last job'
-    print(f'{colorama.Fore.YELLOW}'
-          f'Tailing logs of {job_str} on cluster {cluster_name!r}...'
-          f'{colorama.Style.RESET_ALL}')
+    sky_logging.print(
+        f'{colorama.Fore.YELLOW}'
+        f'Tailing logs of {job_str} on cluster {cluster_name!r}...'
+        f'{colorama.Style.RESET_ALL}')
 
     usage_lib.record_cluster_name_for_current_operation(cluster_name)
     backend.tail_logs(handle, job_id, follow=follow)
 
 
 @usage_lib.entrypoint
 def download_logs(
@@ -482,100 +649,111 @@
     """Download the logs of jobs.
 
     Args:
         cluster_name: (str) name of the cluster.
         job_ids: (List[str]) job ids.
     Returns:
         Dict[str, str]: a mapping of job_id to local log path.
+    Raises:
+        ValueError: if the cluster does not exist.
+        sky.exceptions.ClusterNotUpError: if the cluster is not UP.
+        sky.exceptions.NotSupportedError: if the cluster is not based on
+          CloudVmRayBackend.
+        sky.exceptions.ClusterOwnerIdentityMismatchError: if the current user is
+          not the same as the user who created the cluster.
+        sky.exceptions.CloudUserIdentityError: if we fail to get the current
+          user identity.
     """
     # Check the status of the cluster.
-    handle = _check_cluster_available(cluster_name, 'downloading logs')
+    handle = backend_utils.check_cluster_available(
+        cluster_name,
+        operation='downloading logs',
+    )
     backend = backend_utils.get_backend_from_handle(handle)
+    assert isinstance(backend, backends.CloudVmRayBackend), backend
 
     if job_ids is not None and len(job_ids) == 0:
-        return []
+        return {}
 
     usage_lib.record_cluster_name_for_current_operation(cluster_name)
-    print(f'{colorama.Fore.YELLOW}'
-          'Syncing down logs to local...'
-          f'{colorama.Style.RESET_ALL}')
+    sky_logging.print(f'{colorama.Fore.YELLOW}'
+                      'Syncing down logs to local...'
+                      f'{colorama.Style.RESET_ALL}')
     local_log_dirs = backend.sync_down_logs(handle, job_ids, local_dir)
     return local_log_dirs
 
 
 @usage_lib.entrypoint
-def job_status(
-        cluster_name: str,
-        job_ids: Optional[List[str]],
-        stream_logs: bool = False) -> Dict[str, Optional[job_lib.JobStatus]]:
+def job_status(cluster_name: str,
+               job_ids: Optional[List[int]],
+               stream_logs: bool = False
+              ) -> Dict[Optional[int], Optional[job_lib.JobStatus]]:
     # NOTE(dev): Keep the docstring consistent between the Python API and CLI.
     """Get the status of jobs.
 
     Args:
         cluster_name: (str) name of the cluster.
         job_ids: (List[str]) job ids. If None, get the status of the last job.
     Returns:
-        Dict[str, Optional[job_lib.JobStatus]]: A mapping of job_id to job
-        statuses. The status will be None if the job does not exist.
+        Dict[Optional[int], Optional[job_lib.JobStatus]]: A mapping of job_id to
+        job statuses. The status will be None if the job does not exist.
         If job_ids is None and there is no job on the cluster, it will return
         {None: None}.
+    Raises:
+        ValueError: if the cluster does not exist.
+        sky.exceptions.ClusterNotUpError: if the cluster is not UP.
+        sky.exceptions.NotSupportedError: if the cluster is not based on
+          CloudVmRayBackend.
+        sky.exceptions.ClusterOwnerIdentityMismatchError: if the current user is
+          not the same as the user who created the cluster.
+        sky.exceptions.CloudUserIdentityError: if we fail to get the current
+          user identity.
     """
     # Check the status of the cluster.
-    handle = _check_cluster_available(cluster_name, 'getting job status')
+    handle = backend_utils.check_cluster_available(
+        cluster_name,
+        operation='getting job status',
+    )
     backend = backend_utils.get_backend_from_handle(handle)
+    if not isinstance(backend, backends.CloudVmRayBackend):
+        raise exceptions.NotSupportedError(
+            f'Getting job status is not supported for cluster {cluster_name!r} '
+            f'of type {backend.__class__.__name__!r}.')
+    assert isinstance(handle, backends.CloudVmRayResourceHandle), handle
 
     if job_ids is not None and len(job_ids) == 0:
-        return []
+        return {}
 
-    print(f'{colorama.Fore.YELLOW}'
-          'Getting job status...'
-          f'{colorama.Style.RESET_ALL}')
+    sky_logging.print(f'{colorama.Fore.YELLOW}'
+                      'Getting job status...'
+                      f'{colorama.Style.RESET_ALL}')
 
     usage_lib.record_cluster_name_for_current_operation(cluster_name)
     statuses = backend.get_job_status(handle, job_ids, stream_logs=stream_logs)
     return statuses
 
 
 # =======================
 # = Spot Job Management =
 # =======================
 
 
-def _is_spot_controller_up(
-    stopped_message: str,
-) -> Tuple[Optional[global_user_state.ClusterStatus],
-           Optional[backends.Backend.ResourceHandle]]:
-    controller_status, handle = backend_utils.refresh_cluster_status_handle(
-        spot.SPOT_CONTROLLER_NAME, force_refresh=True)
-    if controller_status is None:
-        print('No managed spot job has been run.')
-    elif controller_status != global_user_state.ClusterStatus.UP:
-        msg = (f'Spot controller {spot.SPOT_CONTROLLER_NAME} '
-               f'is {controller_status.value}.')
-        if controller_status == global_user_state.ClusterStatus.STOPPED:
-            msg += f'\n{stopped_message}'
-        if controller_status == global_user_state.ClusterStatus.INIT:
-            msg += '\nPlease wait for the controller to be ready.'
-        print(msg)
-        handle = None
-    return controller_status, handle
-
-
 @usage_lib.entrypoint
 def spot_status(refresh: bool) -> List[Dict[str, Any]]:
     """[Deprecated] (alias of spot_queue) Get statuses of managed spot jobs."""
-    print(
+    sky_logging.print(
         f'{colorama.Fore.YELLOW}WARNING: `spot_status()` is deprecated. '
         f'Instead, use: spot_queue(){colorama.Style.RESET_ALL}',
         file=sys.stderr)
     return spot_queue(refresh=refresh)
 
 
 @usage_lib.entrypoint
-def spot_queue(refresh: bool) -> List[Dict[str, Any]]:
+def spot_queue(refresh: bool,
+               skip_finished: bool = False) -> List[Dict[str, Any]]:
     # NOTE(dev): Keep the docstring consistent between the Python API and CLI.
     """Get statuses of managed spot jobs.
 
     Please refer to the sky.cli.spot_queue for the documentation.
 
     Returns:
         [
@@ -589,79 +767,102 @@
                 'retry_count': int Number of retries,
                 'status': sky.JobStatus status of the job,
                 'cluster_resources': (str) resources of the cluster,
                 'region': (str) region of the cluster,
             }
         ]
     Raises:
-        sky.exceptions.ClusterNotUpError: the spot controller is not up.
+        sky.exceptions.ClusterNotUpError: the spot controller is not up or
+            does not exist.
+        RuntimeError: if failed to get the spot jobs with ssh.
     """
 
     stop_msg = ''
     if not refresh:
         stop_msg = 'To view the latest job table: sky spot queue --refresh'
-    controller_status, handle = _is_spot_controller_up(stop_msg)
-
-    if controller_status is None:
-        return []
+    controller_status, handle = spot.is_spot_controller_up(stop_msg)
 
     if (refresh and controller_status in [
-            global_user_state.ClusterStatus.STOPPED,
-            global_user_state.ClusterStatus.INIT
+            status_lib.ClusterStatus.STOPPED, status_lib.ClusterStatus.INIT
     ]):
-        print(f'{colorama.Fore.YELLOW}'
-              'Restarting controller for latest status...'
-              f'{colorama.Style.RESET_ALL}')
-
-        handle = _start(spot.SPOT_CONTROLLER_NAME,
-                        idle_minutes_to_autostop=spot.
-                        SPOT_CONTROLLER_IDLE_MINUTES_TO_AUTOSTOP)
+        sky_logging.print(f'{colorama.Fore.YELLOW}'
+                          'Restarting controller for latest status...'
+                          f'{colorama.Style.RESET_ALL}')
+
+        log_utils.force_update_rich_status(
+            '[cyan] Checking spot jobs - restarting '
+            'controller[/]')
+        handle = _start(spot.SPOT_CONTROLLER_NAME)
+        controller_status = status_lib.ClusterStatus.UP
+        log_utils.force_update_rich_status('[cyan] Checking spot jobs[/]')
 
     if handle is None or handle.head_ip is None:
-        raise exceptions.ClusterNotUpError('Spot controller is not up.')
+        # When the controller is STOPPED, the head_ip will be None, as
+        # it will be set in global_user_state.remove_cluster().
+        # We do not directly check for UP because the controller may be
+        # in INIT state during another spot launch, but still have
+        # head_ip available. In this case, we can still try to ssh
+        # into the controller and fetch the job table.
+        raise exceptions.ClusterNotUpError('Spot controller is not up.',
+                                           cluster_status=controller_status)
 
     backend = backend_utils.get_backend_from_handle(handle)
     assert isinstance(backend, backends.CloudVmRayBackend)
 
     code = spot.SpotCodeGen.get_job_table()
     returncode, job_table_payload, stderr = backend.run_on_head(
         handle,
         code,
         require_outputs=True,
         stream_logs=False,
         separate_stderr=True)
     try:
-        subprocess_utils.handle_returncode(
-            returncode, code, 'Failed to fetch managed job statuses', stderr)
+        subprocess_utils.handle_returncode(returncode,
+                                           code,
+                                           'Failed to fetch managed spot jobs',
+                                           stderr,
+                                           stream_logs=False)
     except exceptions.CommandError as e:
         raise RuntimeError(e.error_msg) from e
 
     jobs = spot.load_spot_job_queue(job_table_payload)
+    if skip_finished:
+        # Filter out the finished jobs. If a multi-task job is partially
+        # finished, we will include all its tasks.
+        non_finished_tasks = list(
+            filter(lambda job: not job['status'].is_terminal(), jobs))
+        non_finished_job_ids = {job['job_id'] for job in non_finished_tasks}
+        jobs = list(
+            filter(lambda job: job['job_id'] in non_finished_job_ids, jobs))
     return jobs
 
 
 @usage_lib.entrypoint
 # pylint: disable=redefined-builtin
 def spot_cancel(name: Optional[str] = None,
-                job_ids: Optional[Tuple[int]] = None,
+                job_ids: Optional[List[int]] = None,
                 all: bool = False) -> None:
     # NOTE(dev): Keep the docstring consistent between the Python API and CLI.
     """Cancel managed spot jobs.
 
     Please refer to the sky.cli.spot_cancel for the document.
 
     Raises:
         sky.exceptions.ClusterNotUpError: the spot controller is not up.
         RuntimeError: failed to cancel the job.
     """
     job_ids = [] if job_ids is None else job_ids
-    _, handle = _is_spot_controller_up(
+    cluster_status, handle = spot.is_spot_controller_up(
         'All managed spot jobs should have finished.')
     if handle is None or handle.head_ip is None:
-        raise exceptions.ClusterNotUpError('All jobs finished.')
+        # The error message is already printed in spot.is_spot_controller_up.
+        # TODO(zhwu): Move the error message into the exception.
+        with ux_utils.print_exception_no_traceback():
+            raise exceptions.ClusterNotUpError('',
+                                               cluster_status=cluster_status)
 
     job_id_str = ','.join(map(str, job_ids))
     if sum([len(job_ids) > 0, name is not None, all]) != 1:
         argument_str = f'job_ids={job_id_str}' if len(job_ids) > 0 else ''
         argument_str += f' name={name}' if name is not None else ''
         argument_str += ' all' if all else ''
         raise ValueError('Can only specify one of JOB_IDS or name or all. '
@@ -670,28 +871,29 @@
     backend = backend_utils.get_backend_from_handle(handle)
     assert isinstance(backend, backends.CloudVmRayBackend)
     if all:
         code = spot.SpotCodeGen.cancel_jobs_by_id(None)
     elif job_ids:
         code = spot.SpotCodeGen.cancel_jobs_by_id(job_ids)
     else:
+        assert name is not None, (job_ids, name, all)
         code = spot.SpotCodeGen.cancel_job_by_name(name)
     # The stderr is redirected to stdout
     returncode, stdout, _ = backend.run_on_head(handle,
                                                 code,
                                                 require_outputs=True,
                                                 stream_logs=False)
     try:
         subprocess_utils.handle_returncode(returncode, code,
                                            'Failed to cancel managed spot job',
                                            stdout)
     except exceptions.CommandError as e:
         raise RuntimeError(e.error_msg) from e
 
-    print(stdout)
+    sky_logging.print(stdout)
     if 'Multiple jobs found with name' in stdout:
         with ux_utils.print_exception_no_traceback():
             raise RuntimeError(
                 'Please specify the job ID instead of the job name.')
 
 
 @usage_lib.entrypoint
@@ -703,27 +905,29 @@
     Please refer to the sky.cli.spot_logs for the document.
 
     Raises:
         ValueError: invalid arguments.
         sky.exceptions.ClusterNotUpError: the spot controller is not up.
     """
     # TODO(zhwu): Automatically restart the spot controller
-    controller_status, handle = _is_spot_controller_up(
+    controller_status, handle = spot.is_spot_controller_up(
         'Please restart the spot controller with '
-        f'`sky start {spot.SPOT_CONTROLLER_NAME} -i 5`.')
+        f'`sky start {spot.SPOT_CONTROLLER_NAME}`.')
     if handle is None or handle.head_ip is None:
         msg = 'All jobs finished.'
-        if controller_status == global_user_state.ClusterStatus.INIT:
+        if controller_status == status_lib.ClusterStatus.INIT:
             msg = ''
         with ux_utils.print_exception_no_traceback():
-            raise exceptions.ClusterNotUpError(msg)
+            raise exceptions.ClusterNotUpError(msg,
+                                               cluster_status=controller_status)
 
     if name is not None and job_id is not None:
         raise ValueError('Cannot specify both name and job_id.')
     backend = backend_utils.get_backend_from_handle(handle)
+    assert isinstance(backend, backends.CloudVmRayBackend), backend
     # Stream the realtime logs
     backend.tail_spot_logs(handle, job_id=job_id, job_name=name, follow=follow)
 
 
 # ======================
 # = Storage Management =
 # ======================
@@ -753,14 +957,15 @@
 def storage_delete(name: str) -> None:
     # NOTE(dev): Keep the docstring consistent between the Python API and CLI.
     """Delete a storage.
 
     Raises:
         ValueError: If the storage does not exist.
     """
+    # TODO(zhwu): check the storage owner matches the current user
     handle = global_user_state.get_handle_from_storage_name(name)
     if handle is None:
         raise ValueError(f'Storage name {name!r} not found.')
     else:
         store_object = data.Storage(name=handle.storage_name,
                                     source=handle.source,
                                     sync_on_reconstruction=False)
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/dag.py` & `skypilot-nightly-1.0.0.dev20230713/sky/dag.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """DAGs: user applications to be run."""
 import pprint
 import threading
+from typing import List
 
 
 class Dag:
     """Dag: a user application, represented as a DAG of Tasks.
 
     Examples:
         >>> import sky
@@ -13,14 +14,15 @@
     """
 
     def __init__(self):
         self.tasks = []
         import networkx as nx  # pylint: disable=import-outside-toplevel
 
         self.graph = nx.DiGraph()
+        self.name = None
 
     def add(self, task):
         self.graph.add_node(task)
         self.tasks.append(task)
 
     def remove(self, task):
         self.tasks.remove(task)
@@ -65,15 +67,15 @@
                     visited_zero_out_degree = True
         return is_chain
 
 
 class _DagContext(threading.local):
     """A thread-local stack of Dags."""
     _current_dag = None
-    _previous_dags = []
+    _previous_dags: List[Dag] = []
 
     def push_dag(self, dag):
         if self._current_dag is not None:
             self._previous_dags.append(self._current_dag)
         self._current_dag = dag
 
     def pop_dag(self):
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/data/data_transfer.py` & `skypilot-nightly-1.0.0.dev20230713/sky/data/data_transfer.py`

 * *Files 22% similar despite different names*

```diff
@@ -9,26 +9,27 @@
 - S3 -> Local
 - Local -> GCS
 - GCS -> Local
 - S3 -> GCS
 
 TODO:
 - All combinations of Azure Transfer
+- All combinations of R2 Transfer
 - GCS -> S3
 """
 import json
 import subprocess
 import time
 
 import colorama
 
 from sky import clouds
 from sky import sky_logging
 from sky.adaptors import aws, gcp
-from sky.backends import backend_utils
+from sky.utils import log_utils
 from sky.utils import ux_utils
 
 logger = sky_logging.init_logger(__name__)
 
 MAX_POLLS = 120000
 POLL_INTERVAL = 1
 
@@ -89,15 +90,15 @@
 
     logger.info(f'{colorama.Fore.GREEN}Transfer job scheduled: '
                 f'{colorama.Style.RESET_ALL}'
                 f's3://{s3_bucket_name} -> gs://{gs_bucket_name} ')
     logger.debug(json.dumps(operation, indent=4))
     logger.info('Waiting for the transfer to finish')
     start = time.time()
-    with backend_utils.safe_console_status('Transferring'):
+    with log_utils.safe_rich_status('Transferring'):
         for _ in range(MAX_POLLS):
             result = (storagetransfer.transferOperations().get(
                 name=operation['name']).execute())
             if 'error' in result:
                 with ux_utils.print_exception_no_traceback():
                     raise RuntimeError(result['error'])
 
@@ -112,27 +113,84 @@
                 'Storage Transfer Service console at '
                 'https://cloud.google.com/storage-transfer-service')
             return
     logger.info(
         f'Transfer finished in {(time.time() - start) / 60:.2f} minutes.')
 
 
+def s3_to_r2(s3_bucket_name: str, r2_bucket_name: str) -> None:
+    """Creates a one-time transfer from Amazon S3 to Google Cloud Storage.
+
+    Can be viewed from: https://console.cloud.google.com/transfer/cloud
+    it will block until the transfer is complete.
+
+    Args:
+      s3_bucket_name: str; Name of the Amazon S3 Bucket
+      r2_bucket_name: str; Name of the Cloudflare R2 Bucket
+    """
+    raise NotImplementedError('Moving data directly from clouds to R2 is '
+                              'currently not supported. Please specify '
+                              'a local source for the storage object.')
+
+
 def gcs_to_s3(gs_bucket_name: str, s3_bucket_name: str) -> None:
     """Creates a one-time transfer from Google Cloud Storage to Amazon S3.
 
      Args:
       gs_bucket_name: str; Name of the Google Cloud Storage Bucket
       s3_bucket_name: str; Name of the Amazon S3 Bucket
     """
     sync_command = (f'gsutil -m rsync -rd gs://{gs_bucket_name} '
                     f's3://{s3_bucket_name}')
 
     subprocess.call(sync_command, shell=True)
 
 
+def gcs_to_r2(gs_bucket_name: str, r2_bucket_name: str) -> None:
+    """Creates a one-time transfer from Google Cloud Storage to Amazon S3.
+
+     Args:
+      gs_bucket_name: str; Name of the Google Cloud Storage Bucket
+      r2_bucket_name: str; Name of the Cloudflare R2 Bucket
+    """
+    raise NotImplementedError('Moving data directly from clouds to R2 is '
+                              'currently not supported. Please specify '
+                              'a local source for the storage object.')
+
+
+def r2_to_gcs(r2_bucket_name: str, gs_bucket_name: str) -> None:
+    """Creates a one-time transfer from Cloudflare R2 to Google Cloud Storage.
+
+    Can be viewed from: https://console.cloud.google.com/transfer/cloud
+    it will block until the transfer is complete.
+
+    Args:
+      r2_bucket_name: str; Name of the Cloudflare R2 Bucket
+      gs_bucket_name: str; Name of the Google Cloud Storage Bucket
+    """
+    raise NotImplementedError('Moving data directly from R2 to clouds is '
+                              'currently not supported. Please specify '
+                              'a local source for the storage object.')
+
+
+def r2_to_s3(r2_bucket_name: str, s3_bucket_name: str) -> None:
+    """Creates a one-time transfer from Amazon S3 to Google Cloud Storage.
+
+    Can be viewed from: https://console.cloud.google.com/transfer/cloud
+    it will block until the transfer is complete.
+
+    Args:
+      r2_bucket_name: str; Name of the Cloudflare R2 Bucket\
+      s3_bucket_name: str; Name of the Amazon S3 Bucket
+    """
+    raise NotImplementedError('Moving data directly from R2 to clouds is '
+                              'currently not supported. Please specify '
+                              'a local source for the storage object.')
+
+
 def _add_bucket_iam_member(bucket_name: str, role: str, member: str) -> None:
     storage_client = gcp.storage_client()
     bucket = storage_client.bucket(bucket_name)
 
     policy = bucket.get_iam_policy(requested_policy_version=3)
     policy.bindings.append({'role': role, 'members': {member}})
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/data/data_utils.py` & `skypilot-nightly-1.0.0.dev20230713/sky/data/data_utils.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,19 +1,18 @@
 """Miscellaneous Utils for Sky Data
 """
 from multiprocessing import pool
 import os
 import subprocess
-from pathlib import Path
 from typing import Any, Callable, Dict, List, Optional, Tuple
 import urllib.parse
 
 from sky import exceptions
 from sky import sky_logging
-from sky.adaptors import aws, gcp
+from sky.adaptors import aws, gcp, cloudflare
 from sky.utils import ux_utils
 
 Client = Any
 
 logger = sky_logging.init_logger(__name__)
 
 
@@ -37,14 +36,26 @@
     """
     path_parts = gcs_path.replace('gs://', '').split('/')
     bucket = path_parts.pop(0)
     key = '/'.join(path_parts)
     return bucket, key
 
 
+def split_r2_path(r2_path: str) -> Tuple[str, str]:
+    """Splits R2 Path into Bucket name and Relative Path to Bucket
+
+    Args:
+      r2_path: str; R2 Path, e.g. r2://imagenet/train/
+    """
+    path_parts = r2_path.replace('r2://', '').split('/')
+    bucket = path_parts.pop(0)
+    key = '/'.join(path_parts)
+    return bucket, key
+
+
 def create_s3_client(region: str = 'us-east-2') -> Client:
     """Helper method that connects to Boto3 client for S3 Bucket
 
     Args:
       region: str; Region name, e.g. us-west-1, us-east-2
     """
     return aws.client('s3', region_name=region)
@@ -70,14 +81,34 @@
     try:
         gcp.storage_client().get_bucket(name)
         return True
     except gcp.not_found_exception():
         return False
 
 
+def create_r2_client(region: str = 'auto') -> Client:
+    """Helper method that connects to Boto3 client for R2 Bucket
+
+    Args:
+      region: str; Region for CLOUDFLARE R2 is set to auto
+    """
+    return cloudflare.client('s3', region)
+
+
+def verify_r2_bucket(name: str) -> bool:
+    """Helper method that checks if the R2 bucket exists
+
+    Args:
+      name: str; Name of R2 Bucket (without r2:// prefix)
+    """
+    r2 = cloudflare.resource('s3')
+    bucket = r2.Bucket(name)
+    return bucket in r2.buckets.all()
+
+
 def is_cloud_store_url(url):
     result = urllib.parse.urlsplit(url)
     # '' means non-cloud URLs.
     return result.netloc
 
 
 def _group_files_by_dir(
@@ -91,39 +122,39 @@
     E.g., ['a/b/c.txt', 'a/b/d.txt', 'a/e.txt'] will be grouped into
     {'a/b': ['c.txt', 'd.txt'], 'a': ['e.txt']}, and these three files can be
     uploaded in two rsync calls instead of three.
 
     Args:
         source_list: List[str]; List of paths to group
     """
-    grouped_files = {}
+    grouped_files: Dict[str, List[str]] = {}
     dirs = []
     for source in source_list:
         source = os.path.abspath(os.path.expanduser(source))
         if os.path.isdir(source):
             dirs.append(source)
         else:
             base_path = os.path.dirname(source)
             file_name = os.path.basename(source)
             if base_path not in grouped_files:
                 grouped_files[base_path] = []
             grouped_files[base_path].append(file_name)
     return grouped_files, dirs
 
 
-def parallel_upload(source_path_list: List[Path],
+def parallel_upload(source_path_list: List[str],
                     filesync_command_generator: Callable[[str, List[str]], str],
                     dirsync_command_generator: Callable[[str, str], str],
                     bucket_name: str,
                     access_denied_message: str,
                     create_dirs: bool = False,
                     max_concurrent_uploads: Optional[int] = None) -> None:
     """Helper function to run parallel uploads for a list of paths.
 
-    Used by S3Store and GCSStore to run rsync commands in parallel by
+    Used by S3Store, GCSStore, and R2Store to run rsync commands in parallel by
     providing appropriate command generators.
 
     Args:
         source_path_list: List of paths to local files or directories
         filesync_command_generator: Callable that generates rsync command
             for a list of files belonging to the same dir.
         dirsync_command_generator: Callable that generates rsync command
@@ -167,14 +198,15 @@
     # TODO(zhwu): Use log_lib.run_with_log() and redirect the output
     # to a log file.
     with subprocess.Popen(command,
                           stderr=subprocess.PIPE,
                           stdout=subprocess.DEVNULL,
                           shell=True) as process:
         stderr = []
+        assert process.stderr is not None  # for mypy
         while True:
             line = process.stderr.readline()
             if not line:
                 break
             str_line = line.decode('utf-8')
             stderr.append(str_line)
             if access_denied_message in str_line:
@@ -183,13 +215,13 @@
                     raise PermissionError(
                         'Failed to upload files to '
                         'the remote bucket. The bucket does not have '
                         'write permissions. It is possible that '
                         'the bucket is public.')
         returncode = process.wait()
         if returncode != 0:
-            stderr = '\n'.join(stderr)
+            stderr_str = '\n'.join(stderr)
             with ux_utils.print_exception_no_traceback():
-                logger.error(stderr)
+                logger.error(stderr_str)
                 raise exceptions.StorageUploadError(
                     f'Upload to bucket failed for store {bucket_name}. '
                     'Please check the logs.')
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/data/mounting_utils.py` & `skypilot-nightly-1.0.0.dev20230713/sky/data/mounting_utils.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,16 +1,20 @@
 """Helper functions for object store mounting in Sky Storage"""
 import random
 import textwrap
+from typing import Optional
+
+from sky import exceptions
 
 
 def get_mounting_command(
     mount_path: str,
     install_cmd: str,
     mount_cmd: str,
+    version_check_cmd: Optional[str] = None,
 ) -> str:
     """
     Generates the mounting command for a given bucket. Generated script first
     unmounts any existing mount at the mount path, checks and installs the
     mounting utility if required, creates the mount path and finally mounts
     the bucket.
 
@@ -20,14 +24,17 @@
           single line.
         mount_cmd: Command to mount the bucket. Should be single line.
 
     Returns:
         str: Mounting command with the mounting script as a heredoc.
     """
     mount_binary = mount_cmd.split()[0]
+    installed_check = f'[ -x "$(command -v {mount_binary})" ]'
+    if version_check_cmd is not None:
+        installed_check += f' && {version_check_cmd}'
     script = textwrap.dedent(f"""
         #!/usr/bin/env bash
         set -e
 
         MOUNT_PATH={mount_path}
         MOUNT_BINARY={mount_binary}
 
@@ -35,31 +42,31 @@
         if grep -q $MOUNT_PATH /proc/mounts ; then
             echo "Path already mounted - unmounting..."
             fusermount -uz "$MOUNT_PATH"
             echo "Successfully unmounted $MOUNT_PATH."
         fi
 
         # Install MOUNT_BINARY if not already installed
-        if ! [ -x "$(command -v $MOUNT_BINARY)" ]; then
+        if {installed_check}; then
+          echo "$MOUNT_BINARY already installed. Proceeding..."
+        else
           echo "Installing $MOUNT_BINARY..."
           {install_cmd}
-        else
-          echo "$MOUNT_BINARY already installed. Proceeding..."
         fi
 
         # Check if mount path exists
         if [ ! -d "$MOUNT_PATH" ]; then
           echo "Mount path $MOUNT_PATH does not exist. Creating..."
           sudo mkdir -p $MOUNT_PATH
           sudo chmod 777 $MOUNT_PATH
         else
           # Check if mount path contains files
           if [ "$(ls -A $MOUNT_PATH)" ]; then
-            echo "Mount path $MOUNT_PATH is not empty. Please make sure its empty."
-            exit 1
+            echo "Mount path $MOUNT_PATH is not empty. Please mount to another path or remove it first."
+            exit {exceptions.MOUNT_PATH_NON_EMPTY_CODE}
           fi
         fi
         echo "Mounting $SOURCE_BUCKET to $MOUNT_PATH with $MOUNT_BINARY..."
         {mount_cmd}
         echo "Mounting done."
     """)
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/data/storage.py` & `skypilot-nightly-1.0.0.dev20230713/sky/data/storage.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,57 +1,90 @@
 """Storage and Store Classes for Sky Data."""
 import enum
 import os
+import re
 import subprocess
 import time
-from typing import Any, Dict, Optional, Tuple, Union, List
+import typing
+from typing import Any, Dict, List, Optional, Tuple, Type, Union
 import urllib.parse
 
 import colorama
 
+from sky import check
 from sky import clouds
 from sky.adaptors import aws
 from sky.adaptors import gcp
+from sky.adaptors import cloudflare
 from sky.backends import backend_utils
 from sky.utils import schemas
 from sky.data import data_transfer
 from sky.data import data_utils
 from sky.data import mounting_utils
 from sky import exceptions
 from sky import global_user_state
 from sky import sky_logging
+from sky import status_lib
+from sky.utils import log_utils
 from sky.utils import ux_utils
 
+if typing.TYPE_CHECKING:
+    import boto3  # type: ignore
+    from google.cloud import storage  # type: ignore
+
 logger = sky_logging.init_logger(__name__)
 
-Path = str
 StorageHandle = Any
-StorageStatus = global_user_state.StorageStatus
+StorageStatus = status_lib.StorageStatus
+Path = str
+SourceType = Union[Path, List[Path]]
 
 # Clouds with object storage implemented in this module. Azure Blob
 # Storage isn't supported yet (even though Azure is).
-STORE_ENABLED_CLOUDS = [clouds.AWS(), clouds.GCP()]
-
-# Max number of objects a GCS bucket can be directly deleted with
-_GCS_RM_MAX_OBJS = 256
+# TODO(Doyoung): need to add clouds.CLOUDFLARE() to support
+# R2 to be an option as preferred store type
+STORE_ENABLED_CLOUDS: List[str] = [
+    str(clouds.AWS()), str(clouds.GCP()), cloudflare.NAME
+]
 
 # Maximum number of concurrent rsync upload processes
 _MAX_CONCURRENT_UPLOADS = 32
 
 _BUCKET_FAIL_TO_CONNECT_MESSAGE = (
-    'Failed to connect to an existing bucket {name!r}.\n'
-    'Please check if:\n  1. the bucket name is taken and/or '
-    '\n  2. the bucket permissions are not setup correctly.')
+    'Failed to access existing bucket {name!r}. '
+    'This is likely because it is a private bucket you do not have access to.\n'
+    'To fix: \n'
+    '  1. If you are trying to create a new bucket: use a different name.\n'
+    '  2. If you are trying to connect to an existing bucket: make sure '
+    'your cloud credentials have access to it.')
+
+_BUCKET_EXTERNALLY_DELETED_DEBUG_MESSAGE = (
+    'Bucket {bucket_name!r} does not exist. '
+    'It may have been deleted externally.')
+
+
+def _is_storage_cloud_enabled(cloud_name: str,
+                              try_fix_with_sky_check: bool = True) -> bool:
+    enabled_storage_clouds = global_user_state.get_enabled_storage_clouds()
+    if cloud_name in enabled_storage_clouds:
+        return True
+    if try_fix_with_sky_check:
+        # TODO(zhwu): Only check the specified cloud to speed up.
+        check.check(quiet=True)
+        return _is_storage_cloud_enabled(cloud_name,
+                                         try_fix_with_sky_check=False)
+    return False
 
 
 class StoreType(enum.Enum):
     """Enum for the different types of stores."""
     S3 = 'S3'
     GCS = 'GCS'
     AZURE = 'AZURE'
+    R2 = 'R2'
 
     @classmethod
     def from_cloud(cls, cloud: clouds.Cloud) -> 'StoreType':
         if isinstance(cloud, clouds.AWS):
             return StoreType.S3
         elif isinstance(cloud, clouds.GCP):
             return StoreType.GCS
@@ -62,14 +95,16 @@
 
     @classmethod
     def from_store(cls, store: 'AbstractStore') -> 'StoreType':
         if isinstance(store, S3Store):
             return StoreType.S3
         elif isinstance(store, GcsStore):
             return StoreType.GCS
+        elif isinstance(store, R2Store):
+            return StoreType.R2
         else:
             with ux_utils.print_exception_no_traceback():
                 raise ValueError(f'Unknown store type: {store}')
 
 
 class StorageMode(enum.Enum):
     MOUNT = 'MOUNT'
@@ -80,24 +115,33 @@
     if isinstance(cloud, clouds.AWS):
         return StoreType.S3
     elif isinstance(cloud, clouds.GCP):
         return StoreType.GCS
     elif isinstance(cloud, clouds.Azure):
         with ux_utils.print_exception_no_traceback():
             raise ValueError('Azure Blob Storage is not supported yet.')
+    elif isinstance(cloud, clouds.Lambda):
+        with ux_utils.print_exception_no_traceback():
+            raise ValueError('Lambda Cloud does not provide cloud storage.')
+    elif isinstance(cloud, clouds.SCP):
+        with ux_utils.print_exception_no_traceback():
+            raise ValueError('SCP does not provide cloud storage.')
     else:
         with ux_utils.print_exception_no_traceback():
             raise ValueError(f'Unknown cloud type: {cloud}')
 
 
 def get_store_prefix(storetype: StoreType) -> str:
     if storetype == StoreType.S3:
         return 's3://'
     elif storetype == StoreType.GCS:
         return 'gs://'
+    # R2 storages use 's3://' as a prefix for various aws cli commands
+    elif storetype == StoreType.R2:
+        return 's3://'
     elif storetype == StoreType.AZURE:
         with ux_utils.print_exception_no_traceback():
             raise ValueError('Azure Blob Storage is not supported yet.')
     else:
         with ux_utils.print_exception_no_traceback():
             raise ValueError(f'Unknown store type: {storetype}')
 
@@ -121,15 +165,15 @@
         Allows store objects to be written to and reconstructed from
         global_user_state.
         """
 
         def __init__(self,
                      *,
                      name: str,
-                     source: str,
+                     source: Optional[SourceType],
                      region: Optional[str] = None,
                      is_sky_managed: Optional[bool] = None):
             self.name = name
             self.source = source
             self.region = region
             self.is_sky_managed = is_sky_managed
 
@@ -138,35 +182,41 @@
                     f'\n\tname={self.name},'
                     f'\n\tsource={self.source},'
                     f'\n\tregion={self.region},'
                     f'\n\tis_sky_managed={self.is_sky_managed})')
 
     def __init__(self,
                  name: str,
-                 source: str,
+                 source: Optional[SourceType],
                  region: Optional[str] = None,
-                 is_sky_managed: Optional[bool] = None):
+                 is_sky_managed: Optional[bool] = None,
+                 sync_on_reconstruction: Optional[bool] = True):
         """Initialize AbstractStore
 
         Args:
             name: Store name
             source: Data source for the store
             region: Region to place the bucket in
             is_sky_managed: Whether the store is managed by Sky. If None, it
               must be populated by the implementing class during initialization.
+            sync_on_reconstruction: bool; Whether to sync data if the storage
+              object is found in the global_user_state and reconstructed from
+              there. This is set to false when the Storage object is created not
+              for direct use, e.g. for sky storage delete.
 
         Raises:
             StorageBucketCreateError: If bucket creation fails
             StorageBucketGetError: If fetching existing bucket fails
             StorageInitError: If general initialization fails
         """
         self.name = name
         self.source = source
         self.region = region
         self.is_sky_managed = is_sky_managed
+        self.sync_on_reconstruction = sync_on_reconstruction
         # Whether sky is responsible for the lifecycle of the Store.
         self._validate()
         self.initialize()
 
     @classmethod
     def from_metadata(cls, metadata: StoreMetadata, **override_args):
         """Create a Store from a StoreMetadata object.
@@ -174,15 +224,17 @@
         Used when reconstructing Storage and Store objects from
         global_user_state.
         """
         return cls(name=override_args.get('name', metadata.name),
                    source=override_args.get('source', metadata.source),
                    region=override_args.get('region', metadata.region),
                    is_sky_managed=override_args.get('is_sky_managed',
-                                                    metadata.is_sky_managed))
+                                                    metadata.is_sky_managed),
+                   sync_on_reconstruction=override_args.get(
+                       'sync_on_reconstruction', True))
 
     def get_metadata(self) -> StoreMetadata:
         return self.StoreMetadata(name=self.name,
                                   source=self.source,
                                   region=self.region,
                                   is_sky_managed=self.is_sky_managed)
 
@@ -224,26 +276,15 @@
     def download_remote_dir(self, local_path: str) -> None:
         """Downloads directory from remote bucket to the specified
         local_path
 
         Args:
           local_path: Local path on user's device
         """
-        assert local_path is not None
-        local_path = os.path.expanduser(local_path)
-        iterator = self._remote_filepath_iterator()
-        for remote_path in iterator:
-            remote_path = next(iterator)
-            if remote_path[-1] == '/':
-                continue
-            path = os.path.join(local_path, remote_path)
-            if not os.path.exists(os.path.dirname(path)):
-                os.makedirs(os.path.dirname(path))
-            logger.info(f'Downloading {remote_path} to {path}')
-            self._download_file(remote_path, path)
+        raise NotImplementedError
 
     def _download_file(self, remote_path: str, local_path: str) -> None:
         """Downloads file from remote to local on Store
 
         Args:
           remote_path: str; Remote file path on Store
           local_path: str; Local file path on user's device
@@ -304,15 +345,15 @@
         - (optional) Set of stores managed by sky added to the Storage object
         """
 
         def __init__(
             self,
             *,
             storage_name: Optional[str],
-            source: Optional[str],
+            source: Optional[SourceType],
             sky_stores: Optional[Dict[StoreType,
                                       AbstractStore.StoreMetadata]] = None):
             assert storage_name is not None or source is not None
             self.storage_name = storage_name
             self.source = source
             # Only stores managed by sky are stored here in the
             # global_user_state
@@ -331,19 +372,19 @@
         def remove_store(self, store: AbstractStore) -> None:
             storetype = StoreType.from_store(store)
             if storetype in self.sky_stores:
                 del self.sky_stores[storetype]
 
     def __init__(self,
                  name: Optional[str] = None,
-                 source: Union[Path, List[Path], None] = None,
+                 source: Optional[SourceType] = None,
                  stores: Optional[Dict[StoreType, AbstractStore]] = None,
                  persistent: Optional[bool] = True,
                  mode: StorageMode = StorageMode.MOUNT,
-                 sync_on_reconstruction: Optional[bool] = True):
+                 sync_on_reconstruction: bool = True) -> None:
         """Initializes a Storage object.
 
         Three fields are required: the name of the storage, the source
         path where the data is initially located, and the default mount
         path where the data will be mounted to on the cloud.
 
         Storage object validation depends on the name, source and mount mode.
@@ -373,48 +414,58 @@
           mode: StorageMode; Specify how the storage object is manifested on
             the remote VM. Can be either MOUNT or COPY. Defaults to MOUNT.
           sync_on_reconstruction: bool; Whether to sync the data if the storage
             object is found in the global_user_state and reconstructed from
             there. This is set to false when the Storage object is created not
             for direct use, e.g. for sky storage delete.
         """
-        self.name = name
+        self.name: str
         self.source = source
         self.persistent = persistent
         self.mode = mode
         assert mode in StorageMode
         self.sync_on_reconstruction = sync_on_reconstruction
 
         # TODO(romilb, zhwu): This is a workaround to support storage deletion
         # for spot. Once sky storage supports forced management for external
         # buckets, this can be deprecated.
         self.force_delete = False
 
         # Validate and correct inputs if necessary
-        self._validate_storage_spec()
+        self._validate_storage_spec(name)
 
         # Sky optimizer either adds a storage object instance or selects
         # from existing ones
         self.stores = {} if stores is None else stores
 
         # Logic to rebuild Storage if it is in global user state
-        self.handle = global_user_state.get_handle_from_storage_name(self.name)
-        if self.handle:
+        handle = global_user_state.get_handle_from_storage_name(self.name)
+        if handle is not None:
+            self.handle = handle
             # Reconstruct the Storage object from the global_user_state
             logger.debug('Detected existing storage object, '
                          f'loading Storage: {self.name}')
             for s_type, s_metadata in self.handle.sky_stores.items():
                 # When initializing from global_user_state, we override the
                 # source from the YAML
                 if s_type == StoreType.S3:
-                    store = S3Store.from_metadata(s_metadata,
-                                                  source=self.source)
+                    store = S3Store.from_metadata(
+                        s_metadata,
+                        source=self.source,
+                        sync_on_reconstruction=self.sync_on_reconstruction)
                 elif s_type == StoreType.GCS:
-                    store = GcsStore.from_metadata(s_metadata,
-                                                   source=self.source)
+                    store = GcsStore.from_metadata(
+                        s_metadata,
+                        source=self.source,
+                        sync_on_reconstruction=self.sync_on_reconstruction)
+                elif s_type == StoreType.R2:
+                    store = R2Store.from_metadata(
+                        s_metadata,
+                        source=self.source,
+                        sync_on_reconstruction=self.sync_on_reconstruction)
                 else:
                     with ux_utils.print_exception_no_traceback():
                         raise ValueError(f'Unknown store type: {s_type}')
 
                 self._add_store(store, is_reconstructed=True)
 
             # TODO(romilb): This logic should likely be in add_store to move
@@ -429,38 +480,41 @@
                 self.sync_all_stores()
 
         else:
             # Storage does not exist in global_user_state, create new stores
             sky_managed_stores = {
                 t: s.get_metadata()
                 for t, s in self.stores.items()
-                if s.is_sky_managed()
+                if s.is_sky_managed
             }
             self.handle = self.StorageMetadata(storage_name=self.name,
                                                source=self.source,
                                                sky_stores=sky_managed_stores)
 
             if self.source is not None:
                 # If source is a pre-existing bucket, connect to the bucket
                 # If the bucket does not exist, this will error out
                 if isinstance(self.source, str):
                     if self.source.startswith('s3://'):
                         self.add_store(StoreType.S3)
                     elif self.source.startswith('gs://'):
                         self.add_store(StoreType.GCS)
+                    elif self.source.startswith('r2://'):
+                        self.add_store(StoreType.R2)
 
     @staticmethod
-    def _validate_source(source: str, mode: StorageMode,
-                         sync_on_reconstruction: bool) -> Tuple[str, bool]:
+    def _validate_source(
+            source: SourceType, mode: StorageMode,
+            sync_on_reconstruction: bool) -> Tuple[SourceType, bool]:
         """Validates the source path.
 
         Args:
           source: str; File path where the data is initially stored. Can be a
-            local path or a cloud URI (s3://, gs://, etc.). Local paths do not
-            need to be absolute.
+            local path or a cloud URI (s3://, gs://, r2:// etc.).
+            Local paths do not need to be absolute.
           mode: StorageMode; StorageMode of the storage object
 
         Returns:
           Tuple[source, is_local_source]
           source: str; The source path.
           is_local_path: bool; Whether the source is a local path. False if URI.
         """
@@ -522,15 +576,15 @@
                             'that the file will be uploaded to the root of the '
                             'bucket and will appear at <destination_path>/'
                             f'{os.path.basename(source)}. Alternatively, you '
                             'can directly upload the file to the VM without '
                             'using a bucket by writing <destination_path>: '
                             f'{source} in the file_mounts section of your YAML')
                 is_local_source = True
-            elif split_path.scheme in ['s3', 'gs']:
+            elif split_path.scheme in ['s3', 'gs', 'r2']:
                 is_local_source = False
                 # Storage mounting does not support mounting specific files from
                 # cloud store - ensure path points to only a directory
                 if mode == StorageMode.MOUNT:
                     if split_path.path.strip('/') != '':
                         with ux_utils.print_exception_no_traceback():
                             raise exceptions.StorageModeError(
@@ -538,65 +592,90 @@
                                 ' mounting specific files from cloud'
                                 ' storage. Please use COPY mode or'
                                 ' specify only the bucket name as'
                                 ' the source.')
             else:
                 with ux_utils.print_exception_no_traceback():
                     raise exceptions.StorageSourceError(
-                        f'Supported paths: local, s3://, gs://. Got: {source}')
+                        f'Supported paths: local, s3://, gs://, '
+                        f'r2://. Got: {source}')
         return source, is_local_source
 
-    def _validate_storage_spec(self) -> None:
+    def _validate_storage_spec(self, name: Optional[str]) -> None:
         """
         Validates the storage spec and updates local fields if necessary.
         """
+
+        def validate_name(name):
+            """ Checks for validating the storage name.
+
+            Checks if the name starts the s3, gcs or r2 prefix and raise error
+            if it does. Store specific validation checks (e.g., S3 specific
+            rules) happen in the corresponding store class.
+            """
+            prefix = name.split('://')[0]
+            prefix = prefix.lower()
+            if prefix in ['s3', 'gs', 'r2']:
+                with ux_utils.print_exception_no_traceback():
+                    raise exceptions.StorageNameError(
+                        'Prefix detected: `name` cannot start with '
+                        f'{prefix}://. If you are trying to use an existing '
+                        'bucket created outside of SkyPilot, please specify it '
+                        'using the `source` field (e.g. '
+                        '`source: s3://mybucket/`). If you are trying to '
+                        'create a new bucket, please use the `store` field to '
+                        'specify the store type (e.g. `store: s3`).')
+
         if self.source is None:
             # If the mode is COPY, the source must be specified
             if self.mode == StorageMode.COPY:
                 # Check if a Storage object already exists in global_user_state
                 # (e.g. used as scratch previously). Such storage objects can be
                 # mounted in copy mode even though they have no source in the
                 # yaml spec (the name is the source).
-                handle = global_user_state.get_handle_from_storage_name(
-                    self.name)
-                if handle is not None:
-                    return
-                else:
+                handle = global_user_state.get_handle_from_storage_name(name)
+                if handle is None:
                     with ux_utils.print_exception_no_traceback():
                         raise exceptions.StorageSourceError(
                             'New storage object: source must be specified when '
                             'using COPY mode.')
             else:
                 # If source is not specified in COPY mode, the intent is to
                 # create a bucket and use it as scratch disk. Name must be
                 # specified to create bucket.
-                if not self.name:
+                if not name:
                     with ux_utils.print_exception_no_traceback():
                         raise exceptions.StorageSpecError(
                             'Storage source or storage name must be specified.')
-                else:
-                    # Create bucket and mount
-                    return
+            assert name is not None, handle
+            validate_name(name)
+            self.name = name
+            return
         elif self.source is not None:
             source, is_local_source = Storage._validate_source(
                 self.source, self.mode, self.sync_on_reconstruction)
-
-            if not self.name:
+            if not name:
                 if is_local_source:
                     with ux_utils.print_exception_no_traceback():
                         raise exceptions.StorageNameError(
                             'Storage name must be specified if the source is '
                             'local.')
                 else:
+                    assert isinstance(source, str)
                     # Set name to source bucket name and continue
-                    self.name = urllib.parse.urlsplit(source).netloc
+                    name = urllib.parse.urlsplit(source).netloc
+                    assert name is not None, source
+                    self.name = name
                     return
             else:
                 if is_local_source:
                     # If name is specified and source is local, upload to bucket
+                    assert name is not None, source
+                    validate_name(name)
+                    self.name = name
                     return
                 else:
                     # Both name and source should not be specified if the source
                     # is a URI. Name will be inferred from the URI.
                     with ux_utils.print_exception_no_traceback():
                         raise exceptions.StorageSpecError(
                             'Storage name should not be specified if the '
@@ -608,35 +687,41 @@
     def add_store(self, store_type: Union[str, StoreType]) -> AbstractStore:
         """Initializes and adds a new store to the storage.
 
         Invoked by the optimizer after it has selected a store to
         add it to Storage.
 
         Args:
-          store_type: StoreType; Type of the storage [S3, GCS, AZURE]
+          store_type: StoreType; Type of the storage [S3, GCS, AZURE, R2]
         """
         if isinstance(store_type, str):
             store_type = StoreType(store_type)
 
         if store_type in self.stores:
             logger.info(f'Storage type {store_type} already exists.')
             return self.stores[store_type]
 
+        store_cls: Type[AbstractStore]
         if store_type == StoreType.S3:
             store_cls = S3Store
         elif store_type == StoreType.GCS:
             store_cls = GcsStore
+        elif store_type == StoreType.R2:
+            store_cls = R2Store
         else:
             with ux_utils.print_exception_no_traceback():
                 raise exceptions.StorageSpecError(
                     f'{store_type} not supported as a Store.')
 
         # Initialize store object and get/create bucket
         try:
-            store = store_cls(name=self.name, source=self.source)
+            store = store_cls(
+                name=self.name,
+                source=self.source,
+                sync_on_reconstruction=self.sync_on_reconstruction)
         except exceptions.StorageBucketCreateError:
             # Creation failed, so this must be sky managed store. Add failure
             # to state.
             logger.error(f'Could not create {store_type} store '
                          f'with name {self.name}.')
             global_user_state.set_storage_status(self.name,
                                                  StorageStatus.INIT_FAILED)
@@ -689,15 +774,16 @@
             # We delete a store from the cloud if it's sky managed. Else just
             # remove handle and return
             if is_sky_managed:
                 self.handle.remove_store(store)
                 store.delete()
                 # Check remaining stores - if none is sky managed, remove
                 # the storage from global_user_state.
-                delete = all(s.is_sky_managed is False for s in self.stores)
+                delete = all(
+                    s.is_sky_managed is False for s in self.stores.values())
                 if delete:
                     global_user_state.remove_storage(self.name)
                 else:
                     global_user_state.set_storage_handle(self.name, self.handle)
             elif self.force_delete:
                 store.delete()
             # Remove store from bookkeeping
@@ -716,30 +802,42 @@
     def sync_all_stores(self):
         """Syncs the source and destinations of all stores in the Storage"""
         for _, store in self.stores.items():
             self._sync_store(store)
 
     def _sync_store(self, store: AbstractStore):
         """Runs the upload routine for the store and handles failures"""
+
+        def warn_for_git_dir(source: str):
+            if os.path.isdir(os.path.join(source, '.git')):
+                logger.warning(f'\'.git\' directory under \'{self.source}\' '
+                               'is excluded during sync.')
+
         try:
+            if self.source is not None:
+                if isinstance(self.source, str):
+                    warn_for_git_dir(self.source)
+                else:
+                    for source in self.source:
+                        warn_for_git_dir(source)
             store.upload()
         except exceptions.StorageUploadError:
             logger.error(f'Could not upload {self.source} to store '
                          f'name {store.name}.')
             if store.is_sky_managed:
                 global_user_state.set_storage_status(
                     self.name, StorageStatus.UPLOAD_FAILED)
             raise
 
         # Upload succeeded - update state
         if store.is_sky_managed:
             global_user_state.set_storage_status(self.name, StorageStatus.READY)
 
     @classmethod
-    def from_yaml_config(cls, config: Dict[str, str]) -> 'Storage':
+    def from_yaml_config(cls, config: Dict[str, Any]) -> 'Storage':
         backend_utils.validate_schema(config, schemas.get_storage_schema(),
                                       'Invalid storage YAML: ')
 
         name = config.pop('name', None)
         source = config.pop('source', None)
         store = config.pop('store', None)
         mode_str = config.pop('mode', None)
@@ -764,25 +862,25 @@
             storage_obj.add_store(StoreType(store.upper()))
 
         # Add force deletion flag
         storage_obj.force_delete = force_delete
         return storage_obj
 
     def to_yaml_config(self) -> Dict[str, str]:
-        config = dict()
+        config = {}
 
-        def add_if_not_none(key, value):
+        def add_if_not_none(key: str, value: Optional[Any]):
             if value is not None:
                 config[key] = value
 
-        name = self.name
-        if (self.source is not None and isinstance(self.source, str) and
-                data_utils.is_cloud_store_url(self.source)):
+        name = None
+        if (self.source is None or not isinstance(self.source, str) or
+                not data_utils.is_cloud_store_url(self.source)):
             # Remove name if source is a cloud store URL
-            name = None
+            name = self.name
         add_if_not_none('name', name)
         add_if_not_none('source', self.source)
 
         stores = None
         if len(self.stores) > 0:
             stores = ','.join([store.value for store in self.stores])
         add_if_not_none('store', stores)
@@ -794,38 +892,118 @@
 
 
 class S3Store(AbstractStore):
     """S3Store inherits from Storage Object and represents the backend
     for S3 buckets.
     """
 
-    ACCESS_DENIED_MESSAGE = 'Access Denied'
+    _ACCESS_DENIED_MESSAGE = 'Access Denied'
 
     def __init__(self,
                  name: str,
                  source: str,
                  region: Optional[str] = 'us-east-2',
-                 is_sky_managed: Optional[bool] = None):
-        self.client = None
-        self.bucket = None
-        super().__init__(name, source, region, is_sky_managed)
+                 is_sky_managed: Optional[bool] = None,
+                 sync_on_reconstruction: bool = True):
+        self.client: 'boto3.client.Client'
+        self.bucket: 'StorageHandle'
+        super().__init__(name, source, region, is_sky_managed,
+                         sync_on_reconstruction)
 
     def _validate(self):
         if self.source is not None and isinstance(self.source, str):
             if self.source.startswith('s3://'):
                 assert self.name == data_utils.split_s3_path(self.source)[0], (
                     'S3 Bucket is specified as path, the name should be the'
                     ' same as S3 bucket.')
             elif self.source.startswith('gs://'):
                 assert self.name == data_utils.split_gcs_path(self.source)[0], (
                     'GCS Bucket is specified as path, the name should be '
                     'the same as GCS bucket.')
                 assert data_utils.verify_gcs_bucket(self.name), (
                     f'Source specified as {self.source}, a GCS bucket. ',
                     'GCS Bucket should exist.')
+            elif self.source.startswith('r2://'):
+                assert self.name == data_utils.split_r2_path(self.source)[0], (
+                    'R2 Bucket is specified as path, the name should be '
+                    'the same as R2 bucket.')
+                assert data_utils.verify_r2_bucket(self.name), (
+                    f'Source specified as {self.source}, a R2 bucket. ',
+                    'R2 Bucket should exist.')
+        # Validate name
+        self.name = self.validate_name(self.name)
+
+        # Check if the storage is enabled
+        if not _is_storage_cloud_enabled(str(clouds.AWS())):
+            with ux_utils.print_exception_no_traceback():
+                raise exceptions.ResourcesUnavailableError(
+                    'Storage \'store: s3\' specified, but ' \
+                    'AWS access is disabled. To fix, enable '\
+                    'AWS by running `sky check`. More info: '\
+                    'https://skypilot.readthedocs.io/en/latest/getting-started/installation.html.' # pylint: disable=line-too-long
+                    )
+
+    @classmethod
+    def validate_name(cls, name) -> str:
+        """Validates the name of the S3 store.
+
+        Source for rules: https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.html # pylint: disable=line-too-long
+        """
+
+        def _raise_no_traceback_name_error(err_str):
+            with ux_utils.print_exception_no_traceback():
+                raise exceptions.StorageNameError(err_str)
+
+        if name is not None and isinstance(name, str):
+            if not 3 <= len(name) <= 63:
+                _raise_no_traceback_name_error(
+                    f'Invalid store name: name {name} must be between 3 (min) '
+                    'and 63 (max) characters long.')
+
+            # Check for valid characters and start/end with a letter or number
+            pattern = r'^[a-z0-9][-a-z0-9.]*[a-z0-9]$'
+            if not re.match(pattern, name):
+                _raise_no_traceback_name_error(
+                    f'Invalid store name: name {name} can consist only of '
+                    'lowercase letters, numbers, dots (.), and hyphens (-). '
+                    'It must begin and end with a letter or number.')
+
+            # Check for two adjacent periods
+            if '..' in name:
+                _raise_no_traceback_name_error(
+                    f'Invalid store name: name {name} must not contain '
+                    'two adjacent periods.')
+
+            # Check for IP address format
+            ip_pattern = r'^(?:\d{1,3}\.){3}\d{1,3}$'
+            if re.match(ip_pattern, name):
+                _raise_no_traceback_name_error(
+                    f'Invalid store name: name {name} must not be formatted as '
+                    'an IP address (for example, 192.168.5.4).')
+
+            # Check for 'xn--' prefix
+            if name.startswith('xn--'):
+                _raise_no_traceback_name_error(
+                    f'Invalid store name: name {name} must not start with the '
+                    'prefix "xn--".')
+
+            # Check for '-s3alias' suffix
+            if name.endswith('-s3alias'):
+                _raise_no_traceback_name_error(
+                    f'Invalid store name: name {name} must not end with the '
+                    'suffix "-s3alias".')
+
+            # Check for '--ol-s3' suffix
+            if name.endswith('--ol-s3'):
+                _raise_no_traceback_name_error(
+                    f'Invalid store name: name {name} must not end with the '
+                    'suffix "--ol-s3".')
+        else:
+            _raise_no_traceback_name_error('Store name must be specified.')
+        return name
 
     def initialize(self):
         """Initializes the S3 store object on the cloud.
 
         Initialization involves fetching bucket if exists, or creating it if
         it does not.
 
@@ -856,25 +1034,32 @@
             if isinstance(self.source, list):
                 self.batch_aws_rsync(self.source, create_dirs=True)
             elif self.source is not None:
                 if self.source.startswith('s3://'):
                     pass
                 elif self.source.startswith('gs://'):
                     self._transfer_to_s3()
+                elif self.source.startswith('r2://'):
+                    self._transfer_to_s3()
                 else:
                     self.batch_aws_rsync([self.source])
         except exceptions.StorageUploadError:
             raise
         except Exception as e:
             raise exceptions.StorageUploadError(
                 f'Upload failed for store {self.name}') from e
 
     def delete(self) -> None:
-        self._delete_s3_bucket(self.name)
-        logger.info(f'{colorama.Fore.GREEN}Deleted S3 bucket {self.name}.'
+        deleted_by_skypilot = self._delete_s3_bucket(self.name)
+        if deleted_by_skypilot:
+            msg_str = f'Deleted S3 bucket {self.name}.'
+        else:
+            msg_str = f'S3 bucket {self.name} may have been deleted ' \
+                      f'externally. Removing from local state.'
+        logger.info(f'{colorama.Fore.GREEN}{msg_str}'
                     f'{colorama.Style.RESET_ALL}')
 
     def get_handle(self) -> StorageHandle:
         return aws.resource('s3').Bucket(self.name)
 
     def batch_aws_rsync(self,
                         source_path_list: List[Path],
@@ -902,48 +1087,55 @@
                 [f'--include "{file_name}"' for file_name in file_names])
             sync_command = ('aws s3 sync --no-follow-symlinks --exclude="*" '
                             f'{includes} {base_dir_path} '
                             f's3://{self.name}')
             return sync_command
 
         def get_dir_sync_command(src_dir_path, dest_dir_name):
-            sync_command = ('aws s3 sync --no-follow-symlinks '
-                            f'{src_dir_path} '
-                            f's3://{self.name}/{dest_dir_name}')
+            # we exclude .git directory from the sync
+            sync_command = (
+                'aws s3 sync --no-follow-symlinks --exclude ".git/*" '
+                f'{src_dir_path} '
+                f's3://{self.name}/{dest_dir_name}')
             return sync_command
 
         # Generate message for upload
         if len(source_path_list) > 1:
             source_message = f'{len(source_path_list)} paths'
         else:
             source_message = source_path_list[0]
 
-        with backend_utils.safe_console_status(
+        with log_utils.safe_rich_status(
                 f'[bold cyan]Syncing '
                 f'[green]{source_message}[/] to [green]s3://{self.name}/[/]'):
             data_utils.parallel_upload(
                 source_path_list,
                 get_file_sync_command,
                 get_dir_sync_command,
                 self.name,
-                self.ACCESS_DENIED_MESSAGE,
+                self._ACCESS_DENIED_MESSAGE,
                 create_dirs=create_dirs,
                 max_concurrent_uploads=_MAX_CONCURRENT_UPLOADS)
 
     def _transfer_to_s3(self) -> None:
+        assert isinstance(self.source, str), self.source
         if self.source.startswith('gs://'):
             data_transfer.gcs_to_s3(self.name, self.name)
+        elif self.source.startswith('r2://'):
+            data_transfer.r2_to_s3(self.name, self.name)
 
-    def _get_bucket(self) -> Tuple[StorageHandle, bool]:
+    def _get_bucket(self) -> Tuple[Optional[StorageHandle], bool]:
         """Obtains the S3 bucket.
 
-        If the bucket exists, this method will connect to the bucket.
-        If the bucket does not exist, there are two cases:
+        If the bucket exists, this method will return the bucket.
+        If the bucket does not exist, there are three cases:
           1) Raise an error if the bucket source starts with s3://
-          2) Create a new bucket otherwise
+          2) Return None if bucket has been externally deleted and
+             sync_on_reconstruction is False
+          3) Create and return a new bucket otherwise
 
         Raises:
             StorageBucketCreateError: If creating the bucket fails
             StorageBucketGetError: If fetching a bucket fails
         """
         s3 = aws.resource('s3')
         bucket = s3.Bucket(self.name)
@@ -951,36 +1143,40 @@
         try:
             # Try Public bucket case.
             # This line does not error out if the bucket is an external public
             # bucket or if it is a user's bucket that is publicly
             # accessible.
             self.client.head_bucket(Bucket=self.name)
             return bucket, False
-        except aws.client_exception() as e:
+        except aws.botocore_exceptions().ClientError as e:
             error_code = e.response['Error']['Code']
             # AccessDenied error for buckets that are private and not owned by
             # user.
             if error_code == '403':
                 command = f'aws s3 ls {self.name}'
                 with ux_utils.print_exception_no_traceback():
                     raise exceptions.StorageBucketGetError(
                         _BUCKET_FAIL_TO_CONNECT_MESSAGE.format(name=self.name) +
-                        f' To debug, consider using {command}.') from e
+                        f' To debug, consider running `{command}`.') from e
 
         if isinstance(self.source, str) and self.source.startswith('s3://'):
             with ux_utils.print_exception_no_traceback():
                 raise exceptions.StorageBucketGetError(
                     'Attempted to connect to a non-existent bucket: '
                     f'{self.source}. Consider using `aws s3 ls '
                     f'{self.source}` to debug.')
 
         # If bucket cannot be found in both private and public settings,
-        # the bucket is created by Sky.
-        bucket = self._create_s3_bucket(self.name)
-        return bucket, True
+        # the bucket is to be created by Sky. However, creation is skipped if
+        # Store object is being reconstructed for deletion.
+        if self.sync_on_reconstruction:
+            bucket = self._create_s3_bucket(self.name)
+            return bucket, True
+        else:
+            return None, False
 
     def _download_file(self, remote_path: str, local_path: str) -> None:
         """Downloads file from remote to local on s3 bucket
         using the boto3 API
 
         Args:
           remote_path: str; Remote path on S3 bucket
@@ -1023,65 +1219,79 @@
             if region is None:
                 s3_client.create_bucket(Bucket=bucket_name)
             else:
                 location = {'LocationConstraint': region}
                 s3_client.create_bucket(Bucket=bucket_name,
                                         CreateBucketConfiguration=location)
                 logger.info(f'Created S3 bucket {bucket_name} in {region}')
-        except aws.client_exception() as e:
+        except aws.botocore_exceptions().ClientError as e:
             with ux_utils.print_exception_no_traceback():
                 raise exceptions.StorageBucketCreateError(
                     f'Attempted to create a bucket '
                     f'{self.name} but failed.') from e
         return aws.resource('s3').Bucket(bucket_name)
 
-    def _delete_s3_bucket(self, bucket_name: str) -> None:
+    def _delete_s3_bucket(self, bucket_name: str) -> bool:
         """Deletes S3 bucket, including all objects in bucket
 
         Args:
           bucket_name: str; Name of bucket
+
+        Returns:
+         bool; True if bucket was deleted, False if it was deleted externally.
         """
         # Deleting objects is very slow programatically
         # (i.e. bucket.objects.all().delete() is slow).
         # In addition, standard delete operations (i.e. via `aws s3 rm`)
         # are slow, since AWS puts deletion markers.
         # https://stackoverflow.com/questions/49239351/why-is-it-so-much-slower-to-delete-objects-in-aws-s3-than-it-is-to-create-them
         # The fastest way to delete is to run `aws s3 rb --force`,
         # which removes the bucket by force.
         remove_command = f'aws s3 rb s3://{bucket_name} --force'
         try:
-            with backend_utils.safe_console_status(
+            with log_utils.safe_rich_status(
                     f'[bold cyan]Deleting S3 bucket {bucket_name}[/]'):
-                subprocess.check_output(remove_command.split(' '))
+                subprocess.check_output(remove_command.split(' '),
+                                        stderr=subprocess.STDOUT)
         except subprocess.CalledProcessError as e:
-            logger.error(e.output)
-            with ux_utils.print_exception_no_traceback():
-                raise exceptions.StorageBucketDeleteError(
-                    f'Failed to delete S3 bucket {bucket_name}.')
+            if 'NoSuchBucket' in e.output.decode('utf-8'):
+                logger.debug(
+                    _BUCKET_EXTERNALLY_DELETED_DEBUG_MESSAGE.format(
+                        bucket_name=bucket_name))
+                return False
+            else:
+                logger.error(e.output)
+                with ux_utils.print_exception_no_traceback():
+                    raise exceptions.StorageBucketDeleteError(
+                        f'Failed to delete S3 bucket {bucket_name}.')
 
         # Wait until bucket deletion propagates on AWS servers
         while data_utils.verify_s3_bucket(bucket_name):
             time.sleep(0.1)
+        return True
 
 
 class GcsStore(AbstractStore):
     """GcsStore inherits from Storage Object and represents the backend
     for GCS buckets.
     """
 
-    ACCESS_DENIED_MESSAGE = 'AccessDeniedException'
+    _ACCESS_DENIED_MESSAGE = 'AccessDeniedException'
+    GCSFUSE_VERSION = '0.42.3'
 
     def __init__(self,
                  name: str,
                  source: str,
                  region: Optional[str] = 'us-central1',
-                 is_sky_managed: Optional[bool] = None):
-        self.client = None
-        self.bucket = None
-        super().__init__(name, source, region, is_sky_managed)
+                 is_sky_managed: Optional[bool] = None,
+                 sync_on_reconstruction: Optional[bool] = True):
+        self.client: 'storage.Client'
+        self.bucket: StorageHandle
+        super().__init__(name, source, region, is_sky_managed,
+                         sync_on_reconstruction)
 
     def _validate(self):
         if self.source is not None:
             if isinstance(self.source, str):
                 if self.source.startswith('s3://'):
                     assert self.name == data_utils.split_s3_path(
                         self.source
@@ -1093,14 +1303,90 @@
                         'S3 Bucket should exist.')
                 elif self.source.startswith('gs://'):
                     assert self.name == data_utils.split_gcs_path(
                         self.source
                     )[0], (
                         'GCS Bucket is specified as path, the name should be '
                         'the same as GCS bucket.')
+                elif self.source.startswith('r2://'):
+                    assert self.name == data_utils.split_r2_path(
+                        self.source
+                    )[0], ('R2 Bucket is specified as path, the name should be '
+                           'the same as R2 bucket.')
+                    assert data_utils.verify_r2_bucket(self.name), (
+                        f'Source specified as {self.source}, a R2 bucket. ',
+                        'R2 Bucket should exist.')
+        # Validate name
+        self.name = self.validate_name(self.name)
+        # Check if the storage is enabled
+        if not _is_storage_cloud_enabled(str(clouds.GCP())):
+            with ux_utils.print_exception_no_traceback():
+                raise exceptions.ResourcesUnavailableError(
+                    'Storage \'store: gcs\' specified, but '
+                    'GCP access is disabled. To fix, enable '
+                    'GCP by running `sky check`. '
+                    'More info: https://skypilot.readthedocs.io/en/latest/getting-started/installation.html.')  # pylint: disable=line-too-long
+
+    @classmethod
+    def validate_name(cls, name) -> str:
+        """Validates the name of the GCS store.
+
+        Source for rules: https://cloud.google.com/storage/docs/buckets#naming
+        """
+
+        def _raise_no_traceback_name_error(err_str):
+            with ux_utils.print_exception_no_traceback():
+                raise exceptions.StorageNameError(err_str)
+
+        if name is not None and isinstance(name, str):
+            # Check for overall length
+            if not 3 <= len(name) <= 222:
+                _raise_no_traceback_name_error(
+                    f'Invalid store name: name {name} must contain 3-222 '
+                    'characters.')
+
+            # Check for valid characters and start/end with a number or letter
+            pattern = r'^[a-z0-9][-a-z0-9._]*[a-z0-9]$'
+            if not re.match(pattern, name):
+                _raise_no_traceback_name_error(
+                    f'Invalid store name: name {name} can only contain '
+                    'lowercase letters, numeric characters, dashes (-), '
+                    'underscores (_), and dots (.). Spaces are not allowed. '
+                    'Names must start and end with a number or letter.')
+
+            # Check for 'goog' prefix and 'google' in the name
+            if name.startswith('goog') or any(
+                    s in name
+                    for s in ['google', 'g00gle', 'go0gle', 'g0ogle']):
+                _raise_no_traceback_name_error(
+                    f'Invalid store name: name {name} cannot begin with the '
+                    '"goog" prefix or contain "google" in various forms.')
+
+            # Check for dot-separated components length
+            components = name.split('.')
+            if any(len(component) > 63 for component in components):
+                _raise_no_traceback_name_error(
+                    'Invalid store name: Dot-separated components in name '
+                    f'{name} can be no longer than 63 characters.')
+
+            if '..' in name or '.-' in name or '-.' in name:
+                _raise_no_traceback_name_error(
+                    f'Invalid store name: name {name} must not contain two '
+                    'adjacent periods or a dot next to a hyphen.')
+
+            # Check for IP address format
+            ip_pattern = r'^(?:\d{1,3}\.){3}\d{1,3}$'
+            if re.match(ip_pattern, name):
+                _raise_no_traceback_name_error(
+                    f'Invalid store name: name {name} cannot be represented as '
+                    'an IP address in dotted-decimal notation '
+                    '(for example, 192.168.5.4).')
+        else:
+            _raise_no_traceback_name_error('Store name must be specified.')
+        return name
 
     def initialize(self):
         """Initializes the GCS store object on the cloud.
 
         Initialization involves fetching bucket if exists, or creating it if
         it does not.
 
@@ -1131,27 +1417,34 @@
             if isinstance(self.source, list):
                 self.batch_gsutil_rsync(self.source, create_dirs=True)
             elif self.source is not None:
                 if self.source.startswith('gs://'):
                     pass
                 elif self.source.startswith('s3://'):
                     self._transfer_to_gcs()
+                elif self.source.startswith('r2://'):
+                    self._transfer_to_gcs()
                 else:
                     # If a single directory is specified in source, upload
                     # contents to root of bucket by suffixing /*.
                     self.batch_gsutil_rsync([self.source])
         except exceptions.StorageUploadError:
             raise
         except Exception as e:
             raise exceptions.StorageUploadError(
                 f'Upload failed for store {self.name}') from e
 
     def delete(self) -> None:
-        self._delete_gcs_bucket(self.name)
-        logger.info(f'{colorama.Fore.GREEN}Deleted GCS bucket {self.name}.'
+        deleted_by_skypilot = self._delete_gcs_bucket(self.name)
+        if deleted_by_skypilot:
+            msg_str = f'Deleted GCS bucket {self.name}.'
+        else:
+            msg_str = f'GCS bucket {self.name} may have been deleted ' \
+                      f'externally. Removing from local state.'
+        logger.info(f'{colorama.Fore.GREEN}{msg_str}'
                     f'{colorama.Style.RESET_ALL}')
 
     def get_handle(self) -> StorageHandle:
         return self.client.get_bucket(self.name)
 
     def batch_gsutil_cp(self,
                         source_path_list: List[Path],
@@ -1164,15 +1457,15 @@
         However, unlike rsync, files are compared based on just their filename,
         and any updates to a file would not be copied to the bucket.
         """
         # Generate message for upload
         if len(source_path_list) > 1:
             source_message = f'{len(source_path_list)} paths'
         else:
-            source_message = source_path_list
+            source_message = source_path_list[0]
 
         # If the source_path list contains a directory, then gsutil cp -n
         # copies the dir as is to the root of the bucket. To copy the
         # contents of directory to the root, add /* to the directory path
         # e.g., /mydir/*
         source_path_list = [
             str(path) + '/*' if
@@ -1180,19 +1473,19 @@
             for path in source_path_list
         ]
         copy_list = '\n'.join(
             os.path.abspath(os.path.expanduser(p)) for p in source_path_list)
         sync_command = (f'echo "{copy_list}" | '
                         f'gsutil -m cp -e -n -r -I gs://{self.name}')
 
-        with backend_utils.safe_console_status(
+        with log_utils.safe_rich_status(
                 f'[bold cyan]Syncing '
                 f'[green]{source_message}[/] to [green]gs://{self.name}/[/]'):
             data_utils.run_upload_cli(sync_command,
-                                      self.ACCESS_DENIED_MESSAGE,
+                                      self._ACCESS_DENIED_MESSAGE,
                                       bucket_name=self.name)
 
     def batch_gsutil_rsync(self,
                            source_path_list: List[Path],
                            create_dirs: bool = False) -> None:
         """Invokes gsutil rsync to batch upload a list of local paths
 
@@ -1210,52 +1503,58 @@
                 root of the bucket. If the local_path is a directory and this is
                 set to True, the directory is created in the bucket root and
                 contents are uploaded to it.
         """
 
         def get_file_sync_command(base_dir_path, file_names):
             sync_format = '|'.join(file_names)
-            sync_command = (f'gsutil -m rsync -x \'^(?!{sync_format}$).*\' '
+            sync_command = (f'gsutil -m rsync -e -x \'^(?!{sync_format}$).*\' '
                             f'{base_dir_path} gs://{self.name}')
             return sync_command
 
         def get_dir_sync_command(src_dir_path, dest_dir_name):
-            sync_command = (f'gsutil -m rsync -r {src_dir_path} '
-                            f'gs://{self.name}/{dest_dir_name}')
+            # we exclude .git directory from the sync
+            sync_command = (
+                f'gsutil -m rsync -e -r -x \'.git/*\' {src_dir_path} '
+                f'gs://{self.name}/{dest_dir_name}')
             return sync_command
 
         # Generate message for upload
         if len(source_path_list) > 1:
             source_message = f'{len(source_path_list)} paths'
         else:
             source_message = source_path_list[0]
 
-        with backend_utils.safe_console_status(
+        with log_utils.safe_rich_status(
                 f'[bold cyan]Syncing '
                 f'[green]{source_message}[/] to [green]gs://{self.name}/[/]'):
             data_utils.parallel_upload(
                 source_path_list,
                 get_file_sync_command,
                 get_dir_sync_command,
                 self.name,
-                self.ACCESS_DENIED_MESSAGE,
+                self._ACCESS_DENIED_MESSAGE,
                 create_dirs=create_dirs,
                 max_concurrent_uploads=_MAX_CONCURRENT_UPLOADS)
 
     def _transfer_to_gcs(self) -> None:
-        if self.source.startswith('s3://'):
+        if isinstance(self.source, str) and self.source.startswith('s3://'):
             data_transfer.s3_to_gcs(self.name, self.name)
+        elif isinstance(self.source, str) and self.source.startswith('r2://'):
+            data_transfer.r2_to_gcs(self.name, self.name)
 
     def _get_bucket(self) -> Tuple[StorageHandle, bool]:
         """Obtains the GCS bucket.
         If the bucket exists, this method will connect to the bucket.
 
-        If the bucket does not exist, there are two cases:
+        If the bucket does not exist, there are three cases:
           1) Raise an error if the bucket source starts with gs://
-          2) Create a new bucket otherwise
+          2) Return None if bucket has been externally deleted and
+             sync_on_reconstruction is False
+          3) Create and return a new bucket otherwise
 
         Raises:
             StorageBucketCreateError: If creating the bucket fails
             StorageBucketGetError: If fetching a bucket fails
         """
         try:
             bucket = self.client.get_bucket(self.name)
@@ -1263,16 +1562,23 @@
         except gcp.not_found_exception() as e:
             if isinstance(self.source, str) and self.source.startswith('gs://'):
                 with ux_utils.print_exception_no_traceback():
                     raise exceptions.StorageBucketGetError(
                         'Attempted to connect to a non-existent bucket: '
                         f'{self.source}') from e
             else:
-                bucket = self._create_gcs_bucket(self.name)
-                return bucket, True
+
+                # If bucket cannot be found (i.e., does not exist), it is to be
+                # created by Sky. However, creation is skipped if Store object
+                # is being reconstructed for deletion.
+                if self.sync_on_reconstruction:
+                    bucket = self._create_gcs_bucket(self.name)
+                    return bucket, True
+                else:
+                    return None, False
         except gcp.forbidden_exception():
             # Try public bucket to see if bucket exists
             logger.info(
                 'External Bucket detected; Connecting to external bucket...')
             try:
                 a_client = gcp.anonymous_storage_client()
                 bucket = a_client.bucket(self.name)
@@ -1280,37 +1586,40 @@
                 next(bucket.list_blobs())
                 return bucket, False
             except (gcp.not_found_exception(), ValueError) as e:
                 command = f'gsutil ls gs://{self.name}'
                 with ux_utils.print_exception_no_traceback():
                     raise exceptions.StorageBucketGetError(
                         _BUCKET_FAIL_TO_CONNECT_MESSAGE.format(name=self.name) +
-                        f' To debug, consider using {command}.') from e
+                        f' To debug, consider running `{command}`.') from e
 
     def mount_command(self, mount_path: str) -> str:
         """Returns the command to mount the bucket to the mount_path.
 
         Uses gcsfuse to mount the bucket.
 
         Args:
           mount_path: str; Path to mount the bucket to.
         """
         install_cmd = ('wget -nc https://github.com/GoogleCloudPlatform/gcsfuse'
-                       '/releases/download/v0.41.2/gcsfuse_0.41.2_amd64.deb '
+                       f'/releases/download/v{self.GCSFUSE_VERSION}/'
+                       f'gcsfuse_{self.GCSFUSE_VERSION}_amd64.deb '
                        '-O /tmp/gcsfuse.deb && '
                        'sudo dpkg --install /tmp/gcsfuse.deb')
         mount_cmd = ('gcsfuse -o allow_other '
                      '--implicit-dirs '
                      f'--stat-cache-capacity {self._STAT_CACHE_CAPACITY} '
                      f'--stat-cache-ttl {self._STAT_CACHE_TTL} '
                      f'--type-cache-ttl {self._TYPE_CACHE_TTL} '
                      f'--rename-dir-limit {self._RENAME_DIR_LIMIT} '
                      f'{self.bucket.name} {mount_path}')
+        version_check_cmd = (
+            f'gcsfuse --version | grep -q {self.GCSFUSE_VERSION}')
         return mounting_utils.get_mounting_command(mount_path, install_cmd,
-                                                   mount_cmd)
+                                                   mount_cmd, version_check_cmd)
 
     def _download_file(self, remote_path: str, local_path: str) -> None:
         """Downloads file from remote to local on GS bucket
 
         Args:
           remote_path: str; Remote path on GS bucket
           local_path: str; Local path on user's device
@@ -1337,34 +1646,393 @@
                     f'Attempted to create a bucket {self.name} but failed.'
                 ) from e
         logger.info(
             f'Created GCS bucket {new_bucket.name} in {new_bucket.location} '
             f'with storage class {new_bucket.storage_class}')
         return new_bucket
 
-    def _delete_gcs_bucket(self, bucket_name: str) -> None:
+    def _delete_gcs_bucket(self, bucket_name: str) -> bool:
         """Deletes GCS bucket, including all objects in bucket
 
         Args:
           bucket_name: str; Name of bucket
+
+        Returns:
+         bool; True if bucket was deleted, False if it was deleted externally.
         """
-        try:
-            self.client.get_bucket(bucket_name)
-        except gcp.forbidden_exception() as e:
-            # Try public bucket to see if bucket exists
-            with ux_utils.print_exception_no_traceback():
-                raise PermissionError(
-                    'External Bucket detected. User not allowed to delete '
-                    'external bucket.') from e
 
-        try:
-            with backend_utils.safe_console_status(
-                    f'[bold cyan]Deleting GCS bucket {bucket_name}[/]'):
+        with log_utils.safe_rich_status(
+                f'[bold cyan]Deleting GCS bucket {bucket_name}[/]'):
+            try:
+                self.client.get_bucket(bucket_name)
+            except gcp.forbidden_exception() as e:
+                # Try public bucket to see if bucket exists
+                with ux_utils.print_exception_no_traceback():
+                    raise PermissionError(
+                        'External Bucket detected. User not allowed to delete '
+                        'external bucket.') from e
+            except gcp.not_found_exception():
+                # If bucket does not exist, it may have been deleted externally.
+                # Do a no-op in that case.
+                logger.debug(
+                    _BUCKET_EXTERNALLY_DELETED_DEBUG_MESSAGE.format(
+                        bucket_name=bucket_name))
+                return False
+            try:
                 remove_obj_command = ('gsutil -m rm -r'
                                       f' gs://{bucket_name}')
                 subprocess.check_output(remove_obj_command.split(' '),
                                         stderr=subprocess.STDOUT)
-        except subprocess.CalledProcessError as e:
-            logger.error(e.output)
+                return True
+            except subprocess.CalledProcessError as e:
+                logger.error(e.output)
+                with ux_utils.print_exception_no_traceback():
+                    raise exceptions.StorageBucketDeleteError(
+                        f'Failed to delete GCS bucket {bucket_name}.')
+
+
+class R2Store(AbstractStore):
+    """R2Store inherits from S3Store Object and represents the backend
+    for R2 buckets.
+    """
+
+    _ACCESS_DENIED_MESSAGE = 'Access Denied'
+
+    def __init__(self,
+                 name: str,
+                 source: str,
+                 region: Optional[str] = 'auto',
+                 is_sky_managed: Optional[bool] = None,
+                 sync_on_reconstruction: Optional[bool] = True):
+        self.client: 'boto3.client.Client'
+        self.bucket: 'StorageHandle'
+        super().__init__(name, source, region, is_sky_managed,
+                         sync_on_reconstruction)
+
+    def _validate(self):
+        if self.source is not None and isinstance(self.source, str):
+            if self.source.startswith('s3://'):
+                assert self.name == data_utils.split_s3_path(self.source)[0], (
+                    'S3 Bucket is specified as path, the name should be the'
+                    ' same as S3 bucket.')
+                assert data_utils.verify_s3_bucket(self.name), (
+                    f'Source specified as {self.source}, a S3 bucket. ',
+                    'S3 Bucket should exist.')
+            elif self.source.startswith('gs://'):
+                assert self.name == data_utils.split_gcs_path(self.source)[0], (
+                    'GCS Bucket is specified as path, the name should be '
+                    'the same as GCS bucket.')
+                assert data_utils.verify_gcs_bucket(self.name), (
+                    f'Source specified as {self.source}, a GCS bucket. ',
+                    'GCS Bucket should exist.')
+            elif self.source.startswith('r2://'):
+                assert self.name == data_utils.split_r2_path(self.source)[0], (
+                    'R2 Bucket is specified as path, the name should be '
+                    'the same as R2 bucket.')
+        # Validate name
+        self.name = S3Store.validate_name(self.name)
+        # Check if the storage is enabled
+        if not _is_storage_cloud_enabled(cloudflare.NAME):
             with ux_utils.print_exception_no_traceback():
-                raise exceptions.StorageBucketDeleteError(
-                    f'Failed to delete GCS bucket {bucket_name}.')
+                raise exceptions.ResourcesUnavailableError(
+                    'Storage \'store: r2\' specified, but ' \
+                    'Cloudflare R2 access is disabled. To fix, '\
+                    'enable Cloudflare R2 by running `sky check`. '\
+                    'More info: https://skypilot.readthedocs.io/en/latest/getting-started/installation.html.'  # pylint: disable=line-too-long
+                    )
+
+    def initialize(self):
+        """Initializes the R2 store object on the cloud.
+
+        Initialization involves fetching bucket if exists, or creating it if
+        it does not.
+
+        Raises:
+          StorageBucketCreateError: If bucket creation fails
+          StorageBucketGetError: If fetching existing bucket fails
+          StorageInitError: If general initialization fails.
+        """
+        self.client = data_utils.create_r2_client(self.region)
+        self.bucket, is_new_bucket = self._get_bucket()
+        if self.is_sky_managed is None:
+            # If is_sky_managed is not specified, then this is a new storage
+            # object (i.e., did not exist in global_user_state) and we should
+            # set the is_sky_managed property.
+            # If is_sky_managed is specified, then we take no action.
+            self.is_sky_managed = is_new_bucket
+
+    def upload(self):
+        """Uploads source to store bucket.
+
+        Upload must be called by the Storage handler - it is not called on
+        Store initialization.
+
+        Raises:
+            StorageUploadError: if upload fails.
+        """
+        try:
+            if isinstance(self.source, list):
+                self.batch_aws_rsync(self.source, create_dirs=True)
+            elif self.source is not None:
+                if self.source.startswith('s3://'):
+                    self._transfer_to_r2()
+                elif self.source.startswith('gs://'):
+                    self._transfer_to_r2()
+                elif self.source.startswith('r2://'):
+                    pass
+                else:
+                    self.batch_aws_rsync([self.source])
+        except exceptions.StorageUploadError:
+            raise
+        except Exception as e:
+            raise exceptions.StorageUploadError(
+                f'Upload failed for store {self.name}') from e
+
+    def delete(self) -> None:
+        deleted_by_skypilot = self._delete_r2_bucket(self.name)
+        if deleted_by_skypilot:
+            msg_str = f'Deleted R2 bucket {self.name}.'
+        else:
+            msg_str = f'R2 bucket {self.name} may have been deleted ' \
+                      f'externally. Removing from local state.'
+        logger.info(f'{colorama.Fore.GREEN}{msg_str}'
+                    f'{colorama.Style.RESET_ALL}')
+
+    def get_handle(self) -> StorageHandle:
+        return cloudflare.resource('s3').Bucket(self.name)
+
+    def batch_aws_rsync(self,
+                        source_path_list: List[Path],
+                        create_dirs: bool = False) -> None:
+        """Invokes aws s3 sync to batch upload a list of local paths to S3
+
+        AWS Sync by default uses 10 threads to upload files to the bucket.  To
+        increase parallelism, modify max_concurrent_requests in your aws config
+        file (Default path: ~/.aws/config).
+
+        Since aws s3 sync does not support batch operations, we construct
+        multiple commands to be run in parallel.
+
+        Args:
+            source_path_list: List of paths to local files or directories
+            create_dirs: If the local_path is a directory and this is set to
+                False, the contents of the directory are directly uploaded to
+                root of the bucket. If the local_path is a directory and this is
+                set to True, the directory is created in the bucket root and
+                contents are uploaded to it.
+        """
+
+        def get_file_sync_command(base_dir_path, file_names):
+            includes = ' '.join(
+                [f'--include "{file_name}"' for file_name in file_names])
+            endpoint_url = cloudflare.create_endpoint()
+            sync_command = ('AWS_SHARED_CREDENTIALS_FILE='
+                            f'{cloudflare.R2_CREDENTIALS_PATH} '
+                            'aws s3 sync --no-follow-symlinks --exclude="*" '
+                            f'{includes} {base_dir_path} '
+                            f's3://{self.name} '
+                            f'--endpoint {endpoint_url} '
+                            f'--profile={cloudflare.R2_PROFILE_NAME}')
+            return sync_command
+
+        def get_dir_sync_command(src_dir_path, dest_dir_name):
+            # we exclude .git directory from the sync
+            endpoint_url = cloudflare.create_endpoint()
+            sync_command = (
+                'AWS_SHARED_CREDENTIALS_FILE='
+                f'{cloudflare.R2_CREDENTIALS_PATH} '
+                'aws s3 sync --no-follow-symlinks --exclude ".git/*" '
+                f'{src_dir_path} '
+                f's3://{self.name}/{dest_dir_name} '
+                f'--endpoint {endpoint_url} '
+                f'--profile={cloudflare.R2_PROFILE_NAME}')
+            return sync_command
+
+        # Generate message for upload
+        if len(source_path_list) > 1:
+            source_message = f'{len(source_path_list)} paths'
+        else:
+            source_message = source_path_list[0]
+
+        with log_utils.safe_rich_status(
+                f'[bold cyan]Syncing '
+                f'[green]{source_message}[/] to [green]r2://{self.name}/[/]'):
+            data_utils.parallel_upload(
+                source_path_list,
+                get_file_sync_command,
+                get_dir_sync_command,
+                self.name,
+                self._ACCESS_DENIED_MESSAGE,
+                create_dirs=create_dirs,
+                max_concurrent_uploads=_MAX_CONCURRENT_UPLOADS)
+
+    def _transfer_to_r2(self) -> None:
+        assert isinstance(self.source, str), self.source
+        if self.source.startswith('gs://'):
+            data_transfer.gcs_to_r2(self.name, self.name)
+        elif self.source.startswith('s3://'):
+            data_transfer.s3_to_r2(self.name, self.name)
+
+    def _get_bucket(self) -> Tuple[StorageHandle, bool]:
+        """Obtains the R2 bucket.
+
+        If the bucket exists, this method will return the bucket.
+        If the bucket does not exist, there are three cases:
+          1) Raise an error if the bucket source starts with s3://
+          2) Return None if bucket has been externally deleted and
+             sync_on_reconstruction is False
+          3) Create and return a new bucket otherwise
+
+        Raises:
+            StorageBucketCreateError: If creating the bucket fails
+            StorageBucketGetError: If fetching a bucket fails
+        """
+        r2 = cloudflare.resource('s3')
+        bucket = r2.Bucket(self.name)
+        endpoint_url = cloudflare.create_endpoint()
+        try:
+            # Try Public bucket case.
+            # This line does not error out if the bucket is an external public
+            # bucket or if it is a user's bucket that is publicly
+            # accessible.
+            self.client.head_bucket(Bucket=self.name)
+            return bucket, False
+        except aws.botocore_exceptions().ClientError as e:
+            error_code = e.response['Error']['Code']
+            # AccessDenied error for buckets that are private and not owned by
+            # user.
+            if error_code == '403':
+                command = ('AWS_SHARED_CREDENTIALS_FILE='
+                           f'{cloudflare.R2_CREDENTIALS_PATH} '
+                           f'aws s3 ls s3://{self.name} '
+                           f'--endpoint {endpoint_url} '
+                           f'--profile={cloudflare.R2_PROFILE_NAME}')
+                with ux_utils.print_exception_no_traceback():
+                    raise exceptions.StorageBucketGetError(
+                        _BUCKET_FAIL_TO_CONNECT_MESSAGE.format(name=self.name) +
+                        f' To debug, consider running `{command}`.') from e
+
+        if isinstance(self.source, str) and self.source.startswith('r2://'):
+            with ux_utils.print_exception_no_traceback():
+                raise exceptions.StorageBucketGetError(
+                    'Attempted to connect to a non-existent bucket: '
+                    f'{self.source}. Consider using '
+                    '`AWS_SHARED_CREDENTIALS_FILE='
+                    f'{cloudflare.R2_CREDENTIALS_PATH} aws s3 ls '
+                    f's3://{self.name} '
+                    f'--endpoint {endpoint_url} '
+                    f'--profile={cloudflare.R2_PROFILE_NAME}\' '
+                    'to debug.')
+
+        # If bucket cannot be found in both private and public settings,
+        # the bucket is to be created by Sky. However, skip creation if
+        # Store object is being reconstructed for deletion.
+        if self.sync_on_reconstruction:
+            bucket = self._create_r2_bucket(self.name)
+            return bucket, True
+        else:
+            return None, False
+
+    def _download_file(self, remote_path: str, local_path: str) -> None:
+        """Downloads file from remote to local on r2 bucket
+        using the boto3 API
+
+        Args:
+          remote_path: str; Remote path on R2 bucket
+          local_path: str; Local path on user's device
+        """
+        self.bucket.download_file(remote_path, local_path)
+
+    def mount_command(self, mount_path: str) -> str:
+        """Returns the command to mount the bucket to the mount_path.
+
+        Uses goofys to mount the bucket.
+
+        Args:
+          mount_path: str; Path to mount the bucket to.
+        """
+        install_cmd = ('sudo wget -nc https://github.com/romilbhardwaj/goofys/'
+                       'releases/download/0.24.0-romilb-upstream/goofys '
+                       '-O /usr/local/bin/goofys && '
+                       'sudo chmod +x /usr/local/bin/goofys')
+        endpoint_url = cloudflare.create_endpoint()
+        mount_cmd = (
+            f'AWS_SHARED_CREDENTIALS_FILE={cloudflare.R2_CREDENTIALS_PATH} '
+            f'AWS_PROFILE={cloudflare.R2_PROFILE_NAME} goofys -o allow_other '
+            f'--stat-cache-ttl {self._STAT_CACHE_TTL} '
+            f'--type-cache-ttl {self._TYPE_CACHE_TTL} '
+            f'--endpoint {endpoint_url} '
+            f'{self.bucket.name} {mount_path}')
+        return mounting_utils.get_mounting_command(mount_path, install_cmd,
+                                                   mount_cmd)
+
+    def _create_r2_bucket(self,
+                          bucket_name: str,
+                          region='auto') -> StorageHandle:
+        """Creates R2 bucket with specific name in specific region
+
+        Args:
+          bucket_name: str; Name of bucket
+          region: str; Region name, r2 automatically sets region
+        Raises:
+          StorageBucketCreateError: If bucket creation fails.
+        """
+        r2_client = self.client
+        try:
+            if region is None:
+                r2_client.create_bucket(Bucket=bucket_name)
+            else:
+                location = {'LocationConstraint': region}
+                r2_client.create_bucket(Bucket=bucket_name,
+                                        CreateBucketConfiguration=location)
+                logger.info(f'Created R2 bucket {bucket_name} in {region}')
+        except aws.botocore_exceptions().ClientError as e:
+            with ux_utils.print_exception_no_traceback():
+                raise exceptions.StorageBucketCreateError(
+                    f'Attempted to create a bucket '
+                    f'{self.name} but failed.') from e
+        return cloudflare.resource('s3').Bucket(bucket_name)
+
+    def _delete_r2_bucket(self, bucket_name: str) -> bool:
+        """Deletes R2 bucket, including all objects in bucket
+
+        Args:
+          bucket_name: str; Name of bucket
+
+        Returns:
+         bool; True if bucket was deleted, False if it was deleted externally.
+        """
+        # Deleting objects is very slow programatically
+        # (i.e. bucket.objects.all().delete() is slow).
+        # In addition, standard delete operations (i.e. via `aws s3 rm`)
+        # are slow, since AWS puts deletion markers.
+        # https://stackoverflow.com/questions/49239351/why-is-it-so-much-slower-to-delete-objects-in-aws-s3-than-it-is-to-create-them
+        # The fastest way to delete is to run `aws s3 rb --force`,
+        # which removes the bucket by force.
+        endpoint_url = cloudflare.create_endpoint()
+        remove_command = (
+            f'AWS_SHARED_CREDENTIALS_FILE={cloudflare.R2_CREDENTIALS_PATH} '
+            f'aws s3 rb s3://{bucket_name} --force '
+            f'--endpoint {endpoint_url} '
+            f'--profile={cloudflare.R2_PROFILE_NAME}')
+        try:
+            with log_utils.safe_rich_status(
+                    f'[bold cyan]Deleting R2 bucket {bucket_name}[/]'):
+                subprocess.check_output(remove_command,
+                                        stderr=subprocess.STDOUT,
+                                        shell=True)
+        except subprocess.CalledProcessError as e:
+            if 'NoSuchBucket' in e.output.decode('utf-8'):
+                logger.debug(
+                    _BUCKET_EXTERNALLY_DELETED_DEBUG_MESSAGE.format(
+                        bucket_name=bucket_name))
+                return False
+            else:
+                logger.error(e.output)
+                with ux_utils.print_exception_no_traceback():
+                    raise exceptions.StorageBucketDeleteError(
+                        f'Failed to delete R2 bucket {bucket_name}.')
+
+        # Wait until bucket deletion propagates on AWS servers
+        while data_utils.verify_r2_bucket(bucket_name):
+            time.sleep(0.1)
+        return True
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/optimizer.py` & `skypilot-nightly-1.0.0.dev20230713/sky/optimizer.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 """Optimizer: assigns best resources to user tasks."""
 import collections
 import enum
 import typing
-from typing import Dict, List, Optional, Tuple
+from typing import Any, Dict, Iterable, List, Optional, Tuple
 
 import colorama
 import numpy as np
 import prettytable
 
 from sky import check
 from sky import clouds
@@ -17,29 +17,28 @@
 from sky import task as task_lib
 from sky.backends import backend_utils
 from sky.utils import env_options
 from sky.utils import ux_utils
 from sky.utils import log_utils
 
 if typing.TYPE_CHECKING:
-    from sky import dag as dag_lib
+    import networkx as nx
+    from sky import dag as dag_lib  # pylint: disable=ungrouped-imports
 
 logger = sky_logging.init_logger(__name__)
 
-Task = task_lib.Task
-
 _DUMMY_SOURCE_NAME = 'skypilot-dummy-source'
 _DUMMY_SINK_NAME = 'skypilot-dummy-sink'
 
 # task -> resources -> estimated cost or time.
-_TaskToCostMap = Dict[Task, Dict[resources_lib.Resources, float]]
+_TaskToCostMap = Dict[task_lib.Task, Dict[resources_lib.Resources, float]]
 # cloud -> list of resources that have the same accelerators.
 _PerCloudCandidates = Dict[clouds.Cloud, List[resources_lib.Resources]]
 # task -> per-cloud candidates
-_TaskToPerCloudCandidates = Dict[Task, _PerCloudCandidates]
+_TaskToPerCloudCandidates = Dict[task_lib.Task, _PerCloudCandidates]
 
 
 # Constants: minimize what target?
 class OptimizeTarget(enum.Enum):
     COST = 0
     TIME = 1
 
@@ -87,25 +86,38 @@
         else:
             egress_time = 0.0
         return egress_time
 
     @staticmethod
     def optimize(dag: 'dag_lib.Dag',
                  minimize=OptimizeTarget.COST,
-                 blocked_launchable_resources: Optional[List[
+                 blocked_resources: Optional[Iterable[
                      resources_lib.Resources]] = None,
                  quiet: bool = False):
+        """Find the best execution plan for the given DAG.
+
+        Args:
+            dag: the DAG to optimize.
+            minimize: whether to minimize cost or time.
+            blocked_resources: a list of resources that should not be used.
+            quiet: whether to suppress logging.
+
+        Raises:
+            exceptions.ResourcesUnavailableError: if no resources are available
+                for a task.
+            exceptions.NoCloudAccessError: if no public clouds are enabled.
+        """
         # This function is effectful: mutates every node in 'dag' by setting
         # node.best_resources if it is None.
         Optimizer._add_dummy_source_sink_nodes(dag)
         try:
             unused_best_plan = Optimizer._optimize_objective(
                 dag,
                 minimize_cost=minimize == OptimizeTarget.COST,
-                blocked_launchable_resources=blocked_launchable_resources,
+                blocked_resources=blocked_resources,
                 quiet=quiet)
         finally:
             # Make sure to remove the dummy source/sink nodes, even if the
             # optimization fails.
             Optimizer._remove_dummy_source_sink_nodes(dag)
         return dag
 
@@ -132,15 +144,15 @@
             if in_degree == 0:
                 zero_indegree_nodes.append(node)
         for node, out_degree in graph.out_degree():
             if out_degree == 0:
                 zero_outdegree_nodes.append(node)
 
         def make_dummy(name):
-            dummy = Task(name)
+            dummy = task_lib.Task(name)
             dummy.set_resources({DummyResources(DummyCloud(), None)})
             dummy.set_time_estimator(lambda _: 0)
             return dummy
 
         with dag:
             source = make_dummy(_DUMMY_SOURCE_NAME)
             for real_source_node in zero_indegree_nodes:
@@ -158,56 +170,57 @@
             return
         assert len(source) == len(sink) == 1, dag.tasks
         dag.remove(source[0])
         dag.remove(sink[0])
 
     @staticmethod
     def _get_egress_info(
-        parent: Task,
+        parent: task_lib.Task,
         parent_resources: resources_lib.Resources,
-        node: Task,
+        node: task_lib.Task,
         resources: resources_lib.Resources,
-    ) -> Tuple[clouds.Cloud, clouds.Cloud, float]:
+    ) -> Tuple[Optional[clouds.Cloud], Optional[clouds.Cloud], float]:
         if isinstance(parent_resources.cloud, DummyCloud):
             # Special case.  The current 'node' is a real
             # source node, and its input may be on a different
             # cloud from 'resources'.
             if node.get_inputs() is None:
-                # A Task may have no inputs specified.
+                # A task_lib.Task may have no inputs specified.
                 return None, None, 0
             src_cloud = node.get_inputs_cloud()
             nbytes = node.get_estimated_inputs_size_gigabytes()
         else:
             src_cloud = parent_resources.cloud
             nbytes = parent.get_estimated_outputs_size_gigabytes()
         dst_cloud = resources.cloud
         return src_cloud, dst_cloud, nbytes
 
     @staticmethod
-    def _egress_cost_or_time(minimize_cost: bool, parent: Task,
+    def _egress_cost_or_time(minimize_cost: bool, parent: task_lib.Task,
                              parent_resources: resources_lib.Resources,
-                             node: Task, resources: resources_lib.Resources):
+                             node: task_lib.Task,
+                             resources: resources_lib.Resources):
         """Computes the egress cost or time depending on 'minimize_cost'."""
         src_cloud, dst_cloud, nbytes = Optimizer._get_egress_info(
             parent, parent_resources, node, resources)
         if nbytes == 0:
             return 0
+        assert src_cloud is not None and dst_cloud is not None
 
         if minimize_cost:
             fn = Optimizer._egress_cost
         else:
             fn = Optimizer._egress_time
         return fn(src_cloud, dst_cloud, nbytes)
 
     @staticmethod
     def _estimate_nodes_cost_or_time(
-        topo_order: List[Task],
+        topo_order: List[task_lib.Task],
         minimize_cost: bool = True,
-        blocked_launchable_resources: Optional[List[
-            resources_lib.Resources]] = None,
+        blocked_resources: Optional[Iterable[resources_lib.Resources]] = None,
     ) -> Tuple[_TaskToCostMap, _TaskToPerCloudCandidates]:
         """Estimates the cost/time of each task-resource mapping in the DAG.
 
         Note that the egress cost/time is not considered in this function.
         The estimated run time of a task running on a resource is given by
         `task.estimate_runtime(resources)` or 1 hour by default.
         The estimated cost is `task.num_nodes * resources.get_cost(runtime)`.
@@ -232,30 +245,41 @@
                 logger.debug('#### {} ####'.format(node))
 
             if node_i < len(topo_order) - 1:
                 # Convert partial resource labels to launchable resources.
                 launchable_resources, cloud_candidates = \
                     _fill_in_launchable_resources(
                         node,
-                        blocked_launchable_resources
+                        blocked_resources
                     )
                 node_to_candidate_map[node] = cloud_candidates
             else:
                 # Dummy sink node.
-                launchable_resources = node.get_resources()
-                launchable_resources = {
-                    list(node.get_resources())[0]: launchable_resources
-                }
+                node_resources = node.get_resources()
+                launchable_resources = {list(node_resources)[0]: node_resources}
 
             num_resources = len(node.get_resources())
             for orig_resources, launchable_list in launchable_resources.items():
                 if not launchable_list:
+                    location_hint = ''
+                    if node.get_resources():
+                        specified_resources = list(node.get_resources())[0]
+                        if specified_resources.zone is not None:
+                            location_hint = (
+                                f' Zone: {specified_resources.zone}.')
+                        elif specified_resources.region:
+                            location_hint = (
+                                f' Region: {specified_resources.region}.')
+
                     error_msg = (
-                        f'No launchable resource found for task {node}. '
-                        'To fix: relax its resource requirements.\n'
+                        'No launchable resource found for task '
+                        f'{node}.{location_hint}\nThis means the '
+                        'catalog does not contain any instance types that '
+                        'satisfy this request.\n'
+                        'To fix: relax or change the resource requirements.\n'
                         'Hint: \'sky show-gpus --all\' '
                         'to list available accelerators.\n'
                         '      \'sky check\' to check the enabled clouds.')
                     with ux_utils.print_exception_no_traceback():
                         raise exceptions.ResourcesUnavailableError(error_msg)
                 if num_resources == 1 and node.time_estimator_func is None:
                     logger.debug(
@@ -292,23 +316,27 @@
                                 '  estimated_cost (not incl. egress): ${:.1f}'.
                                 format(estimated_cost_or_time))
                     node_to_cost_map[node][resources] = estimated_cost_or_time
         return node_to_cost_map, node_to_candidate_map
 
     @staticmethod
     def _optimize_by_dp(
-        topo_order: List[Task],
+        topo_order: List[task_lib.Task],
         node_to_cost_map: _TaskToCostMap,
         minimize_cost: bool = True,
-    ) -> Tuple[Dict[Task, resources_lib.Resources], float]:
+    ) -> Tuple[Dict[task_lib.Task, resources_lib.Resources], float]:
         """Optimizes a chain DAG using a dynamic programming algorithm."""
         # node -> { resources -> best estimated cost }
-        dp_best_objective = collections.defaultdict(dict)
+        dp_best_objective: Dict[task_lib.Task,
+                                Dict[resources_lib.Resources,
+                                     float]] = collections.defaultdict(dict)
         # node -> { resources -> best parent resources }
-        dp_point_backs = collections.defaultdict(dict)
+        dp_point_backs: Dict[task_lib.Task, Dict[
+            resources_lib.Resources,
+            resources_lib.Resources]] = collections.defaultdict(dict)
 
         # Computes dp_best_objective[node][resources]
         # = my estimated cost + min_phw { dp_best_objective(p, phw) +
         #                                 egress_cost(p, phw, hw) }
         # where p is the parent of the node.
         for node_i, node in enumerate(topo_order):
             if node_i == 0:
@@ -349,19 +377,19 @@
             node.best_resources = best_resources
             if node.name != _DUMMY_SOURCE_NAME:
                 best_resources = dp_point_backs[node][best_resources]
         return best_plan, best_total_objective
 
     @staticmethod
     def _optimize_by_ilp(
-        graph,
-        topo_order: List[Task],
+        graph: 'nx.DiGraph',
+        topo_order: List[task_lib.Task],
         node_to_cost_map: _TaskToCostMap,
         minimize_cost: bool = True,
-    ) -> Tuple[Dict[Task, resources_lib.Resources], float]:
+    ) -> Tuple[Dict[task_lib.Task, resources_lib.Resources], float]:
         """Optimizes a general DAG using an ILP solver.
 
         Notations:
             V: the set of nodes (tasks).
             E: the set of edges (dependencies).
             k: node -> [r.cost for r in node.resources].
             F: (node i, node j) -> the egress cost/time between node i and j.
@@ -415,30 +443,32 @@
         # Prepare the constants.
         V = topo_order  # pylint: disable=invalid-name
         E = graph.edges()  # pylint: disable=invalid-name
         k = {
             node: list(resource_cost_map.values())
             for node, resource_cost_map in node_to_cost_map.items()
         }
-        F = collections.defaultdict(dict)  # pylint: disable=invalid-name
+        F: Dict[Any, Dict[Any, List[float]]] = collections.defaultdict(dict)  # pylint: disable=invalid-name
         for u, v in E:
             F[u][v] = []
             for r_u in node_to_cost_map[u].keys():
                 for r_v in node_to_cost_map[v].keys():
                     F[u][v].append(
                         Optimizer._egress_cost_or_time(minimize_cost, u, r_u, v,
                                                        r_v))
 
         # Define the decision variables.
         c = {
             v: pulp.LpVariable.matrix(v.name, (range(len(k[v])),), cat='Binary')
             for v in V
         }
 
-        e = collections.defaultdict(dict)
+        e: Dict[Any,
+                Dict[Any,
+                     List[pulp.LpVariable]]] = collections.defaultdict(dict)
         for u, v in E:
             num_vars = len(c[u]) * len(c[v])
             e[u][v] = pulp.LpVariable.matrix(f'({u.name}->{v.name})',
                                              (range(num_vars),),
                                              cat='Binary')
 
         # Formulate the constraints.
@@ -498,19 +528,19 @@
             node.best_resources = best_resources
             best_plan[node] = best_resources
         return best_plan, best_total_objective
 
     @staticmethod
     def _compute_total_time(
         graph,
-        topo_order: List[Task],
-        plan: Dict[Task, resources_lib.Resources],
+        topo_order: List[task_lib.Task],
+        plan: Dict[task_lib.Task, resources_lib.Resources],
     ) -> float:
         """Estimates the total time of running the DAG by the plan."""
-        cache_finish_time = {}
+        cache_finish_time: Dict[task_lib.Task, float] = {}
 
         def finish_time(node):
             if node in cache_finish_time:
                 return cache_finish_time[node]
 
             resources = plan[node]
             if node.time_estimator_func is None:
@@ -532,16 +562,16 @@
 
         sink_node = topo_order[-1]
         return finish_time(sink_node)
 
     @staticmethod
     def _compute_total_cost(
         graph,
-        topo_order: List[Task],
-        plan: Dict[Task, resources_lib.Resources],
+        topo_order: List[task_lib.Task],
+        plan: Dict[task_lib.Task, resources_lib.Resources],
     ) -> float:
         """Estimates the total cost of running the DAG by the plan."""
         total_cost = 0
         for node in topo_order:
             resources = plan[node]
             if node.time_estimator_func is None:
                 execution_time = 1 * 3600
@@ -592,16 +622,16 @@
             table = _create_table(['SOURCE', 'TARGET', 'SIZE (GB)', metric])
             table.add_rows(reversed(message_data))
             logger.info(f'Egress plan:\n{table}\n')
 
     @staticmethod
     def print_optimized_plan(
         graph,
-        topo_order: List[Task],
-        best_plan: Dict[Task, resources_lib.Resources],
+        topo_order: List[task_lib.Task],
+        best_plan: Dict[task_lib.Task, resources_lib.Resources],
         total_time: float,
         total_cost: float,
         node_to_cost_map: _TaskToCostMap,
         minimize_cost: bool,
     ):
         logger.info('== Optimizer ==')
         ordered_node_to_cost_map = collections.OrderedDict()
@@ -641,30 +671,46 @@
             if accelerators is None:
                 accelerators = '-'
             elif isinstance(accelerators, dict) and len(accelerators) == 1:
                 accelerators, count = list(accelerators.items())[0]
                 accelerators = f'{accelerators}:{count}'
             spot = '[Spot]' if resources.use_spot else ''
             cloud = resources.cloud
-            vcpus = cloud.get_vcpus_from_instance_type(resources.instance_type)
-            if vcpus is None:
-                vcpus = '-'
-            elif vcpus.is_integer():
-                vcpus = str(int(vcpus))
+            vcpus, mem = cloud.get_vcpus_mem_from_instance_type(
+                resources.instance_type)
+
+            def format_number(x):
+                if x is None:
+                    return '-'
+                elif x.is_integer():
+                    return str(int(x))
+                else:
+                    return f'{x:.1f}'
+
+            vcpus = format_number(vcpus)
+            mem = format_number(mem)
+
+            if resources.zone is None:
+                region_or_zone = resources.region
             else:
-                vcpus = f'{vcpus:.1f}'
+                region_or_zone = resources.zone
             return [
                 str(cloud),
                 resources.instance_type + spot,
                 vcpus,
+                mem,
                 str(accelerators),
+                str(region_or_zone),
             ]
 
         # Print the list of resouces that the optimizer considered.
-        resource_fields = ['CLOUD', 'INSTANCE', 'vCPUs', 'ACCELERATORS']
+        resource_fields = [
+            'CLOUD', 'INSTANCE', 'vCPUs', 'Mem(GB)', 'ACCELERATORS',
+            'REGION/ZONE'
+        ]
         # Do not print Source or Sink.
         best_plan_rows = [[t, t.num_nodes] + _get_resources_element_list(r)
                           for t, r in ordered_best_plan.items()]
         if len(best_plan_rows) > 1:
             logger.info(
                 f'{colorama.Style.BRIGHT}Best plan: {colorama.Style.RESET_ALL}')
             best_plan_table = _create_table(['TASK', '#NODES'] +
@@ -676,35 +722,56 @@
         Optimizer._print_egress_plan(graph, best_plan, minimize_cost)
 
         metric = 'COST ($)' if minimize_cost else 'TIME (hr)'
         field_names = resource_fields + [metric, 'CHOSEN']
 
         num_tasks = len(ordered_node_to_cost_map)
         for task, v in ordered_node_to_cost_map.items():
-            task_str = f'for Task {repr(task)!r}' if num_tasks > 1 else ''
+            task_str = (f'for task_lib.Task {repr(task)!r}'
+                        if num_tasks > 1 else '')
             plural = 's' if task.num_nodes > 1 else ''
             logger.info(
                 f'{colorama.Style.BRIGHT}Considered resources {task_str}'
                 f'({task.num_nodes} node{plural}):'
                 f'{colorama.Style.RESET_ALL}')
-            rows = []
+
+            # Only print 1 row per cloud.
+            best_per_cloud: Dict[str, Tuple[resources_lib.Resources,
+                                            float]] = {}
             for resources, cost in v.items():
+                cloud = str(resources.cloud)
+                if cloud in best_per_cloud:
+                    if cost < best_per_cloud[cloud][1]:
+                        best_per_cloud[cloud] = (resources, cost)
+                else:
+                    best_per_cloud[cloud] = (resources, cost)
+
+            # If the DAG has multiple tasks, the chosen resources may not be
+            # the best resources for the task.
+            chosen_resources = best_plan[task]
+            best_per_cloud[str(chosen_resources.cloud)] = (chosen_resources,
+                                                           v[chosen_resources])
+
+            rows = []
+            for resources, cost in best_per_cloud.values():
                 if minimize_cost:
-                    cost = f'{cost:.2f}'
+                    cost_str = f'{cost:.2f}'
                 else:
-                    cost = f'{cost / 3600:.2f}'
+                    cost_str = f'{cost / 3600:.2f}'
 
-                row = [*_get_resources_element_list(resources), cost, '']
+                row = [*_get_resources_element_list(resources), cost_str, '']
                 if resources == best_plan[task]:
                     # Use tick sign for the chosen resources.
                     row[-1] = (colorama.Fore.GREEN + '   ' + u'\u2714' +
                                colorama.Style.RESET_ALL)
                 rows.append(row)
 
-            rows = sorted(rows, key=lambda x: x[-2])
+            # NOTE: we've converted the cost to a string above, so we should
+            # convert it back to float for sorting.
+            rows = sorted(rows, key=lambda x: float(x[-2]))
             # Highlight the chosen resources.
             for row in rows:
                 if row[-1] != '':
                     for i, cell in enumerate(row):
                         row[i] = (f'{colorama.Style.BRIGHT}{cell}'
                                   f'{colorama.Style.RESET_ALL}')
                     break
@@ -735,18 +802,17 @@
                 logger.info(
                     f'To list more details, run \'sky show-gpus {acc_name}\'.')
 
     @staticmethod
     def _optimize_objective(
         dag: 'dag_lib.Dag',
         minimize_cost: bool = True,
-        blocked_launchable_resources: Optional[List[
-            resources_lib.Resources]] = None,
+        blocked_resources: Optional[Iterable[resources_lib.Resources]] = None,
         quiet: bool = False,
-    ) -> Dict[Task, resources_lib.Resources]:
+    ) -> Dict[task_lib.Task, resources_lib.Resources]:
         """Finds the optimal task-resource mapping for the entire DAG.
 
         The optimal mapping should consider the egress cost/time so that
         the total estimated cost/time of the DAG becomes the minimum.
         """
         import networkx as nx  # pylint: disable=import-outside-toplevel
         # TODO: The output of this function is useful. Should generate a
@@ -755,15 +821,15 @@
         graph = dag.get_graph()
         topo_order = list(nx.topological_sort(graph))
 
         node_to_cost_map, node_to_candidate_map = \
             Optimizer._estimate_nodes_cost_or_time(
                 topo_order,
                 minimize_cost,
-                blocked_launchable_resources)
+                blocked_resources)
 
         if dag.is_chain():
             best_plan, best_total_objective = Optimizer._optimize_by_dp(
                 topo_order, node_to_cost_map, minimize_cost)
         else:
             best_plan, best_total_objective = Optimizer._optimize_by_ilp(
                 graph, topo_order, node_to_cost_map, minimize_cost)
@@ -794,97 +860,170 @@
 
     def get_cost(self, seconds):
         return 0
 
 
 class DummyCloud(clouds.Cloud):
     """A dummy Cloud that has zero egress cost from/to."""
-    _REPR = 'DummyCloud'
     pass
 
 
-def _cloud_in_list(cloud: clouds.Cloud, lst: List[clouds.Cloud]) -> bool:
+def _cloud_in_list(cloud: clouds.Cloud, lst: Iterable[clouds.Cloud]) -> bool:
     return any(cloud.is_same_cloud(c) for c in lst)
 
 
+def _make_launchables_for_valid_region_zones(
+    launchable_resources: resources_lib.Resources
+) -> List[resources_lib.Resources]:
+    assert launchable_resources.is_launchable()
+    # In principle, all provisioning requests should be made at the granularity
+    # of a single zone. However, for on-demand instances, we batch the requests
+    # to the zones in the same region in order to leverage the region-level
+    # provisioning APIs of AWS and Azure. This way, we can reduce the number of
+    # API calls, and thus the overall failover time. Note that this optimization
+    # does not affect the user cost since the clouds charge the same prices for
+    # on-demand instances in the same region regardless of the zones. On the
+    # other hand, for spot instances, we do not batch the requests because the
+    # "AWS" spot prices may vary across zones.
+
+    # NOTE(woosuk): GCP does not support region-level provisioning APIs. Thus,
+    # while we return per-region resources here, the provisioner will still
+    # issue the request for one zone at a time.
+    # NOTE(woosuk): If we support Azure spot instances, we should batch the
+    # requests since Azure spot prices are region-level.
+    # TODO(woosuk): Batch the per-zone AWS spot instance requests if they are
+    # in the same region and have the same price.
+    # TODO(woosuk): A better design is to implement batching at a higher level
+    # (e.g., in provisioner or optimizer), not here.
+    launchables = []
+    regions = launchable_resources.get_valid_regions_for_launchable()
+    for region in regions:
+        if launchable_resources.use_spot and region.zones is not None:
+            # Spot instances.
+            # Do not batch the per-zone requests.
+            for zone in region.zones:
+                launchables.append(
+                    launchable_resources.copy(region=region.name,
+                                              zone=zone.name))
+        else:
+            # On-demand instances.
+            # Batch the requests at the granularity of a single region.
+            launchables.append(launchable_resources.copy(region=region.name))
+    return launchables
+
+
 def _filter_out_blocked_launchable_resources(
-        launchable_resources: List[resources_lib.Resources],
-        blocked_launchable_resources: List[resources_lib.Resources]):
+        launchable_resources: Iterable[resources_lib.Resources],
+        blocked_resources: Iterable[resources_lib.Resources]):
     """Whether the resources are blocked."""
     available_resources = []
     for resources in launchable_resources:
-        for blocked_resources in blocked_launchable_resources:
-            if resources.is_launchable_fuzzy_equal(blocked_resources):
+        for blocked in blocked_resources:
+            if resources.should_be_blocked_by(blocked):
                 break
-        else:  # non-blokced launchable resources. (no break)
+        else:  # non-blocked launchable resources. (no break)
             available_resources.append(resources)
     return available_resources
 
 
 def _fill_in_launchable_resources(
-    task: Task,
-    blocked_launchable_resources: Optional[List[resources_lib.Resources]],
+    task: task_lib.Task,
+    blocked_resources: Optional[Iterable[resources_lib.Resources]],
     try_fix_with_sky_check: bool = True,
 ) -> Tuple[Dict[resources_lib.Resources, List[resources_lib.Resources]],
            _PerCloudCandidates]:
     backend_utils.check_public_cloud_enabled()
     enabled_clouds = global_user_state.get_enabled_clouds()
     launchable = collections.defaultdict(list)
-    cloud_candidates = collections.defaultdict(resources_lib.Resources)
-    if blocked_launchable_resources is None:
-        blocked_launchable_resources = []
+    cloud_candidates: Dict[clouds.Cloud,
+                           resources_lib.Resources] = collections.defaultdict(
+                               resources_lib.Resources)
+    if blocked_resources is None:
+        blocked_resources = []
     for resources in task.get_resources():
         if resources.cloud is not None and not _cloud_in_list(
                 resources.cloud, enabled_clouds):
             if try_fix_with_sky_check:
+                # Explicitly check again to update the enabled cloud list.
                 check.check(quiet=True)
-                return _fill_in_launchable_resources(
-                    task, blocked_launchable_resources, False)
+                return _fill_in_launchable_resources(task, blocked_resources,
+                                                     False)
             with ux_utils.print_exception_no_traceback():
                 raise exceptions.ResourcesUnavailableError(
-                    f'Task {task} requires {resources.cloud} which is not '
-                    f'enabled. To enable access, run {colorama.Style.BRIGHT}'
+                    f'task_lib.Task {task} requires {resources.cloud} which is '
+                    'not enabled. To enable access, run '
+                    f'{colorama.Style.BRIGHT}'
                     f'sky check {colorama.Style.RESET_ALL}, or change the '
                     'cloud requirement')
         elif resources.is_launchable():
             if isinstance(resources.cloud, clouds.GCP):
                 # Check if the host VM satisfies the max vCPU and memory limits.
                 clouds.GCP.check_accelerator_attachable_to_host(
                     resources.instance_type, resources.accelerators,
                     resources.zone)
-            launchable[resources] = [resources]
+            # If the user has specified a GCP zone and the zone does not support
+            # the host-accelerator combination, then an error will be raised by
+            # the above check_accelerator_attachable_to_host() call.
+            # If the user has not specified any zone, a launchable will be made
+            # for every zone even if some of the zones do not support the
+            # host-accelerator combination. Then the provisioner may try to
+            # launch the instance, and fail over to other zones. We find this
+            # behavior acceptable because this will happen only when the user
+            # requested GCP 4:P100 or 8:K80 with a very large host VM.
+            elif isinstance(resources.cloud, clouds.SCP):
+                # Check if the host VM satisfies the min/max disk size limits.
+                is_allowed, launchable[resources] = \
+                    clouds.SCP.is_disk_size_allowed(resources)
+                if not is_allowed:
+                    continue
+
+            launchable[resources] = _make_launchables_for_valid_region_zones(
+                resources)
         else:
-            clouds_list = [resources.cloud
-                          ] if resources.cloud is not None else enabled_clouds
+            clouds_list = ([resources.cloud]
+                           if resources.cloud is not None else enabled_clouds)
             # Hack: When >=2 cloud candidates, always remove local cloud from
             # possible candidates. This is so the optimizer will consider
             # public clouds, except local. Local will be included as part of
             # optimizer in a future PR.
             # TODO(mluo): Add on-prem to cloud spillover.
             if len(clouds_list) >= 2:
                 clouds_list = [
                     c for c in clouds_list if not isinstance(c, clouds.Local)
                 ]
             all_fuzzy_candidates = set()
             for cloud in clouds_list:
-                (feasible_resources, fuzzy_candidate_list
-                ) = cloud.get_feasible_launchable_resources(resources)
+                (feasible_resources, fuzzy_candidate_list) = (
+                    cloud.get_feasible_launchable_resources(resources))
                 if len(feasible_resources) > 0:
-                    # Assume feasible_resources is sorted by prices and
-                    # only append the cheapest option for each cloud
-                    launchable[resources].append(feasible_resources[0])
+                    # Assume feasible_resources is sorted by prices.
+                    cheapest = feasible_resources[0]
+                    # Generate region/zone-specified resources.
+                    launchable[resources].extend(
+                        _make_launchables_for_valid_region_zones(cheapest))
                     cloud_candidates[cloud] = feasible_resources
                 else:
                     all_fuzzy_candidates.update(fuzzy_candidate_list)
             if len(launchable[resources]) == 0:
-                logger.info(f'No resource satisfying {resources.accelerators} '
-                            f'on {clouds_list}.')
+                clouds_str = str(clouds_list) if len(clouds_list) > 1 else str(
+                    clouds_list[0])
+                logger.info(f'No resource satisfying {resources} '
+                            f'on {clouds_str}.')
                 if len(all_fuzzy_candidates) > 0:
                     logger.info('Did you mean: '
                                 f'{colorama.Fore.CYAN}'
                                 f'{sorted(all_fuzzy_candidates)}'
                                 f'{colorama.Style.RESET_ALL}')
+                else:
+                    if resources.cpus is not None:
+                        logger.info('Try specifying a different CPU count, '
+                                    'or add "+" to the end of the CPU count '
+                                    'to allow for larger instances.')
+                    if resources.memory is not None:
+                        logger.info('Try specifying a different memory size, '
+                                    'or add "+" to the end of the memory size '
+                                    'to allow for larger instances.')
 
         launchable[resources] = _filter_out_blocked_launchable_resources(
-            launchable[resources], blocked_launchable_resources)
+            launchable[resources], blocked_resources)
 
     return launchable, cloud_candidates
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/setup_files/setup.py` & `skypilot-nightly-1.0.0.dev20230713/sky/setup_files/setup.py`

 * *Files 20% similar despite different names*

```diff
@@ -14,14 +14,15 @@
 """
 
 import io
 import os
 import platform
 import re
 import warnings
+from typing import Dict, List
 
 import setuptools
 
 ROOT_DIR = os.path.dirname(__file__)
 
 system = platform.system()
 if system == 'Darwin':
@@ -60,63 +61,93 @@
         re.MULTILINE)
     readme = mode_re.sub(r'<img\1>', readme)
     return readme
 
 
 install_requires = [
     'wheel',
-    # NOTE: ray 2.0.1 requires click<=8.0.4,>=7.0; We disable the
-    # shell completion for click<8.0 for backward compatibility.
+    # NOTE: ray requires click>=7.0. Also, click 8.1.x makes our rendered CLI
+    # docs display weird blockquotes.
+    # TODO(zongheng): investigate how to make click 8.1.x display nicely and
+    # remove the upper bound.
     'click<=8.0.4,>=7.0',
     # NOTE: required by awscli. To avoid ray automatically installing
     # the latest version.
     'colorama<0.4.5',
     'cryptography',
-    'jinja2',
+    # Jinja has a bug in older versions because of the lack of pinning
+    # the version of the underlying markupsafe package. See:
+    # https://github.com/pallets/jinja/issues/1585
+    'jinja2>=3.0',
     'jsonschema',
     'networkx',
     'oauth2client',
     'pandas',
     'pendulum',
-    'PrettyTable',
-    # Lower local ray version is not fully supported, due to the
-    # autoscaler issues (also tracked in #537).
-    'ray[default]>=1.9.0,<=2.0.1',
+    # PrettyTable with version >=2.0.0 is required for the support of
+    # `add_rows` method.
+    'PrettyTable>=2.0.0',
+    # Lower version of ray will cause dependency conflict for
+    # click/grpcio/protobuf.
+    'ray[default]>=2.2.0,<=2.4.0',
     'rich',
     'tabulate',
-    'filelock',  # TODO(mraheja): Enforce >=3.6.0 when python version is >= 3.7
-    # This is used by ray. The latest 1.44.0 will generate an error
-    # `Fork support is only compatible with the epoll1 and poll
-    # polling strategies`
-    'grpcio>=1.32.0,<=1.43.0',
+    # Light weight requirement, can be replaced with "typing" once
+    # we deprecate Python 3.7 (this will take a while).
+    "typing_extensions; python_version < '3.8'",
+    'filelock>=3.6.0',
+    # Adopted from ray's setup.py: https://github.com/ray-project/ray/blob/ray-2.4.0/python/setup.py
+    # SkyPilot: != 1.48.0 is required to avoid the error where ray dashboard fails to start when
+    # ray start is called (#2054).
+    # Tracking issue: https://github.com/ray-project/ray/issues/30984
+    "grpcio >= 1.32.0, <= 1.49.1, != 1.48.0; python_version < '3.10' and sys_platform == 'darwin'",  # noqa:E501
+    "grpcio >= 1.42.0, <= 1.49.1, != 1.48.0; python_version >= '3.10' and sys_platform == 'darwin'",  # noqa:E501
+    # Original issue: https://github.com/ray-project/ray/issues/33833
+    "grpcio >= 1.32.0, <= 1.51.3, != 1.48.0; python_version < '3.10' and sys_platform != 'darwin'",  # noqa:E501
+    "grpcio >= 1.42.0, <= 1.51.3, != 1.48.0; python_version >= '3.10' and sys_platform != 'darwin'",  # noqa:E501
     'packaging',
-    # The latest 4.21.1 will break ray. Enforce < 4.0.0 until Ray releases the
-    # fix.
-    # https://github.com/ray-project/ray/pull/25211
-    'protobuf<4.0.0',
+    # Adopted from ray's setup.py:
+    # https://github.com/ray-project/ray/blob/86fab1764e618215d8131e8e5068f0d493c77023/python/setup.py#L326
+    'protobuf >= 3.15.3, != 3.19.5',
     'psutil',
     'pulp',
+    # Ray job has an issue with pydantic>2.0.0, due to API changes of pydantic. See
+    # https://github.com/ray-project/ray/issues/36990
+    'pydantic<2.0'
 ]
 
-# NOTE: Change the templates/spot-controller.yaml.j2 file if any of the following
-# packages dependencies are changed.
-extras_require = {
-    'aws': [
-        'awscli',
-        'boto3',
-        # 'Crypto' module used in authentication.py for AWS.
-        'pycryptodome==3.12.0',
-    ],
+# NOTE: Change the templates/spot-controller.yaml.j2 file if any of the
+# following packages dependencies are changed.
+aws_dependencies = [
+    # NOTE: this installs CLI V1. To use AWS SSO (e.g., `aws sso login`), users
+    # should instead use CLI V2 which is not pip-installable. See
+    # https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html.
+    'awscli',
+    'boto3',
+    # 'Crypto' module used in authentication.py for AWS.
+    'pycryptodome==3.12.0',
+]
+extras_require: Dict[str, List[str]] = {
+    'aws': aws_dependencies,
     # TODO(zongheng): azure-cli is huge and takes a long time to install.
     # Tracked in: https://github.com/Azure/azure-cli/issues/7387
-    # azure-cli need to be pinned to 2.31.0 due to later versions
-    # do not have azure-identity (used in node_provider) installed
-    'azure': ['azure-cli==2.31.0', 'azure-core'],
+    # azure-identity is needed in node_provider.
+    # We need azure-identity>=1.13.0 to enable the customization of the
+    # timeout of AzureCliCredential.
+    'azure': [
+        'azure-cli>=2.31.0', 'azure-core', 'azure-identity>=1.13.0',
+        'azure-mgmt-network'
+    ],
     'gcp': ['google-api-python-client', 'google-cloud-storage'],
+    'ibm': ['ibm-cloud-sdk-core', 'ibm-vpc', 'ibm-platform-services'],
     'docker': ['docker'],
+    'lambda': [],
+    'cloudflare': aws_dependencies,
+    'scp': [],
+    'oci': ['oci'],
 }
 
 extras_require['all'] = sum(extras_require.values(), [])
 
 # Install aws requirements by default, as it is the most common cloud provider,
 # and the installation is quick.
 install_requires += extras_require['aws']
@@ -139,23 +170,22 @@
     author='SkyPilot Team',
     license='Apache 2.0',
     readme='README.md',
     description='SkyPilot: An intercloud broker for the clouds',
     long_description=long_description,
     long_description_content_type='text/markdown',
     setup_requires=['wheel'],
-    requires_python='>=3.6',
+    requires_python='>=3.7',
     install_requires=install_requires,
     extras_require=extras_require,
     entry_points={
         'console_scripts': ['sky = sky.cli:cli'],
     },
     include_package_data=True,
     classifiers=[
-        'Programming Language :: Python :: 3.6',
         'Programming Language :: Python :: 3.7',
         'Programming Language :: Python :: 3.8',
         'Programming Language :: Python :: 3.9',
         'Programming Language :: Python :: 3.10',
         'License :: OSI Approved :: Apache Software License',
         'Operating System :: OS Independent',
         'Topic :: Software Development :: Libraries :: Python Modules',
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/sky_logging.py` & `skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/ibm/utils.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,41 +1,38 @@
-"""Logging utilities."""
-import logging
-import sys
-
-from sky.utils import env_options
-
-# If the SKYPILOT_MINIMIZE_LOGGING environment variable is set to True,
-# remove logging prefixes and unnecessary information in optimizer
-FORMAT = (None if env_options.Options.MINIMIZE_LOGGING.get() else
-          '%(levelname).1s %(asctime)s %(filename)s:%(lineno)d] %(message)s')
-DATE_FORMAT = '%m-%d %H:%M:%S'
-
+"""holds common utility function/constants to be used by the providers."""
 
-class NewLineFormatter(logging.Formatter):
-    """Adds logging prefix to newlines to align multi-line messages."""
-
-    def __init__(self, fmt, datefmt=None):
-        logging.Formatter.__init__(self, fmt, datefmt)
-
-    def format(self, record):
-        msg = logging.Formatter.format(self, record)
-        if record.message != '':
-            parts = msg.split(record.message)
-            msg = msg.replace('\n', '\r\n' + parts[0])
-        return msg
+import logging
+import os
+import time
 
+RAY_RECYCLABLE = "ray-recyclable"
 
-def init_logger(name: str):
-    h = logging.StreamHandler(sys.stdout)
-    h.flush = sys.stdout.flush
 
-    fmt = NewLineFormatter(FORMAT, datefmt=DATE_FORMAT)
-    h.setFormatter(fmt)
+def get_logger(caller_name):
+    """
+    Configures the logger of this module for console output and file output
+    logs of level DEBUG and higher will be directed to file under LOGS_FOLDER.
+    logs of level INFO and higher will be directed to console output.
+    """
+    logger = logging.getLogger(caller_name)
+    LOGS_FOLDER = "/tmp/connector_logs/"  # this node_provider's logs location.
+    logger.setLevel(logging.DEBUG)
+
+    if not os.path.exists(LOGS_FOLDER):
+        os.mkdir(LOGS_FOLDER)
+    logs_path = LOGS_FOLDER + caller_name + time.strftime("%Y-%m-%d--%H-%M-%S")
+    # pylint: disable=line-too-long
+    file_formatter = logging.Formatter(
+        "%(asctime)s %(levelname)-8s %(message)s", datefmt="%Y-%m-%d %H:%M:%S"
+    )
+
+    file_handler = logging.FileHandler(logs_path)
+    file_handler.setFormatter(file_formatter)
+    file_handler.setLevel(logging.DEBUG)
+
+    console_output_handler = logging.StreamHandler()
+    console_output_handler.setFormatter(file_formatter)
+    console_output_handler.setLevel(logging.INFO)
 
-    logger = logging.getLogger(name)
-    logger.addHandler(h)
-    if env_options.Options.SHOW_DEBUG_INFO.get():
-        logger.setLevel(logging.DEBUG)
-    else:
-        logger.setLevel(logging.INFO)
+    logger.addHandler(file_handler)
+    logger.addHandler(console_output_handler)
     return logger
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/skylet/LICENCE` & `skypilot-nightly-1.0.0.dev20230713/sky/skylet/LICENSE`

 * *Files 2% similar despite different names*

```diff
@@ -199,27 +199,27 @@
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.
 
 --------------------------------------------------------------------------------
 
 Code in providers/azure from
-https://github.com/ray-project/ray/tree/ray-2.0.1/python/ray/autoscaler/_private/_azure
-Git commit of the release 2.0.1: 03b6bc7b5a305877501110ec04710a9c57011479
+https://github.com/ray-project/ray/tree/ray-2.4.0/python/ray/autoscaler/_private/_azure
+Git commit of the release 2.4.0: a777a028b8dbd7bbae9a7393c98f6cd65f98a5f5
 
 Code in providers/gcp from
-https://github.com/ray-project/ray/tree/ray-2.0.1/python/ray/autoscaler/_private/gcp
-Git commit of the release 2.0.1: 03b6bc7b5a305877501110ec04710a9c57011479
+https://github.com/ray-project/ray/tree/ray-2.4.0/python/ray/autoscaler/_private/gcp
+Git commit of the release 2.4.0: 45ffe6eb99d96488fdec187bb47a4a78d9b5ee92
 
 Code in providers/aws from
-https://github.com/ray-project/ray/tree/ray-2.0.1/python/ray/autoscaler/_private/aws
-Git commit of the release 2.0.1: 03b6bc7b5a305877501110ec04710a9c57011479
+https://github.com/ray-project/ray/tree/ray-2.4.0/python/ray/autoscaler/_private/aws
+Git commit of the release 2.4.0: c27859fa49f6470b98743bdce8288c7242d89699
 
 
-Copyright 2016-2022 Ray developers
+Copyright 2016-2023 Ray developers
 
 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
 You may obtain a copy of the License at
 
     https://www.apache.org/licenses/LICENSE-2.0
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/skylet/job_lib.py` & `skypilot-nightly-1.0.0.dev20230713/sky/skylet/job_lib.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,20 +1,25 @@
 """Sky job lib, backed by a sqlite database.
 
 This is a remote utility module that provides job queue functionality.
 """
 import enum
+import json
 import os
 import pathlib
+import psutil
 import shlex
+import subprocess
 import time
 import typing
-from typing import Any, Dict, List, Optional
+from typing import Any, Dict, List, Optional, Tuple
 
+import colorama
 import filelock
+import getpass
 
 from sky import sky_logging
 from sky.skylet import constants
 from sky.utils import common_utils
 from sky.utils import db_utils
 from sky.utils import log_utils
 
@@ -56,42 +61,54 @@
         job_name TEXT,
         username TEXT,
         submitted_at FLOAT,
         status TEXT,
         run_timestamp TEXT CANDIDATE KEY,
         start_at FLOAT DEFAULT -1)""")
 
+    cursor.execute("""CREATE TABLE IF NOT EXISTS pending_jobs(
+        job_id INTEGER,
+        run_cmd TEXT,
+        submit INTEGER,
+        created_time INTEGER
+    )""")
+
     db_utils.add_column_to_table(cursor, conn, 'jobs', 'end_at', 'FLOAT')
     db_utils.add_column_to_table(cursor, conn, 'jobs', 'resources', 'TEXT')
 
     conn.commit()
 
 
 _DB = db_utils.SQLiteConn(_DB_PATH, create_table)
 _CURSOR = _DB.cursor
 _CONN = _DB.conn
 
 
 class JobStatus(enum.Enum):
     """Job status"""
+
     # 3 in-flux states: each can transition to any state below it.
     # The `job_id` has been generated, but the generated ray program has
     # not started yet. skylet can transit the state from INIT to FAILED
     # directly, if the ray program fails to start.
+    # In the 'jobs' table, the `submitted_at` column will be set to the current
+    # time, when the job is firstly created (in the INIT state).
     INIT = 'INIT'
+    # The job is waiting for the required resources. (`ray job status`
+    # shows RUNNING as the generated ray program has started, but blocked
+    # by the placement constraints.)
+    PENDING = 'PENDING'
     # Running the user's setup script (only in effect if --detach-setup is
     # set). Our update_job_status() can temporarily (for a short period) set
     # the status to SETTING_UP, if the generated ray program has not set
     # the status to PENDING or RUNNING yet.
     SETTING_UP = 'SETTING_UP'
-    # The job is waiting for the required resources. (`ray job status`
-    # shows RUNNING as the generated ray program has started, but blocked
-    # by the placement constraints.)
-    PENDING = 'PENDING'
     # The job is running.
+    # In the 'jobs' table, the `start_at` column will be set to the current
+    # time, when the job is firstly transitioned to RUNNING.
     RUNNING = 'RUNNING'
     # 3 terminal states below: once reached, they do not transition.
     # The job finished successfully.
     SUCCEEDED = 'SUCCEEDED'
     # The job fails due to the user code or a system restart.
     FAILED = 'FAILED'
     # The job setup failed (only in effect if --detach-setup is set). It
@@ -110,33 +127,120 @@
 
     def is_terminal(self):
         return self not in self.nonterminal_statuses()
 
     def __lt__(self, other):
         return list(JobStatus).index(self) < list(JobStatus).index(other)
 
+    def colored_str(self):
+        color = _JOB_STATUS_TO_COLOR[self]
+        return f'{color}{self.value}{colorama.Style.RESET_ALL}'
+
+
+# Only update status of the jobs after this many seconds of job submission,
+# to avoid race condition with `ray job` to make sure it job has been
+# correctly updated.
+# TODO(zhwu): This number should be tuned based on heuristics.
+_PENDING_SUBMIT_GRACE_PERIOD = 60
+
+_PRE_RESOURCE_STATUSES = [JobStatus.PENDING]
+
+
+class JobScheduler:
+    """Base class for job scheduler"""
+
+    def queue(self, job_id: int, cmd: str) -> None:
+        _CURSOR.execute('INSERT INTO pending_jobs VALUES (?,?,?,?)',
+                        (job_id, cmd, 0, int(time.time())))
+        _CONN.commit()
+        set_status(job_id, JobStatus.PENDING)
+        self.schedule_step()
+
+    def remove_job_no_lock(self, job_id: int) -> None:
+        _CURSOR.execute(f'DELETE FROM pending_jobs WHERE job_id={job_id!r}')
+        _CONN.commit()
+
+    def _run_job(self, job_id: int, run_cmd: str):
+        _CURSOR.execute((f'UPDATE pending_jobs SET submit={int(time.time())} '
+                         f'WHERE job_id={job_id!r}'))
+        _CONN.commit()
+        subprocess.Popen(run_cmd, shell=True, stdout=subprocess.DEVNULL)
+
+    def schedule_step(self) -> None:
+        job_owner = getpass.getuser()
+        jobs = self._get_jobs()
+        if len(jobs) > 0:
+            update_status(job_owner)
+        # TODO(zhwu, mraheja): One optimization can be allowing more than one
+        # job staying in the pending state after ray job submit, so that to be
+        # faster to schedule a large amount of jobs.
+        for job_id, run_cmd, submit, created_time in jobs:
+            with filelock.FileLock(_get_lock_path(job_id)):
+                status = get_status_no_lock(job_id)
+                if (status not in _PRE_RESOURCE_STATUSES or
+                        created_time < psutil.boot_time()):
+                    # Job doesn't exist, is running/cancelled, or created
+                    # before the last reboot.
+                    self.remove_job_no_lock(job_id)
+                    continue
+                if submit:
+                    # Next job waiting for resources
+                    return
+                self._run_job(job_id, run_cmd)
+                return
+
+    def _get_jobs(self) -> List[Tuple[int, str, int, int]]:
+        """Returns the metadata for jobs in the pending jobs table
+
+        The information contains job_id, run command, submit time,
+        creation time.
+        """
+        raise NotImplementedError
+
+
+class FIFOScheduler(JobScheduler):
+    """First in first out job scheduler"""
+
+    def _get_jobs(self) -> List[Tuple[int, str, int, int]]:
+        return list(
+            _CURSOR.execute('SELECT * FROM pending_jobs ORDER BY job_id'))
+
+
+scheduler = FIFOScheduler()
+
+_JOB_STATUS_TO_COLOR = {
+    JobStatus.INIT: colorama.Fore.BLUE,
+    JobStatus.SETTING_UP: colorama.Fore.BLUE,
+    JobStatus.PENDING: colorama.Fore.BLUE,
+    JobStatus.RUNNING: colorama.Fore.GREEN,
+    JobStatus.SUCCEEDED: colorama.Fore.GREEN,
+    JobStatus.FAILED: colorama.Fore.RED,
+    JobStatus.FAILED_SETUP: colorama.Fore.RED,
+    JobStatus.CANCELLED: colorama.Fore.YELLOW,
+}
 
 _RAY_TO_JOB_STATUS_MAP = {
-    # These are intentionally set to one status before, because:
+    # These are intentionally set this way, because:
     # 1. when the ray status indicates the job is PENDING the generated
-    # python program should not be started yet, i.e. the job should be INIT.
+    # python program has been `ray job submit` from the job queue
+    # and is now PENDING
     # 2. when the ray status indicates the job is RUNNING the job can be in
     # setup or resources may not be allocated yet, i.e. the job should be
-    # SETTING_UP.
-    # For case 2, update_job_status() would compare this mapped SETTING_UP to
+    # PENDING.
+    # For case 2, update_job_status() would compare this mapped PENDING to
     # the status in our jobs DB and take the max. This is because the job's
     # generated ray program is the only place that can determine a job has
     # reserved resources and actually started running: it will set the
-    # status in the DB to RUNNING.
+    # status in the DB to SETTING_UP or RUNNING.
     # If there is no setup specified in the task, as soon as it is started
     # (ray's status becomes RUNNING), i.e. it will be very rare that the job
     # will be set to SETTING_UP by the update_job_status, as our generated
     # ray program will set the status to PENDING immediately.
-    'PENDING': JobStatus.INIT,
-    'RUNNING': JobStatus.SETTING_UP,
+    'PENDING': JobStatus.PENDING,
+    'RUNNING': JobStatus.PENDING,
     'SUCCEEDED': JobStatus.SUCCEEDED,
     'FAILED': JobStatus.FAILED,
     'STOPPED': JobStatus.CANCELLED,
 }
 
 
 def _create_ray_job_submission_client():
@@ -148,15 +252,17 @@
         raise
     try:
         from ray import job_submission  # pylint: disable=import-outside-toplevel
     except ImportError:
         logger.error(
             f'Failed to import job_submission with ray=={ray.__version__}')
         raise
-    return job_submission.JobSubmissionClient(address='http://127.0.0.1:8265')
+    port = get_job_submission_port()
+    return job_submission.JobSubmissionClient(
+        address=f'http://127.0.0.1:{port}')
 
 
 def make_ray_job_id(sky_job_id: int, job_owner: str) -> str:
     return f'{sky_job_id}-{job_owner}'
 
 
 def make_job_command_with_user_switching(username: str,
@@ -218,28 +324,29 @@
     with filelock.FileLock(_get_lock_path(job_id)):
         _CURSOR.execute(
             'UPDATE jobs SET status=(?), start_at=(?), end_at=NULL '
             'WHERE job_id=(?)', (JobStatus.RUNNING.value, time.time(), job_id))
         _CONN.commit()
 
 
-def get_status_no_lock(job_id: int) -> JobStatus:
+def get_status_no_lock(job_id: int) -> Optional[JobStatus]:
     """Get the status of the job with the given id.
 
     This function can return a stale status if there is a concurrent update.
     Make sure the caller will not be affected by the stale status, e.g. getting
     the status in a while loop as in `log_lib._follow_job_logs`. Otherwise, use
     `get_status`.
     """
     rows = _CURSOR.execute('SELECT status FROM jobs WHERE job_id=(?)',
                            (job_id,))
     for (status,) in rows:
         if status is None:
             return None
         return JobStatus(status)
+    return None
 
 
 def get_status(job_id: int) -> Optional[JobStatus]:
     # TODO(mraheja): remove pylint disabling when filelock version updated.
     # pylint: disable=abstract-class-instantiated
     with filelock.FileLock(_get_lock_path(job_id)):
         return get_status_no_lock(job_id)
@@ -254,35 +361,77 @@
         job_ids)
     statuses = {job_id: None for job_id in job_ids}
     for (job_id, status) in rows:
         statuses[job_id] = status
     return common_utils.encode_payload(statuses)
 
 
-def load_statuses_payload(statuses_payload: str) -> Dict[int, JobStatus]:
+def load_statuses_payload(
+        statuses_payload: str) -> Dict[Optional[int], Optional[JobStatus]]:
     statuses = common_utils.decode_payload(statuses_payload)
     for job_id, status in statuses.items():
         if status is not None:
             statuses[job_id] = JobStatus(status)
     return statuses
 
 
 def get_latest_job_id() -> Optional[int]:
     rows = _CURSOR.execute(
         'SELECT job_id FROM jobs ORDER BY job_id DESC LIMIT 1')
     for (job_id,) in rows:
         return job_id
+    return None
 
 
-def get_job_time_payload(job_id: int, is_end: bool) -> Optional[int]:
-    field = 'end_at' if is_end else 'start_at'
+def get_job_submitted_or_ended_timestamp_payload(job_id: int,
+                                                 get_ended_time: bool) -> str:
+    """Get the job submitted/ended timestamp.
+
+    This function should only be called by the spot controller,
+    which is ok to use `submitted_at` instead of `start_at`,
+    because the spot job duration need to include both setup
+    and running time and the job will not stay in PENDING
+    state.
+
+    The normal job duration will use `start_at` instead of
+    `submitted_at` (in `format_job_queue()`), because the job
+    may stay in PENDING if the cluster is busy.
+    """
+    field = 'end_at' if get_ended_time else 'submitted_at'
     rows = _CURSOR.execute(f'SELECT {field} FROM jobs WHERE job_id=(?)',
                            (job_id,))
     for (timestamp,) in rows:
         return common_utils.encode_payload(timestamp)
+    return common_utils.encode_payload(None)
+
+
+def get_ray_port():
+    """Get the port Skypilot-internal Ray cluster uses.
+
+    If the port file does not exist, the cluster was launched before #1790,
+    return the default port.
+    """
+    port_path = os.path.expanduser(constants.SKY_REMOTE_RAY_PORT_FILE)
+    if not os.path.exists(port_path):
+        return 6379
+    port = json.load(open(port_path))['ray_port']
+    return port
+
+
+def get_job_submission_port():
+    """Get the dashboard port Skypilot-internal Ray cluster uses.
+
+    If the port file does not exist, the cluster was launched before #1790,
+    return the default port.
+    """
+    port_path = os.path.expanduser(constants.SKY_REMOTE_RAY_PORT_FILE)
+    if not os.path.exists(port_path):
+        return 8265
+    port = json.load(open(port_path))['ray_dashboard_port']
+    return port
 
 
 def _get_records_from_rows(rows) -> List[Dict[str, Any]]:
     records = []
     for row in rows:
         if row[0] is None:
             break
@@ -297,37 +446,36 @@
             'start_at': row[JobInfoLoc.START_AT.value],
             'end_at': row[JobInfoLoc.END_AT.value],
             'resources': row[JobInfoLoc.RESOURCES.value],
         })
     return records
 
 
-def _get_jobs(username: Optional[str],
-              status_list: Optional[List[JobStatus]] = None,
-              submitted_gap_sec: int = 0) -> List[Dict[str, Any]]:
+def _get_jobs(
+        username: Optional[str],
+        status_list: Optional[List[JobStatus]] = None) -> List[Dict[str, Any]]:
     if status_list is None:
         status_list = list(JobStatus)
     status_str_list = [status.value for status in status_list]
     if username is None:
         rows = _CURSOR.execute(
             f"""\
             SELECT * FROM jobs
             WHERE status IN ({','.join(['?'] * len(status_list))})
-            AND submitted_at <= (?)
             ORDER BY job_id DESC""",
-            (*status_str_list, time.time() - submitted_gap_sec),
+            (*status_str_list,),
         )
     else:
         rows = _CURSOR.execute(
             f"""\
             SELECT * FROM jobs
             WHERE status IN ({','.join(['?'] * len(status_list))})
-            AND username=(?) AND submitted_at <= (?)
+            AND username=(?)
             ORDER BY job_id DESC""",
-            (*status_str_list, username, time.time() - submitted_gap_sec),
+            (*status_str_list, username),
         )
 
     records = _get_records_from_rows(rows)
     return records
 
 
 def _get_jobs_by_ids(job_ids: List[int]) -> List[Dict[str, Any]]:
@@ -338,79 +486,105 @@
         ORDER BY job_id DESC""",
         (*job_ids,),
     )
     records = _get_records_from_rows(rows)
     return records
 
 
+def _get_pending_jobs():
+    rows = _CURSOR.execute(
+        'SELECT job_id, created_time, submit FROM pending_jobs')
+    rows = list(rows)
+    return {
+        job_id: {
+            'created_time': created_time,
+            'submit': submit
+        } for job_id, created_time, submit in rows
+    }
+
+
 def update_job_status(job_owner: str,
                       job_ids: List[int],
                       silent: bool = False) -> List[JobStatus]:
-    """Updates and returns the job statuses matching our `JobStatus` semantics
-
-    "True" statuses: this function queries `ray job status` and processes
-    those results to match our semantics.
+    """Updates and returns the job statuses matching our `JobStatus` semantics.
 
-    This function queries `ray job status` and processes those results to
-    match our semantics.
+    This function queries `ray job status` and processes those results to match
+    our semantics.
 
-    Though we update job status actively in ray program and job cancelling,
-    we still need this to handle staleness problem, caused by instance
-    restarting and other corner cases (if any).
+    Though we update job status actively in the generated ray program and
+    during job cancelling, we still need this to handle the staleness problem,
+    caused by instance restarting and other corner cases (if any).
 
-    This function should only be run on the remote instance with ray==2.0.1.
+    This function should only be run on the remote instance with ray==2.4.0.
     """
     if len(job_ids) == 0:
         return []
 
     # TODO: if too slow, directly query against redis.
     ray_job_ids = [make_ray_job_id(job_id, job_owner) for job_id in job_ids]
 
     job_client = _create_ray_job_submission_client()
 
-    # In ray 2.0.1, job_client.list_jobs returns a list of JobDetails,
+    # In ray 2.4.0, job_client.list_jobs returns a list of JobDetails,
     # which contains the job status (str) and submission_id (str).
     job_detail_lists: List['ray_pydantic.JobDetails'] = job_client.list_jobs()
 
-    job_details = dict()
+    pending_jobs = _get_pending_jobs()
+    job_details = {}
     ray_job_ids_set = set(ray_job_ids)
     for job_detail in job_detail_lists:
         if job_detail.submission_id in ray_job_ids_set:
             job_details[job_detail.submission_id] = job_detail
-    job_statuses: List[JobStatus] = [None] * len(ray_job_ids)
+    job_statuses: List[Optional[JobStatus]] = [None] * len(ray_job_ids)
     for i, ray_job_id in enumerate(ray_job_ids):
+        job_id = job_ids[i]
         if ray_job_id in job_details:
             ray_status = job_details[ray_job_id].status
             job_statuses[i] = _RAY_TO_JOB_STATUS_MAP[ray_status]
+        if job_id in pending_jobs:
+            if pending_jobs[job_id]['created_time'] < psutil.boot_time():
+                # The job is stale as it is created before the instance
+                # is booted, e.g. the instance is rebooted.
+                job_statuses[i] = JobStatus.FAILED
+            # Gives a 60 second grace period between job being submit from
+            # the pending table until appearing in ray jobs.
+            if (pending_jobs[job_id]['submit'] > 0 and
+                    pending_jobs[job_id]['submit'] <
+                    time.time() - _PENDING_SUBMIT_GRACE_PERIOD):
+                # For jobs submitted outside of the grace period, we will
+                # consider the ray job status.
+                continue
+            else:
+                # Reset the job status to PENDING even though it may not appear
+                # in the ray jobs, so that it will not be considered as stale.
+                job_statuses[i] = JobStatus.PENDING
 
     assert len(job_statuses) == len(job_ids), (job_statuses, job_ids)
 
     statuses = []
     for job_id, status in zip(job_ids, job_statuses):
         # Per-job status lock is required because between the job status
         # query and the job status update, the job status in the databse
         # can be modified by the generated ray program.
-        # TODO(mraheja): remove pylint disabling when filelock version
-        # updated
-        # pylint: disable=abstract-class-instantiated
         with filelock.FileLock(_get_lock_path(job_id)):
+            original_status = get_status_no_lock(job_id)
+            assert original_status is not None, (job_id, status)
             if status is None:
-                original_status = get_status_no_lock(job_id)
                 status = original_status
-                if not original_status.is_terminal():
+                if (original_status is not None and
+                        not original_status.is_terminal()):
                     # The job may be stale, when the instance is restarted
                     # (the ray redis is volatile). We need to reset the
                     # status of the task to FAILED if its original status
                     # is RUNNING or PENDING.
                     status = JobStatus.FAILED
                     _set_status_no_lock(job_id, status)
                     if not silent:
                         logger.info(f'Updated job {job_id} status to {status}')
             else:
-                original_status = get_status_no_lock(job_id)
                 # Taking max of the status is necessary because:
                 # 1. It avoids race condition, where the original status has
                 # already been set to later state by the job. We skip the
                 # update.
                 # 2. _RAY_TO_JOB_STATUS_MAP would map `ray job status`'s
                 # `RUNNING` to our JobStatus.SETTING_UP; if a job has already
                 # been set to JobStatus.PENDING or JobStatus.RUNNING by the
@@ -434,24 +608,23 @@
         f"""\
         UPDATE jobs SET status=(?)
         WHERE status IN ({','.join(['?'] * len(in_progress_status))})
         """, (JobStatus.FAILED.value, *in_progress_status))
     _CONN.commit()
 
 
-def update_status(job_owner: str, submitted_gap_sec: int = 0) -> None:
+def update_status(job_owner: str) -> None:
     # This will be called periodically by the skylet to update the status
     # of the jobs in the database, to avoid stale job status.
     # NOTE: there might be a INIT job in the database set to FAILED by this
     # function, as the ray job status does not exist due to the app
     # not submitted yet. It will be then reset to PENDING / RUNNING when the
     # app starts.
     nonterminal_jobs = _get_jobs(username=None,
-                                 status_list=JobStatus.nonterminal_statuses(),
-                                 submitted_gap_sec=submitted_gap_sec)
+                                 status_list=JobStatus.nonterminal_statuses())
     nonterminal_job_ids = [job['job_id'] for job in nonterminal_jobs]
 
     update_job_status(job_owner, nonterminal_job_ids)
 
 
 def is_cluster_idle() -> bool:
     """Returns if the cluster is idle (no in-flight jobs)."""
@@ -461,14 +634,15 @@
     rows = _CURSOR.execute(
         f"""\
         SELECT COUNT(*) FROM jobs
         WHERE status IN ({','.join(['?'] * len(in_progress_status))})
         """, in_progress_status)
     for (count,) in rows:
         return count == 0
+    assert False, 'Should not reach here'
 
 
 def format_job_queue(jobs: List[Dict[str, Any]]):
     """Format the job queue for display.
 
     Usage:
         jobs = get_job_queue()
@@ -484,28 +658,30 @@
             job['job_name'],
             log_utils.readable_time_duration(job['submitted_at']),
             log_utils.readable_time_duration(job['start_at']),
             log_utils.readable_time_duration(job['start_at'],
                                              job['end_at'],
                                              absolute=True),
             job['resources'],
-            job['status'].value,
+            job['status'].colored_str(),
             job['log_path'],
         ])
     return job_table
 
 
 def dump_job_queue(username: Optional[str], all_jobs: bool) -> str:
     """Get the job queue in encoded json format.
 
     Args:
         username: The username to show jobs for. Show all the users if None.
         all_jobs: Whether to show all jobs, not just the pending/running ones.
     """
-    status_list = [JobStatus.SETTING_UP, JobStatus.PENDING, JobStatus.RUNNING]
+    status_list: Optional[List[JobStatus]] = [
+        JobStatus.SETTING_UP, JobStatus.PENDING, JobStatus.RUNNING
+    ]
     if all_jobs:
         status_list = None
 
     jobs = _get_jobs(username, status_list=status_list)
     for job in jobs:
         job['status'] = job['status'].value
         job['log_path'] = os.path.join(constants.SKY_LOGS_DIRECTORY,
@@ -531,38 +707,44 @@
     Args:
         jobs: The job ids to cancel. If None, cancel all the jobs.
     """
     # Update the status of the jobs to avoid setting the status of stale
     # jobs to CANCELLED.
     if jobs is None:
         job_records = _get_jobs(
-            None, [JobStatus.SETTING_UP, JobStatus.PENDING, JobStatus.RUNNING])
+            None, [JobStatus.PENDING, JobStatus.SETTING_UP, JobStatus.RUNNING])
     else:
         job_records = _get_jobs_by_ids(jobs)
 
     # TODO(zhwu): `job_client.stop_job` will wait for the jobs to be killed, but
     # when the memory is not enough, this will keep waiting.
     job_client = _create_ray_job_submission_client()
 
     # Sequentially cancel the jobs to avoid the resource number bug caused by
     # ray cluster (tracked in #1262).
     for job in job_records:
         job_id = make_ray_job_id(job['job_id'], job_owner)
-        try:
-            job_client.stop_job(job_id)
-        except RuntimeError as e:
-            # If the job does not exist or if the request to the
-            # job server fails.
-            logger.warning(str(e))
-            continue
-
-        if job['status'] in [
-                JobStatus.SETTING_UP, JobStatus.PENDING, JobStatus.RUNNING
-        ]:
-            set_status(job['job_id'], JobStatus.CANCELLED)
+        # Job is locked to ensure that pending queue does not start it while
+        # it is being cancelled
+        with filelock.FileLock(_get_lock_path(job['job_id'])):
+            try:
+                job_client.stop_job(job_id)
+            except RuntimeError as e:
+                # If the request to the job server fails, we should not
+                # set the job to CANCELLED.
+                if 'does not exist' not in str(e):
+                    logger.warning(str(e))
+                    continue
+
+            if job['status'] in [
+                    JobStatus.PENDING, JobStatus.SETTING_UP, JobStatus.RUNNING
+            ]:
+                _set_status_no_lock(job['job_id'], JobStatus.CANCELLED)
+
+        scheduler.schedule_step()
 
 
 def get_run_timestamp(job_id: Optional[int]) -> Optional[str]:
     """Returns the relative path to the log file for a job."""
     _CURSOR.execute(
         """\
             SELECT * FROM jobs
@@ -570,24 +752,23 @@
     row = _CURSOR.fetchone()
     if row is None:
         return None
     run_timestamp = row[JobInfoLoc.RUN_TIMESTAMP.value]
     return run_timestamp
 
 
-def run_timestamp_with_globbing_payload(
-        job_ids: List[Optional[str]]) -> Dict[str, str]:
+def run_timestamp_with_globbing_payload(job_ids: List[Optional[str]]) -> str:
     """Returns the relative paths to the log files for job with globbing."""
     query_str = ' OR '.join(['job_id GLOB (?)'] * len(job_ids))
     _CURSOR.execute(
         f"""\
             SELECT * FROM jobs
             WHERE {query_str}""", job_ids)
     rows = _CURSOR.fetchall()
-    run_timestamps = dict()
+    run_timestamps = {}
     for row in rows:
         job_id = row[JobInfoLoc.JOB_ID.value]
         run_timestamp = row[JobInfoLoc.RUN_TIMESTAMP.value]
         run_timestamps[str(job_id)] = run_timestamp
     return common_utils.encode_payload(run_timestamps)
 
 
@@ -598,29 +779,36 @@
 
       >> codegen = JobLibCodeGen.add_job(...)
     """
 
     _PREFIX = ['import os', 'from sky.skylet import job_lib, log_lib']
 
     @classmethod
-    def add_job(cls, job_name: str, username: str, run_timestamp: str,
+    def add_job(cls, job_name: Optional[str], username: str, run_timestamp: str,
                 resources_str: str) -> str:
         if job_name is None:
             job_name = '-'
         code = [
             'job_id = job_lib.add_job('
             f'{job_name!r}, '
             f'{username!r}, '
             f'{run_timestamp!r}, '
             f'{resources_str!r})',
             'print("Job ID: " + str(job_id), flush=True)',
         ]
         return cls._build(code)
 
     @classmethod
+    def queue_job(cls, job_id: int, cmd: str) -> str:
+        code = ['job_lib.scheduler.queue('
+                f'{job_id!r},'
+                f'{cmd!r})']
+        return cls._build(code)
+
+    @classmethod
     def update_status(cls, job_owner: str) -> str:
         code = [
             f'job_lib.update_status({job_owner!r})',
         ]
         return cls._build(code)
 
     @classmethod
@@ -644,22 +832,20 @@
 
     @classmethod
     def tail_logs(cls,
                   job_owner: str,
                   job_id: Optional[int],
                   spot_job_id: Optional[int],
                   follow: bool = True) -> str:
+        # pylint: disable=line-too-long
         code = [
-            f'job_id = {job_id} if {job_id} is not None '
-            'else job_lib.get_latest_job_id()',
+            f'job_id = {job_id} if {job_id} is not None else job_lib.get_latest_job_id()',
             'run_timestamp = job_lib.get_run_timestamp(job_id)',
-            (f'log_dir = os.path.join({constants.SKY_LOGS_DIRECTORY!r}, '
-             'run_timestamp)'),
-            (f'log_lib.tail_logs({job_owner!r},'
-             f'job_id, log_dir, {spot_job_id!r}, follow={follow})'),
+            f'log_dir = None if run_timestamp is None else os.path.join({constants.SKY_LOGS_DIRECTORY!r}, run_timestamp)',
+            f'log_lib.tail_logs({job_owner!r}, job_id, log_dir, {spot_job_id!r}, follow={follow})',
         ]
         return cls._build(code)
 
     @classmethod
     def get_job_status(cls, job_ids: Optional[List[int]] = None) -> str:
         # Prints "Job <id> <status>" for UX; caller should parse the last token.
         code = [
@@ -667,21 +853,24 @@
             'else [job_lib.get_latest_job_id()]',
             'job_statuses = job_lib.get_statuses_payload(job_ids)',
             'print(job_statuses, flush=True)',
         ]
         return cls._build(code)
 
     @classmethod
-    def get_job_time_payload(cls,
-                             job_id: Optional[int] = None,
-                             is_end: bool = False) -> str:
+    def get_job_submitted_or_ended_timestamp_payload(
+            cls,
+            job_id: Optional[int] = None,
+            get_ended_time: bool = False) -> str:
         code = [
             f'job_id = {job_id} if {job_id} is not None '
             'else job_lib.get_latest_job_id()',
-            f'job_time = job_lib.get_job_time_payload(job_id, {is_end})',
+            'job_time = '
+            'job_lib.get_job_submitted_or_ended_timestamp_payload('
+            f'job_id, {get_ended_time})',
             'print(job_time, flush=True)',
         ]
         return cls._build(code)
 
     @classmethod
     def get_run_timestamp_with_globbing(cls,
                                         job_ids: Optional[List[str]]) -> str:
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/skylet/log_lib.py` & `skypilot-nightly-1.0.0.dev20230713/sky/skylet/log_lib.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,124 +1,144 @@
 """Sky logging library.
 
 This is a remote utility module that provides logging functionality.
 """
+import copy
 import io
+import multiprocessing.pool
 import os
-import selectors
 import subprocess
 import sys
 import time
 import textwrap
 import tempfile
 from typing import Dict, Iterator, List, Optional, Tuple, Union
 
 import colorama
 
 from sky import sky_logging
 from sky.skylet import constants
 from sky.skylet import job_lib
 from sky.utils import log_utils
+from sky.utils import subprocess_utils
 
 _SKY_LOG_WAITING_GAP_SECONDS = 1
 _SKY_LOG_WAITING_MAX_RETRY = 5
 _SKY_LOG_TAILING_GAP_SECONDS = 0.2
 
 logger = sky_logging.init_logger(__name__)
 
 
-def process_subprocess_stream(
-    proc,
-    log_path: str,
-    stream_logs: bool,
-    start_streaming_at: str = '',
-    end_streaming_at: Optional[str] = None,
-    skip_lines: Optional[List[str]] = None,
-    replace_crlf: bool = False,
-    line_processor: Optional[log_utils.LineProcessor] = None,
-    streaming_prefix: Optional[str] = None,
-) -> Tuple[str, str]:
-    """Redirect the process's filtered stdout/stderr to both stream and file"""
-    if line_processor is None:
-        line_processor = log_utils.LineProcessor()
+class _ProcessingArgs:
+    """Arguments for processing logs."""
 
-    sel = selectors.DefaultSelector()
-    out_io = io.TextIOWrapper(proc.stdout,
+    def __init__(self,
+                 log_path: str,
+                 stream_logs: bool,
+                 start_streaming_at: str = '',
+                 end_streaming_at: Optional[str] = None,
+                 skip_lines: Optional[List[str]] = None,
+                 replace_crlf: bool = False,
+                 line_processor: Optional[log_utils.LineProcessor] = None,
+                 streaming_prefix: Optional[str] = None) -> None:
+        self.log_path = log_path
+        self.stream_logs = stream_logs
+        self.start_streaming_at = start_streaming_at
+        self.end_streaming_at = end_streaming_at
+        self.skip_lines = skip_lines
+        self.replace_crlf = replace_crlf
+        self.line_processor = line_processor
+        self.streaming_prefix = streaming_prefix
+
+
+def _handle_io_stream(io_stream, out_stream, args: _ProcessingArgs):
+    """Process the stream of a process."""
+    out_io = io.TextIOWrapper(io_stream,
                               encoding='utf-8',
                               newline='',
-                              errors='replace')
-    sel.register(out_io, selectors.EVENT_READ)
-    if proc.stderr is not None:
-        err_io = io.TextIOWrapper(proc.stderr,
-                                  encoding='utf-8',
-                                  newline='',
-                                  errors='replace')
-        sel.register(err_io, selectors.EVENT_READ)
-
-    stdout = ''
-    stderr = ''
-
-    if streaming_prefix is None:
-        streaming_prefix = ''
+                              errors='replace',
+                              write_through=True)
 
     start_streaming_flag = False
     end_streaming_flag = False
-    with line_processor:
-        with open(log_path, 'a') as fout:
-            while len(sel.get_map()) > 0:
-                events = sel.select()
-                for key, _ in events:
-                    line = key.fileobj.readline()
-                    if not line:
-                        # Unregister the io when EOF reached
-                        sel.unregister(key.fileobj)
-                        continue
-                    # TODO(zhwu,gmittal): Put replace_crlf, skip_lines, and
-                    # start_streaming_at logic in processor.process_line(line)
-                    if replace_crlf and line.endswith('\r\n'):
-                        # Replace CRLF with LF to avoid ray logging to the same
-                        # line due to separating lines with '\n'.
-                        line = line[:-2] + '\n'
-                    if (skip_lines is not None and
-                            any(skip in line for skip in skip_lines)):
-                        continue
-                    if start_streaming_at in line:
-                        start_streaming_flag = True
-                    if (end_streaming_at is not None and
-                            end_streaming_at in line):
-                        # Keep executing the loop, only stop streaming.
-                        # E.g., this is used for `sky bench` to hide the
-                        # redundant messages of `sky launch` while
-                        # saving them in log files.
-                        end_streaming_flag = True
-                    if key.fileobj is out_io:
-                        stdout += line
-                        out_stream = sys.stdout
-                    else:
-                        stderr += line
-                        out_stream = sys.stderr
-                    if (stream_logs and start_streaming_flag and
-                            not end_streaming_flag):
-                        out_stream.write(streaming_prefix + line)
-                        out_stream.flush()
-                    if log_path != '/dev/null':
-                        fout.write(line)
-                        fout.flush()
-                    line_processor.process_line(line)
+    streaming_prefix = args.streaming_prefix if args.streaming_prefix else ''
+    line_processor = (log_utils.LineProcessor()
+                      if args.line_processor is None else args.line_processor)
+
+    out = []
+    with open(args.log_path, 'a') as fout:
+        with line_processor:
+            while True:
+                line = out_io.readline()
+                if not line:
+                    break
+                # start_streaming_at logic in processor.process_line(line)
+                if args.replace_crlf and line.endswith('\r\n'):
+                    # Replace CRLF with LF to avoid ray logging to the same
+                    # line due to separating lines with '\n'.
+                    line = line[:-2] + '\n'
+                if (args.skip_lines is not None and
+                        any(skip in line for skip in args.skip_lines)):
+                    continue
+                if args.start_streaming_at in line:
+                    start_streaming_flag = True
+                if (args.end_streaming_at is not None and
+                        args.end_streaming_at in line):
+                    # Keep executing the loop, only stop streaming.
+                    # E.g., this is used for `sky bench` to hide the
+                    # redundant messages of `sky launch` while
+                    # saving them in log files.
+                    end_streaming_flag = True
+                if (args.stream_logs and start_streaming_flag and
+                        not end_streaming_flag):
+                    print(streaming_prefix + line,
+                          end='',
+                          file=out_stream,
+                          flush=True)
+                if args.log_path != '/dev/null':
+                    fout.write(line)
+                    fout.flush()
+                line_processor.process_line(line)
+                out.append(line)
+    return ''.join(out)
+
+
+def process_subprocess_stream(proc, args: _ProcessingArgs) -> Tuple[str, str]:
+    """Redirect the process's filtered stdout/stderr to both stream and file"""
+    if proc.stderr is not None:
+        # Asyncio does not work as the output processing can be executed in a
+        # different thread.
+        # selectors is possible to handle the multiplexing of stdout/stderr,
+        # but it introduces buffering making the output not streaming.
+        with multiprocessing.pool.ThreadPool(processes=1) as pool:
+            err_args = copy.copy(args)
+            err_args.line_processor = None
+            stderr_fut = pool.apply_async(_handle_io_stream,
+                                          args=(proc.stderr, sys.stderr,
+                                                err_args))
+            # Do not launch a thread for stdout as the rich.status does not
+            # work in a thread, which is used in
+            # log_utils.RayUpLineProcessor.
+            stdout = _handle_io_stream(proc.stdout, sys.stdout, args)
+            stderr = stderr_fut.get()
+    else:
+        stdout = _handle_io_stream(proc.stdout, sys.stdout, args)
+        stderr = ''
     return stdout, stderr
 
 
 def run_with_log(
     cmd: Union[List[str], str],
     log_path: str,
+    *,
+    require_outputs: bool = False,
     stream_logs: bool = False,
     start_streaming_at: str = '',
     end_streaming_at: Optional[str] = None,
     skip_lines: Optional[List[str]] = None,
-    require_outputs: bool = False,
     shell: bool = False,
     with_ray: bool = False,
     process_stream: bool = True,
     line_processor: Optional[log_utils.LineProcessor] = None,
     streaming_prefix: Optional[str] = None,
     ray_job_id: Optional[str] = None,
     use_sudo: bool = False,
@@ -128,16 +148,16 @@
 
     Args:
         cmd: The command to run.
         log_path: The path to the log file.
         stream_logs: Whether to stream the logs to stdout/stderr.
         require_outputs: Whether to return the stdout/stderr of the command.
         process_stream: Whether to post-process the stdout/stderr of the
-          command. If enabled, lines are printed only when '\r' or '\n' is
-          found.
+            command, such as replacing or skipping lines on the fly. If
+            enabled, lines are printed only when '\r' or '\n' is found.
         ray_job_id: The id for a ray job.
         use_sudo: Whether to use sudo to create log_path.
 
     Returns the returncode or returncode, stdout and stderr of the command.
       Note that the stdout and stderr is already decoded.
     """
     assert process_stream or not require_outputs, (
@@ -157,90 +177,97 @@
         # subprocess.Popen in local mode with shell=True does not work,
         # as it does not understand what -H means for sudo.
         shell = False
     else:
         os.makedirs(dirname, exist_ok=True)
     # Redirect stderr to stdout when using ray, to preserve the order of
     # stdout and stderr.
-    stdout = stderr = None
+    stdout_arg = stderr_arg = None
     if process_stream:
-        stdout = subprocess.PIPE
-        stderr = subprocess.PIPE if not with_ray else subprocess.STDOUT
+        stdout_arg = subprocess.PIPE
+        stderr_arg = subprocess.PIPE if not with_ray else subprocess.STDOUT
     with subprocess.Popen(cmd,
-                          stdout=stdout,
-                          stderr=stderr,
+                          stdout=stdout_arg,
+                          stderr=stderr_arg,
                           start_new_session=True,
                           shell=shell,
                           **kwargs) as proc:
-        # The proc can be defunct if the python program is killed. Here we
-        # open a new subprocess to gracefully kill the proc, SIGTERM
-        # and then SIGKILL the process group.
-        # Adapted from ray/dashboard/modules/job/job_manager.py#L154
-        parent_pid = os.getpid()
-        daemon_script = os.path.join(
-            os.path.dirname(os.path.abspath(job_lib.__file__)),
-            'subprocess_daemon.py')
-        daemon_cmd = [
-            'python3',
-            daemon_script,
-            '--parent-pid',
-            str(parent_pid),
-            '--proc-pid',
-            str(proc.pid),
-        ]
-        # Bool use_sudo is true in the Sky On-prem case.
-        # In this case, subprocess_daemon.py should run on the root user
-        # and the Ray job id should be passed for daemon to poll for
-        # job status (as `ray job stop` does not work in the
-        # multitenant case).
-        if use_sudo:
-            daemon_cmd.insert(0, 'sudo')
-            daemon_cmd.extend(['--local-ray-job-id', str(ray_job_id)])
-        subprocess.Popen(
-            daemon_cmd,
-            start_new_session=True,
-            # Suppress output
-            stdout=subprocess.DEVNULL,
-            stderr=subprocess.DEVNULL,
-            # Disable input
-            stdin=subprocess.DEVNULL,
-        )
-        stdout = ''
-        stderr = ''
-
-        if process_stream:
-            if skip_lines is None:
-                skip_lines = []
-            # Skip these lines caused by `-i` option of bash. Failed to
-            # find other way to turn off these two warning.
-            # https://stackoverflow.com/questions/13300764/how-to-tell-bash-not-to-issue-warnings-cannot-set-terminal-process-group-and # pylint: disable=line-too-long
-            # `ssh -T -i -tt` still cause the problem.
-            skip_lines += [
-                'bash: cannot set terminal process group',
-                'bash: no job control in this shell',
+        try:
+            # The proc can be defunct if the python program is killed. Here we
+            # open a new subprocess to gracefully kill the proc, SIGTERM
+            # and then SIGKILL the process group.
+            # Adapted from ray/dashboard/modules/job/job_manager.py#L154
+            parent_pid = os.getpid()
+            daemon_script = os.path.join(
+                os.path.dirname(os.path.abspath(job_lib.__file__)),
+                'subprocess_daemon.py')
+            daemon_cmd = [
+                'python3',
+                daemon_script,
+                '--parent-pid',
+                str(parent_pid),
+                '--proc-pid',
+                str(proc.pid),
             ]
-            # We need this even if the log_path is '/dev/null' to ensure the
-            # progress bar is shown.
-            # NOTE: Lines are printed only when '\r' or '\n' is found.
-            stdout, stderr = process_subprocess_stream(
-                proc,
-                log_path,
-                stream_logs,
-                start_streaming_at=start_streaming_at,
-                end_streaming_at=end_streaming_at,
-                skip_lines=skip_lines,
-                line_processor=line_processor,
-                # Replace CRLF when the output is logged to driver by ray.
-                replace_crlf=with_ray,
-                streaming_prefix=streaming_prefix,
+            # Bool use_sudo is true in the Sky On-prem case.
+            # In this case, subprocess_daemon.py should run on the root user
+            # and the Ray job id should be passed for daemon to poll for
+            # job status (as `ray job stop` does not work in the
+            # multitenant case).
+            if use_sudo:
+                daemon_cmd.insert(0, 'sudo')
+                daemon_cmd.extend(['--local-ray-job-id', str(ray_job_id)])
+            subprocess.Popen(
+                daemon_cmd,
+                start_new_session=True,
+                # Suppress output
+                stdout=subprocess.DEVNULL,
+                stderr=subprocess.DEVNULL,
+                # Disable input
+                stdin=subprocess.DEVNULL,
             )
-        proc.wait()
-        if require_outputs:
-            return proc.returncode, stdout, stderr
-        return proc.returncode
+            stdout = ''
+            stderr = ''
+
+            if process_stream:
+                if skip_lines is None:
+                    skip_lines = []
+                # Skip these lines caused by `-i` option of bash. Failed to
+                # find other way to turn off these two warning.
+                # https://stackoverflow.com/questions/13300764/how-to-tell-bash-not-to-issue-warnings-cannot-set-terminal-process-group-and # pylint: disable=line-too-long
+                # `ssh -T -i -tt` still cause the problem.
+                skip_lines += [
+                    'bash: cannot set terminal process group',
+                    'bash: no job control in this shell',
+                ]
+                # We need this even if the log_path is '/dev/null' to ensure the
+                # progress bar is shown.
+                # NOTE: Lines are printed only when '\r' or '\n' is found.
+                args = _ProcessingArgs(
+                    log_path=log_path,
+                    stream_logs=stream_logs,
+                    start_streaming_at=start_streaming_at,
+                    end_streaming_at=end_streaming_at,
+                    skip_lines=skip_lines,
+                    line_processor=line_processor,
+                    # Replace CRLF when the output is logged to driver by ray.
+                    replace_crlf=with_ray,
+                    streaming_prefix=streaming_prefix,
+                )
+                stdout, stderr = process_subprocess_stream(proc, args)
+            proc.wait()
+            if require_outputs:
+                return proc.returncode, stdout, stderr
+            return proc.returncode
+        except KeyboardInterrupt:
+            # Kill the subprocess directly, otherwise, the underlying
+            # process will only be killed after the python program exits,
+            # causing the stream handling stuck at `readline`.
+            subprocess_utils.kill_children_processes()
+            raise
 
 
 def make_task_bash_script(codegen: str,
                           env_vars: Optional[Dict[str, str]] = None) -> str:
     # set -a is used for exporting all variables functions to the environment
     # so that bash `user_script` can access `conda activate`. Detail: #436.
     # Reference: https://www.gnu.org/software/bash/manual/html_node/The-Set-Builtin.html # pylint: disable=line-too-long
@@ -280,16 +307,16 @@
         if env_var in env_dict:
             env_vars[env_var] = env_dict[env_var]
     return env_vars
 
 
 def run_bash_command_with_log(bash_command: str,
                               log_path: str,
-                              job_owner: str,
-                              job_id: int,
+                              job_owner: Optional[str] = None,
+                              job_id: Optional[int] = None,
                               env_vars: Optional[Dict[str, str]] = None,
                               stream_logs: bool = False,
                               with_ray: bool = False,
                               use_sudo: bool = False):
     with tempfile.NamedTemporaryFile('w', prefix='sky_app_',
                                      delete=False) as fp:
         if use_sudo:
@@ -298,29 +325,34 @@
         fp.write(bash_command)
         fp.flush()
         script_path = fp.name
 
         # Need this `-i` option to make sure `source ~/.bashrc` work.
         inner_command = f'/bin/bash -i {script_path}'
 
-        if use_sudo:
+        subprocess_cmd: Union[str, List[str]]
+        if use_sudo and job_owner is not None:
             subprocess.run(f'chmod a+rwx {script_path}', shell=True, check=True)
             subprocess_cmd = job_lib.make_job_command_with_user_switching(
                 job_owner, inner_command)
         else:
             subprocess_cmd = inner_command
 
-        return run_with_log(subprocess_cmd,
-                            log_path,
-                            ray_job_id=job_lib.make_ray_job_id(
-                                job_id, job_owner),
-                            stream_logs=stream_logs,
-                            with_ray=with_ray,
-                            use_sudo=use_sudo,
-                            shell=True)
+        ray_job_id = job_lib.make_ray_job_id(job_id,
+                                             job_owner) if job_id else None
+        return run_with_log(
+            subprocess_cmd,
+            log_path,
+            ray_job_id=ray_job_id,
+            stream_logs=stream_logs,
+            with_ray=with_ray,
+            use_sudo=use_sudo,
+            # Disable input to avoid blocking.
+            stdin=subprocess.DEVNULL,
+            shell=True)
 
 
 def _follow_job_logs(file,
                      job_id: int,
                      start_streaming_at: str = '') -> Iterator[str]:
     """Yield each line from a file as they are written.
 
@@ -355,15 +387,16 @@
                     job_lib.JobStatus.RUNNING
             ]:
                 if wait_last_logs:
                     # Wait all the logs are printed before exit.
                     time.sleep(1 + _SKY_LOG_TAILING_GAP_SECONDS)
                     wait_last_logs = False
                     continue
-                print(f'INFO: Job finished (status: {status.value}).')
+                status_str = status.value if status is not None else 'None'
+                print(f'INFO: Job finished (status: {status_str}).')
                 return
 
             time.sleep(_SKY_LOG_TAILING_GAP_SECONDS)
             status = job_lib.get_status_no_lock(job_id)
 
 
 def tail_logs(job_owner: str,
@@ -379,22 +412,22 @@
         log_dir: The log directory of the job.
         spot_job_id: The spot job id (for logging info only to avoid confusion).
         follow: Whether to follow the logs or print the logs so far and exit.
     """
     job_str = f'job {job_id}'
     if spot_job_id is not None:
         job_str = f'spot job {spot_job_id}'
-    logger.debug(f'Tailing logs for job, real job_id {job_id}, spot_job_id '
-                 f'{spot_job_id}.')
-    logger.info(f'{colorama.Fore.YELLOW}Start streaming logs for {job_str}.'
-                f'{colorama.Style.RESET_ALL}')
     if log_dir is None:
         print(f'{job_str.capitalize()} not found (see `sky queue`).',
               file=sys.stderr)
         return
+    logger.debug(f'Tailing logs for job, real job_id {job_id}, spot_job_id '
+                 f'{spot_job_id}.')
+    logger.info(f'{colorama.Fore.YELLOW}Start streaming logs for {job_str}.'
+                f'{colorama.Style.RESET_ALL}')
     log_path = os.path.join(log_dir, 'run.log')
     log_path = os.path.expanduser(log_path)
 
     status = job_lib.update_job_status(job_owner, [job_id], silent=True)[0]
 
     # Wait for the log to be written. This is needed due to the `ray submit`
     # will take some time to start the job and write the log.
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/skylet/providers/aws/cloudwatch/cloudwatch_helper.py` & `skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/aws/cloudwatch/cloudwatch_helper.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,14 +1,15 @@
 import copy
 import hashlib
 import json
 import logging
 import os
 import time
-from typing import Any, Dict, List, Tuple, Union
+from enum import Enum
+from typing import Any, Callable, Dict, List, Union
 
 import botocore
 
 from sky.skylet.providers.aws.utils import client_cache, resource_cache
 from ray.autoscaler.tags import NODE_KIND_HEAD, TAG_RAY_CLUSTER_NAME, TAG_RAY_NODE_KIND
 
 logger = logging.getLogger(__name__)
@@ -17,40 +18,61 @@
 CLOUDWATCH_RAY_INSTANCE_PROFILE = RAY + "-cloudwatch-v1"
 CLOUDWATCH_RAY_IAM_ROLE = RAY + "-cloudwatch-v1"
 CLOUDWATCH_AGENT_INSTALLED_AMI_TAG = "T6Iq2faj"
 CLOUDWATCH_AGENT_INSTALLED_TAG = "cloudwatch-agent-installed"
 CLOUDWATCH_CONFIG_HASH_TAG_BASE = "cloudwatch-config-hash"
 
 
+class CloudwatchConfigType(str, Enum):
+    AGENT = "agent"
+    DASHBOARD = "dashboard"
+    ALARM = "alarm"
+
+
 class CloudwatchHelper:
     def __init__(
         self, provider_config: Dict[str, Any], node_id: str, cluster_name: str
     ) -> None:
         self.node_id = node_id
         self.cluster_name = cluster_name
         self.provider_config = provider_config
         region = provider_config["region"]
         self.ec2_resource = resource_cache("ec2", region)
         self.ec2_client = self.ec2_resource.meta.client
         self.ssm_client = client_cache("ssm", region)
         cloudwatch_resource = resource_cache("cloudwatch", region)
         self.cloudwatch_client = cloudwatch_resource.meta.client
+        self.CLOUDWATCH_CONFIG_TYPE_TO_CONFIG_VARIABLE_REPLACE_FUNC: Dict[
+            str, Callable
+        ] = {
+            CloudwatchConfigType.AGENT.value: self._replace_cwa_config_vars,
+            CloudwatchConfigType.DASHBOARD.value: self._replace_dashboard_config_vars,
+            CloudwatchConfigType.ALARM.value: self._load_config_file,
+        }
+        self.CLOUDWATCH_CONFIG_TYPE_TO_UPDATE_FUNC_HEAD_NODE: Dict[str, Callable] = {
+            CloudwatchConfigType.AGENT.value: self._restart_cloudwatch_agent,
+            CloudwatchConfigType.DASHBOARD.value: self._put_cloudwatch_dashboard,
+            CloudwatchConfigType.ALARM.value: self._put_cloudwatch_alarm,
+        }
+        self.CLOUDWATCH_CONFIG_TYPE_TO_UPDATE_FUNC_WORKER_NODE: Dict[str, Callable] = {
+            CloudwatchConfigType.AGENT.value: self._restart_cloudwatch_agent,
+            CloudwatchConfigType.ALARM.value: self._put_cloudwatch_alarm,
+        }
 
     def update_from_config(self, is_head_node: bool) -> None:
         """Discovers and applies CloudWatch config updates as required.
 
         Args:
             is_head_node: whether this node is the head node.
         """
-        if CloudwatchHelper.cloudwatch_config_exists(self.provider_config, "agent"):
-            self._update_cloudwatch_config(is_head_node, "agent")
-        if CloudwatchHelper.cloudwatch_config_exists(self.provider_config, "dashboard"):
-            self._update_cloudwatch_config(is_head_node, "dashboard")
-        if CloudwatchHelper.cloudwatch_config_exists(self.provider_config, "alarm"):
-            self._update_cloudwatch_config(is_head_node, "alarm")
+        for config_type in CloudwatchConfigType:
+            if CloudwatchHelper.cloudwatch_config_exists(
+                self.provider_config, config_type.value
+            ):
+                self._update_cloudwatch_config(config_type.value, is_head_node)
 
     def _ec2_health_check_waiter(self, node_id: str) -> None:
         # wait for all EC2 instance checks to complete
         try:
             logger.info(
                 "Waiting for EC2 instance health checks to complete before "
                 "configuring Unified Cloudwatch Agent. This may take a few "
@@ -62,69 +84,67 @@
             logger.error(
                 "Failed while waiting for EC2 instance checks to complete: {}".format(
                     e.message
                 )
             )
             raise e
 
-    def _update_cloudwatch_config(self, is_head_node: bool, config_type: str) -> None:
-        """Update remote CloudWatch configs at Parameter Store,
-        update hash tag value on node and perform associated operations
-        at CloudWatch console if local CloudWatch configs change.
-
-        Args:
-            is_head_node: whether this node is the head node.
-            config_type: CloudWatch config file type.
+    def _update_cloudwatch_config(self, config_type: str, is_head_node: bool) -> None:
+        """
+        check whether update operations are needed in
+        cloudwatch related configs
         """
         cwa_installed = self._setup_cwa()
         param_name = self._get_ssm_param_name(config_type)
         if cwa_installed:
             if is_head_node:
                 cw_config_ssm = self._set_cloudwatch_ssm_config_param(
                     param_name, config_type
                 )
                 cur_cw_config_hash = self._sha1_hash_file(config_type)
                 ssm_cw_config_hash = self._sha1_hash_json(cw_config_ssm)
-                # check if user updated Unified Cloudwatch Agent config file.
+                # check if user updated cloudwatch related config files.
                 # if so, perform corresponding actions.
                 if cur_cw_config_hash != ssm_cw_config_hash:
                     logger.info(
                         "Cloudwatch {} config file has changed.".format(config_type)
                     )
                     self._upload_config_to_ssm_and_set_hash_tag(config_type)
-                    if config_type == "agent":
-                        self._restart_cloudwatch_agent()
-                    elif config_type == "dashboard":
-                        self._put_cloudwatch_dashboard()
-                    elif config_type == "alarm":
-                        self._put_cloudwatch_alarm()
+                    self.CLOUDWATCH_CONFIG_TYPE_TO_UPDATE_FUNC_HEAD_NODE.get(
+                        config_type
+                    )()
             else:
                 head_node_hash = self._get_head_node_config_hash(config_type)
                 cur_node_hash = self._get_cur_node_config_hash(config_type)
                 if head_node_hash != cur_node_hash:
                     logger.info(
                         "Cloudwatch {} config file has changed.".format(config_type)
                     )
-                    if config_type == "agent":
-                        self._restart_cloudwatch_agent()
-                    if config_type == "alarm":
-                        self._put_cloudwatch_alarm()
+                    update_func = (
+                        self.CLOUDWATCH_CONFIG_TYPE_TO_UPDATE_FUNC_WORKER_NODE.get(
+                            config_type
+                        )
+                    )
+                    if update_func:
+                        update_func()
                     self._update_cloudwatch_hash_tag_value(
                         self.node_id, head_node_hash, config_type
                     )
 
     def _put_cloudwatch_dashboard(self) -> Dict[str, Any]:
         """put dashboard to cloudwatch console"""
 
         cloudwatch_config = self.provider_config["cloudwatch"]
         dashboard_config = cloudwatch_config.get("dashboard", {})
         dashboard_name_cluster = dashboard_config.get("name", self.cluster_name)
         dashboard_name = self.cluster_name + "-" + dashboard_name_cluster
 
-        widgets = self._replace_dashboard_config_variables()
+        widgets = self._replace_dashboard_config_vars(
+            CloudwatchConfigType.DASHBOARD.value
+        )
 
         response = self.cloudwatch_client.put_dashboard(
             DashboardName=dashboard_name, DashboardBody=json.dumps({"widgets": widgets})
         )
         issue_count = len(response.get("DashboardValidationMessages", []))
         if issue_count > 0:
             for issue in response.get("DashboardValidationMessages"):
@@ -140,29 +160,29 @@
             )
         else:
             logger.info("Successfully put dashboard to CloudWatch console")
         return response
 
     def _put_cloudwatch_alarm(self) -> None:
         """put CloudWatch metric alarms read from config"""
-        param_name = self._get_ssm_param_name("alarm")
+        param_name = self._get_ssm_param_name(CloudwatchConfigType.ALARM.value)
         data = json.loads(self._get_ssm_param(param_name))
         for item in data:
             item_out = copy.deepcopy(item)
             self._replace_all_config_variables(
                 item_out,
                 self.node_id,
                 self.cluster_name,
                 self.provider_config["region"],
             )
             self.cloudwatch_client.put_metric_alarm(**item_out)
         logger.info("Successfully put alarms to CloudWatch console")
 
     def _send_command_to_node(
-        self, document_name: str, parameters: List[str], node_id: str
+        self, document_name: str, parameters: Dict[str, List[str]], node_id: str
     ) -> Dict[str, Any]:
         """send SSM command to the given nodes"""
         logger.debug(
             "Sending SSM command to {} node(s). Document name: {}. "
             "Parameters: {}.".format(node_id, document_name, parameters)
         )
         response = self.ssm_client.send_command(
@@ -173,30 +193,32 @@
             MaxErrors="0",
         )
         return response
 
     def _ssm_command_waiter(
         self,
         document_name: str,
-        parameters: List[str],
+        parameters: Dict[str, List[str]],
         node_id: str,
         retry_failed: bool = True,
-    ) -> bool:
+    ) -> Dict[str, Any]:
         """wait for SSM command to complete on all cluster nodes"""
 
         # This waiter differs from the built-in SSM.Waiter by
         # optimistically waiting for the command invocation to
         # exist instead of failing immediately, and by resubmitting
         # any failed command until all retry attempts are exhausted
         # by default.
         response = self._send_command_to_node(document_name, parameters, node_id)
         command_id = response["Command"]["CommandId"]
 
         cloudwatch_config = self.provider_config["cloudwatch"]
-        agent_retryer_config = cloudwatch_config.get("agent").get("retryer", {})
+        agent_retryer_config = cloudwatch_config.get(
+            CloudwatchConfigType.AGENT.value
+        ).get("retryer", {})
         max_attempts = agent_retryer_config.get("max_attempts", 120)
         delay_seconds = agent_retryer_config.get("delay_seconds", 30)
         num_attempts = 0
         cmd_invocation_res = {}
         while True:
             num_attempts += 1
             logger.debug(
@@ -279,34 +301,40 @@
             string = string.replace("{cluster_name}", cluster_name)
         if region:
             string = string.replace("{region}", region)
         return string
 
     def _replace_all_config_variables(
         self,
-        collection: Union[dict, list],
+        collection: Union[Dict[str, Any], str],
         node_id: str,
         cluster_name: str,
         region: str,
-    ) -> Tuple[(Union[dict, list], int)]:
+    ) -> Union[str, Dict[str, Any]]:
         """
         Replace known config variable occurrences in the input collection.
-
         The input collection must be either a dict or list.
         Returns a tuple consisting of the output collection and the number of
         modified strings in the collection (which is not necessarily equal to
         the number of variables replaced).
         """
+
         for key in collection:
             if type(collection) is dict:
                 value = collection.get(key)
                 index_key = key
             elif type(collection) is list:
                 value = key
                 index_key = collection.index(key)
+            else:
+                raise ValueError(
+                    f"Can't replace CloudWatch config variables "
+                    f"in unsupported collection type: {type(collection)}."
+                    f"Please check your CloudWatch JSON config files."
+                )
             if type(value) is str:
                 collection[index_key] = self._replace_config_variables(
                     value, node_id, cluster_name, region
                 )
             elif type(value) is dict or type(value) is list:
                 collection[index_key] = self._replace_all_config_variables(
                     value, node_id, cluster_name, region
@@ -340,16 +368,16 @@
                     "Checking for Unified CloudWatch Agent installation".format(
                         config_type
                     )
                 )
                 return self._get_default_empty_config_file_hash()
             else:
                 logger.info(
-                    "Failed to fetch CloudWatch {} config from SSM "
-                    "parameter store.".format(config_type)
+                    "Failed to fetch Unified CloudWatch Agent config from SSM "
+                    "parameter store."
                 )
                 logger.error(e)
                 raise e
         return parameter_value
 
     def _get_default_empty_config_file_hash(self):
         default_cw_config = "{}"
@@ -364,52 +392,46 @@
         logger.info("Successfully fetch ssm parameter: {}".format(parameter_name))
         res = response.get("Parameter", {})
         cwa_parameter = res.get("Value", {})
         return cwa_parameter
 
     def _sha1_hash_json(self, value: str) -> str:
         """calculate the json string sha1 hash"""
-        hash = hashlib.new("sha1")
+        sha1_hash = hashlib.new("sha1")
         binary_value = value.encode("ascii")
-        hash.update(binary_value)
-        sha1_res = hash.hexdigest()
+        sha1_hash.update(binary_value)
+        sha1_res = sha1_hash.hexdigest()
         return sha1_res
 
     def _sha1_hash_file(self, config_type: str) -> str:
         """calculate the config file sha1 hash"""
-        if config_type == "agent":
-            config = self._replace_cwa_config_variables()
-        if config_type == "dashboard":
-            config = self._replace_dashboard_config_variables()
-        if config_type == "alarm":
-            config = self._load_config_file("alarm")
+        config = self.CLOUDWATCH_CONFIG_TYPE_TO_CONFIG_VARIABLE_REPLACE_FUNC.get(
+            config_type
+        )(config_type)
         value = json.dumps(config)
         sha1_res = self._sha1_hash_json(value)
         return sha1_res
 
     def _upload_config_to_ssm_and_set_hash_tag(self, config_type: str):
-        if config_type == "agent":
-            data = self._replace_cwa_config_variables()
-        if config_type == "dashboard":
-            data = self._replace_dashboard_config_variables()
-        if config_type == "alarm":
-            data = self._load_config_file("alarm")
+        data = self.CLOUDWATCH_CONFIG_TYPE_TO_CONFIG_VARIABLE_REPLACE_FUNC.get(
+            config_type
+        )(config_type)
         sha1_hash_value = self._sha1_hash_file(config_type)
         self._upload_config_to_ssm(data, config_type)
         self._update_cloudwatch_hash_tag_value(
             self.node_id, sha1_hash_value, config_type
         )
 
     def _add_cwa_installed_tag(self, node_id: str) -> None:
         self.ec2_client.create_tags(
             Resources=[node_id],
             Tags=[{"Key": CLOUDWATCH_AGENT_INSTALLED_TAG, "Value": "True"}],
         )
         logger.info(
-            "Successfully add Unified Cloudwatch Agent installed "
+            "Successfully add Unified CloudWatch Agent installed "
             "tag on {}".format(node_id)
         )
 
     def _update_cloudwatch_hash_tag_value(
         self, node_id: str, sha1_hash_value: str, config_type: str
     ):
         hash_key_value = "-".join([CLOUDWATCH_CONFIG_HASH_TAG_BASE, config_type])
@@ -440,69 +462,68 @@
             Tier="Intelligent-Tiering",
         )
 
     def _upload_config_to_ssm(self, param: Dict[str, Any], config_type: str):
         param_name = self._get_ssm_param_name(config_type)
         self._put_ssm_param(param, param_name)
 
-    def _replace_cwa_config_variables(self) -> Dict[str, Any]:
+    def _replace_cwa_config_vars(self, config_type: str) -> Dict[str, Any]:
         """
         replace {instance_id}, {region}, {cluster_name}
         variable occurrences in Unified Cloudwatch Agent config file
         """
-        cwa_config = self._load_config_file("agent")
+        cwa_config = self._load_config_file(config_type)
         self._replace_all_config_variables(
             cwa_config,
             self.node_id,
             self.cluster_name,
             self.provider_config["region"],
         )
         return cwa_config
 
-    def _replace_dashboard_config_variables(self) -> List[Dict[str, Any]]:
+    def _replace_dashboard_config_vars(self, config_type: str) -> List[str]:
         """
         replace known variable occurrences in CloudWatch Dashboard config file
         """
-        data = self._load_config_file("dashboard")
+        data = self._load_config_file(config_type)
         widgets = []
         for item in data:
             item_out = self._replace_all_config_variables(
                 item,
                 self.node_id,
                 self.cluster_name,
                 self.provider_config["region"],
             )
-            item_out = copy.deepcopy(item)
             widgets.append(item_out)
         return widgets
 
-    def _replace_alarm_config_variables(self) -> List[Dict[str, Any]]:
+    def _replace_alarm_config_vars(self, config_type: str) -> List[str]:
         """
         replace {instance_id}, {region}, {cluster_name}
         variable occurrences in cloudwatch alarm config file
         """
-        data = self._load_config_file("alarm")
+        data = self._load_config_file(config_type)
         param_data = []
         for item in data:
             item_out = copy.deepcopy(item)
             self._replace_all_config_variables(
                 item_out,
                 self.node_id,
                 self.cluster_name,
                 self.provider_config["region"],
             )
             param_data.append(item_out)
         return param_data
 
     def _restart_cloudwatch_agent(self) -> None:
-        """restart Unified Cloudwatch Agent"""
-        cwa_param_name = self._get_ssm_param_name("agent")
+        """restart Unified CloudWatch Agent"""
+        cwa_param_name = self._get_ssm_param_name(CloudwatchConfigType.AGENT.value)
         logger.info(
-            "Restarting Unified Cloudwatch Agent package on {} node(s).".format(
-                (self.node_id)
+            "Restarting Unified CloudWatch Agent package on node {}.".format(
+                self.node_id
             )
         )
         self._stop_cloudwatch_agent()
         self._start_cloudwatch_agent(cwa_param_name)
 
     def _stop_cloudwatch_agent(self) -> None:
         """stop Unified CloudWatch Agent"""
@@ -687,15 +708,17 @@
 
         Returns:
             default cloudwatch instance profile name if cloudwatch config file
                 exists.
             default ray instance profile name if cloudwatch config file
                 doesn't exist.
         """
-        cwa_cfg_exists = CloudwatchHelper.cloudwatch_config_exists(config, "agent")
+        cwa_cfg_exists = CloudwatchHelper.cloudwatch_config_exists(
+            config, CloudwatchConfigType.AGENT.value
+        )
         return (
             CLOUDWATCH_RAY_INSTANCE_PROFILE
             if cwa_cfg_exists
             else default_instance_profile_name
         )
 
     @staticmethod
@@ -708,15 +731,17 @@
             config: provider section of cluster config file.
             default_iam_role_name: default ray iam role name.
 
         Returns:
             default cloudwatch iam role name if cloudwatch config file exists.
             default ray iam role name if cloudwatch config file doesn't exist.
         """
-        cwa_cfg_exists = CloudwatchHelper.cloudwatch_config_exists(config, "agent")
+        cwa_cfg_exists = CloudwatchHelper.cloudwatch_config_exists(
+            config, CloudwatchConfigType.AGENT.value
+        )
         return CLOUDWATCH_RAY_IAM_ROLE if cwa_cfg_exists else default_iam_role_name
 
     @staticmethod
     def resolve_policy_arns(
         config: Dict[str, Any], iam: Any, default_policy_arns: List[str]
     ) -> List[str]:
         """Attach necessary AWS policies for CloudWatch related operations.
@@ -727,15 +752,17 @@
             default_policy_arns: List of default ray AWS policies.
 
         Returns:
             list of policy arns including additional policies for CloudWatch
                 related operations if cloudwatch agent config is specifed in
                 cluster config file.
         """
-        cwa_cfg_exists = CloudwatchHelper.cloudwatch_config_exists(config, "agent")
+        cwa_cfg_exists = CloudwatchHelper.cloudwatch_config_exists(
+            config, CloudwatchConfigType.AGENT.value
+        )
         if cwa_cfg_exists:
             cloudwatch_managed_policy = {
                 "Version": "2012-10-17",
                 "Statement": [
                     {
                         "Effect": "Allow",
                         "Action": [
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/skylet/providers/aws/config.py` & `skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/aws/config.py`

 * *Files 15% similar despite different names*

```diff
@@ -28,44 +28,58 @@
 logger = logging.getLogger(__name__)
 
 RAY = "ray-autoscaler"
 DEFAULT_RAY_INSTANCE_PROFILE = RAY + "-v1"
 DEFAULT_RAY_IAM_ROLE = RAY + "-v1"
 SECURITY_GROUP_TEMPLATE = RAY + "-{}"
 
+SKYPILOT = "skypilot"
+DEFAULT_SKYPILOT_INSTANCE_PROFILE = SKYPILOT + "-v1"
+DEFAULT_SKYPILOT_IAM_ROLE = SKYPILOT + "-v1"
+
 # V61.0 has CUDA 11.2
 DEFAULT_AMI_NAME = "AWS Deep Learning AMI (Ubuntu 18.04) V61.0"
 
 # Obtained from https://aws.amazon.com/marketplace/pp/B07Y43P7X5 on 6/10/2022.
+# TODO(alex) : write a unit test to make sure we update AMI version used in
+# ray/autoscaler/aws/example-full.yaml whenever we update this dict.
 # NOTE(skypilot): these are not used; skypilot instead uses the default AMIs in aws.py.
 DEFAULT_AMI = {
     "us-east-1": "ami-0dd6adfad4ad37eec",  # US East (N. Virginia)
     "us-east-2": "ami-0c77cd5ca05bf1281",  # US East (Ohio)
     "us-west-1": "ami-020ab1b368a5ed1db",  # US West (N. California)
     "us-west-2": "ami-0387d929287ab193e",  # US West (Oregon)
     "ca-central-1": "ami-07dbafdbd38f18d98",  # Canada (Central)
     "eu-central-1": "ami-0383bd0c1fc4c63ec",  # EU (Frankfurt)
     "eu-west-1": "ami-0a074b0a311a837ac",  # EU (Ireland)
     "eu-west-2": "ami-094ba2b4651f761ca",  # EU (London)
     "eu-west-3": "ami-031da10fbf225bf5f",  # EU (Paris)
     "sa-east-1": "ami-0be7c1f1dd96d7337",  # SA (Sao Paulo)
+    "ap-northeast-1": "ami-0d69b2fd9641af433",  # Asia Pacific (Tokyo)
+    "ap-northeast-2": "ami-0d6d00bd58046ff91",  # Asia Pacific (Seoul)
+    "ap-northeast-3": "ami-068feab7122f7558d",  # Asia Pacific (Osaka)
+    "ap-southeast-1": "ami-05006b266c1be4e8f",  # Asia Pacific (Singapore)
+    "ap-southeast-2": "ami-066aa744514f9f95c",  # Asia Pacific (Sydney)
 }
 
 # todo: cli_logger should handle this assert properly
 # this should probably also happens somewhere else
 assert StrictVersion(boto3.__version__) >= StrictVersion(
     "1.4.8"
 ), "Boto3 version >= 1.4.8 required, try `pip install -U boto3`"
 
 
 def key_pair(i, region, key_name):
     """
     If key_name is not None, key_pair will be named after key_name.
     Returns the ith default (aws_key_pair_name, key_pair_path).
     """
+    # SkyPilot: we don't use this, as we explicitly set the key already.
+    # For backwards compatibility, we'll just return the key pair with
+    # the previous name.
     if i == 0:
         key_pair_name = "{}_{}".format(RAY, region) if key_name is None else key_name
         return (
             key_pair_name,
             os.path.expanduser("~/.ssh/{}.pem".format(key_pair_name)),
         )
 
@@ -211,15 +225,15 @@
             list_value=True,
         )
         print_info("EC2 AMI", "ImageId", "ami_src", allowed_tags=["dlami"])
 
     cli_logger.newline()
 
 
-def bootstrap_aws(config):
+def bootstrap_aws(config, skypilot_iam_role: bool = False):
     # create a copy of the input config to modify
     config = copy.deepcopy(config)
 
     # Log warnings if user included deprecated `head_node` or `worker_nodes`
     # fields. Raise error if no `available_node_types`
     check_legacy_fields(config)
     # Used internally to store head IAM role.
@@ -227,19 +241,24 @@
 
     # If a LaunchTemplate is provided, extract the necessary fields for the
     # config stages below.
     config = _configure_from_launch_template(config)
 
     # If NetworkInterfaces are provided, extract the necessary fields for the
     # config stages below.
+    # This basically adds two fields 'SubnetIds', 'SecurityGroupIds' to the
+    # node_config dict.
     config = _configure_from_network_interfaces(config)
 
     # The head node needs to have an IAM role that allows it to create further
     # EC2 instances.
-    config = _configure_iam_role(config)
+    #
+    # If skypilot_iam_role is True, we use our own IAM role for both head and
+    # workers.
+    config = _configure_iam_role(config, skypilot_iam_role=skypilot_iam_role)
 
     # Configure SSH access, using an existing key pair if possible.
     config = _configure_key_pair(config)
     global_event_system.execute_callback(
         CreateClusterEvent.ssh_keypair_downloaded,
         {"ssh_key_path": config["auth"]["ssh_private_key"]},
     )
@@ -253,25 +272,38 @@
 
     # Provide a helpful message for missing AMI.
     _check_ami(config)
 
     return config
 
 
-def _configure_iam_role(config):
+def _configure_iam_role(config, skypilot_iam_role: bool):
+    default_instance_profile = DEFAULT_RAY_INSTANCE_PROFILE
+    default_iam_role = DEFAULT_RAY_IAM_ROLE
+    if skypilot_iam_role:
+        default_instance_profile = DEFAULT_SKYPILOT_INSTANCE_PROFILE
+        default_iam_role = DEFAULT_SKYPILOT_IAM_ROLE
+
     head_node_type = config["head_node_type"]
     head_node_config = config["available_node_types"][head_node_type]["node_config"]
     if "IamInstanceProfile" in head_node_config:
         _set_config_info(head_instance_profile_src="config")
+        if skypilot_iam_role:
+            # SkyPilot: let the workers use the same role as the head node, so that they
+            # can access private S3 buckets.
+            for node_type in config["available_node_types"].values():
+                node_type["node_config"]["IamInstanceProfile"] = head_node_config[
+                    "IamInstanceProfile"
+                ]
         return config
     _set_config_info(head_instance_profile_src="default")
 
     instance_profile_name = cwh.resolve_instance_profile_name(
         config["provider"],
-        DEFAULT_RAY_INSTANCE_PROFILE,
+        default_instance_profile,
     )
     profile = _get_instance_profile(instance_profile_name, config)
 
     if profile is None:
         cli_logger.verbose(
             "Creating new IAM instance profile {} for use as the default.",
             cf.bold(instance_profile_name),
@@ -283,15 +315,15 @@
 
     cli_logger.doassert(
         profile is not None, "Failed to create instance profile."
     )  # todo: err msg
     assert profile is not None, "Failed to create instance profile"
 
     if not profile.roles:
-        role_name = cwh.resolve_iam_role_name(config["provider"], DEFAULT_RAY_IAM_ROLE)
+        role_name = cwh.resolve_iam_role_name(config["provider"], default_iam_role)
         role = _get_role(role_name, config)
         if role is None:
             cli_logger.verbose(
                 "Creating new IAM role {} for use as the default instance role.",
                 cf.bold(role_name),
             )
             iam = _resource("iam", config)
@@ -322,19 +354,50 @@
             )  # todo: err msg
 
             assert role is not None, "Failed to create role"
 
             for policy_arn in attach_policy_arns:
                 role.attach_policy(PolicyArn=policy_arn)
 
+            # SkyPilot: "PassRole" is required by the head node to pass the role to
+            # the workers, so we can access S3 buckets on the workers. "Resource"
+            # is to limit the role to only able to pass itself to the workers.
+            skypilot_pass_role_policy_doc = {
+                "Statement": [
+                    {
+                        "Effect": "Allow",
+                        "Action": [
+                            "iam:GetRole",
+                            "iam:PassRole",
+                        ],
+                        "Resource": role.arn,
+                    },
+                    {
+                        "Effect": "Allow",
+                        "Action": "iam:GetInstanceProfile",
+                        "Resource": profile.arn,
+                    },
+                ]
+            }
+            if skypilot_iam_role:
+                role.Policy("SkyPilotPassRolePolicy").put(
+                    PolicyDocument=json.dumps(skypilot_pass_role_policy_doc)
+                )
+
         profile.add_role(RoleName=role.name)
         time.sleep(15)  # wait for propagation
-    # Add IAM role to "head_node" field so that it is applied only to
-    # the head node -- not to workers with the same node type as the head.
-    config["head_node"]["IamInstanceProfile"] = {"Arn": profile.arn}
+    if skypilot_iam_role:
+        # SkyPilot: let the workers use the same role as the head node, so that they
+        # can access private S3 buckets.
+        for node_type in config["available_node_types"].values():
+            node_type["node_config"]["IamInstanceProfile"] = {"Arn": profile.arn}
+    else:
+        # Add IAM role to "head_node" field so that it is applied only to
+        # the head node -- not to workers with the same node type as the head.
+        config["head_node"]["IamInstanceProfile"] = {"Arn": profile.arn}
 
     return config
 
 
 def _configure_key_pair(config):
     node_types = config["available_node_types"]
 
@@ -368,15 +431,15 @@
     ec2 = _resource("ec2", config)
 
     # Writing the new ssh key to the filesystem fails if the ~/.ssh
     # directory doesn't already exist.
     os.makedirs(os.path.expanduser("~/.ssh"), exist_ok=True)
 
     # Try a few times to get or create a good key pair.
-    MAX_NUM_KEYS = 30
+    MAX_NUM_KEYS = 60
     for i in range(MAX_NUM_KEYS):
 
         key_name = config["provider"].get("key_pair", {}).get("key_name")
 
         key_name, key_path = key_pair(i, config["provider"]["region"], key_name)
         key = _get_key(key_name, config)
 
@@ -395,15 +458,15 @@
             # permissions. In order to do that we need to change the default
             # os.open behavior to include the mode we want.
             with open(key_path, "w", opener=partial(os.open, mode=0o600)) as f:
                 f.write(key.key_material)
             break
 
     if not key:
-        cli_logger.abort(
+        _skypilot_log_error_and_exit_for_failover(
             "No matching local key file for any of the key pairs in this "
             "account with ids from 0..{}. "
             "Consider deleting some unused keys pairs from your account.",
             key_name,
         )
 
     cli_logger.doassert(
@@ -462,114 +525,170 @@
         )
 
     def _get_pruned_subnets(current_subnets: List[Any]) -> Set[str]:
         current_subnet_ids = {s.subnet_id for s in current_subnets}
         user_specified_subnet_ids = {s.subnet_id for s in user_specified_subnets}
         return user_specified_subnet_ids - current_subnet_ids
 
+    def _subnet_name_tag_contains(subnet, substr: str) -> bool:
+        tags = subnet.meta.data["Tags"]
+        for tag in tags:
+            if tag["Key"] == "Name":
+                name = tag["Value"]
+                return substr in name
+        return False
+
     try:
         candidate_subnets = (
             user_specified_subnets
             if user_specified_subnets is not None
             else all_subnets
         )
         if vpc_id_of_sg:
             candidate_subnets = [
                 s for s in candidate_subnets if s.vpc_id == vpc_id_of_sg
             ]
+
         subnets = sorted(
             (
                 s
                 for s in candidate_subnets
                 if s.state == "available"
-                and (use_internal_ips or s.map_public_ip_on_launch)
+                and (
+                    # If using internal IPs, the subnets must not assign public
+                    # IPs. Additionally, requires that each eligible subnet
+                    # contain a name tag which includes the substring
+                    # 'private'. This is a HACK; see below.
+                    #
+                    # Reason: the first two checks alone are not enough. For
+                    # example, the VPC creation helper from AWS will create a
+                    # "public" and a "private" subnet per AZ. However, the
+                    # created "public" subnet by default has
+                    # map_public_ip_on_launch set to False as well. This means
+                    # we could've launched in that subnet, which will make any
+                    # instances not able to send outbound traffic to the
+                    # Internet, due to the way route tables/gateways are set up
+                    # for that public subnet. The "public" subnets are NOT
+                    # intended to host data plane VMs, while the "private"
+                    # subnets are.
+                    #
+                    # An alternative to the subnet name hack is to ensure
+                    # there's a route (dest=0.0.0.0/0, target=nat-*) in the
+                    # subnet's route table so that outbound connections
+                    # work. This seems hard to do, given a ec2.Subnet
+                    # object. (Easy to see in console though.) So we opt for
+                    # the subnet name requirement for now.
+                    (
+                        use_internal_ips
+                        and not s.map_public_ip_on_launch
+                        and _subnet_name_tag_contains(s, "private")
+                    )
+                    or
+                    # Or if using public IPs, the subnets must assign public
+                    # IPs.
+                    (not use_internal_ips and s.map_public_ip_on_launch)
+                    # NOTE: SkyPilot also changes the semantics of
+                    # 'use_internal_ips' through the above two conditions.
+                    # Previously, this flag by itself does not enforce only
+                    # choosing subnets that do not assign public IPs.  Now we
+                    # do so.
+                    #
+                    # In both before and now, this flag makes Ray communicate
+                    # between the client and the head node using the latter's
+                    # private ip.
+                )
             ),
             reverse=True,  # sort from Z-A
             key=lambda subnet: subnet.availability_zone,
         )
     except botocore.exceptions.ClientError as exc:
         handle_boto_error(exc, "Failed to fetch available subnets from AWS.")
         raise exc
 
     if not subnets:
-        cli_logger.abort(
+        _skypilot_log_error_and_exit_for_failover(
             f"No usable subnets found for node type {node_type_key}, try "
             "manually creating an instance in your specified region to "
-            "populate the list of subnets and trying this again.\n"
+            "populate the list of subnets and trying this again. "
             "Note that the subnet must map public IPs "
             "on instance launch unless you set `use_internal_ips: true` in "
             "the `provider` config."
         )
     elif _are_user_subnets_pruned(subnets):
-        cli_logger.abort(
+        _skypilot_log_error_and_exit_for_failover(
             f"The specified subnets for node type {node_type_key} are not "
             f"usable: {_get_pruned_subnets(subnets)}"
         )
 
     if azs is not None:
         azs = [az.strip() for az in azs.split(",")]
         subnets = [
             s
             for az in azs  # Iterate over AZs first to maintain the ordering
             for s in subnets
             if s.availability_zone == az
         ]
         if not subnets:
-            cli_logger.abort(
+            _skypilot_log_error_and_exit_for_failover(
                 f"No usable subnets matching availability zone {azs} found "
-                f"for node type {node_type_key}.\nChoose a different "
+                f"for node type {node_type_key}. Choose a different "
                 "availability zone or try manually creating an instance in "
                 "your specified region to populate the list of subnets and "
-                "trying this again."
+                "trying this again. If you have set `use_internal_ips`, check "
+                "that this zone has a subnet that (1) has the substring 'private' in its name tag "
+                "and (2) does not assign public IPs (`map_public_ip_on_launch` is False)."
             )
         elif _are_user_subnets_pruned(subnets):
-            cli_logger.abort(
+            _skypilot_log_error_and_exit_for_failover(
                 f"MISMATCH between specified subnets and Availability Zones! "
                 "The following Availability Zones were specified in the "
-                f"`provider section`: {azs}.\n The following subnets for node "
+                f"`provider section`: {azs}. The following subnets for node "
                 f"type `{node_type_key}` have no matching availability zone: "
                 f"{list(_get_pruned_subnets(subnets))}."
             )
 
     # Use subnets in only one VPC, so that _configure_security_groups only
     # needs to create a security group in this one VPC. Otherwise, we'd need
     # to set up security groups in all of the user's VPCs and set up networking
     # rules to allow traffic between these groups.
     # See https://github.com/ray-project/ray/pull/14868.
     first_subnet_vpc_id = subnets[0].vpc_id
     subnets = [s.subnet_id for s in subnets if s.vpc_id == subnets[0].vpc_id]
     if _are_user_subnets_pruned(subnets):
         subnet_vpcs = {s.subnet_id: s.vpc_id for s in user_specified_subnets}
-        cli_logger.abort(
+        _skypilot_log_error_and_exit_for_failover(
             f"Subnets specified in more than one VPC for node type `{node_type_key}`! "
             f"Please ensure that all subnets share the same VPC and retry your "
-            "request. Subnet VPCs: {}",
-            subnet_vpcs,
+            f"request. Subnet VPCs: {subnet_vpcs}"
         )
     return subnets, first_subnet_vpc_id
 
 
 def _configure_subnet(config):
     ec2 = _resource("ec2", config)
 
     # If head or worker security group is specified, filter down to subnets
     # belonging to the same VPC as the security group.
     sg_ids = []
     for node_type in config["available_node_types"].values():
         node_config = node_type["node_config"]
         sg_ids.extend(node_config.get("SecurityGroupIds", []))
-    if sg_ids:
+
+    if "vpc_name" in config["provider"]:
+        # NOTE: This is a new field added by SkyPilot and parsed by our own AWSNodeProvider.
+        vpc_id_of_sg = _get_vpc_id_by_name(config["provider"]["vpc_name"], config)
+    elif sg_ids:
         vpc_id_of_sg = _get_vpc_id_of_sg(sg_ids, config)
     else:
         vpc_id_of_sg = None
 
     # map from node type key -> source of SubnetIds field
     subnet_src_info = {}
     _set_config_info(subnet_src=subnet_src_info)
-    all_subnets = list(ec2.subnets.all())
+    all_subnets = list(ec2.subnets.all())  # All subnets of this region.
     # separate node types with and without user-specified subnets
     node_types_subnets = []
     node_types_no_subnets = []
     for key, node_type in config["available_node_types"].items():
         if "SubnetIds" in node_type["node_config"]:
             node_types_subnets.append((key, node_type))
         else:
@@ -608,14 +727,52 @@
         )
         subnet_src_info[key] = "default"
         node_config["SubnetIds"] = subnet_ids
 
     return config
 
 
+def _skypilot_log_error_and_exit_for_failover(error: str) -> None:
+    """Logs an message then raises a specific RuntimeError to trigger failover.
+
+    Mainly used for handling VPC/subnet errors before nodes are launched.
+    """
+    # NOTE: keep. The backend looks for this to know no nodes are launched.
+    prefix = "SKYPILOT_ERROR_NO_NODES_LAUNCHED: "
+    raise RuntimeError(prefix + error)
+
+
+def _get_vpc_id_by_name(vpc_name: str, config: Dict[str, Any]) -> str:
+    """Returns the VPC ID of the unique VPC with a given name.
+
+    Exits with code 1 if:
+      - No VPC with the given name is found in the current region.
+      - More than 1 VPC with the given name are found in the current region.
+    """
+    ec2 = _resource("ec2", config)
+    # Look in the "Name" tag (shown as Name column in console).
+    filters = [{"Name": "tag:Name", "Values": [vpc_name]}]
+    vpcs = [vpc for vpc in ec2.vpcs.filter(Filters=filters)]
+    if not vpcs:
+        _skypilot_log_error_and_exit_for_failover(
+            f"No VPC with name {vpc_name!r} is found "
+            f'in {config["provider"]["region"]}. '
+            "To fix: specify a correct VPC name."
+        )
+    elif len(vpcs) > 1:
+        _skypilot_log_error_and_exit_for_failover(
+            f"Multiple VPCs with name {vpc_name!r} "
+            f'found in {config["provider"]["region"]}: {vpcs}. '
+            "It is ambiguous as to which VPC to use. To fix: specify a "
+            "VPC name that is uniquely identifying."
+        )
+    assert len(vpcs) == 1, vpcs
+    return vpcs[0].id
+
+
 def _get_vpc_id_of_sg(sg_ids: List[str], config: Dict[str, Any]) -> str:
     """Returns the VPC id of the security groups with the provided security
     group ids.
 
     Errors if the provided security groups belong to multiple VPCs.
     Errors if no security group with any of the provided ids is identified.
     """
@@ -626,15 +783,17 @@
     filters = [{"Name": "group-id", "Values": sg_ids}]
     security_groups = ec2.security_groups.filter(Filters=filters)
     vpc_ids = [sg.vpc_id for sg in security_groups]
     vpc_ids = list(set(vpc_ids))
 
     multiple_vpc_msg = (
         "All security groups specified in the cluster config "
-        "should belong to the same VPC."
+        "should belong to the same VPC.\n"
+        f"Security group IDs: {sg_ids}\n"
+        f"Their VPC IDs (expected 1 element): {vpc_ids}\n"
     )
     cli_logger.doassert(len(vpc_ids) <= 1, multiple_vpc_msg)
     assert len(vpc_ids) <= 1, multiple_vpc_msg
 
     no_sg_msg = (
         "Failed to detect a security group with id equal to any of "
         "the configured SecurityGroupIds."
@@ -688,15 +847,15 @@
     default_ami = DEFAULT_AMI.get(region)
 
     for key, node_type in config["available_node_types"].items():
         node_config = node_type["node_config"]
         node_ami = node_config.get("ImageId", "").lower()
         if node_ami in ["", "latest_dlami"]:
             if not default_ami:
-                cli_logger.abort(
+                _skypilot_log_error_and_exit_for_failover(
                     f"Node type `{key}` has no ImageId in its node_config "
                     f"and no default AMI is available for the region `{region}`. "
                     "ImageId will need to be set manually in your cluster config."
                 )
             else:
                 node_config["ImageId"] = default_ami
                 ami_src_info[key] = "dlami"
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/skylet/providers/aws/node_provider.py` & `skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/aws/node_provider.py`

 * *Files 12% similar despite different names*

```diff
@@ -93,14 +93,19 @@
         )
         final_instance_types.extend(copy.deepcopy(instance_types["InstanceTypes"]))
 
     return final_instance_types
 
 
 class AWSNodeProvider(NodeProvider):
+    """Deprecated for SkyPilot and kept for backward compatibility.
+
+    The cluster launch template has been updated to use AWSNodeProviderV2.
+    """
+
     max_terminate_nodes = 1000
 
     def __init__(self, provider_config, cluster_name):
         NodeProvider.__init__(self, provider_config, cluster_name)
         self.cache_stopped_nodes = provider_config.get("cache_stopped_nodes", True)
         aws_credentials = provider_config.get("aws_credentials")
 
@@ -272,29 +277,70 @@
                     "Name": "tag:{}".format(TAG_RAY_CLUSTER_NAME),
                     "Values": [self.cluster_name],
                 },
                 {
                     "Name": "tag:{}".format(TAG_RAY_NODE_KIND),
                     "Values": [tags[TAG_RAY_NODE_KIND]],
                 },
-                {
-                    "Name": "tag:{}".format(TAG_RAY_LAUNCH_CONFIG),
-                    "Values": [tags[TAG_RAY_LAUNCH_CONFIG]],
-                },
+                # SkyPilot: removed TAG_RAY_LAUNCH_CONFIG to allow reusing nodes
+                # with different launch configs.
+                # Reference: https://github.com/skypilot-org/skypilot/pull/1671
             ]
             # This tag may not always be present.
             if TAG_RAY_USER_NODE_TYPE in tags:
                 filters.append(
                     {
                         "Name": "tag:{}".format(TAG_RAY_USER_NODE_TYPE),
                         "Values": [tags[TAG_RAY_USER_NODE_TYPE]],
                     }
                 )
+            filters_with_launch_config = copy.copy(filters)
+            filters_with_launch_config.append(
+                {
+                    "Name": "tag:{}".format(TAG_RAY_LAUNCH_CONFIG),
+                    "Values": [tags[TAG_RAY_LAUNCH_CONFIG]],
+                }
+            )
+
+            # SkyPilot: We try to use the instances with the same matching launch_config first. If
+            # there is not enough instances with matching launch_config, we then use all the
+            # instances with the same matching launch_config plus some instances with wrong
+            # launch_config.
+            nodes_matching_launch_config = list(
+                self.ec2.instances.filter(Filters=filters_with_launch_config)
+            )
+            # launch_time is the latest launch time of the node, rather than the
+            # initial launch time.
+            nodes_matching_launch_config.sort(key=lambda n: n.launch_time, reverse=True)
+            if len(nodes_matching_launch_config) >= count:
+                reuse_nodes = nodes_matching_launch_config[:count]
+            else:
+                nodes_all = list(self.ec2.instances.filter(Filters=filters))
+                nodes_matching_launch_config_ids = set(
+                    n.id for n in nodes_matching_launch_config
+                )
+                nodes_non_matching_launch_config = [
+                    n for n in nodes_all if n.id not in nodes_matching_launch_config_ids
+                ]
+                # This `sort` is for backward compatibility, where the user already has leaked
+                # stopped nodes with the different launch config before update to #1671,
+                # and the total number of the leaked nodes is greater than the number of
+                # nodes to be created. With this, we will make sure we will reuse the
+                # most recently used nodes.
+                # This can be removed in the future when we are sure all the users
+                # have updated to #1671.
+                nodes_non_matching_launch_config.sort(
+                    key=lambda n: n.launch_time, reverse=True
+                )
+                reuse_nodes = (
+                    nodes_matching_launch_config + nodes_non_matching_launch_config
+                )
+                # The total number of reusable nodes can be less than the number of nodes to be created.
+                reuse_nodes = reuse_nodes[:count]
 
-            reuse_nodes = list(self.ec2.instances.filter(Filters=filters))[:count]
             reuse_node_ids = [n.id for n in reuse_nodes]
             reused_nodes_dict = {n.id: n for n in reuse_nodes}
             if reuse_nodes:
                 cli_logger.print(
                     # todo: handle plural vs singular?
                     "Reusing nodes {}. "
                     "To disable reuse, set `cache_stopped_nodes: False` "
@@ -449,14 +495,19 @@
                                 state=instance.state["Name"],
                                 info=state_reason["Message"],
                             ),
                         )
                 break
             except botocore.exceptions.ClientError as exc:
                 if attempt == max_tries:
+                    # SkyPilot: do not adopt the changes from upstream in
+                    # https://github.com/ray-project/ray/commit/c2abfdb2f7eee7f3e4320cb0d9e8e3bd639d5680#diff-eeb7bc1d8342583cf12c40536240dbcc67f089466a18a37bd60f187265a2dc94
+                    # which replaces the exception to NodeLaunchException. As we directly
+                    # handle the exception output in
+                    # cloud_vm_ray_backend._update_blocklist_on_aws_error
                     cli_logger.abort(
                         "Failed to launch instances. Max attempts exceeded.",
                         exc=exc,
                     )
                 else:
                     cli_logger.warning(
                         "create_instances: Attempt failed with {}, retrying.", exc
@@ -658,7 +709,26 @@
                     "Instance type "
                     + instance_type
                     + " is not available in AWS region: "
                     + cluster_config["provider"]["region"]
                     + "."
                 )
         return cluster_config
+
+
+class AWSNodeProviderV2(AWSNodeProvider):
+    """Same as V1, except head and workers use a SkyPilot IAM role.
+
+    The new version of the AWS node provider supports AWS SSO
+    (see #1489), by using a new IAM role with different permissions
+    than the original ray-autoscaler-v1 for both the head node and
+    worker nodes.
+
+    We did not overwrite the original AWSNodeProvider class to avoid
+    breaking existing clusters. Otherwise, the existing clusters will
+    have a new launch_hash and will have new node(s) launched, causing
+    the existing nodes to leak.
+    """
+
+    @staticmethod
+    def bootstrap_config(cluster_config):
+        return bootstrap_aws(cluster_config, skypilot_iam_role=True)
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/skylet/providers/aws/utils.py` & `skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/aws/utils.py`

 * *Files identical despite different names*

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/skylet/providers/azure/azure-vm-template.json` & `skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/azure/azure-vm-template.json`

 * *Files 6% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9443646071589322%*

 * *Differences: {"'parameters'": "{'msi': OrderedDict([('type', 'string'), ('metadata', "*

 * *                 "OrderedDict([('description', 'Managed service identity resource id.')]))]), "*

 * *                 "'nsg': OrderedDict([('type', 'string'), ('metadata', "*

 * *                 "OrderedDict([('description', 'Network security group resource id.')]))]), "*

 * *                 "'subnet': OrderedDict([('type', 'string'), ('metadata', "*

 * *                 "OrderedDict([('descriptions', 'Subnet resource id.')]))]), 'osDiskTier': "*

 * *       […]*

```diff
@@ -52,20 +52,43 @@
         },
         "imageVersion": {
             "metadata": {
                 "description": "The version of the VM image"
             },
             "type": "string"
         },
+        "msi": {
+            "metadata": {
+                "description": "Managed service identity resource id."
+            },
+            "type": "string"
+        },
+        "nsg": {
+            "metadata": {
+                "description": "Network security group resource id."
+            },
+            "type": "string"
+        },
         "osDiskSizeGB": {
             "metadata": {
                 "description": "OS disk size in GBs."
             },
             "type": "int"
         },
+        "osDiskTier": {
+            "allowedValues": [
+                "Premium_LRS",
+                "StandardSSD_LRS",
+                "Standard_LRS"
+            ],
+            "metadata": {
+                "description": "OS disk tier."
+            },
+            "type": "string"
+        },
         "priority": {
             "defaultValue": "Regular",
             "metadata": {
                 "description": "Specifies the priority for the virtual machine."
             },
             "type": "string"
         },
@@ -78,14 +101,20 @@
         },
         "publicKey": {
             "metadata": {
                 "description": "SSH Key for the Virtual Machine"
             },
             "type": "securestring"
         },
+        "subnet": {
+            "metadata": {
+                "descriptions": "Subnet resource id."
+            },
+            "type": "string"
+        },
         "vmCount": {
             "metadata": {
                 "description": "Number of VMs to deploy"
             },
             "type": "int"
         },
         "vmName": {
@@ -126,21 +155,21 @@
                         "name": "[variables('networkIpConfig')]",
                         "properties": {
                             "privateIPAllocationMethod": "Dynamic",
                             "publicIpAddress": {
                                 "id": "[resourceId('Microsoft.Network/publicIPAddresses', concat(variables('publicIPAddressName'), copyIndex()))]"
                             },
                             "subnet": {
-                                "id": "[variables('subnetRef')]"
+                                "id": "[parameters('subnet')]"
                             }
                         }
                     }
                 ],
                 "networkSecurityGroup": {
-                    "id": "[resourceId('Microsoft.Network/networkSecurityGroups','ray-nsg')]"
+                    "id": "[parameters('nsg')]"
                 }
             },
             "type": "Microsoft.Network/networkInterfaces"
         },
         {
             "apiVersion": "2020-06-01",
             "condition": "[not(parameters('provisionPublicIp'))]",
@@ -153,21 +182,21 @@
             "properties": {
                 "ipConfigurations": [
                     {
                         "name": "[variables('networkIpConfig')]",
                         "properties": {
                             "privateIPAllocationMethod": "Dynamic",
                             "subnet": {
-                                "id": "[variables('subnetRef')]"
+                                "id": "[parameters('subnet')]"
                             }
                         }
                     }
                 ],
                 "networkSecurityGroup": {
-                    "id": "[resourceId('Microsoft.Network/networkSecurityGroups','ray-nsg')]"
+                    "id": "[parameters('nsg')]"
                 }
             },
             "type": "Microsoft.Network/networkInterfaces"
         },
         {
             "apiVersion": "2019-02-01",
             "condition": "[parameters('provisionPublicIp')]",
@@ -195,15 +224,15 @@
             },
             "dependsOn": [
                 "[resourceId('Microsoft.Network/networkInterfaces/', concat(variables('networkInterfaceName'), copyIndex()))]"
             ],
             "identity": {
                 "type": "UserAssigned",
                 "userAssignedIdentities": {
-                    "[resourceId('Microsoft.ManagedIdentity/userAssignedIdentities', 'ray-msi-user-identity')]": {}
+                    "[parameters('msi')]": {}
                 }
             },
             "location": "[variables('location')]",
             "name": "[concat(parameters('vmName'), copyIndex())]",
             "properties": {
                 "billingProfile": "[parameters('billingProfile')]",
                 "hardwareProfile": {
@@ -240,27 +269,25 @@
                         "sku": "[parameters('imageSku')]",
                         "version": "[parameters('imageVersion')]"
                     },
                     "osDisk": {
                         "createOption": "fromImage",
                         "diskSizeGB": "[parameters('osDiskSizeGB')]",
                         "managedDisk": {
-                            "storageAccountType": "[variables('osDiskType')]"
+                            "storageAccountType": "[parameters('osDiskTier')]"
                         }
                     }
                 }
             },
             "tags": "[parameters('vmTags')]",
             "type": "Microsoft.Compute/virtualMachines"
         }
     ],
     "variables": {
         "location": "[resourceGroup().location]",
         "networkInterfaceName": "[if(parameters('provisionPublicIp'), variables('networkInterfaceNamePublic'), variables('networkInterfaceNamePrivate'))]",
-        "networkInterfaceNamePrivate": "[concat(parameters('vmName'),'-nic')]",
-        "networkInterfaceNamePublic": "[concat(parameters('vmName'),'-nic-public')]",
+        "networkInterfaceNamePrivate": "[concat(parameters('vmName'), '-nic')]",
+        "networkInterfaceNamePublic": "[concat(parameters('vmName'), '-nic-public')]",
         "networkIpConfig": "[guid(resourceGroup().id, parameters('vmName'))]",
-        "osDiskType": "Standard_LRS",
-        "publicIpAddressName": "[concat(parameters('vmName'), '-ip' )]",
-        "subnetRef": "[resourceId('Microsoft.Network/virtualNetworks/subnets', 'ray-vnet', 'ray-subnet')]"
+        "publicIpAddressName": "[concat(parameters('vmName'), '-ip')]"
     }
 }
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/skylet/providers/azure/config.py` & `skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/azure/config.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,36 +1,35 @@
 import json
 import logging
 import random
+from hashlib import sha256
 from pathlib import Path
 from typing import Any, Callable
 
 from azure.common.credentials import get_cli_profile
 from azure.identity import AzureCliCredential
 from azure.mgmt.resource import ResourceManagementClient
 from azure.mgmt.resource.resources.models import DeploymentMode
 
-RETRIES = 30
-MSI_NAME = "ray-msi-user-identity"
-NSG_NAME = "ray-nsg"
-SUBNET_NAME = "ray-subnet"
-VNET_NAME = "ray-vnet"
+UNIQUE_ID_LEN = 4
 
 logger = logging.getLogger(__name__)
 
 
 def get_azure_sdk_function(client: Any, function_name: str) -> Callable:
     """Retrieve a callable function from Azure SDK client object.
 
     Newer versions of the various client SDKs renamed function names to
     have a begin_ prefix. This function supports both the old and new
     versions of the SDK by first trying the old name and falling back to
     the prefixed new name.
     """
-    func = getattr(client, function_name, getattr(client, f"begin_{function_name}"))
+    func = getattr(
+        client, function_name, getattr(client, f"begin_{function_name}", None)
+    )
     if func is None:
         raise AttributeError(
             "'{obj}' object has no {func} or begin_{func} attribute".format(
                 obj={client.__name__}, func=function_name
             )
         )
     return func
@@ -44,15 +43,20 @@
 
 def _configure_resource_group(config):
     # TODO: look at availability sets
     # https://docs.microsoft.com/en-us/azure/virtual-machines/windows/tutorial-availability-sets
     subscription_id = config["provider"].get("subscription_id")
     if subscription_id is None:
         subscription_id = get_cli_profile().get_subscription_id()
-    resource_client = ResourceManagementClient(AzureCliCredential(), subscription_id)
+    # Increase the timeout to fix the Azure get-access-token (used by ray azure
+    # node_provider) timeout issue.
+    # Tracked in https://github.com/Azure/azure-cli/issues/20404#issuecomment-1249575110
+    resource_client = ResourceManagementClient(
+        AzureCliCredential(process_timeout=30), subscription_id
+    )
     config["provider"]["subscription_id"] = subscription_id
     logger.info("Using subscription id: %s", subscription_id)
 
     assert (
         "resource_group" in config["provider"]
     ), "Provider config must include resource_group field"
     resource_group = config["provider"]["resource_group"]
@@ -61,68 +65,77 @@
         "location" in config["provider"]
     ), "Provider config must include location field"
     params = {"location": config["provider"]["location"]}
 
     if "tags" in config["provider"]:
         params["tags"] = config["provider"]["tags"]
 
-    logger.info("Creating/Updating Resource Group: %s", resource_group)
-    resource_client.resource_groups.create_or_update(
-        resource_group_name=resource_group, parameters=params
+    logger.info("Creating/Updating resource group: %s", resource_group)
+    rg_create_or_update = get_azure_sdk_function(
+        client=resource_client.resource_groups, function_name="create_or_update"
     )
+    rg_create_or_update(resource_group_name=resource_group, parameters=params)
 
     # load the template file
     current_path = Path(__file__).parent
     template_path = current_path.joinpath("azure-config-template.json")
     with open(template_path, "r") as template_fp:
         template = json.load(template_fp)
 
-    # choose a random subnet, skipping most common value of 0
-    random.seed(resource_group)
-    subnet_mask = "10.{}.0.0/16".format(random.randint(1, 254))
+    logger.info("Using cluster name: %s", config["cluster_name"])
+
+    # set unique id for resources in this cluster
+    unique_id = config["provider"].get("unique_id")
+    if unique_id is None:
+        hasher = sha256()
+        hasher.update(config["provider"]["resource_group"].encode("utf-8"))
+        unique_id = hasher.hexdigest()[:UNIQUE_ID_LEN]
+    else:
+        unique_id = str(unique_id)
+    config["provider"]["unique_id"] = unique_id
+    logger.info("Using unique id: %s", unique_id)
+    cluster_id = "{}-{}".format(config["cluster_name"], unique_id)
+
+    subnet_mask = config["provider"].get("subnet_mask")
+    if subnet_mask is None:
+        # choose a random subnet, skipping most common value of 0
+        random.seed(unique_id)
+        subnet_mask = "10.{}.0.0/16".format(random.randint(1, 254))
+    logger.info("Using subnet mask: %s", subnet_mask)
 
     parameters = {
         "properties": {
             "mode": DeploymentMode.incremental,
             "template": template,
-            "parameters": {"subnet": {"value": subnet_mask}},
+            "parameters": {
+                "subnet": {"value": subnet_mask},
+                "clusterId": {"value": cluster_id},
+            },
         }
     }
 
     create_or_update = get_azure_sdk_function(
         client=resource_client.deployments, function_name="create_or_update"
     )
-    create_or_update(
-        resource_group_name=resource_group,
-        deployment_name="ray-config",
-        parameters=parameters,
-    ).wait()
+    outputs = (
+        create_or_update(
+            resource_group_name=resource_group,
+            deployment_name="ray-config",
+            parameters=parameters,
+        )
+        .result()
+        .properties.outputs
+    )
+
+    # append output resource ids to be used with vm creation
+    config["provider"]["msi"] = outputs["msi"]["value"]
+    config["provider"]["nsg"] = outputs["nsg"]["value"]
+    config["provider"]["subnet"] = outputs["subnet"]["value"]
 
     return config
 
 
 def _configure_key_pair(config):
-    ssh_user = config["auth"]["ssh_user"]
-    public_key = None
-    # search if the keys exist
-    for key_type in ["ssh_private_key", "ssh_public_key"]:
-        try:
-            key_path = Path(config["auth"][key_type]).expanduser()
-        except KeyError:
-            raise Exception("Config must define {}".format(key_type))
-        except TypeError:
-            raise Exception("Invalid config value for {}".format(key_type))
-
-        assert key_path.is_file(), "Could not find ssh key: {}".format(key_path)
-
-        if key_type == "ssh_public_key":
-            with open(key_path, "r") as f:
-                public_key = f.read()
-
-    for node_type in config["available_node_types"].values():
-        azure_arm_parameters = node_type["node_config"].setdefault(
-            "azure_arm_parameters", {}
-        )
-        azure_arm_parameters["adminUsername"] = ssh_user
-        azure_arm_parameters["publicKey"] = public_key
-
+    # SkyPilot: The original checks and configurations are no longer
+    # needed, since we have already set them up in the upper level
+    # SkyPilot codes. See sky/templates/azure-ray.yml.j2
     return config
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/skylet/providers/azure/node_provider.py` & `skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/azure/node_provider.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,7 +1,8 @@
+import copy
 import json
 import logging
 from pathlib import Path
 from threading import RLock
 from uuid import uuid4
 
 from azure.identity import AzureCliCredential
@@ -20,15 +21,15 @@
     TAG_RAY_LAUNCH_CONFIG,
     TAG_RAY_NODE_KIND,
     TAG_RAY_NODE_NAME,
     TAG_RAY_USER_NODE_TYPE,
 )
 
 VM_NAME_MAX_LEN = 64
-VM_NAME_UUID_LEN = 8
+UNIQUE_ID_LEN = 4
 
 logger = logging.getLogger(__name__)
 azure_logger = logging.getLogger("azure.core.pipeline.policies.http_logging_policy")
 azure_logger.setLevel(logging.WARNING)
 
 
 def synchronized(f):
@@ -58,32 +59,41 @@
         NodeProvider.__init__(self, provider_config, cluster_name)
         # TODO(suquark): This is a temporary patch for resource group.
         # By default, Ray autoscaler assumes the resource group is still here even
         # after the whole cluster is destroyed. However, now we deletes the resource
         # group after tearing down the cluster. To comfort the autoscaler, we need
         # to create/update it here, so the resource group always exists.
         from sky.skylet.providers.azure.config import _configure_resource_group
-        _configure_resource_group({"provider": provider_config})
+
+        _configure_resource_group(
+            {"cluster_name": cluster_name, "provider": provider_config}
+        )
         subscription_id = provider_config["subscription_id"]
         self.cache_stopped_nodes = provider_config.get("cache_stopped_nodes", True)
         # Sky only supports Azure CLI credential for now.
-        credential = AzureCliCredential()
+        # Increase the timeout to fix the Azure get-access-token (used by ray azure
+        # node_provider) timeout issue.
+        # Tracked in https://github.com/Azure/azure-cli/issues/20404#issuecomment-1249575110
+        credential = AzureCliCredential(process_timeout=30)
         self.compute_client = ComputeManagementClient(credential, subscription_id)
         self.network_client = NetworkManagementClient(credential, subscription_id)
         self.resource_client = ResourceManagementClient(credential, subscription_id)
 
         self.lock = RLock()
 
         # cache node objects
         self.cached_nodes = {}
 
     @synchronized
     def _get_filtered_nodes(self, tag_filters):
+        # add cluster name filter to only get nodes from this cluster
+        cluster_tag_filters = {**tag_filters, TAG_RAY_CLUSTER_NAME: self.cluster_name}
+
         def match_tags(vm):
-            for k, v in tag_filters.items():
+            for k, v in cluster_tag_filters.items():
                 if vm.tags.get(k) != v:
                     return False
             return True
 
         vms = self.compute_client.virtual_machines.list(
             resource_group_name=self.provider_config["resource_group"]
         )
@@ -140,15 +150,18 @@
 
         This list must not include terminated nodes. For performance reasons,
         providers are allowed to cache the result of a call to nodes() to
         serve single-node queries (e.g. is_running(node_id)). This means that
         nodes() must be called again to refresh results.
 
         Examples:
-            >>> provider.non_terminated_nodes({TAG_RAY_NODE_KIND: "worker"})
+            >>> from ray.autoscaler.tags import TAG_RAY_NODE_KIND
+            >>> provider = ... # doctest: +SKIP
+            >>> provider.non_terminated_nodes( # doctest: +SKIP
+            ...     {TAG_RAY_NODE_KIND: "worker"})
             ["node-1", "node-2"]
         """
         nodes = self._get_filtered_nodes(tag_filters=tag_filters)
         return [k for k, v in nodes.items() if not v["status"].startswith("deallocat")]
 
     def is_running(self, node_id):
         """Return whether the specified node is running."""
@@ -185,19 +198,56 @@
     def create_node(self, node_config, tags, count):
         resource_group = self.provider_config["resource_group"]
 
         if self.cache_stopped_nodes:
             VALIDITY_TAGS = [
                 TAG_RAY_CLUSTER_NAME,
                 TAG_RAY_NODE_KIND,
-                TAG_RAY_LAUNCH_CONFIG,
                 TAG_RAY_USER_NODE_TYPE,
             ]
             filters = {tag: tags[tag] for tag in VALIDITY_TAGS if tag in tags}
-            reuse_nodes = self.stopped_nodes(filters)[:count]
+            filters_with_launch_config = copy.copy(filters)
+            if TAG_RAY_LAUNCH_CONFIG in tags:
+                filters_with_launch_config[TAG_RAY_LAUNCH_CONFIG] = tags[
+                    TAG_RAY_LAUNCH_CONFIG
+                ]
+
+            # SkyPilot: We try to use the instances with the same matching launch_config first. If
+            # there is not enough instances with matching launch_config, we then use all the
+            # instances with the same matching launch_config plus some instances with wrong
+            # launch_config.
+            nodes_matching_launch_config = self.stopped_nodes(
+                filters_with_launch_config
+            )
+            nodes_matching_launch_config.sort(reverse=True)
+            if len(nodes_matching_launch_config) >= count:
+                reuse_nodes = nodes_matching_launch_config[:count]
+            else:
+                nodes_all = self.stopped_nodes(filters)
+                nodes_non_matching_launch_config = [
+                    n for n in nodes_all if n not in nodes_matching_launch_config
+                ]
+                # This sort is for backward compatibility, where the user already has
+                # leaked stopped nodes with the different launch config before update
+                # to #1671, and the total number of the leaked nodes is greater than
+                # the number of nodes to be created. With this, we make sure the nodes
+                # are reused in a deterministic order (sorting by str IDs; we cannot
+                # get the launch time info here; otherwise, sort by the launch time
+                # is more accurate.)
+                # This can be removed in the future when we are sure all the users
+                # have updated to #1671.
+                nodes_non_matching_launch_config.sort(reverse=True)
+                reuse_nodes = (
+                    nodes_matching_launch_config + nodes_non_matching_launch_config
+                )
+                # The total number of reusable nodes can be less than the number of nodes to be created.
+                # This `[:count]` is fine, as it will get all the reusable nodes, even if there are
+                # less nodes.
+                reuse_nodes = reuse_nodes[:count]
+
             logger.info(
                 f"Reusing nodes {list(reuse_nodes)}. "
                 "To disable reuse, set `cache_stopped_nodes: False` "
                 "under `provider` in the cluster configuration.",
             )
             start = get_azure_sdk_function(
                 client=self.compute_client.virtual_machines, function_name="start"
@@ -221,24 +271,29 @@
             template = json.load(template_fp)
 
         # get the tags
         config_tags = node_config.get("tags", {}).copy()
         config_tags.update(tags)
         config_tags[TAG_RAY_CLUSTER_NAME] = self.cluster_name
 
-        name_tag = config_tags.get(TAG_RAY_NODE_NAME, "node")
-        unique_id = uuid4().hex[:VM_NAME_UUID_LEN]
-        vm_name = "{name}-{id}".format(name=name_tag, id=unique_id)
+        vm_name = "{node}-{unique_id}-{vm_id}".format(
+            node=config_tags.get(TAG_RAY_NODE_NAME, "node"),
+            unique_id=self.provider_config["unique_id"],
+            vm_id=uuid4().hex[:UNIQUE_ID_LEN],
+        )[:VM_NAME_MAX_LEN]
         use_internal_ips = self.provider_config.get("use_internal_ips", False)
 
         template_params = node_config["azure_arm_parameters"].copy()
         template_params["vmName"] = vm_name
         template_params["provisionPublicIp"] = not use_internal_ips
         template_params["vmTags"] = config_tags
         template_params["vmCount"] = count
+        template_params["msi"] = self.provider_config["msi"]
+        template_params["nsg"] = self.provider_config["nsg"]
+        template_params["subnet"] = self.provider_config["subnet"]
 
         parameters = {
             "properties": {
                 "mode": DeploymentMode.incremental,
                 "template": template,
                 "parameters": {
                     key: {"value": value} for key, value in template_params.items()
@@ -248,15 +303,15 @@
 
         # TODO: we could get the private/public ips back directly
         create_or_update = get_azure_sdk_function(
             client=self.resource_client.deployments, function_name="create_or_update"
         )
         create_or_update(
             resource_group_name=resource_group,
-            deployment_name="ray-vm-{}".format(name_tag),
+            deployment_name=vm_name,
             parameters=parameters,
         ).wait()
 
     @synchronized
     def set_node_tags(self, node_id, tags):
         """Sets the tag values (string dict) for the specified node."""
         node_tags = self._get_cached_node(node_id)["tags"]
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/skylet/providers/gcp/config.py` & `skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/gcp/config.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,36 +1,59 @@
 import copy
 import json
 import logging
 import os
 import time
 from functools import partial
+from typing import Dict, List, Set, Tuple
 
 from cryptography.hazmat.backends import default_backend
 from cryptography.hazmat.primitives import serialization
 from cryptography.hazmat.primitives.asymmetric import rsa
 from google.oauth2 import service_account
 from google.oauth2.credentials import Credentials as OAuthCredentials
 from googleapiclient import discovery, errors
 
-from sky.skylet.providers.gcp.node import MAX_POLLS, POLL_INTERVAL, GCPNodeType
+from sky.skylet.providers.gcp.node import (
+    MAX_POLLS,
+    POLL_INTERVAL,
+    GCPNodeType,
+    GCPCompute,
+)
+from sky.skylet.providers.gcp.constants import (
+    SKYPILOT_VPC_NAME,
+    VPC_TEMPLATE,
+    FIREWALL_RULES_TEMPLATE,
+    FIREWALL_RULES_REQUIRED,
+    VM_MINIMAL_PERMISSIONS,
+    TPU_MINIMAL_PERMISSIONS,
+)
 from ray.autoscaler._private.util import check_legacy_fields
 
 logger = logging.getLogger(__name__)
 
 VERSION = "v1"
 TPU_VERSION = "v2alpha"  # change once v2 is stable
 
 RAY = "ray-autoscaler"
 DEFAULT_SERVICE_ACCOUNT_ID = RAY + "-sa-" + VERSION
 SERVICE_ACCOUNT_EMAIL_TEMPLATE = "{account_id}@{project_id}.iam.gserviceaccount.com"
 DEFAULT_SERVICE_ACCOUNT_CONFIG = {
     "displayName": "Ray Autoscaler Service Account ({})".format(VERSION),
 }
 
+SKYPILOT = "skypilot"
+SKYPILOT_SERVICE_ACCOUNT_ID = SKYPILOT + "-" + VERSION
+SKYPILOT_SERVICE_ACCOUNT_EMAIL_TEMPLATE = (
+    "{account_id}@{project_id}.iam.gserviceaccount.com"
+)
+SKYPILOT_SERVICE_ACCOUNT_CONFIG = {
+    "displayName": "SkyPilot Service Account ({})".format(VERSION),
+}
+
 # Those roles will be always added.
 # NOTE: `serviceAccountUser` allows the head node to create workers with
 # a serviceAccount. `roleViewer` allows the head node to run bootstrap_gcp.
 DEFAULT_SERVICE_ACCOUNT_ROLES = [
     "roles/storage.objectAdmin",
     "roles/compute.admin",
     "roles/iam.serviceAccountUser",
@@ -60,15 +83,16 @@
 
     if "machineType" not in node and "acceleratorType" not in node:
         raise ValueError(
             "Invalid node. For a Compute instance, 'machineType' is "
             "required. "
             "For a TPU instance, 'acceleratorType' and no 'machineType' "
             "is required. "
-            f"Got {list(node)}")
+            f"Got {list(node)}"
+        )
 
     if "machineType" not in node and "acceleratorType" in node:
         return GCPNodeType.TPU
     return GCPNodeType.COMPUTE
 
 
 def wait_for_crm_operation(operation, crm):
@@ -118,15 +142,15 @@
         time.sleep(POLL_INTERVAL)
 
     return result
 
 
 def key_pair_name(i, region, project_id, ssh_user):
     """Returns the ith default gcp_key_pair_name."""
-    key_name = "{}_gcp_{}_{}_{}_{}".format(RAY, region, project_id, ssh_user, i)
+    key_name = "{}_gcp_{}_{}_{}_{}".format(SKYPILOT, region, project_id, ssh_user, i)
     return key_name
 
 
 def key_pair_paths(key_name):
     """Returns public and private key paths for a given key_name."""
     public_key_path = os.path.expanduser("~/.ssh/{}.pub".format(key_name))
     private_key_path = os.path.expanduser("~/.ssh/{}.pem".format(key_name))
@@ -275,15 +299,14 @@
     config["head_node"] = {}
 
     # Check if we have any TPUs defined, and if so,
     # insert that information into the provider config
     if _has_tpus_in_node_configs(config):
         config["provider"][HAS_TPU_PROVIDER_FIELD] = True
 
-
     crm, iam, compute, tpu = construct_clients_from_provider_config(config["provider"])
 
     config = _configure_project(config, crm)
     config = _configure_iam_role(config, crm, iam)
     config = _configure_key_pair(config, compute)
     config = _configure_subnet(config, compute)
 
@@ -317,58 +340,169 @@
     ), "Project status needs to be ACTIVE, got {}".format(project["lifecycleState"])
 
     config["provider"]["project_id"] = project["projectId"]
 
     return config
 
 
+def _is_permission_satisfied(
+    service_account, crm, iam, required_permissions, required_roles
+):
+    """Check if either of the roles or permissions are satisfied."""
+    if service_account is None:
+        return False, None
+
+    project_id = service_account["projectId"]
+    email = service_account["email"]
+
+    member_id = "serviceAccount:" + email
+
+    required_permissions = set(required_permissions)
+    policy = crm.projects().getIamPolicy(resource=project_id, body={}).execute()
+    original_policy = copy.deepcopy(policy)
+    already_configured = True
+
+    logger.info(f"_configure_iam_role: Checking permissions for {email}...")
+
+    # Check the roles first, as checking the permission requires more API calls and
+    # permissions.
+    for role in required_roles:
+        role_exists = False
+        for binding in policy["bindings"]:
+            if binding["role"] == role:
+                if member_id not in binding["members"]:
+                    binding["members"].append(member_id)
+                    already_configured = False
+                role_exists = True
+
+        if not role_exists:
+            already_configured = False
+            policy["bindings"].append(
+                {
+                    "members": [member_id],
+                    "role": role,
+                }
+            )
+
+    if already_configured:
+        # In some managed environments, an admin needs to grant the
+        # roles, so only call setIamPolicy if needed.
+        return True, policy
+
+    for binding in original_policy["bindings"]:
+        if member_id in binding["members"]:
+            role = binding["role"]
+            try:
+                role_definition = iam.projects().roles().get(name=role).execute()
+            except TypeError as e:
+                if "does not match the pattern" in str(e):
+                    logger.info(
+                        f"_configure_iam_role: fail to check permission for built-in role {role}. skipped."
+                    )
+                    permissions = []
+                else:
+                    raise
+            else:
+                permissions = role_definition["includedPermissions"]
+            required_permissions -= set(permissions)
+        if not required_permissions:
+            break
+    if not required_permissions:
+        # All required permissions are already granted.
+        return True, policy
+    logger.info(f"_configure_iam_role: missing permisisons {required_permissions}")
+
+    return False, policy
+
+
 def _configure_iam_role(config, crm, iam):
     """Setup a gcp service account with IAM roles.
 
     Creates a gcp service acconut and binds IAM roles which allow it to control
     control storage/compute services. Specifically, the head node needs to have
     an IAM role that allows it to create further gce instances and store items
     in google cloud storage.
 
     TODO: Allow the name/id of the service account to be configured
     """
     config = copy.deepcopy(config)
 
-    email = SERVICE_ACCOUNT_EMAIL_TEMPLATE.format(
-        account_id=DEFAULT_SERVICE_ACCOUNT_ID,
+    email = SKYPILOT_SERVICE_ACCOUNT_EMAIL_TEMPLATE.format(
+        account_id=SKYPILOT_SERVICE_ACCOUNT_ID,
         project_id=config["provider"]["project_id"],
     )
     service_account = _get_service_account(email, config, iam)
 
-    if service_account is None:
+    permissions = VM_MINIMAL_PERMISSIONS
+    roles = DEFAULT_SERVICE_ACCOUNT_ROLES
+    if config["provider"].get(HAS_TPU_PROVIDER_FIELD, False):
+        roles = DEFAULT_SERVICE_ACCOUNT_ROLES + TPU_SERVICE_ACCOUNT_ROLES
+        permissions = VM_MINIMAL_PERMISSIONS + TPU_MINIMAL_PERMISSIONS
+
+    satisfied, policy = _is_permission_satisfied(
+        service_account, crm, iam, permissions, roles
+    )
+
+    if not satisfied:
+        # SkyPilot: Fallback to the old ray service account name for
+        # backwards compatibility. Users using GCP before #2112 have
+        # the old service account setup setup in their GCP project,
+        # and the user may not have the permissions to create the
+        # new service account. This is to ensure that the old service
+        # account is still usable.
+        email = SERVICE_ACCOUNT_EMAIL_TEMPLATE.format(
+            account_id=DEFAULT_SERVICE_ACCOUNT_ID,
+            project_id=config["provider"]["project_id"],
+        )
+        logger.info(f"_configure_iam_role: Fallback to service account {email}")
+
+        ray_service_account = _get_service_account(email, config, iam)
+        ray_satisfied, _ = _is_permission_satisfied(
+            ray_service_account, crm, iam, permissions, roles
+        )
         logger.info(
             "_configure_iam_role: "
-            "Creating new service account {}".format(DEFAULT_SERVICE_ACCOUNT_ID)
+            f"Fallback to service account {email} succeeded? {ray_satisfied}"
         )
 
-        service_account = _create_service_account(
-            DEFAULT_SERVICE_ACCOUNT_ID, DEFAULT_SERVICE_ACCOUNT_CONFIG, config, iam
-        )
+        if ray_satisfied:
+            service_account = ray_service_account
+            satisfied = ray_satisfied
+        elif service_account is None:
+            logger.info(
+                "_configure_iam_role: "
+                "Creating new service account {}".format(SKYPILOT_SERVICE_ACCOUNT_ID)
+            )
+            # SkyPilot: a GCP user without the permission to create a service
+            # account will fail here.
+            service_account = _create_service_account(
+                SKYPILOT_SERVICE_ACCOUNT_ID,
+                SKYPILOT_SERVICE_ACCOUNT_CONFIG,
+                config,
+                iam,
+            )
+            satisfied, policy = _is_permission_satisfied(
+                service_account, crm, iam, permissions, roles
+            )
 
     assert service_account is not None, "Failed to create service account"
 
-    if config["provider"].get(HAS_TPU_PROVIDER_FIELD, False):
-        roles = DEFAULT_SERVICE_ACCOUNT_ROLES + TPU_SERVICE_ACCOUNT_ROLES
-    else:
-        roles = DEFAULT_SERVICE_ACCOUNT_ROLES
-
-    _add_iam_policy_binding(service_account, roles, crm)
+    if not satisfied:
+        logger.info(
+            "_configure_iam_role: " f"Adding roles to service account {email}..."
+        )
+        _add_iam_policy_binding(service_account, policy, crm, iam)
 
     account_dict = {
         "email": service_account["email"],
         # NOTE: The amount of access is determined by the scope + IAM
         # role of the service account. Even if the cloud-platform scope
         # gives (scope) access to the whole cloud-platform, the service
         # account is limited by the IAM rights specified below.
-        "scopes": ["https://www.googleapis.com/auth/cloud-platform"]
+        "scopes": ["https://www.googleapis.com/auth/cloud-platform"],
     }
     if _is_head_node_a_tpu(config):
         # SKY: The API for TPU VM is slightly different from normal compute instances.
         # See https://cloud.google.com/tpu/docs/reference/rest/v2alpha1/projects.locations.nodes#Node
         account_dict["scope"] = account_dict["scopes"]
         account_dict.pop("scopes")
         config["head_node"]["serviceAccount"] = account_dict
@@ -484,14 +618,184 @@
     )
 
     config["auth"]["ssh_private_key"] = private_key_path
 
     return config
 
 
+def _check_firewall_rules(vpc_name, config, compute):
+    """Check if the firewall rules in the VPC are sufficient."""
+    required_rules = FIREWALL_RULES_REQUIRED.copy()
+
+    operation = compute.networks().getEffectiveFirewalls(
+        project=config["provider"]["project_id"], network=vpc_name
+    )
+    response = operation.execute()
+    if len(response) == 0:
+        return False
+    effective_rules = response["firewalls"]
+
+    def _merge_and_refine_rule(rules):
+        """Returns the reformatted rules from the firewall rules
+
+        The function translates firewall rules fetched from the cloud provider
+        to a format for simple comparison.
+
+        Example of firewall rules from the cloud:
+        [
+            {
+                ...
+                "direction": "INGRESS",
+                "allowed": [
+                    {"IPProtocol": "tcp", "ports": ['80', '443']},
+                    {"IPProtocol": "udp", "ports": ['53']},
+                ],
+                "sourceRanges": ["10.128.0.0/9"],
+            },
+            {
+                ...
+                "direction": "INGRESS",
+                "allowed": [{
+                    "IPProtocol": "tcp",
+                    "ports": ["22"],
+                }],
+                "sourceRanges": ["0.0.0.0/0"],
+            },
+        ]
+
+        Returns:
+            source2rules: Dict[(direction, sourceRanges) -> Dict(protocol -> Set[ports])]
+                Example {
+                    ("INGRESS", "10.128.0.0/9"): {"tcp": {80, 443}, "udp": {53}},
+                    ("INGRESS", "0.0.0.0/0"): {"tcp": {22}},
+                }
+        """
+        source2rules: Dict[Tuple[str, str], Dict[str, Set[int]]] = {}
+        source2allowed_list: Dict[Tuple[str, str], List[Dict[str, str]]] = {}
+        for rule in rules:
+            direction = rule.get("direction", "")
+            sources = rule.get("sourceRanges", [])
+            allowed = rule.get("allowed", [])
+            for source in sources:
+                key = (direction, source)
+                source2allowed_list[key] = source2allowed_list.get(key, []) + allowed
+        for direction_source, allowed_list in source2allowed_list.items():
+            source2rules[direction_source] = {}
+            for allowed in allowed_list:
+                # Example of port_list: ["20", "50-60"]
+                # If list is empty, it means all ports
+                port_list = allowed.get("ports", [])
+                port_set = set()
+                if port_list == []:
+                    port_set.update(set(range(1, 65536)))
+                else:
+                    for port_range in port_list:
+                        parse_ports = port_range.split("-")
+                        if len(parse_ports) == 1:
+                            port_set.add(int(parse_ports[0]))
+                        else:
+                            assert (
+                                len(parse_ports) == 2
+                            ), f"Failed to parse the port range: {port_range}"
+                            port_set.update(
+                                set(range(int(parse_ports[0]), int(parse_ports[1]) + 1))
+                            )
+                if allowed["IPProtocol"] not in source2rules[direction_source]:
+                    source2rules[direction_source][allowed["IPProtocol"]] = set()
+                source2rules[direction_source][allowed["IPProtocol"]].update(port_set)
+        return source2rules
+
+    effective_rules = _merge_and_refine_rule(effective_rules)
+    required_rules = _merge_and_refine_rule(required_rules)
+
+    for direction_source, allowed_req in required_rules.items():
+        if direction_source not in effective_rules:
+            return False
+        allowed_eff = effective_rules[direction_source]
+        # Special case: "all" means allowing all traffic
+        if "all" in allowed_eff:
+            continue
+        # Check if the required ports are a subset of the effective ports
+        for protocol, ports_req in allowed_req.items():
+            ports_eff = allowed_eff.get(protocol, set())
+            if not ports_req.issubset(ports_eff):
+                return False
+    return True
+
+
+def get_usable_vpc(config):
+    """Return a usable VPC.
+
+    If not found, create a new one with sufficient firewall rules.
+    """
+    _, _, compute, _ = construct_clients_from_provider_config(config["provider"])
+
+    # For backward compatibility, reuse the VPC if the VM is launched.
+    resource = GCPCompute(
+        compute,
+        config["provider"]["project_id"],
+        config["provider"]["availability_zone"],
+        config["cluster_name"],
+    )
+    node = resource._list_instances(label_filters=None, status_filter=None)
+    if len(node) > 0:
+        netInterfaces = node[0].get("networkInterfaces", [])
+        if len(netInterfaces) > 0:
+            vpc_name = netInterfaces[0]["network"].split("/")[-1]
+            return vpc_name
+
+    vpcnets_all = _list_vpcnets(config, compute)
+
+    usable_vpc_name = None
+    for vpc in vpcnets_all:
+        if _check_firewall_rules(vpc["name"], config, compute):
+            usable_vpc_name = vpc["name"]
+            break
+
+    if usable_vpc_name is None:
+        logger.info(f"Creating a default VPC network, {SKYPILOT_VPC_NAME}...")
+
+        proj_id = config["provider"]["project_id"]
+        # Create a SkyPilot VPC network if it doesn't exist
+        vpc_list = _list_vpcnets(config, compute, filter=f"name={SKYPILOT_VPC_NAME}")
+        if len(vpc_list) == 0:
+            body = VPC_TEMPLATE.copy()
+            body["name"] = body["name"].format(VPC_NAME=SKYPILOT_VPC_NAME)
+            body["selfLink"] = body["selfLink"].format(
+                PROJ_ID=proj_id, VPC_NAME=SKYPILOT_VPC_NAME
+            )
+            _create_vpcnet(config, compute, body)
+
+        # Create firewall rules
+        for rule in FIREWALL_RULES_TEMPLATE:
+            # Query firewall rule by its name (unique in a project).
+            # If the rule already exists, delete it first.
+            rule_name = rule["name"].format(VPC_NAME=SKYPILOT_VPC_NAME)
+            rule_list = _list_firewall_rules(
+                config, compute, filter=f"(name={rule_name})"
+            )
+            if len(rule_list) > 0:
+                _delete_firewall_rule(config, compute, rule_name)
+
+            body = rule.copy()
+            body["name"] = body["name"].format(VPC_NAME=SKYPILOT_VPC_NAME)
+            body["network"] = body["network"].format(
+                PROJ_ID=proj_id, VPC_NAME=SKYPILOT_VPC_NAME
+            )
+            body["selfLink"] = body["selfLink"].format(
+                PROJ_ID=proj_id, VPC_NAME=SKYPILOT_VPC_NAME
+            )
+            _create_firewall_rule(config, compute, body)
+
+        usable_vpc_name = SKYPILOT_VPC_NAME
+        logger.info(f"A VPC network {SKYPILOT_VPC_NAME} created.")
+
+    return usable_vpc_name
+
+
 def _configure_subnet(config, compute):
     """Pick a reasonable subnet if not specified by the config."""
     config = copy.deepcopy(config)
 
     node_configs = [
         node_type["node_config"]
         for node_type in config["available_node_types"].values()
@@ -502,22 +806,17 @@
     # networkInterfaces is compute, networkConfig is TPU
     if all(
         "networkInterfaces" in node_config or "networkConfig" in node_config
         for node_config in node_configs
     ):
         return config
 
-    subnets = _list_subnets(config, compute)
-
-    if not subnets:
-        raise NotImplementedError("Should be able to create subnet.")
-
-    # TODO: make sure that we have usable subnet. Maybe call
-    # compute.subnetworks().listUsable? For some reason it didn't
-    # work out-of-the-box
+    # SkyPilot: make sure there's a usable VPC
+    usable_vpc_name = get_usable_vpc(config)
+    subnets = _list_subnets(config, compute, filter=f'(name="{usable_vpc_name}")')
     default_subnet = subnets[0]
 
     default_interfaces = [
         {
             "subnetwork": default_subnet["selfLink"],
             "accessConfigs": [
                 {
@@ -538,25 +837,87 @@
         if "networkConfig" not in node_config:
             node_config["networkConfig"] = copy.deepcopy(default_interfaces)[0]
             node_config["networkConfig"].pop("accessConfigs")
 
     return config
 
 
-def _list_subnets(config, compute):
+def _create_firewall_rule(config, compute, body):
+    operation = (
+        compute.firewalls()
+        .insert(project=config["provider"]["project_id"], body=body)
+        .execute()
+    )
+    response = wait_for_compute_global_operation(
+        config["provider"]["project_id"], operation, compute
+    )
+    return response
+
+
+def _delete_firewall_rule(config, compute, name):
+    operation = (
+        compute.firewalls()
+        .delete(project=config["provider"]["project_id"], firewall=name)
+        .execute()
+    )
+    response = wait_for_compute_global_operation(
+        config["provider"]["project_id"], operation, compute
+    )
+    return response
+
+
+def _list_firewall_rules(config, compute, filter=None):
+    response = (
+        compute.firewalls()
+        .list(
+            project=config["provider"]["project_id"],
+            filter=filter,
+        )
+        .execute()
+    )
+    return response["items"] if "items" in response else []
+
+
+def _create_vpcnet(config, compute, body):
+    operation = (
+        compute.networks()
+        .insert(project=config["provider"]["project_id"], body=body)
+        .execute()
+    )
+    response = wait_for_compute_global_operation(
+        config["provider"]["project_id"], operation, compute
+    )
+    return response
+
+
+def _list_vpcnets(config, compute, filter=None):
+    response = (
+        compute.networks()
+        .list(
+            project=config["provider"]["project_id"],
+            filter=filter,
+        )
+        .execute()
+    )
+
+    return response["items"] if "items" in response else []
+
+
+def _list_subnets(config, compute, filter=None):
     response = (
         compute.subnetworks()
         .list(
             project=config["provider"]["project_id"],
             region=config["provider"]["region"],
+            filter=filter,
         )
         .execute()
     )
 
-    return response["items"]
+    return response["items"] if "items" in response else []
 
 
 def _get_subnet(config, subnet_id, compute):
     subnet = (
         compute.subnetworks()
         .get(
             project=config["provider"]["project_id"],
@@ -596,15 +957,18 @@
     project_id = config["provider"]["project_id"]
     full_name = "projects/{project_id}/serviceAccounts/{account}".format(
         project_id=project_id, account=account
     )
     try:
         service_account = iam.projects().serviceAccounts().get(name=full_name).execute()
     except errors.HttpError as e:
-        if e.resp.status != 404:
+        if e.resp.status not in [403, 404]:
+            # SkyPilot: added 403, which means the service account doesn't exist,
+            # or not accessible by the current account, which is fine, as we do the
+            # fallback in the caller.
             raise
         service_account = None
 
     return service_account
 
 
 def _create_service_account(account_id, account_config, config, iam):
@@ -622,45 +986,17 @@
         )
         .execute()
     )
 
     return service_account
 
 
-def _add_iam_policy_binding(service_account, roles, crm):
+def _add_iam_policy_binding(service_account, policy, crm, iam):
     """Add new IAM roles for the service account."""
     project_id = service_account["projectId"]
-    email = service_account["email"]
-    member_id = "serviceAccount:" + email
-
-    policy = crm.projects().getIamPolicy(resource=project_id, body={}).execute()
-
-    already_configured = True
-    for role in roles:
-        role_exists = False
-        for binding in policy["bindings"]:
-            if binding["role"] == role:
-                if member_id not in binding["members"]:
-                    binding["members"].append(member_id)
-                    already_configured = False
-                role_exists = True
-
-        if not role_exists:
-            already_configured = False
-            policy["bindings"].append(
-                {
-                    "members": [member_id],
-                    "role": role,
-                }
-            )
-
-    if already_configured:
-        # In some managed environments, an admin needs to grant the
-        # roles, so only call setIamPolicy if needed.
-        return
 
     result = (
         crm.projects()
         .setIamPolicy(
             resource=project_id,
             body={
                 "policy": policy,
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/skylet/providers/gcp/node.py` & `skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/gcp/node.py`

 * *Files 2% similar despite different names*

```diff
@@ -46,15 +46,15 @@
 INSTANCE_NAME_UUID_LEN = 8
 MAX_POLLS = 12
 # TPUs take a long while to respond, so we increase the MAX_POLLS
 # considerably - this probably could be smaller
 # TPU deletion uses MAX_POLLS
 MAX_POLLS_TPU = MAX_POLLS * 8
 # Stopping instances can take several minutes, so we increase the timeout
-MAX_POLLS_STOP =  MAX_POLLS * 8
+MAX_POLLS_STOP = MAX_POLLS * 8
 POLL_INTERVAL = 5
 
 
 def _retry_on_exception(
     exception: Union[Exception, Tuple[Exception]],
     regex: Optional[str] = None,
     max_retries: int = MAX_POLLS,
@@ -313,24 +313,21 @@
             ]
         else:
             results = operations
 
         return results
 
     @abc.abstractmethod
-    def start_instance(self, node_id: str,
-                       wait_for_operation: bool = True) -> dict:
+    def start_instance(self, node_id: str, wait_for_operation: bool = True) -> dict:
         """Start an instance and return result."""
 
     @abc.abstractmethod
-    def stop_instance(self, node_id: str,
-                      wait_for_operation: bool = True) -> dict:
+    def stop_instance(self, node_id: str, wait_for_operation: bool = True) -> dict:
         """Stop an instance and return result."""
 
-
     @abc.abstractmethod
     def delete_instance(self, node_id: str, wait_for_operation: bool = True) -> dict:
         """Deletes an instance and returns result."""
         return
 
 
 class GCPCompute(GCPResource):
@@ -368,21 +365,24 @@
                 )
                 break
 
             time.sleep(poll_interval)
 
         return result
 
-    def list_instances(self, label_filters: Optional[dict] = None,
-                       ) -> List[GCPComputeNode]:
+    def list_instances(
+        self,
+        label_filters: Optional[dict] = None,
+    ) -> List[GCPComputeNode]:
         non_terminated_status = list(GCPComputeNode.NON_TERMINATED_STATUSES)
         return self._list_instances(label_filters, non_terminated_status)
 
-    def _list_instances(self, label_filters: Optional[dict],
-                        status_filter: List[str]) -> List[GCPComputeNode]:
+    def _list_instances(
+        self, label_filters: Optional[dict], status_filter: Optional[List[str]]
+    ) -> List[GCPComputeNode]:
         label_filters = label_filters or {}
 
         if label_filters:
             label_filter_expr = (
                 "("
                 + " AND ".join(
                     [
@@ -391,24 +391,27 @@
                     ]
                 )
                 + ")"
             )
         else:
             label_filter_expr = ""
 
-        instance_state_filter_expr = (
-            "("
-            + " OR ".join(
-                [
-                    "(status = {status})".format(status=status)
-                    for status in status_filter
-                ]
+        if status_filter:
+            instance_state_filter_expr = (
+                "("
+                + " OR ".join(
+                    [
+                        "(status = {status})".format(status=status)
+                        for status in status_filter
+                    ]
+                )
+                + ")"
             )
-            + ")"
-        )
+        else:
+            instance_state_filter_expr = ""
 
         cluster_name_filter_expr = "(labels.{key} = {value})".format(
             key=TAG_RAY_CLUSTER_NAME, value=self.cluster_name
         )
 
         not_empty_filters = [
             f
@@ -559,45 +562,51 @@
 
         if wait_for_operation:
             result = self.wait_for_operation(operation)
         else:
             result = operation
 
         return result, name
-    def start_instance(self, node_id: str,
-                       wait_for_operation: bool = True) -> dict:
-        operation = self.resource.instances().start(
-            project=self.project_id,
-            zone=self.availability_zone,
-            instance=node_id,
-        ).execute()
+
+    def start_instance(self, node_id: str, wait_for_operation: bool = True) -> dict:
+        operation = (
+            self.resource.instances()
+            .start(
+                project=self.project_id,
+                zone=self.availability_zone,
+                instance=node_id,
+            )
+            .execute()
+        )
 
         if wait_for_operation:
             result = self.wait_for_operation(operation)
         else:
             result = operation
 
         return result
 
-    def stop_instance(self, node_id: str,
-                      wait_for_operation: bool = True) -> dict:
-        operation = self.resource.instances().stop(
-            project=self.project_id,
-            zone=self.availability_zone,
-            instance=node_id,
-        ).execute()
+    def stop_instance(self, node_id: str, wait_for_operation: bool = True) -> dict:
+        operation = (
+            self.resource.instances()
+            .stop(
+                project=self.project_id,
+                zone=self.availability_zone,
+                instance=node_id,
+            )
+            .execute()
+        )
 
         if wait_for_operation:
             result = self.wait_for_operation(operation)
         else:
             result = operation
 
         return result
 
-
     def delete_instance(self, node_id: str, wait_for_operation: bool = True) -> dict:
         operation = (
             self.resource.instances()
             .delete(
                 project=self.project_id,
                 zone=self.availability_zone,
                 instance=node_id,
@@ -656,24 +665,30 @@
 
         return result
 
     def list_instances(self, label_filters: Optional[dict] = None) -> List[GCPTPUNode]:
         non_terminated_status = list(GCPTPUNode.NON_TERMINATED_STATUSES)
         return self._list_instances(label_filters, non_terminated_status)
 
-    def _list_instances(self, label_filters: Optional[dict],
-                    status_filter: Optional[List[str]]) -> List[GCPTPUNode]:
+    def _list_instances(
+        self, label_filters: Optional[dict], status_filter: Optional[List[str]]
+    ) -> List[GCPTPUNode]:
         try:
-            response = self.resource.projects().locations().nodes().list(
-                parent=self.path).execute()
+            response = (
+                self.resource.projects()
+                .locations()
+                .nodes()
+                .list(parent=self.path)
+                .execute()
+            )
         except HttpError as e:
             # SKY: Catch HttpError when accessing unauthorized region.
             # Return empty list instead of raising exception to not break
             # ray down.
-            logger.warning(f'googleapiclient.errors.HttpError: {e.reason}')
+            logger.warning(f"googleapiclient.errors.HttpError: {e.reason}")
             return []
 
         instances = response.get("nodes", [])
         instances = [GCPTPUNode(i, self) for i in instances]
 
         # filter_expr cannot be passed directly to API
         # so we need to filter the results ourselves
@@ -759,48 +774,54 @@
             config["networkConfig"] = {}
         if "enableExternalIps" not in config["networkConfig"]:
             # this is required for SSH to work, per google documentation
             # https://cloud.google.com/tpu/docs/users-guide-tpu-vm#create-curl
             config["networkConfig"]["enableExternalIps"] = True
 
         try:
-            operation = self.resource.projects().locations().nodes().create(
-                parent=self.path,
-                body=config,
-                nodeId=name,
-            ).execute()
+            operation = (
+                self.resource.projects()
+                .locations()
+                .nodes()
+                .create(
+                    parent=self.path,
+                    body=config,
+                    nodeId=name,
+                )
+                .execute()
+            )
         except HttpError as e:
             # SKY: Catch HttpError when accessing unauthorized region.
-            logger.error(f'googleapiclient.errors.HttpError: {e.reason}')
+            logger.error(f"googleapiclient.errors.HttpError: {e.reason}")
             raise e
 
         if wait_for_operation:
             result = self.wait_for_operation(operation)
         else:
             result = operation
 
         return result, name
 
-    def start_instance(self, node_id: str,
-                       wait_for_operation: bool = True) -> dict:
-        operation = self.resource.projects().locations().nodes().start(
-            name=node_id).execute()
+    def start_instance(self, node_id: str, wait_for_operation: bool = True) -> dict:
+        operation = (
+            self.resource.projects().locations().nodes().start(name=node_id).execute()
+        )
 
         # No need to increase MAX_POLLS for deletion
         if wait_for_operation:
             result = self.wait_for_operation(operation, max_polls=MAX_POLLS)
         else:
             result = operation
 
         return result
 
-    def stop_instance(self, node_id: str,
-                      wait_for_operation: bool = True) -> dict:
-        operation = self.resource.projects().locations().nodes().stop(
-            name=node_id).execute()
+    def stop_instance(self, node_id: str, wait_for_operation: bool = True) -> dict:
+        operation = (
+            self.resource.projects().locations().nodes().stop(name=node_id).execute()
+        )
 
         # No need to increase MAX_POLLS for deletion
         if wait_for_operation:
             result = self.wait_for_operation(operation, max_polls=MAX_POLLS)
         else:
             result = operation
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/skylet/providers/gcp/node_provider.py` & `skypilot-nightly-1.0.0.dev20230713/sky/skylet/providers/gcp/node_provider.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,24 +1,27 @@
 import logging
 import time
 import copy
 from functools import wraps
 from threading import RLock
-from typing import Dict, List, Tuple
+from typing import Dict, List
 
 import googleapiclient
 
 from sky.skylet.providers.gcp.config import (
     bootstrap_gcp,
     construct_clients_from_provider_config,
     get_node_type,
 )
 
-from ray.autoscaler.tags import (TAG_RAY_LAUNCH_CONFIG, TAG_RAY_NODE_KIND,
-                                 TAG_RAY_USER_NODE_TYPE)
+from ray.autoscaler.tags import (
+    TAG_RAY_LAUNCH_CONFIG,
+    TAG_RAY_NODE_KIND,
+    TAG_RAY_USER_NODE_TYPE,
+)
 from ray.autoscaler._private.cli_logger import cf, cli_logger
 
 
 # The logic has been abstracted away here to allow for different GCP resources
 # (API endpoints), which can differ widely, making it impossible to use
 # the same logic for everything.
 from sky.skylet.providers.gcp.node import (  # noqa
@@ -71,16 +74,15 @@
         NodeProvider.__init__(self, provider_config, cluster_name)
         self.lock = RLock()
         self._construct_clients()
 
         # Cache of node objects from the last nodes() call. This avoids
         # excessive DescribeInstances requests.
         self.cached_nodes: Dict[str, GCPNode] = {}
-        self.cache_stopped_nodes = provider_config.get("cache_stopped_nodes",
-                                                       True)
+        self.cache_stopped_nodes = provider_config.get("cache_stopped_nodes", True)
 
     def _construct_clients(self):
         _, _, compute, tpu = construct_clients_from_provider_config(
             self.provider_config
         )
 
         # Dict of different resources provided by GCP.
@@ -178,40 +180,95 @@
     def create_node(self, base_config: dict, tags: dict, count: int) -> Dict[str, dict]:
         """Creates instances.
 
         Returns dict mapping instance id to each create operation result for the created
         instances.
         """
         with self.lock:
-            result_dict = dict()
+            result_dict = {}
             labels = tags  # gcp uses "labels" instead of aws "tags"
             labels = dict(sorted(copy.deepcopy(labels).items()))
 
-
             node_type = get_node_type(base_config)
             resource = self.resources[node_type]
 
             # Try to reuse previously stopped nodes with compatible configs
             if self.cache_stopped_nodes:
                 filters = {
                     TAG_RAY_NODE_KIND: labels[TAG_RAY_NODE_KIND],
-                    TAG_RAY_LAUNCH_CONFIG: labels[TAG_RAY_LAUNCH_CONFIG]
+                    # SkyPilot: removed TAG_RAY_LAUNCH_CONFIG to allow reusing nodes
+                    # with different launch configs.
+                    # Reference: https://github.com/skypilot-org/skypilot/pull/1671
                 }
                 # This tag may not always be present.
                 if TAG_RAY_USER_NODE_TYPE in labels:
                     filters[TAG_RAY_USER_NODE_TYPE] = labels[TAG_RAY_USER_NODE_TYPE]
+                filters_with_launch_config = copy.copy(filters)
+                filters_with_launch_config[TAG_RAY_LAUNCH_CONFIG] = labels[
+                    TAG_RAY_LAUNCH_CONFIG
+                ]
+
                 # SKY: "TERMINATED" for compute VM, "STOPPED" for TPU VM
                 # "STOPPING" means the VM is being stopped, which needs
                 # to be included to avoid creating a new VM.
                 if isinstance(resource, GCPCompute):
                     STOPPED_STATUS = ["TERMINATED", "STOPPING"]
                 else:
                     STOPPED_STATUS = ["STOPPED", "STOPPING"]
-                reuse_nodes = resource._list_instances(
-                    filters, STOPPED_STATUS)[:count]
+
+                # SkyPilot: We try to use the instances with the same matching launch_config first. If
+                # there is not enough instances with matching launch_config, we then use all the
+                # instances with the same matching launch_config plus some instances with wrong
+                # launch_config.
+                def get_order_key(node):
+                    import datetime
+
+                    timestamp = node.get("lastStartTimestamp")
+                    if timestamp is not None:
+                        return datetime.datetime.strptime(
+                            timestamp, "%Y-%m-%dT%H:%M:%S.%f%z"
+                        )
+                    return node.id
+
+                nodes_matching_launch_config = resource._list_instances(
+                    filters_with_launch_config, STOPPED_STATUS
+                )
+                nodes_matching_launch_config.sort(
+                    key=lambda n: get_order_key(n), reverse=True
+                )
+                if len(nodes_matching_launch_config) >= count:
+                    reuse_nodes = nodes_matching_launch_config[:count]
+                else:
+                    nodes_all = resource._list_instances(filters, STOPPED_STATUS)
+                    nodes_matching_launch_config_ids = set(
+                        n.id for n in nodes_matching_launch_config
+                    )
+                    nodes_non_matching_launch_config = [
+                        n
+                        for n in nodes_all
+                        if n.id not in nodes_matching_launch_config_ids
+                    ]
+                    # This is for backward compatibility, where the uesr already has leaked
+                    # stopped nodes with the different launch config before update to #1671,
+                    # and the total number of the leaked nodes is greater than the number of
+                    # nodes to be created. With this, we will make sure we will reuse the
+                    # most recently used nodes.
+                    # This can be removed in the future when we are sure all the users
+                    # have updated to #1671.
+                    nodes_non_matching_launch_config.sort(
+                        key=lambda n: get_order_key(n), reverse=True
+                    )
+                    reuse_nodes = (
+                        nodes_matching_launch_config + nodes_non_matching_launch_config
+                    )
+                    # The total number of reusable nodes can be less than the number of nodes to be created.
+                    # This `[:count]` is fine, as it will get all the reusable nodes, even if there are
+                    # less nodes.
+                    reuse_nodes = reuse_nodes[:count]
+
                 reuse_node_ids = [n.id for n in reuse_nodes]
                 if reuse_nodes:
                     # TODO(suquark): Some instances could still be stopping.
                     # We may wait until these instances stop.
                     cli_logger.print(
                         # TODO: handle plural vs singular?
                         f"Reusing nodes {cli_logger.render_list(reuse_node_ids)}. "
@@ -222,21 +279,23 @@
                         result = resource.start_instance(node_id)
                         result_dict[node_id] = {node_id: result}
                     for node_id in reuse_node_ids:
                         self.set_node_tags(node_id, tags)
                     count -= len(reuse_node_ids)
             if count:
                 results = resource.create_instances(base_config, labels, count)
-                result_dict.update({instance_id: result for result, instance_id in results})
+                result_dict.update(
+                    {instance_id: result for result, instance_id in results}
+                )
             return result_dict
 
-
     @_retry
     def terminate_node(self, node_id: str):
         with self.lock:
+            result = None
             resource = self._get_resource_depending_on_node_name(node_id)
             try:
                 if self.cache_stopped_nodes:
                     cli_logger.print(
                         f"Stopping instance {node_id} "
                         + cf.dimmed(
                             "(to terminate instead, "
@@ -253,35 +312,39 @@
                         instance = resource.get_instance(node_id=node_id)
                         if instance.is_stopped():
                             logger.info(f"Instance {node_id} is stopped.")
                             break
                         elif instance.is_stopping():
                             time.sleep(POLL_INTERVAL)
                         else:
-                            raise RuntimeError(f"Unexpected instance status."
-                                               " Details: {instance}")
+                            raise RuntimeError(
+                                f"Unexpected instance status." " Details: {instance}"
+                            )
 
                     if instance.is_stopping():
-                        raise RuntimeError(f"Maximum number of polls: "
-                                           f"{MAX_POLLS_STOP} reached. "
-                                           f"Instance {node_id} is still in "
-                                           "STOPPING status.")
+                        raise RuntimeError(
+                            f"Maximum number of polls: "
+                            f"{MAX_POLLS_STOP} reached. "
+                            f"Instance {node_id} is still in "
+                            "STOPPING status."
+                        )
                 else:
                     result = resource.delete_instance(
                         node_id=node_id,
                     )
             except googleapiclient.errors.HttpError as http_error:
                 if http_error.resp.status == 404:
                     logger.warning(
                         f"Tried to delete the node with id {node_id} "
                         "but it was already gone."
                     )
                 else:
                     raise http_error from None
-            return result
+
+        return result
 
     @_retry
     def _get_node(self, node_id: str) -> GCPNode:
         self.non_terminated_nodes({})  # Side effect: updates cache
 
         with self.lock:
             if node_id in self.cached_nodes:
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/skylet/ray_patches/__init__.py` & `skypilot-nightly-1.0.0.dev20230713/sky/skylet/ray_patches/__init__.py`

 * *Files 21% similar despite different names*

```diff
@@ -10,15 +10,15 @@
 
 To get original versions, go to the Ray branch with version:
 
   sky.constants.SKY_REMOTE_RAY_VERSION
 
 Example workflow:
 
-  >> wget https://raw.githubusercontent.com/ray-project/ray/releases/2.0.1/python/ray/autoscaler/_private/command_runner.py
+  >> wget https://raw.githubusercontent.com/ray-project/ray/releases/2.4.0/python/ray/autoscaler/_private/command_runner.py
   >> cp command_runner.py command_runner.py.1
 
   >> # Make some edits to command_runner.py.1...
 
   >> diff command_runner.py command_runner.py.1 >command_runner.py.patch
 
   >> # Inspect command_runner.py.patch.
@@ -65,35 +65,22 @@
 
     from ray._private import worker
     _run_patch(worker.__file__, _to_absolute('worker.py.patch'))
 
     from ray.dashboard.modules.job import cli
     _run_patch(cli.__file__, _to_absolute('cli.py.patch'))
 
-    from ray.dashboard.modules.job import job_manager
-    _run_patch(job_manager.__file__, _to_absolute('job_manager.py.patch'))
-
     from ray.autoscaler._private import autoscaler
     _run_patch(autoscaler.__file__, _to_absolute('autoscaler.py.patch'))
 
     from ray.autoscaler._private import command_runner
     _run_patch(command_runner.__file__, _to_absolute('command_runner.py.patch'))
 
     from ray.autoscaler._private import resource_demand_scheduler
     _run_patch(resource_demand_scheduler.__file__,
                _to_absolute('resource_demand_scheduler.py.patch'))
 
     from ray.autoscaler._private import updater
     _run_patch(updater.__file__, _to_absolute('updater.py.patch'))
 
-    # Fix the Azure get-access-token (used by ray azure node_provider) timeout issue,
-    # by increasing the timeout.
-    # Tracked in https://github.com/Azure/azure-cli/issues/20404#issuecomment-1249575110
-    # Only patch it if azure cli is installed.
-    try:
-        import azure
-        from azure.identity._credentials import azure_cli
-        version = pkg_resources.get_distribution('azure-cli').version
-        _run_patch(azure_cli.__file__, _to_absolute('azure_cli.py.patch'),
-                   version)
-    except ImportError:
-        pass
+    from ray.dashboard.modules.job import job_head
+    _run_patch(job_head.__file__, _to_absolute('job_head.py.patch'))
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/skylet/ray_patches/resource_demand_scheduler.py.patch` & `skypilot-nightly-1.0.0.dev20230713/sky/skylet/ray_patches/resource_demand_scheduler.py.patch`

 * *Files 22% similar despite different names*

```diff
@@ -1,17 +1,17 @@
 0a1,5
-> # From https://github.com/ray-project/ray/blob/ray-2.0.1/python/ray/autoscaler/_private/resource_demand_scheduler.py
+> # From https://github.com/ray-project/ray/blob/ray-2.4.0/python/ray/autoscaler/_private/resource_demand_scheduler.py
 > # Sky patch changes:
 > #  - no new nodes are allowed to be launched launched when the upscaling_speed is 0
 > #  - comment out "assert not unfulfilled": this seems a buggy assert
 > 
-509c514,517
+450c455,458
 <             if upper_bound > 0:
 ---
 >             # NOTE(sky): do not autoscale when upsclaing speed is 0.
 >             if self.upscaling_speed == 0:
 >                 upper_bound = 0
 >             if upper_bound >= 0:
-646c654
+594c602
 <             assert not unfulfilled
 ---
 >             # assert not unfulfilled  # NOTE(sky): buggy assert.
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/skylet/skylet.py` & `skypilot-nightly-1.0.0.dev20230713/sky/skylet/skylet.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,20 +1,23 @@
 """skylet: a daemon running on the head node of a cluster."""
 
 import time
 
 from sky import sky_logging
 from sky.skylet import events
 
-logger = sky_logging.init_logger(__name__)
+# Use the explicit logger name so that the logger is under the
+# `sky.skylet.skylet` namespace when executed directly, so as
+# to inherit the setup from the `sky` logger.
+logger = sky_logging.init_logger('sky.skylet.skylet')
 logger.info('skylet started')
 
 EVENTS = [
     events.AutostopEvent(),
-    events.JobUpdateEvent(),
+    events.JobSchedulerEvent(),
     # The spot job update event should be after the job update event.
     # Otherwise, the abnormal spot job status update will be delayed
     # until the next job update event.
     events.SpotJobUpdateEvent(),
 ]
 
 while True:
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/skylet/subprocess_daemon.py` & `skypilot-nightly-1.0.0.dev20230713/sky/skylet/subprocess_daemon.py`

 * *Files 4% similar despite different names*

```diff
@@ -6,14 +6,16 @@
 
 import argparse
 import requests
 import sys
 import time
 
 import psutil
+
+from sky.skylet import job_lib
 from ray.dashboard.modules.job import common as job_common
 from ray.dashboard.modules.job import sdk as job_sdk
 
 if __name__ == '__main__':
 
     parser = argparse.ArgumentParser()
     parser.add_argument('--parent-pid', type=int, required=True)
@@ -38,15 +40,16 @@
     # If Local Ray job id is passed in, wait until the job
     # is done/cancelled/failed.
     if local_ray_job_id is None:
         wait_for_process = True
     else:
         try:
             # Polls the Job submission client to check job status.
-            client = job_sdk.JobSubmissionClient('http://127.0.0.1:8265')
+            port = job_lib.get_job_submission_port()
+            client = job_sdk.JobSubmissionClient(f'http://127.0.0.1:{port}')
             while True:
                 status_info = client.get_job_status(local_ray_job_id)
                 status = status_info.status
                 if status in {
                         job_common.JobStatus.SUCCEEDED,
                         job_common.JobStatus.STOPPED,
                         job_common.JobStatus.FAILED
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/spot/recovery_strategy.py` & `skypilot-nightly-1.0.0.dev20230713/sky/spot/recovery_strategy.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,37 +1,61 @@
 """The strategy to handle launching/recovery/termination of spot clusters."""
 import time
+import traceback
 import typing
-from typing import Optional
+from typing import Optional, Tuple
 
 import sky
 from sky import exceptions
 from sky import global_user_state
 from sky import sky_logging
+from sky import status_lib
+from sky import backends
 from sky.backends import backend_utils
 from sky.skylet import job_lib
 from sky.spot import spot_utils
 from sky.usage import usage_lib
 from sky.utils import common_utils
 from sky.utils import ux_utils
 
 if typing.TYPE_CHECKING:
-    from sky import backends
     from sky import task as task_lib
 
 logger = sky_logging.init_logger(__name__)
 
-SPOT_STRATEGIES = dict()
+SPOT_STRATEGIES = {}
 SPOT_DEFAULT_STRATEGY = None
 
 # Waiting time for job from INIT/PENDING to RUNNING
 # 10 * JOB_STARTED_STATUS_CHECK_GAP_SECONDS = 10 * 5 = 50 seconds
 MAX_JOB_CHECKING_RETRY = 10
 
 
+def terminate_cluster(cluster_name: str, max_retry: int = 3) -> None:
+    """Terminate the spot cluster."""
+    retry_cnt = 0
+    while True:
+        try:
+            usage_lib.messages.usage.set_internal()
+            sky.down(cluster_name)
+            return
+        except ValueError:
+            # The cluster is already down.
+            return
+        except Exception as e:  # pylint: disable=broad-except
+            retry_cnt += 1
+            if retry_cnt >= max_retry:
+                raise RuntimeError('Failed to terminate the spot cluster '
+                                   f'{cluster_name}.') from e
+            logger.error('Failed to terminate the spot cluster '
+                         f'{cluster_name}. Retrying.'
+                         f'Details: {common_utils.format_exception(e)}')
+            logger.error(f'  Traceback: {traceback.format_exc()}')
+
+
 class StrategyExecutor:
     """Handle each launching, recovery and termination of the spot clusters."""
 
     RETRY_INIT_GAP_SECONDS = 60
 
     def __init__(self, cluster_name: str, backend: 'backends.Backend',
                  task: 'task_lib.Task', retry_until_up: bool) -> None:
@@ -39,14 +63,16 @@
 
         Args:
             cluster_name: The name of the cluster.
             backend: The backend to use. Only CloudVMRayBackend is supported.
             task: The task to execute.
             retry_until_up: Whether to retry until the cluster is up.
         """
+        assert isinstance(backend, backends.CloudVmRayBackend), (
+            'Only CloudVMRayBackend is supported.')
         self.dag = sky.Dag()
         self.dag.add(task)
         self.cluster_name = cluster_name
         self.backend = backend
         self.retry_until_up = retry_until_up
 
     def __init_subclass__(cls, name: str, default: bool = False):
@@ -57,255 +83,364 @@
                 'Only one strategy can be default.')
             SPOT_DEFAULT_STRATEGY = name
 
     @classmethod
     def make(cls, cluster_name: str, backend: 'backends.Backend',
              task: 'task_lib.Task', retry_until_up: bool) -> 'StrategyExecutor':
         """Create a strategy from a task."""
-        resources = task.resources
-        assert len(resources) == 1, 'Only one resource is supported.'
-        resources: 'sky.Resources' = list(resources)[0]
+        task_resources = task.resources
+        assert len(task_resources) == 1, 'Only one resource is supported.'
+        resources: 'sky.Resources' = list(task_resources)[0]
 
         spot_recovery = resources.spot_recovery
         assert spot_recovery is not None, (
             'spot_recovery is required to use spot strategy.')
         # Remove the spot_recovery field from the resources, as the strategy
         # will be handled by the strategy class.
         task.set_resources({resources.copy(spot_recovery=None)})
         return SPOT_STRATEGIES[spot_recovery](cluster_name, backend, task,
                                               retry_until_up)
 
-    def launch(self) -> Optional[float]:
+    def launch(self) -> float:
         """Launch the spot cluster for the first time.
 
         It can fail if resource is not available. Need to check the cluster
         status, after calling.
 
-        Returns: The job's start timestamp, or None if failed to start.
+        Returns: The job's submit timestamp, on success (otherwise, an
+            exception is raised).
+
+        Raises: Please refer to the docstring of self._launch().
         """
+
         if self.retry_until_up:
-            return self._launch(max_retry=None)
-        return self._launch()
+            job_submit_at = self._launch(max_retry=None)
+        else:
+            job_submit_at = self._launch()
+        assert job_submit_at is not None
+        return job_submit_at
 
     def recover(self) -> float:
         """Relaunch the spot cluster after failure and wait until job starts.
 
         When recover() is called the cluster should be in STOPPED status (i.e.
         partially down).
 
         Returns: The timestamp job started.
         """
         raise NotImplementedError
 
-    def terminate_cluster(self, max_retry: int = 3) -> None:
-        """Terminate the spot cluster."""
-        retry_cnt = 0
-        while True:
-            try:
-                handle = global_user_state.get_handle_from_cluster_name(
-                    self.cluster_name)
-                if handle is None:
-                    return
-                self.backend.teardown(handle, terminate=True)
-                return
-            except Exception as e:  # pylint: disable=broad-except
-                retry_cnt += 1
-                if retry_cnt >= max_retry:
-                    raise RuntimeError('Failed to terminate the spot cluster '
-                                       f'{self.cluster_name}.') from e
-                logger.error('Failed to terminate the spot cluster '
-                             f'{self.cluster_name}. Retrying.')
-
     def _try_cancel_all_jobs(self):
         handle = global_user_state.get_handle_from_cluster_name(
             self.cluster_name)
+        if handle is None:
+            return
         try:
-            self.backend.cancel_jobs(handle, jobs=None)
+            usage_lib.messages.usage.set_internal()
+            # Note that `sky.cancel()` may not go through for a variety of
+            # reasons:
+            # (1) head node is preempted; or
+            # (2) somehow user programs escape the cancel codepath's kill.
+            # The latter is silent and is a TODO.
+            #
+            # For the former, an exception will be thrown, in which case we
+            # fallback to terminate_cluster() in the except block below. This
+            # is because in the event of recovery on the same set of remaining
+            # worker nodes, we don't want to leave some old job processes
+            # running.
+            # TODO(zhwu): This is non-ideal and we should figure out another way
+            # to reliably cancel those processes and not have to down the
+            # remaining nodes first.
+            #
+            # In the case where the worker node is preempted, the `sky.cancel()`
+            # should be functional with the `_try_cancel_if_cluster_is_init`
+            # flag, i.e. it sends the cancel signal to the head node, which will
+            # then kill the user process on remaining worker nodes.
+            sky.cancel(cluster_name=self.cluster_name,
+                       all=True,
+                       _try_cancel_if_cluster_is_init=True)
         except Exception as e:  # pylint: disable=broad-except
-            # Ignore the failure as the cluster can be totally stopped, and the
-            # job canceling can get connection error.
-            logger.info(
-                f'Ignoring the job cancellation failure (Exception: {e}); '
-                'the spot cluster is likely completely stopped.')
+            logger.info('Failed to cancel the job on the cluster. The cluster '
+                        'might be already down or the head node is preempted.'
+                        '\n  Detailed exception: '
+                        f'{common_utils.format_exception(e)}\n'
+                        'Terminating the cluster explicitly to ensure no '
+                        'remaining job process interferes with recovery.')
+            terminate_cluster(self.cluster_name)
+
+    def _wait_until_job_starts_on_cluster(self) -> Optional[float]:
+        """Wait for MAX_JOB_CHECKING_RETRY times until job starts on the cluster
+
+        Returns:
+            The timestamp of when the job is submitted, or None if failed to
+            submit.
+        """
+        status = None
+        job_checking_retry_cnt = 0
+        while job_checking_retry_cnt < MAX_JOB_CHECKING_RETRY:
+            # Avoid the infinite loop, if any bug happens.
+            job_checking_retry_cnt += 1
+            try:
+                cluster_status, _ = (
+                    backend_utils.refresh_cluster_status_handle(
+                        self.cluster_name,
+                        force_refresh_statuses=set(status_lib.ClusterStatus)))
+            except Exception as e:  # pylint: disable=broad-except
+                # If any unexpected error happens, retry the job checking
+                # loop.
+                # TODO(zhwu): log the unexpected error to usage collection
+                # for future debugging.
+                logger.info(f'Unexpected exception: {e}\nFailed to get the '
+                            'refresh the cluster status. Retrying.')
+                continue
+            if cluster_status != status_lib.ClusterStatus.UP:
+                # The cluster can be preempted before the job is
+                # launched.
+                # Break to let the retry launch kick in.
+                logger.info('The cluster is preempted before the job '
+                            'is submitted.')
+                # TODO(zhwu): we should recover the preemption with the
+                # recovery strategy instead of the current while loop.
+                break
+
+            try:
+                status = spot_utils.get_job_status(self.backend,
+                                                   self.cluster_name)
+            except Exception as e:  # pylint: disable=broad-except
+                # If any unexpected error happens, retry the job checking
+                # loop.
+                # Note: the CommandError is already handled in the
+                # get_job_status, so it should not happen here.
+                # TODO(zhwu): log the unexpected error to usage collection
+                # for future debugging.
+                logger.info(f'Unexpected exception: {e}\nFailed to get the '
+                            'job status. Retrying.')
+                continue
 
-    def _launch(self, max_retry=3, raise_on_failure=True) -> Optional[float]:
+            # Check the job status until it is not in initialized status
+            if status is not None and status > job_lib.JobStatus.INIT:
+                try:
+                    job_submitted_at = spot_utils.get_job_timestamp(
+                        self.backend, self.cluster_name, get_end_time=False)
+                    return job_submitted_at
+                except Exception as e:  # pylint: disable=broad-except
+                    # If we failed to get the job timestamp, we will retry
+                    # job checking loop.
+                    logger.info(f'Unexpected Exception: {e}\nFailed to get '
+                                'the job start timestamp. Retrying.')
+                    continue
+            # Wait for the job to be started
+            time.sleep(spot_utils.JOB_STARTED_STATUS_CHECK_GAP_SECONDS)
+        return None
+
+    def _launch(self,
+                max_retry: Optional[int] = 3,
+                raise_on_failure: bool = True) -> Optional[float]:
         """Implementation of launch().
 
         The function will wait until the job starts running, but will leave the
         handling for the preemption to the caller.
 
         Args:
             max_retry: The maximum number of retries. If None, retry forever.
             raise_on_failure: Whether to raise an exception if the launch fails.
+
+        Returns:
+            The job's submit timestamp, or None if failed to submit the job
+            (either provisioning fails or any error happens in job submission)
+            and raise_on_failure is False.
+
+        Raises:
+            non-exhaustive list of exceptions:
+            exceptions.ProvisionPrechecksError: This will be raised when the
+                underlying `sky.launch` fails due to precheck errors only.
+                I.e., none of the failover exceptions, if
+                any, is due to resources unavailability. This exception
+                includes the following cases:
+                1. The optimizer cannot find a feasible solution.
+                2. Precheck errors: invalid cluster name, failure in getting
+                cloud user identity, or unsupported feature.
+            exceptions.SpotJobReachedMaxRetryError: This will be raised when
+                all prechecks passed but the maximum number of retries is
+                reached for `sky.launch`. The failure of `sky.launch` can be
+                due to:
+                1. Any of the underlying failover exceptions is due to resources
+                unavailability.
+                2. The cluster is preempted before the job is submitted.
+                3. Any unexpected error happens during the `sky.launch`.
+        Other exceptions may be raised depending on the backend.
         """
         # TODO(zhwu): handle the failure during `preparing sky runtime`.
         retry_cnt = 0
         backoff = common_utils.Backoff(self.RETRY_INIT_GAP_SECONDS)
         while True:
             retry_cnt += 1
-            retry_launch = False
-            exception = None
             try:
                 usage_lib.messages.usage.set_internal()
+                # Detach setup, so that the setup failure can be detected
+                # by the controller process (job_status -> FAILED_SETUP).
                 sky.launch(self.dag,
                            cluster_name=self.cluster_name,
-                           detach_run=True)
+                           detach_setup=True,
+                           detach_run=True,
+                           _is_launched_by_spot_controller=True)
                 logger.info('Spot cluster launched.')
-            except exceptions.InvalidClusterNameError as e:
-                # The cluster name is too long.
-                raise exceptions.ResourcesUnavailableError(str(e)) from e
+            except (exceptions.InvalidClusterNameError,
+                    exceptions.NoCloudAccessError) as e:
+                logger.error('Failure happened before provisioning. '
+                             f'{common_utils.format_exception(e)}')
+                if raise_on_failure:
+                    raise exceptions.ProvisionPrechecksError(reasons=[e])
+                return None
+            except exceptions.ResourcesUnavailableError as e:
+                # This is raised when the launch fails due to prechecks or
+                # after failing over through all the candidates.
+                # Please refer to the docstring of `sky.launch` for more
+                # details of how the exception will be structured.
+                if not any(
+                        isinstance(err, exceptions.ResourcesUnavailableError)
+                        for err in e.failover_history):
+                    # _launch() (this function) should fail/exit directly, if
+                    # none of the failover reasons were because of resource
+                    # unavailability or no failover was attempted (the optimizer
+                    # cannot find feasible resources for requested resources),
+                    # i.e., e.failover_history is empty.
+                    # Failing directly avoids the infinite loop of retrying
+                    # the launch when, e.g., an invalid cluster name is used
+                    # and --retry-until-up is specified.
+                    reasons = (e.failover_history
+                               if e.failover_history else [e])
+                    reasons_str = '; '.join(
+                        common_utils.format_exception(err) for err in reasons)
+                    logger.error(
+                        'Failure happened before provisioning. Failover '
+                        f'reasons: {reasons_str}')
+                    if raise_on_failure:
+                        raise exceptions.ProvisionPrechecksError(
+                            reasons=reasons)
+                    return None
+                logger.info('Failed to launch the spot cluster with error: '
+                            f'{common_utils.format_exception(e)})')
             except Exception as e:  # pylint: disable=broad-except
                 # If the launch fails, it will be recovered by the following
                 # code.
                 logger.info('Failed to launch the spot cluster with error: '
-                            f'{type(e)}: {e}')
-                retry_launch = True
-                exception = e
-
-            # At this point, a sky.launch() has succeeded. Cluster may be
-            # UP (no preemption since) or DOWN (newly preempted).
-            status = None
-            job_checking_retry_cnt = 0
-            while not retry_launch:
-                job_checking_retry_cnt += 1
-                if job_checking_retry_cnt >= MAX_JOB_CHECKING_RETRY:
-                    # Avoid the infinite loop, if any bug happens.
-                    retry_launch = True
-                    # TODO(zhwu): log the unexpected error to usage collection
-                    # for future debugging.
-                    logger.info(
-                        'Failed to get the job status, due to unexpected '
-                        'job submission error.')
-                    break
-
-                try:
-                    cluster_status, _ = (
-                        backend_utils.refresh_cluster_status_handle(
-                            self.cluster_name, force_refresh=True))
-                except Exception as e:  # pylint: disable=broad-except
-                    # If any unexpected error happens, retry the job checking
-                    # loop.
-                    # TODO(zhwu): log the unexpected error to usage collection
-                    # for future debugging.
-                    logger.info(f'Unexpected exception: {e}\nFailed to get the '
-                                'refresh the cluster status. Retrying.')
-                    continue
-                if cluster_status != global_user_state.ClusterStatus.UP:
-                    # The cluster can be preempted before the job is launched.
-                    # Break to let the retry launch kick in.
-                    logger.info('The cluster is preempted before the job '
-                                'starts.')
-                    # TODO(zhwu): we should recover the preemption with the
-                    # recovery strategy instead of the current while loop.
-                    retry_launch = True
-                    break
-
-                try:
-                    status = spot_utils.get_job_status(self.backend,
-                                                       self.cluster_name)
-                except Exception as e:  # pylint: disable=broad-except
-                    # If any unexpected error happens, retry the job checking
-                    # loop.
-                    # Note: the CommandError is already handled in the
-                    # get_job_status, so it should not happen here.
-                    # TODO(zhwu): log the unexpected error to usage collection
-                    # for future debugging.
-                    logger.info(f'Unexpected exception: {e}\nFailed to get the '
-                                'job status. Retrying.')
-                    continue
-
-                # Check the job status until it is not in initialized status
-                if status is not None and job_lib.JobStatus.PENDING < status:
-                    try:
-                        launch_time = spot_utils.get_job_timestamp(
-                            self.backend, self.cluster_name, get_end_time=False)
-                        return launch_time
-                    except Exception as e:  # pylint: disable=broad-except
-                        # If we failed to get the job timestamp, we will retry
-                        # job checking loop.
-                        logger.info(f'Unexpected Exception: {e}\nFailed to get '
-                                    'the job start timestamp. Retrying.')
-                        continue
-                # Wait for the job to be started
-                time.sleep(spot_utils.JOB_STARTED_STATUS_CHECK_GAP_SECONDS)
+                            f'{common_utils.format_exception(e)})')
+                logger.info(f'  Traceback: {traceback.format_exc()}')
+            else:  # No exception, the launch succeeds.
+                # At this point, a sky.launch() has succeeded. Cluster may be
+                # UP (no preemption since) or DOWN (newly preempted).
+                job_submitted_at = self._wait_until_job_starts_on_cluster()
+                if job_submitted_at is not None:
+                    return job_submitted_at
+                # The job fails to start on the spot cluster, retry the launch.
+                # TODO(zhwu): log the unexpected error to usage collection
+                # for future debugging.
+                logger.info(
+                    'Failed to successfully submit the job to the '
+                    'launched cluster, due to unexpected submission errors or '
+                    'the cluster being preempted during job submission.')
 
-            assert retry_launch
-
-            self.terminate_cluster()
+            terminate_cluster(self.cluster_name)
             if max_retry is not None and retry_cnt >= max_retry:
                 # Retry forever if max_retry is None.
                 if raise_on_failure:
                     with ux_utils.print_exception_no_traceback():
-                        raise exceptions.ResourcesUnavailableError(
-                            'Failed to launch the spot cluster after '
-                            f'{max_retry} retries.') from exception
+                        raise exceptions.SpotJobReachedMaxRetriesError(
+                            'Resources unavailable: failed to launch the spot '
+                            f'cluster after {max_retry} retries.')
                 else:
                     return None
             gap_seconds = backoff.current_backoff()
             logger.info('Retrying to launch the spot cluster in '
                         f'{gap_seconds:.1f} seconds.')
             time.sleep(gap_seconds)
 
 
 class FailoverStrategyExecutor(StrategyExecutor, name='FAILOVER', default=True):
     """Failover strategy: wait in same region and failover after timout."""
 
     _MAX_RETRY_CNT = 240  # Retry for 4 hours.
 
+    def __init__(self, cluster_name: str, backend: 'backends.Backend',
+                 task: 'task_lib.Task', retry_until_up: bool) -> None:
+        super().__init__(cluster_name, backend, task, retry_until_up)
+        # Note down the cloud/region of the launched cluster, so that we can
+        # first retry in the same cloud/region. (Inside recover() we may not
+        # rely on cluster handle, as it can be None if the cluster is
+        # preempted.)
+        self._launched_cloud_region: Optional[Tuple['sky.clouds.Cloud',
+                                                    'sky.clouds.Region']] = None
+
+    def _launch(self,
+                max_retry: Optional[int] = 3,
+                raise_on_failure: bool = True) -> Optional[float]:
+        job_submitted_at = super()._launch(max_retry, raise_on_failure)
+        if job_submitted_at is not None:
+            # Only record the cloud/region if the launch is successful.
+            handle = global_user_state.get_handle_from_cluster_name(
+                self.cluster_name)
+            assert isinstance(handle, backends.CloudVmRayResourceHandle), (
+                'Cluster should be launched.', handle)
+            launched_resources = handle.launched_resources
+            self._launched_cloud_region = (launched_resources.cloud,
+                                           launched_resources.region)
+        return job_submitted_at
+
     def recover(self) -> float:
         # 1. Cancel the jobs and launch the cluster with the STOPPED status,
         #    so that it will try on the current region first until timeout.
         # 2. Tear down the cluster, if the step 1 failed to launch the cluster.
         # 3. Launch the cluster with no cloud/region constraint or respect the
         #    original user specification.
 
         # Step 1
         self._try_cancel_all_jobs()
 
         # Retry the entire block until the cluster is up, so that the ratio of
         # the time spent in the current region and the time spent in the other
         # region is consistent during the retry.
-        handle = global_user_state.get_handle_from_cluster_name(
-            self.cluster_name)
         while True:
             # Add region constraint to the task, to retry on the same region
-            # first.
-            task = self.dag.tasks[0]
-            resources = list(task.resources)[0]
-            original_resources = resources
-
-            launched_cloud = handle.launched_resources.cloud
-            launched_region = handle.launched_resources.region
-            new_resources = resources.copy(cloud=launched_cloud,
-                                           region=launched_region)
-            task.set_resources({new_resources})
-            # Not using self.launch to avoid the retry until up logic.
-            launched_time = self._launch(raise_on_failure=False)
-            # Restore the original dag, i.e. reset the region constraint.
-            task.set_resources({original_resources})
-            if launched_time is not None:
-                return launched_time
+            # first (if valid).
+            if self._launched_cloud_region is not None:
+                task = self.dag.tasks[0]
+                resources = list(task.resources)[0]
+                original_resources = resources
+
+                launched_cloud, launched_region = self._launched_cloud_region
+                new_resources = resources.copy(cloud=launched_cloud,
+                                               region=launched_region)
+                task.set_resources({new_resources})
+                # Not using self.launch to avoid the retry until up logic.
+                job_submitted_at = self._launch(raise_on_failure=False)
+                # Restore the original dag, i.e. reset the region constraint.
+                task.set_resources({original_resources})
+                if job_submitted_at is not None:
+                    return job_submitted_at
 
             # Step 2
-            logger.debug('Terminating unhealthy spot cluster.')
-            self.terminate_cluster()
+            logger.debug('Terminating unhealthy spot cluster and '
+                         'reset cloud region.')
+            self._launched_cloud_region = None
+            terminate_cluster(self.cluster_name)
 
             # Step 3
             logger.debug('Relaunch the cluster  without constraining to prior '
                          'cloud/region.')
             # Not using self.launch to avoid the retry until up logic.
-            launched_time = self._launch(max_retry=self._MAX_RETRY_CNT,
-                                         raise_on_failure=False)
-            if launched_time is None:
+            job_submitted_at = self._launch(max_retry=self._MAX_RETRY_CNT,
+                                            raise_on_failure=False)
+            if job_submitted_at is None:
                 # Failed to launch the cluster.
                 if self.retry_until_up:
                     gap_seconds = self.RETRY_INIT_GAP_SECONDS
                     logger.info('Retrying to recover the spot cluster in '
                                 f'{gap_seconds:.1f} seconds.')
                     time.sleep(gap_seconds)
                     continue
                 with ux_utils.print_exception_no_traceback():
                     raise exceptions.ResourcesUnavailableError(
                         f'Failed to recover the spot cluster after retrying '
                         f'{self._MAX_RETRY_CNT} times.')
 
-            return launched_time
+            return job_submitted_at
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/task.py` & `skypilot-nightly-1.0.0.dev20230713/sky/task.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,18 +1,18 @@
 """Task: a coarse-grained stage in an application."""
 import inspect
+import json
 import os
 import re
 import typing
 from typing import Any, Callable, Dict, List, Optional, Set, Tuple, Union
 
 import yaml
 
 import sky
-from sky import check
 from sky import clouds
 from sky import exceptions
 from sky import global_user_state
 from sky.backends import backend_utils
 from sky.data import storage as storage_lib
 from sky.data import data_utils
 from sky.skylet import constants
@@ -65,28 +65,63 @@
 
 
 def _is_valid_env_var(name: str) -> bool:
     """Checks if the task environment variable name is valid."""
     return bool(re.fullmatch(_VALID_ENV_VAR_REGEX, name))
 
 
+def _fill_in_env_vars_in_file_mounts(
+    file_mounts: Dict[str, Any],
+    task_envs: Dict[str, str],
+) -> Dict[str, Any]:
+    """Detects env vars in file_mounts and fills them with task_envs.
+
+    Use cases of env vars in file_mounts:
+    - dst/src paths; e.g.,
+        /model_path/llama-${SIZE}b: s3://llama-weights/llama-${SIZE}b
+    - storage's name (bucket name)
+    - storage's source (local path)
+
+    We simply dump file_mounts into a json string, and replace env vars using
+    regex. This should be safe as file_mounts has been schema-validated.
+
+    Env vars of the following forms are detected:
+        - ${ENV}
+        - $ENV
+    where <ENV> must appear in task.envs.
+    """
+    # TODO(zongheng): support ${ENV:-default}?
+    file_mounts_str = json.dumps(file_mounts)
+
+    def replace_var(match):
+        var_name = match.group(1)
+        # If the variable isn't in the dictionary, return it unchanged
+        return task_envs.get(var_name, match.group(0))
+
+    # Pattern for valid env var names in bash.
+    pattern = r'\$\{?\b([a-zA-Z_][a-zA-Z0-9_]*)\b\}?'
+    file_mounts_str = re.sub(pattern, replace_var, file_mounts_str)
+    return json.loads(file_mounts_str)
+
+
 class Task:
     """Task: a computation to be run on the cloud."""
 
     def __init__(
         self,
         name: Optional[str] = None,
         *,
         setup: Optional[str] = None,
         run: Optional[CommandOrCommandGen] = None,
         envs: Optional[Dict[str, str]] = None,
         workdir: Optional[str] = None,
         num_nodes: Optional[int] = None,
         # Advanced:
         docker_image: Optional[str] = None,
+        event_callback: Optional[str] = None,
     ):
         """Initializes a Task.
 
         All fields are optional.  ``Task.run`` is the actual program: either a
         shell command to run (str) or a command generator for different nodes
         (lambda; see below).
 
@@ -136,35 +171,40 @@
             documented above.
           docker_image: (EXPERIMENTAL: Only in effect when LocalDockerBackend
             is used.) The base docker image that this Task will be built on.
             Defaults to 'gpuci/miniforge-cuda:11.4-devel-ubuntu18.04'.
         """
         self.name = name
         self.run = run
-        self.storage_mounts = {}
-        self.storage_plans = {}
+        self.storage_mounts: Dict[str, storage_lib.Storage] = {}
+        self.storage_plans: Dict[storage_lib.Storage,
+                                 storage_lib.StoreType] = {}
         self.setup = setup
-        self._envs = envs or dict()
+        self._envs = envs or {}
         self.workdir = workdir
         self.docker_image = (docker_image if docker_image else
                              'gpuci/miniforge-cuda:11.4-devel-ubuntu18.04')
-        self.num_nodes = num_nodes
+        self.event_callback = event_callback
+        # Ignore type error due to a mypy bug.
+        # https://github.com/python/mypy/issues/3004
+        self.num_nodes = num_nodes  # type: ignore
 
         self.inputs = None
         self.outputs = None
         self.estimated_inputs_size_gigabytes = None
         self.estimated_outputs_size_gigabytes = None
         # Default to CPUNode
         self.resources = {sky.Resources()}
-        self.time_estimator_func = None
-        self.file_mounts = None
-
-        # Only set when 'self' is a spot controller task: 'self.spot_task' is
-        # the underlying managed spot task (Task object).
-        self.spot_task = None
+        self.time_estimator_func: Optional[Callable[['sky.Resources'],
+                                                    int]] = None
+        self.file_mounts: Optional[Dict[str, str]] = None
+
+        # Only set when 'self' is a spot controller task: 'self.spot_dag' is
+        # the underlying managed spot dag (sky.Dag object).
+        self.spot_dag: Optional['sky.Dag'] = None
 
         # Filled in by the optimizer.  If None, this Task is not planned.
         self.best_resources = None
         # Check if the task is legal.
         self._validate()
 
         dag = sky.dag.get_current_dag()
@@ -224,82 +264,81 @@
                 # Symlink to a dir is legal (isdir() follows symlinks).
                 with ux_utils.print_exception_no_traceback():
                     raise ValueError(
                         'Workdir must exist and must be a directory (or '
                         f'a symlink to a directory). {self.workdir} not found.')
 
     @staticmethod
-    def from_yaml(yaml_path: str) -> 'Task':
-        """Initializes a task from a task YAML.
-
-        Example:
-            .. code-block:: python
-
-                task = sky.Task.from_yaml('/path/to/task.yaml')
-
-        Args:
-          yaml_path: file path to a valid task yaml file.
-
-        Raises:
-          ValueError: if the path gets loaded into a str instead of a dict; or
-            if there are any other parsing errors.
-        """
-        with open(os.path.expanduser(yaml_path), 'r') as f:
-            # TODO(zongheng): use
-            #  https://github.com/yaml/pyyaml/issues/165#issuecomment-430074049
-            # to raise errors on duplicate keys.
-            config = yaml.safe_load(f)
-
-        if isinstance(config, str):
-            with ux_utils.print_exception_no_traceback():
-                raise ValueError('YAML loaded as str, not as dict. '
-                                 f'Is it correct? Path: {yaml_path}')
-
-        if config is None:
-            config = {}
+    def from_yaml_config(
+        config: Dict[str, Any],
+        env_overrides: Optional[List[Tuple[str, str]]] = None,
+    ) -> 'Task':
+        if env_overrides is not None:
+            # We must override env vars before constructing the Task, because
+            # the Storage object creation is eager and it (its name/source
+            # fields) may depend on env vars.
+            #
+            # FIXME(zongheng): The eagerness / how we construct Task's from
+            # entrypoint (YAML, CLI args) should be fixed.
+            new_envs = config.get('envs', {})
+            new_envs.update(env_overrides)
+            config['envs'] = new_envs
+
+        # More robust handling for 'envs': explicitly convert keys and values to
+        # str, since users may pass '123' as keys/values which will get parsed
+        # as int causing validate_schema() to fail.
+        envs = config.get('envs')
+        if envs is not None and isinstance(envs, dict):
+            config['envs'] = {str(k): str(v) for k, v in envs.items()}
 
         backend_utils.validate_schema(config, schemas.get_task_schema(),
                                       'Invalid task YAML: ')
 
+        # Fill in any Task.envs into file_mounts (src/dst paths, storage
+        # name/source).
+        if config.get('file_mounts') is not None:
+            config['file_mounts'] = _fill_in_env_vars_in_file_mounts(
+                config['file_mounts'], config.get('envs', {}))
+
         task = Task(
             config.pop('name', None),
             run=config.pop('run', None),
             workdir=config.pop('workdir', None),
             setup=config.pop('setup', None),
             num_nodes=config.pop('num_nodes', None),
             envs=config.pop('envs', None),
+            event_callback=config.pop('event_callback', None),
         )
 
         # Create lists to store storage objects inlined in file_mounts.
         # These are retained in dicts in the YAML schema and later parsed to
         # storage objects with the storage/storage_mount objects.
         fm_storages = []
         file_mounts = config.pop('file_mounts', None)
         if file_mounts is not None:
-            copy_mounts = dict()
+            copy_mounts = {}
             for dst_path, src in file_mounts.items():
                 # Check if it is str path
                 if isinstance(src, str):
                     copy_mounts[dst_path] = src
                 # If the src is not a str path, it is likely a dict. Try to
                 # parse storage object.
                 elif isinstance(src, dict):
                     fm_storages.append((dst_path, src))
                 else:
                     with ux_utils.print_exception_no_traceback():
                         raise ValueError(f'Unable to parse file_mount '
                                          f'{dst_path}:{src}')
             task.set_file_mounts(copy_mounts)
 
-        task_storage_mounts = {}  # type: Dict[str, Storage]
+        task_storage_mounts: Dict[str, storage_lib.Storage] = {}
         all_storages = fm_storages
         for storage in all_storages:
             mount_path = storage[0]
-            assert mount_path, \
-                'Storage mount path cannot be empty.'
+            assert mount_path, 'Storage mount path cannot be empty.'
             try:
                 storage_obj = storage_lib.Storage.from_yaml_config(storage[1])
             except exceptions.StorageSourceError as e:
                 # Patch the error message to include the mount path, if included
                 e.args = (e.args[0].replace('<destination_path>',
                                             mount_path),) + e.args[1:]
                 raise e
@@ -326,40 +365,80 @@
         resources = config.pop('resources', None)
         resources = sky.Resources.from_yaml_config(resources)
 
         task.set_resources({resources})
         assert not config, f'Invalid task args: {config.keys()}'
         return task
 
+    @staticmethod
+    def from_yaml(yaml_path: str) -> 'Task':
+        """Initializes a task from a task YAML.
+
+        Example:
+            .. code-block:: python
+
+                task = sky.Task.from_yaml('/path/to/task.yaml')
+
+        Args:
+          yaml_path: file path to a valid task yaml file.
+
+        Raises:
+          ValueError: if the path gets loaded into a str instead of a dict; or
+            if there are any other parsing errors.
+        """
+        with open(os.path.expanduser(yaml_path), 'r') as f:
+            # TODO(zongheng): use
+            #  https://github.com/yaml/pyyaml/issues/165#issuecomment-430074049
+            # to raise errors on duplicate keys.
+            config = yaml.safe_load(f)
+
+        if isinstance(config, str):
+            with ux_utils.print_exception_no_traceback():
+                raise ValueError('YAML loaded as str, not as dict. '
+                                 f'Is it correct? Path: {yaml_path}')
+
+        if config is None:
+            config = {}
+        return Task.from_yaml_config(config)
+
     @property
     def num_nodes(self) -> int:
         return self._num_nodes
 
+    @num_nodes.setter
+    def num_nodes(self, num_nodes: Optional[int]) -> None:
+        if num_nodes is None:
+            num_nodes = 1
+        if not isinstance(num_nodes, int) or num_nodes <= 0:
+            with ux_utils.print_exception_no_traceback():
+                raise ValueError(
+                    f'num_nodes should be a positive int. Got: {num_nodes}')
+        self._num_nodes = num_nodes
+
     @property
     def envs(self) -> Dict[str, str]:
         return self._envs
 
-    def set_envs(
-            self, envs: Union[None, Tuple[Tuple[str, str]],
+    def update_envs(
+            self, envs: Union[None, List[Tuple[str, str]],
                               Dict[str, str]]) -> 'Task':
-        """Sets the environment variables for use inside the setup/run commands.
+        """Updates environment variables for use inside the setup/run commands.
 
         Args:
           envs: (optional) either a list of ``(env_name, value)`` or a dict
             ``{env_name: value}``.
 
         Returns:
-          self: The current task, with envs set.
+          self: The current task, with envs updated.
 
         Raises:
           ValueError: if various invalid inputs errors are detected.
         """
         if envs is None:
-            self._envs = dict()
-            return self
+            envs = {}
         if isinstance(envs, (list, tuple)):
             keys = set(env[0] for env in envs)
             if len(keys) != len(envs):
                 with ux_utils.print_exception_no_traceback():
                     raise ValueError('Duplicate env keys provided.')
             envs = dict(envs)
         if isinstance(envs, dict):
@@ -371,30 +450,24 @@
                     with ux_utils.print_exception_no_traceback():
                         raise ValueError(f'Invalid env key: {key}')
         else:
             with ux_utils.print_exception_no_traceback():
                 raise ValueError(
                     'envs must be List[Tuple[str, str]] or Dict[str, str]: '
                     f'{envs}')
-        self._envs = envs
+        self._envs.update(envs)
         return self
 
     @property
     def need_spot_recovery(self) -> bool:
         return any(r.spot_recovery is not None for r in self.resources)
 
-    @num_nodes.setter
-    def num_nodes(self, num_nodes: Optional[int]) -> None:
-        if num_nodes is None:
-            num_nodes = 1
-        if not isinstance(num_nodes, int) or num_nodes <= 0:
-            with ux_utils.print_exception_no_traceback():
-                raise ValueError(
-                    f'num_nodes should be a positive int. Got: {num_nodes}')
-        self._num_nodes = num_nodes
+    @property
+    def use_spot(self) -> bool:
+        return any(r.use_spot for r in self.resources)
 
     def set_inputs(self, inputs, estimated_size_gigabytes) -> 'Task':
         # E.g., 's3://bucket', 'gs://bucket', or None.
         self.inputs = inputs
         self.estimated_inputs_size_gigabytes = estimated_size_gigabytes
         return self
 
@@ -441,14 +514,15 @@
             best of these resources" to run this task.
 
         Returns:
           self: The current task, with resources set.
         """
         if isinstance(resources, sky.Resources):
             resources = {resources}
+        # TODO(woosuk): Check if the resources are None.
         self.resources = resources
         return self
 
     def get_resources(self):
         return self.resources
 
     def set_time_estimator(self, func: Callable[['sky.Resources'],
@@ -463,16 +537,16 @@
     def estimate_runtime(self, resources):
         """Returns a func mapping resources to estimated time (secs).
 
         This is EXPERIMENTAL.
         """
         if self.time_estimator_func is None:
             raise NotImplementedError(
-                'Node [{}] does not have a cost model set; '
-                'call set_time_estimator() first'.format(self))
+                f'Node [{self}] does not have a cost model set; '
+                'call set_time_estimator() first')
         return self.time_estimator_func(resources)
 
     def set_file_mounts(self, file_mounts: Optional[Dict[str, str]]) -> 'Task':
         """Sets the file mounts for this task.
 
         Useful for syncing datasets, dotfiles, etc.
 
@@ -566,14 +640,15 @@
           self: the current task, with file mounts updated.
 
         Raises:
           ValueError: if input paths are invalid.
         """
         if self.file_mounts is None:
             self.file_mounts = {}
+        assert self.file_mounts is not None
         self.file_mounts.update(file_mounts)
         # For validation logic:
         return self.set_file_mounts(self.file_mounts)
 
     def set_storage_mounts(
         self,
         storage_mounts: Optional[Dict[str, storage_lib.Storage]],
@@ -604,15 +679,15 @@
         Returns:
           self: The current task, with storage mounts set.
 
         Raises:
           ValueError: if input paths are invalid.
         """
         if storage_mounts is None:
-            self.storage_mounts = None
+            self.storage_mounts = {}
             return self
         for target, _ in storage_mounts.items():
             # TODO(zhwu): /home/username/sky_workdir as the target path need
             # to be filtered out as well.
             if (target == constants.SKY_REMOTE_WORKDIR and
                     self.workdir is not None):
                 with ux_utils.print_exception_no_traceback():
@@ -634,15 +709,15 @@
 
     def update_storage_mounts(
             self, storage_mounts: Dict[str, storage_lib.Storage]) -> 'Task':
         """Updates the storage mounts for this task.
 
         Different from set_storage_mounts(), this function updates into the
         existing storage_mounts (calls ``dict.update()``), rather than
-        overwritting it.
+        overwriting it.
 
         This should be called before provisioning in order to take effect.
 
         Args:
           storage_mounts: an optional dict of ``{mount_path: sky.Storage
             object}``, where mount_path is the path inside the remote VM(s)
             where the Storage object will be mounted on.
@@ -662,39 +737,38 @@
     def get_preferred_store_type(self) -> storage_lib.StoreType:
         # TODO(zhwu, romilb): The optimizer should look at the source and
         #  destination to figure out the right stores to use. For now, we
         #  use a heuristic solution to find the store type by the following
         #  order:
         #  1. cloud decided in best_resources.
         #  2. cloud specified in the task resources.
-        #  3. the first enabled cloud.
+        #  3. if not specified or the task's cloud does not support storage,
+        #     use the first enabled storage cloud.
         # This should be refactored and moved to the optimizer.
         assert len(self.resources) == 1, self.resources
         storage_cloud = None
+
+        backend_utils.check_public_cloud_enabled()
+        enabled_storage_clouds = global_user_state.get_enabled_storage_clouds()
+        if not enabled_storage_clouds:
+            raise ValueError('No enabled cloud for storage, run: sky check')
+
         if self.best_resources is not None:
             storage_cloud = self.best_resources.cloud
         else:
             resources = list(self.resources)[0]
             storage_cloud = resources.cloud
-            if storage_cloud is None:
-                # Get the first enabled cloud.
-                enabled_clouds = global_user_state.get_enabled_clouds()
-                if len(enabled_clouds) == 0:
-                    check.check(quiet=True)
-                    enabled_clouds = global_user_state.get_enabled_clouds()
-                if len(enabled_clouds) == 0:
-                    raise ValueError('No enabled clouds.')
-
-                for cloud in storage_lib.STORE_ENABLED_CLOUDS:
-                    for enabled_cloud in enabled_clouds:
-                        if cloud.is_same_cloud(enabled_cloud):
-                            storage_cloud = cloud
-                            break
+        if storage_cloud is not None:
+            if str(storage_cloud) not in enabled_storage_clouds:
+                storage_cloud = None
+
         if storage_cloud is None:
-            raise ValueError('No available cloud to mount storage.')
+            storage_cloud = clouds.CLOUD_REGISTRY.from_str(
+                enabled_storage_clouds[0])
+
         store_type = storage_lib.get_storetype_from_cloud(storage_cloud)
         return store_type
 
     def sync_storage_mounts(self) -> None:
         """(INTERNAL) Eagerly syncs storage mounts to cloud storage.
 
         After syncing up, COPY-mode storage mounts are translated into regular
@@ -713,33 +787,46 @@
         storage_mounts = self.storage_mounts
         storage_plans = self.storage_plans
         for mnt_path, storage in storage_mounts.items():
             if storage.mode == storage_lib.StorageMode.COPY:
                 store_type = storage_plans[storage]
                 if store_type is storage_lib.StoreType.S3:
                     # TODO: allow for Storage mounting of different clouds
-                    if storage.source is not None and storage.source.startswith(
-                            's3://'):
+                    if isinstance(storage.source,
+                                  str) and storage.source.startswith('s3://'):
                         blob_path = storage.source
                     else:
+                        assert storage.name is not None, storage
                         blob_path = 's3://' + storage.name
                     self.update_file_mounts({
                         mnt_path: blob_path,
                     })
                 elif store_type is storage_lib.StoreType.GCS:
-                    if storage.source.startswith('gs://'):
+                    if isinstance(storage.source,
+                                  str) and storage.source.startswith('gs://'):
                         blob_path = storage.source
                     else:
+                        assert storage.name is not None, storage
                         blob_path = 'gs://' + storage.name
                     self.update_file_mounts({
                         mnt_path: blob_path,
                     })
+                elif store_type is storage_lib.StoreType.R2:
+                    if storage.source is not None and not isinstance(
+                            storage.source,
+                            list) and storage.source.startswith('r2://'):
+                        blob_path = storage.source
+                    else:
+                        blob_path = 'r2://' + storage.name
+                    self.update_file_mounts({
+                        mnt_path: blob_path,
+                    })
                 elif store_type is storage_lib.StoreType.AZURE:
                     # TODO when Azure Blob is done: sync ~/.azure
-                    assert False, 'TODO: Azure Blob not mountable yet'
+                    raise NotImplementedError('Azure Blob not mountable yet')
                 else:
                     with ux_utils.print_exception_no_traceback():
                         raise ValueError(f'Storage Type {store_type} '
                                          'does not exist!')
 
     def get_local_to_remote_file_mounts(self) -> Optional[Dict[str, str]]:
         """Returns file mounts of the form (dst=VM path, src=local path).
@@ -776,15 +863,15 @@
         return d
 
     def to_yaml_config(self) -> Dict[str, Any]:
         """Returns a yaml-style dict representation of the task.
 
         INTERNAL: this method is internal-facing.
         """
-        config = dict()
+        config = {}
 
         def add_if_not_none(key, value, no_empty: bool = False):
             if no_empty and not value:
                 return
             if value is not None:
                 config[key] = value
 
@@ -802,18 +889,19 @@
         if self.outputs is not None:
             add_if_not_none(
                 'outputs',
                 {self.outputs: self.estimated_outputs_size_gigabytes})
 
         add_if_not_none('setup', self.setup)
         add_if_not_none('workdir', self.workdir)
+        add_if_not_none('event_callback', self.event_callback)
         add_if_not_none('run', self.run)
         add_if_not_none('envs', self.envs, no_empty=True)
 
-        add_if_not_none('file_mounts', dict())
+        add_if_not_none('file_mounts', {})
 
         if self.file_mounts is not None:
             config['file_mounts'].update(self.file_mounts)
 
         if self.storage_mounts is not None:
             config['file_mounts'].update({
                 mount_path: storage.to_yaml_config()
@@ -821,32 +909,35 @@
             })
         return config
 
     def __rshift__(self, b):
         sky.dag.get_current_dag().add_edge(self, b)
 
     def __repr__(self):
-        if self.name:
+        if self.name and self.name != 'sky-cmd':  # CLI launch with a command
             return self.name
         if isinstance(self.run, str):
             run_msg = self.run.replace('\n', '\\n')
             if len(run_msg) > 20:
                 run_msg = f'run=\'{run_msg[:20]}...\''
             else:
                 run_msg = f'run=\'{run_msg}\''
         elif self.run is None:
-            run_msg = 'run=None'
+            run_msg = 'run=<empty>'
         else:
             run_msg = 'run=<fn>'
 
         s = f'Task({run_msg})'
         if self.inputs is not None:
             s += f'\n  inputs: {self.inputs}'
         if self.outputs is not None:
             s += f'\n  outputs: {self.outputs}'
         if self.num_nodes > 1:
             s += f'\n  nodes: {self.num_nodes}'
-        if len(self.resources) > 1 or not list(self.resources)[0].is_empty():
+        if len(self.resources) > 1:
             s += f'\n  resources: {self.resources}'
+        elif len(
+                self.resources) == 1 and not list(self.resources)[0].is_empty():
+            s += f'\n  resources: {list(self.resources)[0]}'
         else:
             s += '\n  resources: default instances'
         return s
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/templates/aws-ray.yml.j2` & `skypilot-nightly-1.0.0.dev20230713/sky/templates/ibm-ray.yml.j2`

 * *Files 13% similar despite different names*

```diff
@@ -1,79 +1,73 @@
 cluster_name: {{cluster_name}}
 
 # The maximum number of workers nodes to launch in addition to the head node.
 max_workers: {{num_nodes - 1}}
 upscaling_speed: {{num_nodes - 1}}
 idle_timeout_minutes: 60
 
+
 provider:
   type: external
-  module: sky.skylet.providers.aws.AWSNodeProvider
+  module: sky.skylet.providers.ibm.IBMVPCNodeProvider
   region: {{region}}
   availability_zone: {{zones}}
   # Keep (otherwise cannot reuse when re-provisioning).
   # teardown(terminate=True) will override this.
   cache_stopped_nodes: True
-  security_group:
-    # AWS config file must include security group name
-    GroupName: {{security_group}}
+  iam_api_key: {{iam_api_key}}
+  resource_group_id: {{resource_group_id}}
+  # Disable launch config check for worker nodes as it can cause resource
+  # leakage.
+  # Reference: https://github.com/ray-project/ray/blob/cd1ba65e239360c8a7b130f991ed414eccc063ce/python/ray/autoscaler/_private/autoscaler.py#L1115
+  # The upper-level SkyPilot code has make sure there will not be resource
+  # leakage.
+  disable_launch_config_check: true
 
 auth:
   ssh_user: ubuntu
   ssh_private_key: {{ssh_private_key}}
 
+
 available_node_types:
-  ray.head.default:
-    resources: {}
+  ray_head_default:
+    resources: {{instance_resources}}
     node_config:
-      InstanceType: {{instance_type}}
-      ImageId: {{image_id}}  # Deep Learning AMI (Ubuntu 18.04); see aws.py.
-      # https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ec2.html#EC2.ServiceResource.create_instances
-      BlockDeviceMappings:
-        - DeviceName: /dev/sda1
-          Ebs:
-            VolumeSize: {{disk_size}}
-            # use default Iops for gp3
-            VolumeType: gp3
-            Iops: 3000
-            Throughput: 125 
+      image_id: {{image_id}}  
+      boot_volume_capacity: {{disk_capacity}}
+      volume_tier_name: general-purpose
+      instance_profile_name: {{instance_type}}
+
       {% if use_spot %}
       InstanceMarketOptions:
           MarketType: spot
-          # Additional options can be found in the boto docs, e.g.
-          #   SpotOptions:
-          #       MaxPrice: MAX_HOURLY_PRICE
+
       {% endif %}
 {% if num_nodes > 1 %}
-  ray.worker.default:
+  ray_worker_default:
     min_workers: {{num_nodes - 1}}
     max_workers: {{num_nodes - 1}}
-    resources: {}
+    resources: {{worker_instance_resources}}
     node_config:
-      InstanceType: {{instance_type}}
-      ImageId: {{image_id}}  # Deep Learning AMI (Ubuntu 18.04); see aws.py.
-      BlockDeviceMappings:
-        - DeviceName: /dev/sda1
-          Ebs:
-            VolumeSize: {{disk_size}}
-            VolumeType: gp3
-            Iops: 3000
-            Throughput: 125 
+      image_id: {{image_id}} 
+      boot_volume_capacity: {{disk_capacity}}
+      volume_tier_name: general-purpose
+      instance_profile_name: {{worker_instance_type}}
+
       {% if use_spot %}
       InstanceMarketOptions:
           MarketType: spot
-          # Additional options can be found in the boto docs, e.g.
-          #   SpotOptions:
-          #       MaxPrice: MAX_HOURLY_PRICE
+
       {% endif %}
 {%- endif %}
 
-head_node_type: ray.head.default
+head_node_type: ray_head_default
 
-# Format: `REMOTE_PATH : LOCAL_PATH`
+
+{# Format: `REMOTE_PATH : LOCAL_PATH` #}
 file_mounts: {
   "{{sky_ray_yaml_remote_path}}": "{{sky_ray_yaml_local_path}}",
   "{{sky_remote_path}}/{{sky_wheel_hash}}": "{{sky_local_path}}",
 {%- for remote_path, local_path in credentials.items() %}
   "{{remote_path}}": "{{local_path}}",
 {%- endfor %}
 }
@@ -86,15 +80,14 @@
 # NOTE: these are very performance-sensitive. Each new item opens/closes an SSH
 # connection, which is expensive. Try your best to co-locate commands into fewer
 # items!
 #
 # Increment the following for catching performance bugs easier:
 #   current num items (num SSH connections): 1
 setup_commands:
-  # Disable `unattended-upgrades` to prevent apt-get from hanging. It should be called at the beginning before the process started to avoid being blocked. (This is a temporary fix.)
   # Create ~/.ssh/config file in case the file does not exist in the custom image.
   # Make sure python3 & pip3 are available on this image.
   # We set auto_activate_base to be false for pre-installed conda.
   # This also kills the service that is holding the lock on dpkg (problem only exists on aws/azure, not gcp)
   # Line 'sudo bash ..': set the ulimit as suggested by ray docs for performance. https://docs.ray.io/en/latest/cluster/vms/user-guides/large-cluster-best-practices.html#system-configuration
   # Line 'sudo grep ..': set the number of threads per process to unlimited to avoid ray job submit stucking issue when the number of running ray jobs increase.
   # Line 'mkdir -p ..': disable host key check
@@ -109,40 +102,42 @@
     mkdir -p ~/.ssh; touch ~/.ssh/config;
     pip3 --version > /dev/null 2>&1 || (curl -sSL https://bootstrap.pypa.io/get-pip.py -o get-pip.py && python3 get-pip.py && echo "PATH=$HOME/.local/bin:$PATH" >> ~/.bashrc);
     (type -a python | grep -q python3) || echo 'alias python=python3' >> ~/.bashrc;
     (type -a pip | grep -q pip3) || echo 'alias pip=pip3' >> ~/.bashrc;
     (which conda > /dev/null 2>&1 && conda init > /dev/null && conda config --set auto_activate_base false) || (wget -nc https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh && bash Miniconda3-latest-Linux-x86_64.sh -b && eval "$(~/miniconda3/bin/conda shell.bash hook)" && conda init && conda config --set auto_activate_base true);
     source ~/.bashrc;
     (pip3 list | grep ray | grep {{ray_version}} 2>&1 > /dev/null || pip3 install -U ray[default]=={{ray_version}}) && mkdir -p ~/sky_workdir && mkdir -p ~/.sky/sky_app;
-    (pip3 list | grep skypilot && [ "$(cat {{sky_remote_path}}/current_sky_wheel_hash)" == "{{sky_wheel_hash}}" ]) || (pip3 uninstall skypilot -y; pip3 install "$(echo {{sky_remote_path}}/{{sky_wheel_hash}}/skypilot-{{sky_version}}*.whl)[aws]" && echo "{{sky_wheel_hash}}" > {{sky_remote_path}}/current_sky_wheel_hash || exit 1);
+    (pip3 list | grep skypilot && [ "$(cat {{sky_remote_path}}/current_sky_wheel_hash)" == "{{sky_wheel_hash}}" ]) || (pip3 uninstall skypilot -y; pip3 install "$(echo {{sky_remote_path}}/{{sky_wheel_hash}}/skypilot-{{sky_version}}*.whl)[ibm]" && echo "{{sky_wheel_hash}}" > {{sky_remote_path}}/current_sky_wheel_hash || exit 1);
     sudo bash -c 'rm -rf /etc/security/limits.d; echo "* soft nofile 1048576" >> /etc/security/limits.conf; echo "* hard nofile 1048576" >> /etc/security/limits.conf';
     sudo grep -e '^DefaultTasksMax' /etc/systemd/system.conf || (sudo bash -c 'echo "DefaultTasksMax=infinity" >> /etc/systemd/system.conf'); sudo systemctl set-property user-$(id -u $(whoami)).slice TasksMax=infinity; sudo systemctl daemon-reload;
     mkdir -p ~/.ssh; (grep -Pzo -q "Host \*\n  StrictHostKeyChecking no" ~/.ssh/config) || printf "Host *\n  StrictHostKeyChecking no\n" >> ~/.ssh/config;
     python3 -c "from sky.skylet.ray_patches import patch; patch()" || exit 1;
     [ -f /etc/fuse.conf ] && sudo sed -i 's/#user_allow_other/user_allow_other/g' /etc/fuse.conf || (sudo sh -c 'echo "user_allow_other" > /etc/fuse.conf'); # This is needed for `-o allow_other` option for `goofys`;
 
+
 # Command to start ray on the head node. You don't need to change this.
 # NOTE: these are very performance-sensitive. Each new item opens/closes an SSH
 # connection, which is expensive. Try your best to co-locate commands into fewer
 # items! The same comment applies for worker_start_ray_commands.
 #
 # Increment the following for catching performance bugs easier:
 #   current num items (num SSH connections): 1
 head_start_ray_commands:
   # Start skylet daemon. (Should not place it in the head_setup_commands, otherwise it will run before sky is installed.)
   # NOTE: --disable-usage-stats in `ray start` saves 10 seconds of idle wait.
   # Line "which prlimit ..": increase the limit of the number of open files for the raylet process, as the `ulimit` may not take effect at this point, because it requires
   # all the sessions to be reloaded. This is a workaround.
   - ((ps aux | grep -v nohup | grep -v grep | grep -q -- "python3 -m sky.skylet.skylet") || nohup python3 -m sky.skylet.skylet >> ~/.sky/skylet.log 2>&1 &);
-    ray stop; RAY_SCHEDULER_EVENTS=0 ray start --disable-usage-stats --head --port=6379 --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml {{"--resources='%s'" % custom_resources if custom_resources}} || exit 1;
+    ray stop; RAY_SCHEDULER_EVENTS=0 RAY_DEDUP_LOGS=0 ray start --disable-usage-stats --head --port={{ray_port}} --dashboard-port={{ray_dashboard_port}} --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml {{"--resources='%s'" % custom_resources if custom_resources}} --temp-dir {{ray_temp_dir}} || exit 1;
     which prlimit && for id in $(pgrep -f raylet/raylet); do sudo prlimit --nofile=1048576:1048576 --pid=$id || true; done;
+    {{dump_port_command}};
 
 {%- if num_nodes > 1 %}
 worker_start_ray_commands:
-  - ray stop; RAY_SCHEDULER_EVENTS=0 ray start --disable-usage-stats --address=$RAY_HEAD_IP:6379 --object-manager-port=8076 {{"--resources='%s'" % custom_resources if custom_resources}} || exit 1;
+  - ray stop; RAY_SCHEDULER_EVENTS=0 RAY_DEDUP_LOGS=0 ray start --disable-usage-stats --address=$RAY_HEAD_IP:{{ray_port}} --object-manager-port=8076 {{"--resources='%s'" % custom_resources if custom_resources}} --temp-dir {{ray_temp_dir}} || exit 1;
     which prlimit && for id in $(pgrep -f raylet/raylet); do sudo prlimit --nofile=1048576:1048576 --pid=$id || true; done;
 {%- else %}
 worker_start_ray_commands: []
 {%- endif %}
 
 head_node: {}
 worker_nodes: {}
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/templates/azure-ray.yml.j2` & `skypilot-nightly-1.0.0.dev20230713/sky/templates/azure-ray.yml.j2`

 * *Files 8% similar despite different names*

```diff
@@ -15,53 +15,72 @@
     # for different cluster_name.
     resource_group: {{resource_group}}
     # Keep (otherwise cannot reuse when re-provisioning).
     # teardown(terminate=True) will override this.
     cache_stopped_nodes: True
     # subscription id of the azure user
     subscription_id: {{azure_subscription_id}}
+    # Disable launch config check for worker nodes as it can cause resource
+    # leakage.
+    # Reference: https://github.com/ray-project/ray/blob/cd1ba65e239360c8a7b130f991ed414eccc063ce/python/ray/autoscaler/_private/autoscaler.py#L1115
+    # The upper-level SkyPilot code has make sure there will not be resource
+    # leakage.
+    disable_launch_config_check: true
+
 
 auth:
     ssh_user: azureuser
     ssh_private_key: {{ssh_private_key}}
 
 available_node_types:
     ray.head.default:
       resources: {}
       node_config:
+        tags:
+          skypilot-user: {{ user }}
         azure_arm_parameters:
+            adminUsername: skypilot:ssh_user
+            publicKey: |
+              skypilot:ssh_public_key_content
             vmSize: {{instance_type}}
             # List images https://docs.microsoft.com/en-us/azure/virtual-machines/linux/cli-ps-findimage
             imagePublisher: {{image_publisher}}
             imageOffer: {{image_offer}}
             imageSku: "{{image_sku}}"
             imageVersion: {{image_version}}
             osDiskSizeGB: {{disk_size}}
+            osDiskTier: {{disk_tier}}
             # optionally set priority to use Spot instances
             {%- if use_spot %}
             priority: Spot
             # set a maximum price for spot instances if desired
             # billingProfile:
             #     maxPrice: -1
             {%- endif %}
         # TODO: attach disk
 {% if num_nodes > 1 %}
     ray.worker.default:
       min_workers: {{num_nodes - 1}}
       max_workers: {{num_nodes - 1}}
       resources: {}
       node_config:
+        tags:
+          skypilot-user: {{ user }}
         azure_arm_parameters:
+            adminUsername: skypilot:ssh_user
+            publicKey: |
+              skypilot:ssh_public_key_content
             vmSize: {{instance_type}}
             # List images https://docs.microsoft.com/en-us/azure/virtual-machines/linux/cli-ps-findimage
             imagePublisher: {{image_publisher}}
             imageOffer: {{image_offer}}
             imageSku: "{{image_sku}}"
             imageVersion: {{image_version}}
             osDiskSizeGB: {{disk_size}}
+            osDiskTier: {{disk_tier}}
           {%- if use_spot %}
             priority: Spot
             # set a maximum price for spot instances if desired
             # billingProfile:
             #     maxPrice: -1
           {%- endif %}
 {%- endif %}
@@ -93,29 +112,32 @@
   # Create ~/.ssh/config file in case the file does not exist in the image.
   # Make sure python3 & pip3 are available on this image.
   # Line 'sudo bash ..': set the ulimit as suggested by ray docs for performance. https://docs.ray.io/en/latest/cluster/vms/user-guides/large-cluster-best-practices.html#system-configuration
   # Line 'sudo grep ..': set the number of threads per process to unlimited to avoid ray job submit stucking issue when the number of running ray jobs increase.
   # Line 'mkdir -p ..': disable host key check
   # Line 'python3 -c ..': patch the buggy ray files and enable `-o allow_other` option for `goofys`
   # This also kills the service that is holding the lock on dpkg (problem only exists on aws/azure, not gcp)
-  - sudo systemctl stop unattended-upgrades || true;
+  - function mylsof { p=$(for pid in /proc/{0..9}*; do i=$(basename "$pid"); for file in "$pid"/fd/*; do link=$(readlink -e "$file"); if [ "$link" = "$1" ]; then echo "$i"; fi; done; done); echo "$p"; };
+    sudo systemctl stop unattended-upgrades || true;
     sudo systemctl disable unattended-upgrades || true;
     sudo sed -i 's/Unattended-Upgrade "1"/Unattended-Upgrade "0"/g' /etc/apt/apt.conf.d/20auto-upgrades || true;
-    sudo kill -9 `sudo lsof /var/lib/dpkg/lock-frontend | awk '{print $2}' | tail -n 1` || true;
-    sudo pkill -9 apt-get;
+    p=$(mylsof "/var/lib/dpkg/lock-frontend"); echo "$p";
+    sudo kill -9 `echo "$p" | tail -n 1` || true;
+    sudo rm /var/lib/dpkg/lock-frontend;
     sudo pkill -9 dpkg;
-    sudo dpkg --configure -a;
+    sudo pkill -9 apt-get;
+    sudo dpkg --configure --force-overwrite -a;
     mkdir -p ~/.ssh; touch ~/.ssh/config;
     pip3 --version > /dev/null 2>&1 || (curl -sSL https://bootstrap.pypa.io/get-pip.py -o get-pip.py && python3 get-pip.py && echo "PATH=$HOME/.local/bin:$PATH" >> ~/.bashrc);
     (type -a python | grep -q python3) || echo 'alias python=python3' >> ~/.bashrc;
     (type -a pip | grep -q pip3) || echo 'alias pip=pip3' >> ~/.bashrc;
     which conda > /dev/null 2>&1 || (wget -nc https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh && bash Miniconda3-latest-Linux-x86_64.sh -b && eval "$(/home/azureuser/miniconda3/bin/conda shell.bash hook)" && conda init && conda config --set auto_activate_base true);
     source ~/.bashrc;
-    (pip3 list | grep ray | grep {{ray_version}} 2>&1 > /dev/null || pip3 install -U ray[default]=={{ray_version}}) && mkdir -p ~/sky_workdir && mkdir -p ~/.sky/sky_app && touch ~/.sudo_as_admin_successful;
-    (pip3 list | grep skypilot && [ "$(cat {{sky_remote_path}}/current_sky_wheel_hash)" == "{{sky_wheel_hash}}" ]) || (pip3 uninstall skypilot -y; pip3 install "$(echo {{sky_remote_path}}/{{sky_wheel_hash}}/skypilot-{{sky_version}}*.whl)[azure]" && echo "{{sky_wheel_hash}}" > {{sky_remote_path}}/current_sky_wheel_hash || exit 1);
+    (pip3 list | grep "ray " | grep {{ray_version}} 2>&1 > /dev/null || pip3 install --exists-action w -U ray[default]=={{ray_version}}) && mkdir -p ~/sky_workdir && mkdir -p ~/.sky/sky_app && touch ~/.sudo_as_admin_successful;
+    (pip3 list | grep "skypilot " && [ "$(cat {{sky_remote_path}}/current_sky_wheel_hash)" == "{{sky_wheel_hash}}" ]) || (pip3 uninstall skypilot -y; pip3 install "$(echo {{sky_remote_path}}/{{sky_wheel_hash}}/skypilot-{{sky_version}}*.whl)[azure]" && echo "{{sky_wheel_hash}}" > {{sky_remote_path}}/current_sky_wheel_hash || exit 1);
     sudo bash -c 'rm -rf /etc/security/limits.d; echo "* soft nofile 1048576" >> /etc/security/limits.conf; echo "* hard nofile 1048576" >> /etc/security/limits.conf';
     sudo grep -e '^DefaultTasksMax' /etc/systemd/system.conf || (sudo bash -c 'echo "DefaultTasksMax=infinity" >> /etc/systemd/system.conf'); sudo systemctl set-property user-$(id -u $(whoami)).slice TasksMax=infinity; sudo systemctl daemon-reload;
     mkdir -p ~/.ssh; (grep -Pzo -q "Host \*\n  StrictHostKeyChecking no" ~/.ssh/config) || printf "Host *\n  StrictHostKeyChecking no\n" >> ~/.ssh/config;
     python3 -c "from sky.skylet.ray_patches import patch; patch()" || exit 1;
     [ -f /etc/fuse.conf ] && sudo sed -i 's/#user_allow_other/user_allow_other/g' /etc/fuse.conf || (sudo sh -c 'echo "user_allow_other" > /etc/fuse.conf');
 
 # Command to start ray on the head node. You don't need to change this.
@@ -125,20 +147,21 @@
 #
 # Increment the following for catching performance bugs easier:
 #   current num items (num SSH connections): 2
 head_start_ray_commands:
   # Start skylet daemon. (Should not place it in the head_setup_commands, otherwise it will run before skypilot is installed.)
   # NOTE: --disable-usage-stats in `ray start` saves 10 seconds of idle wait.
   - ((ps aux | grep -v nohup | grep -v grep | grep -q -- "python3 -m sky.skylet.skylet") || nohup python3 -m sky.skylet.skylet >> ~/.sky/skylet.log 2>&1 &);
-    ray stop; RAY_SCHEDULER_EVENTS=0 ray start --disable-usage-stats --head --port=6379 --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml {{"--resources='%s'" % custom_resources if custom_resources}} || exit 1;
+    ray stop; RAY_SCHEDULER_EVENTS=0 RAY_DEDUP_LOGS=0 ray start --disable-usage-stats --head --port={{ray_port}} --dashboard-port={{ray_dashboard_port}} --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml {{"--resources='%s'" % custom_resources if custom_resources}} --temp-dir {{ray_temp_dir}} || exit 1;
     which prlimit && for id in $(pgrep -f raylet/raylet); do sudo prlimit --nofile=1048576:1048576 --pid=$id || true; done;
+    {{dump_port_command}};
 
 {%- if num_nodes > 1 %}
 worker_start_ray_commands:
-  - ray stop; RAY_SCHEDULER_EVENTS=0 ray start --disable-usage-stats --address=$RAY_HEAD_IP:6379 --object-manager-port=8076 {{"--resources='%s'" % custom_resources if custom_resources}} || exit 1;
+  - ray stop; RAY_SCHEDULER_EVENTS=0 RAY_DEDUP_LOGS=0 ray start --disable-usage-stats --address=$RAY_HEAD_IP:{{ray_port}} --object-manager-port=8076 {{"--resources='%s'" % custom_resources if custom_resources}} --temp-dir {{ray_temp_dir}} || exit 1;
     which prlimit && for id in $(pgrep -f raylet/raylet); do sudo prlimit --nofile=1048576:1048576 --pid=$id || true; done;
 {%- else %}
 worker_start_ray_commands: []
 {%- endif %}
 
 head_node: {}
 worker_nodes: {}
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/templates/gcp-ray.yml.j2` & `skypilot-nightly-1.0.0.dev20230713/sky/templates/gcp-ray.yml.j2`

 * *Files 16% similar despite different names*

```diff
@@ -16,23 +16,31 @@
   # teardown(terminate=True) will override this.
   cache_stopped_nodes: True
   # The GCP project ID.
   project_id: {{gcp_project_id}}
 {%- if tpu_vm %}
   _has_tpus: True
 {%- endif %}
+  # Disable launch config check for worker nodes as it can cause resource
+  # leakage.
+  # Reference: https://github.com/ray-project/ray/blob/cd1ba65e239360c8a7b130f991ed414eccc063ce/python/ray/autoscaler/_private/autoscaler.py#L1115
+  # The upper-level SkyPilot code has make sure there will not be resource
+  # leakage.
+  disable_launch_config_check: true
 
 auth:
   ssh_user: gcpuser
   ssh_private_key: {{ssh_private_key}}
 
 available_node_types:
   ray_head_default:
     resources: {}
     node_config:
+      labels:
+        skypilot-user: {{ user }}
 {%- if tpu_vm %}
       acceleratorType: {{tpu_type}}
       runtimeVersion: {{runtime_version}}
   {%- if use_spot %}
       schedulingConfig:
         preemptible: true
   {%- endif %}
@@ -42,35 +50,47 @@
         - boot: true
           autoDelete: true
           type: PERSISTENT
           initializeParams:
             diskSizeGb: {{disk_size}}
             # See https://cloud.google.com/deep-learning-vm/docs/images
             sourceImage: {{image_id}}
+            diskType: zones/{{zones}}/diskTypes/{{disk_tier}}
   {%- if gpu is not none %}
       guestAccelerators:
         - acceleratorType: projects/{{gcp_project_id}}/zones/{{zones}}/acceleratorTypes/{{gpu}}
           acceleratorCount: {{gpu_count}}
+  {%- endif %}
       metadata:
         items:
+          - key: ssh-keys
+            # After replacing the variables, this will become username:ssh_public_key_content.
+            # This is a specific syntax required by GCP https://cloud.google.com/compute/docs/connect/add-ssh-keys
+            value: |
+              skypilot:ssh_user:skypilot:ssh_public_key_content
+  {%- if gpu is not none %}
           - key: install-nvidia-driver
             value: "True"
   {%- endif %}
       scheduling:
   {%- if use_spot %}
         - preemptible: true
   {%- endif %}
+  {%- if gpu is not none %}
         - onHostMaintenance: TERMINATE  # Required for GPU-attached VMs.
+  {%- endif %}
 {%- endif %}
 {% if num_nodes > 1 %}
   ray_worker_default:
     min_workers: {{num_nodes - 1}}
     max_workers: {{num_nodes - 1}}
     resources: {}
     node_config:
+      labels:
+        skypilot-user: {{ user }}
   {%- if tpu_vm %}
       acceleratorType: {{tpu_type}}
       runtimeVersion: {{runtime_version}}
     {%- if use_spot %}
       schedulingConfig:
         preemptible: true
     {%- endif %}
@@ -80,28 +100,36 @@
         - boot: true
           autoDelete: true
           type: PERSISTENT
           initializeParams:
             diskSizeGb: {{disk_size}}
             # See https://cloud.google.com/deep-learning-vm/docs/images
             sourceImage: {{image_id}}
+            diskType: zones/{{zones}}/diskTypes/{{disk_tier}}
     {%- if gpu is not none %}
       guestAccelerators:
         - acceleratorType: projects/{{gcp_project_id}}/zones/{{zones}}/acceleratorTypes/{{gpu}}
           acceleratorCount: {{gpu_count}}
+    {%- endif %}
       metadata:
         items:
+          - key: ssh-keys
+            value: |
+              skypilot:ssh_user:skypilot:ssh_public_key_content
+    {%- if gpu is not none %}
           - key: install-nvidia-driver
             value: "True"
     {%- endif %}
       scheduling:
     {%- if use_spot %}
         - preemptible: true
     {%- endif %}
+    {%- if gpu is not none %}
         - onHostMaintenance: TERMINATE  # Required for GPU-attached VMs.
+    {%- endif %}
   {%- endif %}
 {%- endif %}
 
 head_node_type: ray_head_default
 
 # Format: `REMOTE_PATH : LOCAL_PATH`
 file_mounts: {
@@ -130,38 +158,41 @@
   # Line 'which conda ..': some images (TPU VM) do not install conda by
   # default. 'source ~/.bashrc' is needed so conda takes effect for the next
   # commands.
   # Line 'sudo bash ..': set the ulimit as suggested by ray docs for performance. https://docs.ray.io/en/latest/cluster/vms/user-guides/large-cluster-best-practices.html#system-configuration
   # Line 'sudo grep ..': set the number of threads per process to unlimited to avoid ray job submit stucking issue when the number of running ray jobs increase.
   # Line 'mkdir -p ..': disable host key check
   # Line 'python3 -c ..': patch the buggy ray files and enable `-o allow_other` option for `goofys`
-  - sudo systemctl stop unattended-upgrades || true;
+  - function mylsof { p=$(for pid in /proc/{0..9}*; do i=$(basename "$pid"); for file in "$pid"/fd/*; do link=$(readlink -e "$file"); if [ "$link" = "$1" ]; then echo "$i"; fi; done; done); echo "$p"; };
+    sudo systemctl stop unattended-upgrades || true;
     sudo systemctl disable unattended-upgrades || true;
     sudo sed -i 's/Unattended-Upgrade "1"/Unattended-Upgrade "0"/g' /etc/apt/apt.conf.d/20auto-upgrades || true;
-    sudo kill -9 `sudo lsof /var/lib/dpkg/lock-frontend | awk '{print $2}' | tail -n 1` || true;
-    sudo pkill -9 apt-get;
+    p=$(mylsof "/var/lib/dpkg/lock-frontend"); echo "$p";
+    sudo kill -9 `echo "$p" | tail -n 1` || true;
+    sudo rm /var/lib/dpkg/lock-frontend;
     sudo pkill -9 dpkg;
-    sudo dpkg --configure -a;
+    sudo pkill -9 apt-get;
+    sudo dpkg --configure --force-overwrite -a;
     mkdir -p ~/.ssh; touch ~/.ssh/config;
     pip3 --version > /dev/null 2>&1 || (curl -sSL https://bootstrap.pypa.io/get-pip.py -o get-pip.py && python3 get-pip.py && echo "PATH=$HOME/.local/bin:$PATH" >> ~/.bashrc);
     (type -a python | grep -q python3) || echo 'alias python=python3' >> ~/.bashrc;
     (type -a pip | grep -q pip3) || echo 'alias pip=pip3' >> ~/.bashrc;
     which conda > /dev/null 2>&1 || (wget -nc https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh && bash Miniconda3-latest-Linux-x86_64.sh -b && eval "$(/home/gcpuser/miniconda3/bin/conda shell.bash hook)" && conda init && conda config --set auto_activate_base true);
     source ~/.bashrc;
   {%- if tpu_vm %}
     test -f /home/gcpuser/miniconda3/etc/profile.d/conda.sh && source /home/gcpuser/miniconda3/etc/profile.d/conda.sh && conda activate base || true;
     pip3 install --upgrade google-api-python-client;
   {%- endif %}
-    (pip3 list | grep ray | grep {{ray_version}} 2>&1 > /dev/null || pip3 install -U ray[default]=={{ray_version}}) && mkdir -p ~/sky_workdir && mkdir -p ~/.sky/sky_app;
-    (pip3 list | grep skypilot && [ "$(cat {{sky_remote_path}}/current_sky_wheel_hash)" == "{{sky_wheel_hash}}" ]) || (pip3 uninstall skypilot -y; pip3 install "$(echo {{sky_remote_path}}/{{sky_wheel_hash}}/skypilot-{{sky_version}}*.whl)[gcp]" && echo "{{sky_wheel_hash}}" > {{sky_remote_path}}/current_sky_wheel_hash || exit 1);
+    (pip3 list | grep "ray " | grep {{ray_version}} 2>&1 > /dev/null || pip3 install --exists-action w -U ray[default]=={{ray_version}}) && mkdir -p ~/sky_workdir && mkdir -p ~/.sky/sky_app;
+    (pip3 list | grep "skypilot " && [ "$(cat {{sky_remote_path}}/current_sky_wheel_hash)" == "{{sky_wheel_hash}}" ]) || (pip3 uninstall skypilot -y; pip3 install "$(echo {{sky_remote_path}}/{{sky_wheel_hash}}/skypilot-{{sky_version}}*.whl)[gcp]" && echo "{{sky_wheel_hash}}" > {{sky_remote_path}}/current_sky_wheel_hash || exit 1);
     sudo bash -c 'rm -rf /etc/security/limits.d; echo "* soft nofile 1048576" >> /etc/security/limits.conf; echo "* hard nofile 1048576" >> /etc/security/limits.conf';
     sudo grep -e '^DefaultTasksMax' /etc/systemd/system.conf || (sudo bash -c 'echo "DefaultTasksMax=infinity" >> /etc/systemd/system.conf'); sudo systemctl set-property user-$(id -u $(whoami)).slice TasksMax=infinity; sudo systemctl daemon-reload;
     mkdir -p ~/.ssh; (grep -Pzo -q "Host \*\n  StrictHostKeyChecking no" ~/.ssh/config) || printf "Host *\n  StrictHostKeyChecking no\n" >> ~/.ssh/config;
     python3 -c "from sky.skylet.ray_patches import patch; patch()" || exit 1;
-    [ -f /etc/fuse.conf ] && sudo sed -i 's/#user_allow_other/user_allow_other/g' /etc/fuse.conf || (sudo sh -c 'echo "user_allow_other" > /etc/fuse.conf'); # This is needed for `-o allow_other` option for `gcsfuse`;
+    [ -f /etc/fuse.conf ] && sudo sed -i 's/#user_allow_other/user_allow_other/g' /etc/fuse.conf || (sudo sh -c 'echo "user_allow_other" > /etc/fuse.conf');
 
 # Command to start ray on the head node. You don't need to change this.
 # NOTE: these are very performance-sensitive. Each new item opens/closes an SSH
 # connection, which is expensive. Try your best to co-locate commands into fewer
 # items! The same comment applies for worker_start_ray_commands.
 #
 # Increment the following for catching performance bugs easier:
@@ -169,22 +200,23 @@
 head_start_ray_commands:
   # Start skylet daemon. (Should not place it in the head_setup_commands, otherwise it will run before sky is installed.)
   # NOTE: --disable-usage-stats in `ray start` saves 10 seconds of idle wait.
   # Line "which prlimit ..": increase the limit of the number of open files for the raylet process, as the `ulimit` may not take effect at this point, because it requires
   # all the sessions to be reloaded. This is a workaround.
   - ((ps aux | grep -v nohup | grep -v grep | grep -q -- "python3 -m sky.skylet.skylet") || nohup python3 -m sky.skylet.skylet >> ~/.sky/skylet.log 2>&1 &);
     export SKYPILOT_NUM_GPUS=0 && which nvidia-smi > /dev/null && SKYPILOT_NUM_GPUS=$(nvidia-smi --query-gpu=index,name --format=csv,noheader | wc -l);
-    ray stop; RAY_SCHEDULER_EVENTS=0 ray start --disable-usage-stats --head --port=6379 --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml {{"--resources='%s'" % custom_resources if custom_resources}} --num-gpus=$SKYPILOT_NUM_GPUS || exit 1;
+    ray stop; RAY_SCHEDULER_EVENTS=0 RAY_DEDUP_LOGS=0 ray start --disable-usage-stats --head --port={{ray_port}} --dashboard-port={{ray_dashboard_port}} --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml {{"--resources='%s'" % custom_resources if custom_resources}} --num-gpus=$SKYPILOT_NUM_GPUS --temp-dir {{ray_temp_dir}} || exit 1;
     which prlimit && for id in $(pgrep -f raylet/raylet); do sudo prlimit --nofile=1048576:1048576 --pid=$id || true; done;
+    {{dump_port_command}};
 
 # Worker commands are needed for TPU VM Pods
 {%- if num_nodes > 1 or tpu_vm %}
 worker_start_ray_commands:
   - SKYPILOT_NUM_GPUS=0 && which nvidia-smi > /dev/null && SKYPILOT_NUM_GPUS=$(nvidia-smi --query-gpu=index,name --format=csv,noheader | wc -l);
-    ray stop; RAY_SCHEDULER_EVENTS=0 ray start --disable-usage-stats --address=$RAY_HEAD_IP:6379 --object-manager-port=8076 {{"--resources='%s'" % custom_resources if custom_resources}} --num-gpus=$SKYPILOT_NUM_GPUS || exit 1;
+    ray stop; RAY_SCHEDULER_EVENTS=0 RAY_DEDUP_LOGS=0 ray start --disable-usage-stats --address=$RAY_HEAD_IP:{{ray_port}} --object-manager-port=8076 {{"--resources='%s'" % custom_resources if custom_resources}} --num-gpus=$SKYPILOT_NUM_GPUS --temp-dir {{ray_temp_dir}} || exit 1;
     which prlimit && for id in $(pgrep -f raylet/raylet); do sudo prlimit --nofile=1048576:1048576 --pid=$id || true; done;
 {%- else %}
 worker_start_ray_commands: []
 {%- endif %}
 
 head_node: {}
 worker_nodes: {}
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/templates/local-ray.yml.j2` & `skypilot-nightly-1.0.0.dev20230713/sky/templates/local-ray.yml.j2`

 * *Files identical despite different names*

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/usage/constants.py` & `skypilot-nightly-1.0.0.dev20230713/sky/usage/constants.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 """Constants for usage collection."""
 
-LOG_URL = 'http://54.68.242.202:9090/loki/api/v1/push'  # pylint: disable=line-too-long
+LOG_URL = 'http://usage.skypilot.co:9090/loki/api/v1/push'  # pylint: disable=line-too-long
 
 USAGE_MESSAGE_SCHEMA_VERSION = 1
 
 PRIVACY_POLICY_PATH = '~/.sky/privacy_policy'
 
 USAGE_POLICY_MESSAGE = (
     'SkyPilot collects usage data to improve its services. '
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/usage/usage_lib.py` & `skypilot-nightly-1.0.0.dev20230713/sky/usage/usage_lib.py`

 * *Files 2% similar despite different names*

```diff
@@ -6,26 +6,26 @@
 import datetime
 import inspect
 import json
 import os
 import time
 import traceback
 import typing
-from typing import Any, Dict, List, Optional, Union
+from typing import Any, Callable, Dict, List, Optional, Union
 
 import requests
 
 import sky
 from sky import sky_logging
 from sky.usage import constants
 from sky.utils import common_utils
 from sky.utils import env_options
 
 if typing.TYPE_CHECKING:
-    from sky import global_user_state
+    from sky import status_lib
     from sky import resources as resources_lib
     from sky import task as task_lib
 
 logger = sky_logging.init_logger(__name__)
 
 
 def _get_current_timestamp_ns() -> int:
@@ -43,22 +43,22 @@
     USAGE = 'usage'
     # TODO(zhwu): Add more types, e.g., cluster_lifecycle.
 
 
 class MessageToReport:
     """Abstract class for messages to be sent to Loki."""
 
-    def __init__(self, schema_version: str):
+    def __init__(self, schema_version: int):
         self.schema_version = schema_version
-        self.start_time: int = None
-        self.send_time: int = None
+        self.start_time: Optional[int] = None
+        self.send_time: Optional[int] = None
 
     def start(self):
         if self.start_time is None:
-            self.start_time: int = _get_current_timestamp_ns()
+            self.start_time = _get_current_timestamp_ns()
 
     @property
     def message_sent(self):
         return self.send_time is not None or self.start_time is None
 
     def get_properties(self) -> Dict[str, Any]:
         properties = self.__dict__.copy()
@@ -103,28 +103,29 @@
         self.num_accelerators: Optional[int] = None  # update_cluster_resources
         #: Use spot
         self.use_spot: Optional[bool] = None  # update_cluster_resources
         #: Resources of the cluster.
         self.resources: Optional[Dict[str,
                                       Any]] = None  # update_cluster_resources
         #: Resources of the local cluster.
-        self.local_resources: Optional[Dict[
-            str, Any]] = None  # update_local_cluster_resources
+        self.local_resources: Optional[List[Dict[
+            str, Any]]] = None  # update_local_cluster_resources
         #: The number of nodes in the cluster.
         self.num_nodes: Optional[int] = None  # update_cluster_resources
         #: The status of the cluster.
         self.original_cluster_status: Optional[
             str] = None  # update_cluster_status
         self._original_cluster_status_specified: Optional[
             bool] = False  # update_cluster_status
         self.final_cluster_status: Optional[
             str] = None  # update_final_cluster_status
         #: Whether the cluster is newly launched.
         self.is_new_cluster: bool = False  # set_new_cluster
 
+        self.task_id: Optional[int] = None  # update_task_id
         # Task requested
         #: The number of nodes requested by the task.
         #: Requested cloud
         self.task_cloud: Optional[str] = None  # update_actual_task
         #: Requested region
         self.task_region: Optional[str] = None  # update_actual_task
         #: Requested zone
@@ -139,20 +140,22 @@
         self.task_use_spot: Optional[bool] = None  # update_actual_task
         #: Requested resources
         self.task_resources: Optional[Dict[str,
                                            Any]] = None  # update_actual_task
         #: Requested number of nodes
         self.task_num_nodes: Optional[int] = None  # update_actual_task
         # YAMLs converted to JSON.
-        self.user_task_yaml: Optional[str] = None  # update_user_task_yaml
-        self.actual_task: Optional[Dict[str, Any]] = None  # update_actual_task
+        self.user_task_yaml: Optional[List[Dict[
+            str, Any]]] = None  # update_user_task_yaml
+        self.actual_task: Optional[List[Dict[str,
+                                             Any]]] = None  # update_actual_task
         self.ray_yamls: Optional[List[Dict[str, Any]]] = None
         #: Number of Ray YAML files.
         self.num_tried_regions: Optional[int] = None  # update_ray_yaml
-        self.runtimes: Dict[str, int] = {}  # update_runtime
+        self.runtimes: Dict[str, float] = {}  # update_runtime
         self.stacktrace: Optional[str] = None  # entrypoint_context
 
     def __repr__(self) -> str:
         d = self.get_properties()
         return json.dumps(d)
 
     def update_entrypoint(self, msg: str):
@@ -187,18 +190,21 @@
                 if len(resources.accelerators) > 1:
                     logger.debug('Multiple accelerators are not supported: '
                                  f'{resources.accelerators}.')
                 self.task_accelerators = list(resources.accelerators.keys())[0]
                 self.task_num_accelerators = resources.accelerators[
                     self.task_accelerators]
 
+    def update_task_id(self, task_id: int):
+        self.task_id = task_id
+
     def update_ray_yaml(self, yaml_config_or_path: Union[Dict, str]):
         if self.ray_yamls is None:
             self.ray_yamls = []
-        self.ray_yamls.append(
+        self.ray_yamls.extend(
             prepare_json_from_yaml_config(yaml_config_or_path))
         self.num_tried_regions = len(self.ray_yamls)
 
     def update_cluster_name(self, cluster_name: Union[List[str], str]):
         if isinstance(cluster_name, str):
             self.cluster_names = [cluster_name]
         else:
@@ -226,32 +232,32 @@
         self.resources = resources.to_yaml_config()
 
     def update_local_cluster_resources(
             self, local_resources: List['resources_lib.Resources']):
         self.local_resources = [r.to_yaml_config() for r in local_resources]
 
     def update_cluster_status(
-            self, original_status: Optional['global_user_state.ClusterStatus']):
+            self, original_status: Optional['status_lib.ClusterStatus']):
         status = original_status.value if original_status else None
         if not self._original_cluster_status_specified:
             self.original_cluster_status = status
             self._original_cluster_status_specified = True
         self.final_cluster_status = status
 
     def update_final_cluster_status(
-            self, status: Optional['global_user_state.ClusterStatus']):
+            self, status: Optional['status_lib.ClusterStatus']):
         self.final_cluster_status = status.value if status is not None else None
 
     def set_new_cluster(self):
         self.is_new_cluster = True
 
     @contextlib.contextmanager
     def update_runtime_context(self, name: str):
+        start = time.time()
         try:
-            start = time.time()
             yield
         finally:
             self.runtimes[name] = time.time() - start
 
     def update_runtime(self, name_or_fn: str):
         return common_utils.make_decorator(self.update_runtime_context,
                                            name_or_fn)
@@ -312,15 +318,15 @@
                              timeout=0.5)
     if response.status_code != 204:
         logger.debug(
             f'Grafana Loki failed with response: {response.text}\n{payload}')
     messages.reset(message_type)
 
 
-def _clean_yaml(yaml_info: Dict[str, str]):
+def _clean_yaml(yaml_info: Dict[str, Optional[str]]):
     """Remove sensitive information from user YAML."""
     cleaned_yaml_info = yaml_info.copy()
     for redact_type in constants.USAGE_MESSAGE_REDACT_KEYS:
         if redact_type in cleaned_yaml_info:
             contents = cleaned_yaml_info[redact_type]
             if not contents:
                 cleaned_yaml_info[redact_type] = None
@@ -347,27 +353,31 @@
                 logger.debug(message)
 
             cleaned_yaml_info[redact_type] = message
 
     return cleaned_yaml_info
 
 
-def prepare_json_from_yaml_config(yaml_config_or_path: Union[Dict, str]):
+def prepare_json_from_yaml_config(
+        yaml_config_or_path: Union[Dict, str]) -> List[Dict[str, Any]]:
     """Upload safe contents of YAML file to Loki."""
     if isinstance(yaml_config_or_path, dict):
-        yaml_info = yaml_config_or_path
+        yaml_info = [yaml_config_or_path]
         comment_lines = []
     else:
         with open(yaml_config_or_path, 'r') as f:
             lines = f.readlines()
             comment_lines = [line for line in lines if line.startswith('#')]
-        yaml_info = common_utils.read_yaml(yaml_config_or_path)
+        yaml_info = common_utils.read_yaml_all(yaml_config_or_path)
 
-    yaml_info = _clean_yaml(yaml_info)
-    yaml_info['__redacted_comment_lines'] = len(comment_lines)
+    for i in range(len(yaml_info)):
+        if yaml_info[i] is None:
+            yaml_info[i] = {}
+        yaml_info[i] = _clean_yaml(yaml_info[i])
+        yaml_info[i]['__redacted_comment_lines'] = len(comment_lines)
     return yaml_info
 
 
 def _send_local_messages():
     """Send all messages not been uploaded to Loki."""
     for msg_type, message in messages.items():
         if not message.message_sent:
@@ -420,15 +430,15 @@
         raise
     finally:
         if fallback:
             messages.usage.update_entrypoint(name)
         _send_local_messages()
 
 
-def entrypoint(name_or_fn: str, fallback: bool = False):
+def entrypoint(name_or_fn: Union[str, Callable], fallback: bool = False):
     return common_utils.make_decorator(entrypoint_context,
                                        name_or_fn,
                                        fallback=fallback)
 
 
 # Convenience methods below.
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/utils/accelerator_registry.py` & `skypilot-nightly-1.0.0.dev20230713/sky/utils/accelerator_registry.py`

 * *Files 1% similar despite different names*

```diff
@@ -70,11 +70,11 @@
     # Do not print an error meessage here. Optimizer will handle it.
     if len(names) == 0:
         return accelerator
 
     # Currenlty unreachable.
     # This can happen if catalogs have the same accelerator with
     # different names (e.g., A10g and A10G).
-    if len(names) > 1:
-        with ux_utils.print_exception_no_traceback():
-            raise ValueError(f'Accelerator name {accelerator} is ambiguous. '
-                             f'Please choose one of {names}.')
+    assert len(names) > 1
+    with ux_utils.print_exception_no_traceback():
+        raise ValueError(f'Accelerator name {accelerator!r} is ambiguous. '
+                         f'Please choose one of {names}.')
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/utils/command_runner.py` & `skypilot-nightly-1.0.0.dev20230713/sky/utils/command_runner.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,18 +1,19 @@
 """Runner for commands to be executed on the cluster."""
 import getpass
 import enum
 import hashlib
 import os
 import pathlib
 import shlex
+import time
 from typing import List, Optional, Tuple, Union
 
 from sky import sky_logging
-from sky.utils import subprocess_utils
+from sky.utils import common_utils, subprocess_utils
 from sky.skylet import log_lib
 
 logger = sky_logging.init_logger(__name__)
 
 # The git exclude file to support.
 GIT_EXCLUDE = '.git/info/exclude'
 # Rsync options
@@ -38,15 +39,16 @@
     os.makedirs(path, exist_ok=True)
     return path
 
 
 def ssh_options_list(ssh_private_key: Optional[str],
                      ssh_control_name: Optional[str],
                      *,
-                     timeout=30) -> List[str]:
+                     ssh_proxy_command: Optional[str] = None,
+                     timeout: int = 30) -> List[str]:
     """Returns a list of sane options for 'ssh'."""
     # Forked from Ray SSHOptions:
     # https://github.com/ray-project/ray/blob/master/python/ray/autoscaler/_private/command_runner.py
     arg_dict = {
         # Supresses initial fingerprint verification.
         'StrictHostKeyChecking': 'no',
         # SSH IP and fingerprint pairs no longer added to known_hosts.
@@ -77,14 +79,22 @@
             'ControlPath': f'{_ssh_control_path(ssh_control_name)}/%C',
             'ControlPersist': '300s',
         })
     ssh_key_option = [
         '-i',
         ssh_private_key,
     ] if ssh_private_key is not None else []
+
+    if ssh_proxy_command is not None:
+        logger.debug(f'--- Proxy: {ssh_proxy_command} ---')
+        arg_dict.update({
+            # Due to how log_lib.run_with_log() works (using shell=True) we
+            # must quote this value.
+            'ProxyCommand': shlex.quote(ssh_proxy_command),
+        })
     return ssh_key_option + [
         x for y in (['-o', f'{k}={v}']
                     for k, v in arg_dict.items()
                     if v is not None) for x in y
     ]
 
 
@@ -104,14 +114,15 @@
 
     def __init__(
         self,
         ip: str,
         ssh_user: str,
         ssh_private_key: str,
         ssh_control_name: Optional[str] = '__default__',
+        ssh_proxy_command: Optional[str] = None,
     ):
         """Initialize SSHCommandRunner.
 
         Example Usage:
             runner = SSHCommandRunner(ip, ssh_user, ssh_private_key)
             runner.run('ls -l', mode=SshMode.NON_INTERACTIVE)
             runner.rsync(source, target, up=True)
@@ -120,32 +131,38 @@
             ip: The IP address of the remote machine.
             ssh_private_key: The path to the private key to use for ssh.
             ssh_user: The user to use for ssh.
             ssh_control_name: The files name of the ssh_control to use. This is
                 used to avoid confliction between clusters for creating ssh
                 control files. It can simply be the cluster_name or any name
                 that can distinguish between clusters.
+            ssh_proxy_command: Optional, the value to pass to '-o
+                ProxyCommand'. Useful for communicating with clusters without
+                public IPs using a "jump server".
         """
         self.ip = ip
         self.ssh_user = ssh_user
         self.ssh_private_key = ssh_private_key
         self.ssh_control_name = (
             None if ssh_control_name is None else hashlib.md5(
                 ssh_control_name.encode()).hexdigest()[:_HASH_MAX_LENGTH])
+        self._ssh_proxy_command = ssh_proxy_command
 
     @staticmethod
     def make_runner_list(
-            ip_list: List[str],
-            ssh_user: str,
-            ssh_private_key: str,
-            ssh_control_name: Optional[str] = None) -> List['SSHCommandRunner']:
+        ip_list: List[str],
+        ssh_user: str,
+        ssh_private_key: str,
+        ssh_control_name: Optional[str] = None,
+        ssh_proxy_command: Optional[str] = None,
+    ) -> List['SSHCommandRunner']:
         """Helper function for creating runners with the same ssh credentials"""
         return [
-            SSHCommandRunner(ip, ssh_user, ssh_private_key, ssh_control_name)
-            for ip in ip_list
+            SSHCommandRunner(ip, ssh_user, ssh_private_key, ssh_control_name,
+                             ssh_proxy_command) for ip in ip_list
         ]
 
     def _ssh_base_command(self, *, ssh_mode: SshMode,
                           port_forward: Optional[List[int]]) -> List[str]:
         ssh = ['ssh']
         if ssh_mode == SshMode.NON_INTERACTIVE:
             # Disable pseudo-terminal allocation. Otherwise, the output of
@@ -158,23 +175,25 @@
             for port in port_forward:
                 local = remote = port
                 logger.info(
                     f'Forwarding port {local} to port {remote} on localhost.')
                 ssh += ['-L', f'{remote}:localhost:{local}']
         return ssh + ssh_options_list(
             self.ssh_private_key,
-            self.ssh_control_name) + [f'{self.ssh_user}@{self.ip}']
+            self.ssh_control_name,
+            ssh_proxy_command=self._ssh_proxy_command,
+        ) + [f'{self.ssh_user}@{self.ip}']
 
     def run(
             self,
             cmd: Union[str, List[str]],
             *,
+            require_outputs: bool = False,
             port_forward: Optional[List[int]] = None,
             # Advanced options.
-            require_outputs: bool = False,
             log_path: str = os.devnull,
             # If False, do not redirect stdout/stderr to optimize performance.
             process_stream: bool = True,
             stream_logs: bool = True,
             ssh_mode: SshMode = SshMode.NON_INTERACTIVE,
             separate_stderr: bool = False,
             **kwargs) -> Union[int, Tuple[int, str, str]]:
@@ -236,16 +255,16 @@
                 #  bash: no job control in this shell
                 '| stdbuf -o0 tail -n +5',
                 # This is required to make sure the executor of command can get
                 # correct returncode, since linux pipe is used.
                 '; exit ${PIPESTATUS[0]}'
             ]
 
-        command = ' '.join(command)
-        command = base_ssh_command + [shlex.quote(command)]
+        command_str = ' '.join(command)
+        command = base_ssh_command + [shlex.quote(command_str)]
 
         executable = None
         if not process_stream:
             if stream_logs:
                 command += [
                     f'| tee {log_path}',
                     # This also requires the executor to be '/bin/bash' instead
@@ -254,88 +273,106 @@
                 ]
             else:
                 command += [f'> {log_path}']
             executable = '/bin/bash'
 
         return log_lib.run_with_log(' '.join(command),
                                     log_path,
-                                    stream_logs,
-                                    process_stream=process_stream,
                                     require_outputs=require_outputs,
+                                    stream_logs=stream_logs,
+                                    process_stream=process_stream,
                                     shell=True,
                                     executable=executable,
                                     **kwargs)
 
     def rsync(
         self,
         source: str,
         target: str,
         *,
         up: bool,
         # Advanced options.
         log_path: str = os.devnull,
         stream_logs: bool = True,
+        max_retry: int = 1,
     ) -> None:
         """Uses 'rsync' to sync 'source' to 'target'.
 
         Args:
             source: The source path.
             target: The target path.
             up: The direction of the sync, True for local to cluster, False
               for cluster to local.
             log_path: Redirect stdout/stderr to the log_path.
             stream_logs: Stream logs to the stdout/stderr.
+            max_retry: The maximum number of retries for the rsync command.
+              This value should be non-negative.
 
         Raises:
             exceptions.CommandError: rsync command failed.
         """
         # Build command.
         # TODO(zhwu): This will print a per-file progress bar (with -P),
         # shooting a lot of messages to the output. --info=progress2 is used
         # to get a total progress bar, but it requires rsync>=3.1.0 and Mac
         # OS has a default rsync==2.6.9 (16 years old).
         rsync_command = ['rsync', RSYNC_DISPLAY_OPTION]
 
         # --filter
         rsync_command.append(RSYNC_FILTER_OPTION)
 
-        # --exclude-from
-        resolved_source = pathlib.Path(source).expanduser().resolve()
-        if (resolved_source / GIT_EXCLUDE).exists():
-            # Ensure file exists; otherwise, rsync will error out.
-            rsync_command.append(
-                RSYNC_EXCLUDE_OPTION.format(str(resolved_source / GIT_EXCLUDE)))
-
-        # rsync doesn't support '~' in a quoted target path. need to expand it.
-        full_source_str = str(resolved_source)
-        if resolved_source.is_dir():
-            full_source_str = os.path.join(full_source_str, '')
+        if up:
+            # The source is a local path, so we need to resolve it.
+            # --exclude-from
+            resolved_source = pathlib.Path(source).expanduser().resolve()
+            if (resolved_source / GIT_EXCLUDE).exists():
+                # Ensure file exists; otherwise, rsync will error out.
+                rsync_command.append(
+                    RSYNC_EXCLUDE_OPTION.format(
+                        str(resolved_source / GIT_EXCLUDE)))
 
         ssh_options = ' '.join(
-            ssh_options_list(self.ssh_private_key, self.ssh_control_name))
+            ssh_options_list(
+                self.ssh_private_key,
+                self.ssh_control_name,
+                ssh_proxy_command=self._ssh_proxy_command,
+            ))
         rsync_command.append(f'-e "ssh {ssh_options}"')
         # To support spaces in the path, we need to quote source and target.
+        # rsync doesn't support '~' in a quoted local path, but it is ok to
+        # have '~' in a quoted remote path.
         if up:
+            full_source_str = str(resolved_source)
+            if resolved_source.is_dir():
+                full_source_str = os.path.join(full_source_str, '')
             rsync_command.extend([
                 f'{full_source_str!r}',
                 f'{self.ssh_user}@{self.ip}:{target!r}',
             ])
         else:
             rsync_command.extend([
-                f'{self.ssh_user}@{self.ip}:{full_source_str!r}',
-                f'{target!r}',
+                f'{self.ssh_user}@{self.ip}:{source!r}',
+                f'{os.path.expanduser(target)!r}',
             ])
         command = ' '.join(rsync_command)
 
-        returncode, _, stderr = log_lib.run_with_log(command,
-                                                     log_path=log_path,
-                                                     stream_logs=stream_logs,
-                                                     shell=True,
-                                                     require_outputs=True)
+        backoff = common_utils.Backoff(initial_backoff=5, max_backoff_factor=5)
+        while max_retry >= 0:
+            returncode, _, stderr = log_lib.run_with_log(
+                command,
+                log_path=log_path,
+                stream_logs=stream_logs,
+                shell=True,
+                require_outputs=True)
+            if returncode == 0:
+                break
+            max_retry -= 1
+            time.sleep(backoff.current_backoff())
 
         direction = 'up' if up else 'down'
-        subprocess_utils.handle_returncode(
-            returncode,
-            command,
-            f'Failed to rsync {direction}: {source} -> {target}',
-            stderr=stderr,
-            stream_logs=stream_logs)
+        error_msg = (f'Failed to rsync {direction}: {source} -> {target}. '
+                     'Ensure that the network is stable, then retry.')
+        subprocess_utils.handle_returncode(returncode,
+                                           command,
+                                           error_msg,
+                                           stderr=stderr,
+                                           stream_logs=stream_logs)
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/utils/log_utils.py` & `skypilot-nightly-1.0.0.dev20230713/sky/utils/log_utils.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,20 +1,64 @@
 """Logging utils."""
 import enum
-from typing import List, Optional
+import threading
+from typing import Optional, List
+
+import rich.console as rich_console
 
 import colorama
 import pendulum
 import prettytable
-import rich.status
 
 from sky import sky_logging
 
 logger = sky_logging.init_logger(__name__)
 
+console = rich_console.Console()
+_status = None
+
+
+class _NoOpConsoleStatus:
+    """An empty class for multi-threaded console.status."""
+
+    def __enter__(self):
+        pass
+
+    def __exit__(self, exc_type, exc_val, exc_tb):
+        pass
+
+    def update(self, text):
+        pass
+
+    def stop(self):
+        pass
+
+    def start(self):
+        pass
+
+
+def safe_rich_status(msg: str):
+    """A wrapper for multi-threaded console.status."""
+    if (threading.current_thread() is threading.main_thread() and
+            not sky_logging.is_silent()):
+        global _status
+        if _status is None:
+            _status = console.status(msg)
+        _status.update(msg)
+        return _status
+    return _NoOpConsoleStatus()
+
+
+def force_update_rich_status(msg: str):
+    """Update the status message even if sky_logging.is_silent() is true."""
+    if threading.current_thread() is threading.main_thread():
+        global _status
+        if _status is not None:
+            _status.update(msg)
+
 
 class LineProcessor(object):
     """A processor for log lines."""
 
     def __enter__(self):
         pass
 
@@ -31,15 +75,15 @@
 
     class ProvisionStatus(enum.Enum):
         LAUNCH = 0
         RUNTIME_SETUP = 1
 
     def __enter__(self):
         self.state = self.ProvisionStatus.LAUNCH
-        self.status_display = rich.status.Status('[bold cyan]Launching')
+        self.status_display = safe_rich_status('[bold cyan]Launching')
         self.status_display.start()
 
     def process_line(self, log_line):
         if ('Shared connection to' in log_line and
                 self.state == self.ProvisionStatus.LAUNCH):
             self.status_display.stop()
             logger.info(f'{colorama.Fore.GREEN}Head node is up.'
@@ -63,16 +107,16 @@
                                     field_names=field_names,
                                     **kwargs)
     table.left_padding_width = 0
     table.right_padding_width = 2
     return table
 
 
-def readable_time_duration(start: Optional[int],
-                           end: Optional[int] = None,
+def readable_time_duration(start: Optional[float],
+                           end: Optional[float] = None,
                            absolute: bool = False) -> str:
     """Human readable time duration from timestamps.
 
     Args:
         start: Start timestamp.
         end: End timestamp. If None, current time is used.
         absolute: Whether to return accurate time duration.
@@ -97,14 +141,20 @@
             diff = '< 1 second'
         diff = diff.replace(' seconds', 's')
         diff = diff.replace(' second', 's')
         diff = diff.replace(' minutes', 'm')
         diff = diff.replace(' minute', 'm')
         diff = diff.replace(' hours', 'h')
         diff = diff.replace(' hour', 'h')
+        diff = diff.replace(' days', 'd')
+        diff = diff.replace(' day', 'd')
+        diff = diff.replace(' weeks', 'w')
+        diff = diff.replace(' week', 'w')
+        diff = diff.replace(' months', 'mo')
+        diff = diff.replace(' month', 'mo')
     else:
         diff = start_time.diff_for_humans(end)
         if duration.in_seconds() < 1:
             diff = '< 1 second'
         diff = diff.replace('second', 'sec')
         diff = diff.replace('minute', 'min')
         diff = diff.replace('hour', 'hr')
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/utils/schemas.py` & `skypilot-nightly-1.0.0.dev20230713/sky/utils/schemas.py`

 * *Files 3% similar despite different names*

```diff
@@ -21,14 +21,28 @@
             },
             'region': {
                 'type': 'string',
             },
             'zone': {
                 'type': 'string',
             },
+            'cpus': {
+                'anyOf': [{
+                    'type': 'string',
+                }, {
+                    'type': 'number',
+                }],
+            },
+            'memory': {
+                'anyOf': [{
+                    'type': 'string',
+                }, {
+                    'type': 'number',
+                }],
+            },
             'accelerators': {
                 'anyOf': [{
                     'type': 'string',
                 }, {
                     'type': 'object',
                     'required': [],
                     'maxProperties': 1,
@@ -45,14 +59,17 @@
             },
             'spot_recovery': {
                 'type': 'string',
             },
             'disk_size': {
                 'type': 'integer',
             },
+            'disk_tier': {
+                'type': 'string',
+            },
             'accelerator_args': {
                 'type': 'object',
                 'required': [],
                 'additionalProperties': False,
                 'properties': {
                     'runtime_version': {
                         'type': 'string',
@@ -129,14 +146,17 @@
         'properties': {
             'name': {
                 'type': 'string',
             },
             'workdir': {
                 'type': 'string',
             },
+            'event_callback': {
+                'type': 'string',
+            },
             'num_nodes': {
                 'type': 'integer',
             },
             # resources config is validated separately using RESOURCES_SCHEMA
             'resources': {
                 'type': 'object',
             },
@@ -149,15 +169,21 @@
             },
             'run': {
                 'type': 'string',
             },
             'envs': {
                 'type': 'object',
                 'required': [],
-                'additionalProperties': True,
+                'patternProperties': {
+                    # Checks env keys are valid env var names.
+                    '^[a-zA-Z_][a-zA-Z0-9_]*$': {
+                        'type': 'string'
+                    }
+                },
+                'additionalProperties': False,
             },
             # inputs and outputs are experimental
             'inputs': {
                 'type': 'object',
                 'required': [],
                 'maxProperties': 1,
                 'additionalProperties': {
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/utils/timeline.py` & `skypilot-nightly-1.0.0.dev20230713/sky/utils/timeline.py`

 * *Files 2% similar despite different names*

```diff
@@ -23,15 +23,15 @@
     """Record an event.
 
     Args:
         name: The name of the event.
         message: The message attached to the event.
     """
 
-    def __init__(self, name: str, message: str = None):
+    def __init__(self, name: str, message: Optional[str] = None):
         self._name = name
         self._message = message
         # See the module doc for the event format.
         self._event = {
             'name': self._name,
             'cat': 'event',
             'pid': str(os.getpid()),
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/sky/utils/validator.py` & `skypilot-nightly-1.0.0.dev20230713/sky/utils/validator.py`

 * *Files identical despite different names*

### Comparing `skypilot-nightly-1.0.0.dev20221126/skypilot_nightly.egg-info/PKG-INFO` & `skypilot-nightly-1.0.0.dev20230713/skypilot_nightly.egg-info/PKG-INFO`

 * *Files 12% similar despite different names*

```diff
@@ -1,31 +1,35 @@
 Metadata-Version: 2.1
 Name: skypilot-nightly
-Version: 1.0.0.dev20221126
+Version: 1.0.0.dev20230713
 Summary: SkyPilot: An intercloud broker for the clouds
 Author: SkyPilot Team
 License: Apache 2.0
 Project-URL: Homepage, https://github.com/skypilot-org/skypilot
 Project-URL: Issues, https://github.com/skypilot-org/skypilot/issues
 Project-URL: Discussion, https://github.com/skypilot-org/skypilot/discussions
 Project-URL: Documentation, https://skypilot.readthedocs.io/en/latest/
-Classifier: Programming Language :: Python :: 3.6
 Classifier: Programming Language :: Python :: 3.7
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Operating System :: OS Independent
 Classifier: Topic :: Software Development :: Libraries :: Python Modules
 Classifier: Topic :: System :: Distributed Computing
 Description-Content-Type: text/markdown
 Provides-Extra: aws
 Provides-Extra: azure
 Provides-Extra: gcp
+Provides-Extra: ibm
 Provides-Extra: docker
+Provides-Extra: lambda
+Provides-Extra: cloudflare
+Provides-Extra: scp
+Provides-Extra: oci
 Provides-Extra: all
 License-File: LICENSE
 
 <p align="center">
   <img alt="SkyPilot" src="https://raw.githubusercontent.com/skypilot-org/skypilot/master/docs/source/images/skypilot-wide-light-1k.png" width=55%>
 </p>
 
@@ -34,45 +38,62 @@
     <img alt="Documentation" src="https://readthedocs.org/projects/skypilot/badge/?version=latest">
   </a>
   
   <a href="https://github.com/skypilot-org/skypilot/releases"> 
     <img alt="GitHub Release" src="https://img.shields.io/github/release/skypilot-org/skypilot.svg">
   </a>
   
-  <a href="https://join.slack.com/t/skypilot-org/shared_invite/zt-1i4pa7lyc-g6Lo4_rqqCFWOSXdvwTs3Q"> 
+  <a href="http://slack.skypilot.co"> 
     <img alt="Join Slack" src="https://img.shields.io/badge/SkyPilot-Join%20Slack-blue?logo=slack">
   </a>
   
 </p>
 
 
 <h3 align="center">
-    Run jobs on any cloud, easily and cost effectively
+    Run LLMs and AI on Any Cloud
 </h3>
 
-SkyPilot is a framework for easily and cost effectively running ML workloads<sup>[1]</sup> on any cloud. 
-
-SkyPilot abstracts away cloud infra burden:
-- Launch jobs & clusters on any cloud (AWS, Azure, GCP)
-- Find scarce resources across zones/regions/clouds
-- Queue jobs & use cloud object stores
-
-SkyPilot cuts your cloud costs:
-* [Managed Spot](https://skypilot.readthedocs.io/en/latest/examples/spot-jobs.html): **3x cost savings** using spot VMs, with auto-recovery from preemptions
+----
+:fire: *News* :fire:
+- [June, 2023] Serving LLM **24x Faster On the Cloud** with vLLM and SkyPilot: [**example**](./llm/vllm/), [**blog post**](https://blog.skypilot.co/serving-llm-24x-faster-on-the-cloud-with-vllm-and-skypilot/)
+- [June, 2023] [**Two new clouds supported**](https://skypilot.readthedocs.io/en/latest/getting-started/installation.html): Samsung SCP and Oracle OCI!
+- [April, 2023] **[**SkyPilot YAMLs released**](./llm/vicuna/) for finetuning & serving the Vicuna model with a single command**!
+- [March, 2023] **[Vicuna LLM chatbot](https://lmsys.org/blog/2023-03-30-vicuna/) trained** [**using SkyPilot**](./llm/vicuna/) **for $300 on spot instances!** 
+- [March, 2023] Serve your own LLaMA LLM chatbot (not finetuned) on any cloud: [**example**](./llm/llama-chatbots/), [**repo**](https://github.com/skypilot-org/sky-llama)
+----
+
+SkyPilot is a framework for running LLMs, AI, and batch jobs on any cloud, offering maximum cost savings, highest GPU availability, and managed execution.
+
+SkyPilot **abstracts away cloud infra burdens**:
+- Launch jobs & clusters on any cloud 
+- Easy scale-out: queue and run many jobs, automatically managed
+- Easy access to object stores (S3, GCS, R2)
+
+SkyPilot **maximizes GPU availability for your jobs**:
+* Provision in all zones/regions/clouds you have access to ([the _Sky_](https://arxiv.org/abs/2205.07147)), with automatic failover
+
+SkyPilot **cuts your cloud costs**:
+* [Managed Spot](https://skypilot.readthedocs.io/en/latest/examples/spot-jobs.html): 3-6x cost savings using spot VMs, with auto-recovery from preemptions
+* Optimizer: 2x cost savings by auto-picking the cheapest VM/zone/region/cloud
 * [Autostop](https://skypilot.readthedocs.io/en/latest/reference/auto-stop.html): hands-free cleanup of idle clusters 
-* [Benchmark](https://skypilot.readthedocs.io/en/latest/reference/benchmark/index.html): find best VM types for your jobs
-* Optimizer: **2x cost savings** by auto-picking best prices across zones/regions/clouds
 
 SkyPilot supports your existing GPU, TPU, and CPU workloads, with no code changes. 
 
-Install with pip (choose your clouds) or [from source](https://skypilot.readthedocs.io/en/latest/getting-started/installation.html):
+Install with pip or [from source](https://skypilot.readthedocs.io/en/latest/getting-started/installation.html):
 ```
-pip install "skypilot[aws,gcp,azure]"
+pip install "skypilot[aws,gcp,azure,ibm,oci,scp,lambda]"  # choose your clouds
 ```
 
+Current supported providers (AWS, Azure, GCP, Lambda Cloud, IBM, Samsung, OCI, Cloudflare):
+<p align="center">
+  <img alt="SkyPilot" src="https://raw.githubusercontent.com/skypilot-org/skypilot/master/docs/source/images/cloud-logos-light.png" width=80%>
+</p>
+
+
 ## Getting Started
 You can find our documentation [here](https://skypilot.readthedocs.io/en/latest/).
 - [Installation](https://skypilot.readthedocs.io/en/latest/getting-started/installation.html)
 - [Quickstart](https://skypilot.readthedocs.io/en/latest/getting-started/quickstart.html)
 - [CLI reference](https://skypilot.readthedocs.io/en/latest/reference/cli.html)
 
 ## SkyPilot in 1 minute
@@ -106,15 +127,15 @@
 ```
 
 Prepare the workdir by cloning:
 ```bash
 git clone https://github.com/pytorch/examples.git ~/torch_examples
 ```
 
-Launch with `sky launch`:
+Launch with `sky launch` (note: [access to GPU instances](https://skypilot.readthedocs.io/en/latest/reference/quota.html) is needed for this example):
 ```bash
 sky launch my_task.yaml
 ```
 
 SkyPilot then performs the heavy-lifting for you, including:
 1. Find the lowest priced VM instance type across different clouds
 2. Provision the VM, with auto-failover if the cloud returned capacity errors
@@ -134,21 +155,20 @@
 - [Documentation](https://skypilot.readthedocs.io/en/latest/)
 - [Example: HuggingFace](https://skypilot.readthedocs.io/en/latest/getting-started/tutorial.html) 
 - [Tutorials](https://github.com/skypilot-org/skypilot-tutorial) 
 - [YAML reference](https://skypilot.readthedocs.io/en/latest/reference/yaml-spec.html)
 - Framework examples: [PyTorch DDP](https://github.com/skypilot-org/skypilot/blob/master/examples/resnet_distributed_torch.yaml),  [Distributed](https://github.com/skypilot-org/skypilot/blob/master/examples/resnet_distributed_tf_app.py) [TensorFlow](https://github.com/skypilot-org/skypilot/blob/master/examples/resnet_app_storage.yaml), [JAX/Flax on TPU](https://github.com/skypilot-org/skypilot/blob/master/examples/tpu/tpuvm_mnist.yaml), [Stable Diffusion](https://github.com/skypilot-org/skypilot/tree/master/examples/stable_diffusion), [Detectron2](https://github.com/skypilot-org/skypilot/blob/master/examples/detectron2_docker.yaml), [programmatic grid search](https://github.com/skypilot-org/skypilot/blob/master/examples/huggingface_glue_imdb_grid_search_app.py), [Docker](https://github.com/skypilot-org/skypilot/blob/master/examples/docker/echo_app.yaml), and [many more](./examples).
 
 More information:
-- [Introductory blog post](https://medium.com/@zongheng_yang/skypilot-ml-and-data-science-on-any-cloud-with-massive-cost-savings-244189cc7c0f)
+- [SkyPilot Blog](https://blog.skypilot.co/)
+  - [Introductory blog post](https://blog.skypilot.co/introducing-skypilot/)
+- [NSDI 2023 paper & talk](https://www.usenix.org/conference/nsdi23/presentation/yang-zongheng)
 
 ## Issues, feature requests, and questions
 We are excited to hear your feedback! 
 * For issues and feature requests, please [open a GitHub issue](https://github.com/skypilot-org/skypilot/issues/new).
 * For questions, please use [GitHub Discussions](https://github.com/skypilot-org/skypilot/discussions).
 
-For general discussions, join us on the [SkyPilot Slack](https://join.slack.com/t/skypilot-org/shared_invite/zt-1i4pa7lyc-g6Lo4_rqqCFWOSXdvwTs3Q).
+For general discussions, join us on the [SkyPilot Slack](http://slack.skypilot.co).
 
 ## Contributing
 We welcome and value all contributions to the project! Please refer to [CONTRIBUTING](CONTRIBUTING.md) for how to get involved.
-
-#
-<sup>[1]</sup>: While SkyPilot is currently targeted at machine learning workloads, it supports and has been used for other general batch workloads. We're excited to hear about your use case and how we can better support your requirements; please join us in [this discussion](https://github.com/skypilot-org/skypilot/discussions/1016)!
```

#### html2text {}

```diff
@@ -1,64 +1,83 @@
-Metadata-Version: 2.1 Name: skypilot-nightly Version: 1.0.0.dev20221126
+Metadata-Version: 2.1 Name: skypilot-nightly Version: 1.0.0.dev20230713
 Summary: SkyPilot: An intercloud broker for the clouds Author: SkyPilot Team
 License: Apache 2.0 Project-URL: Homepage, https://github.com/skypilot-org/
 skypilot Project-URL: Issues, https://github.com/skypilot-org/skypilot/issues
 Project-URL: Discussion, https://github.com/skypilot-org/skypilot/discussions
 Project-URL: Documentation, https://skypilot.readthedocs.io/en/latest/
-Classifier: Programming Language :: Python :: 3.6 Classifier: Programming
-Language :: Python :: 3.7 Classifier: Programming Language :: Python :: 3.8
-Classifier: Programming Language :: Python :: 3.9 Classifier: Programming
-Language :: Python :: 3.10 Classifier: License :: OSI Approved :: Apache
-Software License Classifier: Operating System :: OS Independent Classifier:
-Topic :: Software Development :: Libraries :: Python Modules Classifier: Topic
-:: System :: Distributed Computing Description-Content-Type: text/markdown
-Provides-Extra: aws Provides-Extra: azure Provides-Extra: gcp Provides-Extra:
-docker Provides-Extra: all License-File: LICENSE
+Classifier: Programming Language :: Python :: 3.7 Classifier: Programming
+Language :: Python :: 3.8 Classifier: Programming Language :: Python :: 3.9
+Classifier: Programming Language :: Python :: 3.10 Classifier: License :: OSI
+Approved :: Apache Software License Classifier: Operating System :: OS
+Independent Classifier: Topic :: Software Development :: Libraries :: Python
+Modules Classifier: Topic :: System :: Distributed Computing Description-
+Content-Type: text/markdown Provides-Extra: aws Provides-Extra: azure Provides-
+Extra: gcp Provides-Extra: ibm Provides-Extra: docker Provides-Extra: lambda
+Provides-Extra: cloudflare Provides-Extra: scp Provides-Extra: oci Provides-
+Extra: all License-File: LICENSE
                                   [SkyPilot]
                  [Documentation] [GitHub_Release] [Join_Slack]
-         **** Run jobs on any cloud, easily and cost effectively ****
-SkyPilot is a framework for easily and cost effectively running ML workloads[1]
-on any cloud. SkyPilot abstracts away cloud infra burden: - Launch jobs &
-clusters on any cloud (AWS, Azure, GCP) - Find scarce resources across zones/
-regions/clouds - Queue jobs & use cloud object stores SkyPilot cuts your cloud
-costs: * [Managed Spot](https://skypilot.readthedocs.io/en/latest/examples/
-spot-jobs.html): **3x cost savings** using spot VMs, with auto-recovery from
-preemptions * [Autostop](https://skypilot.readthedocs.io/en/latest/reference/
-auto-stop.html): hands-free cleanup of idle clusters * [Benchmark](https://
-skypilot.readthedocs.io/en/latest/reference/benchmark/index.html): find best VM
-types for your jobs * Optimizer: **2x cost savings** by auto-picking best
-prices across zones/regions/clouds SkyPilot supports your existing GPU, TPU,
-and CPU workloads, with no code changes. Install with pip (choose your clouds)
-or [from source](https://skypilot.readthedocs.io/en/latest/getting-started/
-installation.html): ``` pip install "skypilot[aws,gcp,azure]" ``` ## Getting
-Started You can find our documentation [here](https://skypilot.readthedocs.io/
-en/latest/). - [Installation](https://skypilot.readthedocs.io/en/latest/
-getting-started/installation.html) - [Quickstart](https://
-skypilot.readthedocs.io/en/latest/getting-started/quickstart.html) - [CLI
-reference](https://skypilot.readthedocs.io/en/latest/reference/cli.html) ##
-SkyPilot in 1 minute A SkyPilot task specifies: resource requirements, data to
-be synced, setup commands, and the task commands. Once written in this
-[**unified interface**](https://skypilot.readthedocs.io/en/latest/reference/
-yaml-spec.html) (YAML or Python API), the task can be launched on any available
-cloud. This avoids vendor lock-in, and allows easily moving jobs to a different
-provider. Paste the following into a file `my_task.yaml`: ```yaml resources:
-accelerators: V100:1 # 1x NVIDIA V100 GPU num_nodes: 1 # Number of VMs to
-launch # Working directory (optional) containing the project codebase. # Its
-contents are synced to ~/sky_workdir/ on the cluster. workdir: ~/torch_examples
-# Commands to be run before executing the job. # Typical use: pip install -
-r requirements.txt, git clone, etc. setup: | pip install torch torchvision #
-Commands to run as a job. # Typical use: launch the main program. run: | cd
-mnist python main.py --epochs 1 ``` Prepare the workdir by cloning: ```bash git
-clone https://github.com/pytorch/examples.git ~/torch_examples ``` Launch with
-`sky launch`: ```bash sky launch my_task.yaml ``` SkyPilot then performs the
-heavy-lifting for you, including: 1. Find the lowest priced VM instance type
-across different clouds 2. Provision the VM, with auto-failover if the cloud
-returned capacity errors 3. Sync the local `workdir` to the VM 4. Run the
-task's `setup` commands to prepare the VM for running the task 5. Run the
-task's `run` commands
+                    **** Run LLMs and AI on Any Cloud ****
+---- :fire: *News* :fire: - [June, 2023] Serving LLM **24x Faster On the
+Cloud** with vLLM and SkyPilot: [**example**](./llm/vllm/), [**blog post**]
+(https://blog.skypilot.co/serving-llm-24x-faster-on-the-cloud-with-vllm-and-
+skypilot/) - [June, 2023] [**Two new clouds supported**](https://
+skypilot.readthedocs.io/en/latest/getting-started/installation.html): Samsung
+SCP and Oracle OCI! - [April, 2023] **[**SkyPilot YAMLs released**](./llm/
+vicuna/) for finetuning & serving the Vicuna model with a single command**! -
+[March, 2023] **[Vicuna LLM chatbot](https://lmsys.org/blog/2023-03-30-vicuna/
+) trained** [**using SkyPilot**](./llm/vicuna/) **for $300 on spot instances!**
+- [March, 2023] Serve your own LLaMA LLM chatbot (not finetuned) on any cloud:
+[**example**](./llm/llama-chatbots/), [**repo**](https://github.com/skypilot-
+org/sky-llama) ---- SkyPilot is a framework for running LLMs, AI, and batch
+jobs on any cloud, offering maximum cost savings, highest GPU availability, and
+managed execution. SkyPilot **abstracts away cloud infra burdens**: - Launch
+jobs & clusters on any cloud - Easy scale-out: queue and run many jobs,
+automatically managed - Easy access to object stores (S3, GCS, R2) SkyPilot
+**maximizes GPU availability for your jobs**: * Provision in all zones/regions/
+clouds you have access to ([the _Sky_](https://arxiv.org/abs/2205.07147)), with
+automatic failover SkyPilot **cuts your cloud costs**: * [Managed Spot](https:/
+/skypilot.readthedocs.io/en/latest/examples/spot-jobs.html): 3-6x cost savings
+using spot VMs, with auto-recovery from preemptions * Optimizer: 2x cost
+savings by auto-picking the cheapest VM/zone/region/cloud * [Autostop](https://
+skypilot.readthedocs.io/en/latest/reference/auto-stop.html): hands-free cleanup
+of idle clusters SkyPilot supports your existing GPU, TPU, and CPU workloads,
+with no code changes. Install with pip or [from source](https://
+skypilot.readthedocs.io/en/latest/getting-started/installation.html): ``` pip
+install "skypilot[aws,gcp,azure,ibm,oci,scp,lambda]" # choose your clouds ```
+Current supported providers (AWS, Azure, GCP, Lambda Cloud, IBM, Samsung, OCI,
+Cloudflare):
+                                  [SkyPilot]
+## Getting Started You can find our documentation [here](https://
+skypilot.readthedocs.io/en/latest/). - [Installation](https://
+skypilot.readthedocs.io/en/latest/getting-started/installation.html) -
+[Quickstart](https://skypilot.readthedocs.io/en/latest/getting-started/
+quickstart.html) - [CLI reference](https://skypilot.readthedocs.io/en/latest/
+reference/cli.html) ## SkyPilot in 1 minute A SkyPilot task specifies: resource
+requirements, data to be synced, setup commands, and the task commands. Once
+written in this [**unified interface**](https://skypilot.readthedocs.io/en/
+latest/reference/yaml-spec.html) (YAML or Python API), the task can be launched
+on any available cloud. This avoids vendor lock-in, and allows easily moving
+jobs to a different provider. Paste the following into a file `my_task.yaml`:
+```yaml resources: accelerators: V100:1 # 1x NVIDIA V100 GPU num_nodes: 1 #
+Number of VMs to launch # Working directory (optional) containing the project
+codebase. # Its contents are synced to ~/sky_workdir/ on the cluster. workdir:
+~/torch_examples # Commands to be run before executing the job. # Typical use:
+pip install -r requirements.txt, git clone, etc. setup: | pip install torch
+torchvision # Commands to run as a job. # Typical use: launch the main program.
+run: | cd mnist python main.py --epochs 1 ``` Prepare the workdir by cloning:
+```bash git clone https://github.com/pytorch/examples.git ~/torch_examples ```
+Launch with `sky launch` (note: [access to GPU instances](https://
+skypilot.readthedocs.io/en/latest/reference/quota.html) is needed for this
+example): ```bash sky launch my_task.yaml ``` SkyPilot then performs the heavy-
+lifting for you, including: 1. Find the lowest priced VM instance type across
+different clouds 2. Provision the VM, with auto-failover if the cloud returned
+capacity errors 3. Sync the local `workdir` to the VM 4. Run the task's `setup`
+commands to prepare the VM for running the task 5. Run the task's `run`
+commands
                                 [SkyPilot Demo]
 Refer to [Quickstart](https://skypilot.readthedocs.io/en/latest/getting-
 started/quickstart.html) to get started with SkyPilot. ## Learn more -
 [Documentation](https://skypilot.readthedocs.io/en/latest/) - [Example:
 HuggingFace](https://skypilot.readthedocs.io/en/latest/getting-started/
 tutorial.html) - [Tutorials](https://github.com/skypilot-org/skypilot-tutorial)
 - [YAML reference](https://skypilot.readthedocs.io/en/latest/reference/yaml-
@@ -70,22 +89,18 @@
 (https://github.com/skypilot-org/skypilot/blob/master/examples/tpu/
 tpuvm_mnist.yaml), [Stable Diffusion](https://github.com/skypilot-org/skypilot/
 tree/master/examples/stable_diffusion), [Detectron2](https://github.com/
 skypilot-org/skypilot/blob/master/examples/detectron2_docker.yaml),
 [programmatic grid search](https://github.com/skypilot-org/skypilot/blob/
 master/examples/huggingface_glue_imdb_grid_search_app.py), [Docker](https://
 github.com/skypilot-org/skypilot/blob/master/examples/docker/echo_app.yaml),
-and [many more](./examples). More information: - [Introductory blog post]
-(https://medium.com/@zongheng_yang/skypilot-ml-and-data-science-on-any-cloud-
-with-massive-cost-savings-244189cc7c0f) ## Issues, feature requests, and
+and [many more](./examples). More information: - [SkyPilot Blog](https://
+blog.skypilot.co/) - [Introductory blog post](https://blog.skypilot.co/
+introducing-skypilot/) - [NSDI 2023 paper & talk](https://www.usenix.org/
+conference/nsdi23/presentation/yang-zongheng) ## Issues, feature requests, and
 questions We are excited to hear your feedback! * For issues and feature
 requests, please [open a GitHub issue](https://github.com/skypilot-org/
 skypilot/issues/new). * For questions, please use [GitHub Discussions](https://
 github.com/skypilot-org/skypilot/discussions). For general discussions, join us
-on the [SkyPilot Slack](https://join.slack.com/t/skypilot-org/shared_invite/zt-
-1i4pa7lyc-g6Lo4_rqqCFWOSXdvwTs3Q). ## Contributing We welcome and value all
-contributions to the project! Please refer to [CONTRIBUTING](CONTRIBUTING.md)
-for how to get involved. # [1]: While SkyPilot is currently targeted at machine
-learning workloads, it supports and has been used for other general batch
-workloads. We're excited to hear about your use case and how we can better
-support your requirements; please join us in [this discussion](https://
-github.com/skypilot-org/skypilot/discussions/1016)!
+on the [SkyPilot Slack](http://slack.skypilot.co). ## Contributing We welcome
+and value all contributions to the project! Please refer to [CONTRIBUTING]
+(CONTRIBUTING.md) for how to get involved.
```

### Comparing `skypilot-nightly-1.0.0.dev20221126/skypilot_nightly.egg-info/SOURCES.txt` & `skypilot-nightly-1.0.0.dev20230713/skypilot_nightly.egg-info/SOURCES.txt`

 * *Files 21% similar despite different names*

```diff
@@ -10,60 +10,79 @@
 sky/cloud_stores.py
 sky/core.py
 sky/dag.py
 sky/exceptions.py
 sky/execution.py
 sky/global_user_state.py
 sky/optimizer.py
-sky/registry.py
 sky/resources.py
 sky/sky_logging.py
+sky/skypilot_config.py
+sky/status_lib.py
 sky/task.py
 sky/adaptors/__init__.py
 sky/adaptors/aws.py
 sky/adaptors/azure.py
+sky/adaptors/cloudflare.py
 sky/adaptors/docker.py
 sky/adaptors/gcp.py
+sky/adaptors/ibm.py
+sky/adaptors/oci.py
 sky/backends/__init__.py
 sky/backends/backend.py
 sky/backends/backend_utils.py
 sky/backends/cloud_vm_ray_backend.py
 sky/backends/docker_utils.py
 sky/backends/local_docker_backend.py
 sky/backends/onprem_utils.py
 sky/backends/wheel_utils.py
+sky/backends/monkey_patches/monkey_patch_ray_up.py
 sky/benchmark/__init__.py
 sky/benchmark/benchmark_state.py
 sky/benchmark/benchmark_utils.py
 sky/clouds/__init__.py
 sky/clouds/aws.py
 sky/clouds/azure.py
 sky/clouds/cloud.py
 sky/clouds/gcp.py
+sky/clouds/ibm.py
+sky/clouds/lambda_cloud.py
 sky/clouds/local.py
+sky/clouds/oci.py
+sky/clouds/scp.py
 sky/clouds/service_catalog/__init__.py
 sky/clouds/service_catalog/aws_catalog.py
 sky/clouds/service_catalog/azure_catalog.py
 sky/clouds/service_catalog/common.py
+sky/clouds/service_catalog/config.py
 sky/clouds/service_catalog/constants.py
 sky/clouds/service_catalog/gcp_catalog.py
+sky/clouds/service_catalog/ibm_catalog.py
+sky/clouds/service_catalog/lambda_catalog.py
+sky/clouds/service_catalog/oci_catalog.py
+sky/clouds/service_catalog/scp_catalog.py
 sky/clouds/service_catalog/data_fetchers/__init__.py
 sky/clouds/service_catalog/data_fetchers/fetch_aws.py
 sky/clouds/service_catalog/data_fetchers/fetch_azure.py
 sky/clouds/service_catalog/data_fetchers/fetch_gcp.py
+sky/clouds/service_catalog/data_fetchers/fetch_lambda_cloud.py
 sky/data/__init__.py
 sky/data/data_transfer.py
 sky/data/data_utils.py
 sky/data/mounting_utils.py
 sky/data/storage.py
 sky/data/storage_utils.py
+sky/provision/__init__.py
+sky/provision/aws/__init__.py
+sky/provision/aws/instance.py
 sky/setup_files/MANIFEST.in
 sky/setup_files/setup.py
-sky/skylet/LICENCE
+sky/skylet/LICENSE
 sky/skylet/__init__.py
+sky/skylet/attempt_skylet.py
 sky/skylet/autostop_lib.py
 sky/skylet/configs.py
 sky/skylet/constants.py
 sky/skylet/events.py
 sky/skylet/job_lib.py
 sky/skylet/log_lib.py
 sky/skylet/skylet.py
@@ -77,46 +96,70 @@
 sky/skylet/providers/azure/__init__.py
 sky/skylet/providers/azure/azure-config-template.json
 sky/skylet/providers/azure/azure-vm-template.json
 sky/skylet/providers/azure/config.py
 sky/skylet/providers/azure/node_provider.py
 sky/skylet/providers/gcp/__init__.py
 sky/skylet/providers/gcp/config.py
+sky/skylet/providers/gcp/constants.py
 sky/skylet/providers/gcp/node.py
 sky/skylet/providers/gcp/node_provider.py
+sky/skylet/providers/ibm/__init__.py
+sky/skylet/providers/ibm/node_provider.py
+sky/skylet/providers/ibm/utils.py
+sky/skylet/providers/ibm/vpc_provider.py
+sky/skylet/providers/lambda_cloud/__init__.py
+sky/skylet/providers/lambda_cloud/lambda_utils.py
+sky/skylet/providers/lambda_cloud/node_provider.py
+sky/skylet/providers/oci/__init__.py
+sky/skylet/providers/oci/config.py
+sky/skylet/providers/oci/node_provider.py
+sky/skylet/providers/oci/query_helper.py
+sky/skylet/providers/oci/utils.py
+sky/skylet/providers/scp/__init__.py
+sky/skylet/providers/scp/config.py
+sky/skylet/providers/scp/node_provider.py
+sky/skylet/providers/scp/scp_utils.py
 sky/skylet/ray_patches/__init__.py
 sky/skylet/ray_patches/autoscaler.py.patch
-sky/skylet/ray_patches/azure_cli.py.patch
 sky/skylet/ray_patches/cli.py.patch
 sky/skylet/ray_patches/command_runner.py.patch
-sky/skylet/ray_patches/job_manager.py.patch
+sky/skylet/ray_patches/job_head.py.patch
 sky/skylet/ray_patches/log_monitor.py.patch
 sky/skylet/ray_patches/resource_demand_scheduler.py.patch
 sky/skylet/ray_patches/updater.py.patch
 sky/skylet/ray_patches/worker.py.patch
 sky/spot/__init__.py
 sky/spot/constants.py
 sky/spot/controller.py
 sky/spot/recovery_strategy.py
 sky/spot/spot_state.py
 sky/spot/spot_utils.py
+sky/spot/dashboard/dashboard.py
+sky/spot/dashboard/static/favicon.ico
+sky/spot/dashboard/templates/index.html
 sky/templates/aws-ray.yml.j2
 sky/templates/azure-ray.yml.j2
 sky/templates/gcp-ray.yml.j2
 sky/templates/gcp-tpu-create.sh.j2
 sky/templates/gcp-tpu-delete.sh.j2
+sky/templates/ibm-ray.yml.j2
+sky/templates/lambda-ray.yml.j2
 sky/templates/local-ray.yml.j2
+sky/templates/oci-ray.yml.j2
+sky/templates/scp-ray.yml.j2
 sky/templates/spot-controller.yaml.j2
 sky/usage/__init__.py
 sky/usage/constants.py
 sky/usage/usage_lib.py
 sky/utils/__init__.py
 sky/utils/accelerator_registry.py
 sky/utils/command_runner.py
 sky/utils/common_utils.py
+sky/utils/dag_utils.py
 sky/utils/db_utils.py
 sky/utils/env_options.py
 sky/utils/log_utils.py
 sky/utils/schemas.py
 sky/utils/subprocess_utils.py
 sky/utils/timeline.py
 sky/utils/tpu_utils.py
@@ -125,8 +168,20 @@
 sky/utils/cli_utils/__init__.py
 sky/utils/cli_utils/status_utils.py
 skypilot_nightly.egg-info/PKG-INFO
 skypilot_nightly.egg-info/SOURCES.txt
 skypilot_nightly.egg-info/dependency_links.txt
 skypilot_nightly.egg-info/entry_points.txt
 skypilot_nightly.egg-info/requires.txt
-skypilot_nightly.egg-info/top_level.txt
+skypilot_nightly.egg-info/top_level.txt
+tests/test_cli.py
+tests/test_config.py
+tests/test_global_user_state.py
+tests/test_list_accelerators.py
+tests/test_onprem.py
+tests/test_optimizer_dryruns.py
+tests/test_optimizer_random_dag.py
+tests/test_pycryptodome_version.py
+tests/test_smoke.py
+tests/test_spot.py
+tests/test_storage.py
+tests/test_wheels.py
```

