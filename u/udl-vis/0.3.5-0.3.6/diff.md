# Comparing `tmp/udl_vis-0.3.5-py3-none-any.whl.zip` & `tmp/udl_vis-0.3.6-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,38 +1,39 @@
-Zip file size: 423863 bytes, number of entries: 226
+Zip file size: 430825 bytes, number of entries: 227
 -rw-rw-rw-  2.0 fat      250 b- defN 23-Jun-09 10:56 udl_vis/__init__.py
 -rw-rw-rw-  2.0 fat       15 b- defN 23-Jun-09 09:55 udl_vis/AutoDL/__init__.py
--rw-rw-rw-  2.0 fat    14554 b- defN 23-Jun-22 19:22 udl_vis/AutoDL/trainer.py
+-rw-rw-rw-  2.0 fat    13274 b- defN 23-Jul-01 01:36 udl_vis/AutoDL/base_runner.py
+-rw-rw-rw-  2.0 fat    14215 b- defN 23-Jul-09 15:52 udl_vis/AutoDL/trainer.py
 -rw-rw-rw-  2.0 fat       15 b- defN 23-Jun-09 09:55 udl_vis/Basis/__init__.py
 -rw-rw-rw-  2.0 fat     2851 b- defN 22-Oct-04 08:46 udl_vis/Basis/cal_ssim.py
 -rw-rw-rw-  2.0 fat    29004 b- defN 23-Jun-07 03:16 udl_vis/Basis/config.py
 -rw-rw-rw-  2.0 fat     4087 b- defN 23-Jun-07 03:16 udl_vis/Basis/criterion_metrics.py
 -rw-rw-rw-  2.0 fat     6622 b- defN 22-Oct-04 08:46 udl_vis/Basis/dist_utils.py
 -rw-rw-rw-  2.0 fat    14882 b- defN 22-Oct-04 08:46 udl_vis/Basis/launch.py
--rw-rw-rw-  2.0 fat     8772 b- defN 23-Jun-21 16:46 udl_vis/Basis/logger.py
+-rw-rw-rw-  2.0 fat     9077 b- defN 23-Jun-24 10:26 udl_vis/Basis/logger.py
 -rw-rw-rw-  2.0 fat     3145 b- defN 23-Jun-07 03:16 udl_vis/Basis/metrics.py
 -rw-rw-rw-  2.0 fat    16806 b- defN 23-Jun-19 15:32 udl_vis/Basis/module.py
 -rw-rw-rw-  2.0 fat     9926 b- defN 22-Oct-04 08:46 udl_vis/Basis/optim.py
--rw-rw-rw-  2.0 fat     6333 b- defN 23-Jun-21 17:55 udl_vis/Basis/option.py
--rw-rw-rw-  2.0 fat    17306 b- defN 23-Jun-07 03:16 udl_vis/Basis/postprocess.py
--rw-rw-rw-  2.0 fat     5187 b- defN 23-Jun-09 18:39 udl_vis/Basis/python_sub_class.py
+-rw-rw-rw-  2.0 fat     6464 b- defN 23-Jul-08 06:01 udl_vis/Basis/option.py
+-rw-rw-rw-  2.0 fat    17556 b- defN 23-Jul-09 16:03 udl_vis/Basis/postprocess.py
+-rw-rw-rw-  2.0 fat     8175 b- defN 23-Jun-30 22:00 udl_vis/Basis/python_sub_class.py
 -rw-rw-rw-  2.0 fat     3145 b- defN 23-Jun-07 03:16 udl_vis/Basis/variance_sacling_initializer.py
 -rw-rw-rw-  2.0 fat      134 b- defN 22-Oct-04 08:46 udl_vis/Basis/auxiliary/__init__.py
 -rw-rw-rw-  2.0 fat     1084 b- defN 22-Oct-04 08:46 udl_vis/Basis/auxiliary/base.py
 -rw-rw-rw-  2.0 fat     7303 b- defN 22-Oct-04 08:46 udl_vis/Basis/auxiliary/fp16_utils.py
 -rw-rw-rw-  2.0 fat    11956 b- defN 23-Jun-07 03:14 udl_vis/Basis/auxiliary/utils.py
--rw-rw-rw-  2.0 fat      787 b- defN 22-Oct-04 08:46 udl_vis/Basis/auxiliary/torchstat/__init__.py
+-rw-rw-rw-  2.0 fat      724 b- defN 23-Jul-03 14:47 udl_vis/Basis/auxiliary/torchstat/__init__.py
 -rw-rw-rw-  2.0 fat     1142 b- defN 22-Oct-04 08:46 udl_vis/Basis/auxiliary/torchstat/__main__.py
--rw-rw-rw-  2.0 fat     9296 b- defN 22-Oct-04 08:46 udl_vis/Basis/auxiliary/torchstat/compute_flops.py
--rw-rw-rw-  2.0 fat     6085 b- defN 22-Oct-04 08:46 udl_vis/Basis/auxiliary/torchstat/compute_madd.py
--rw-rw-rw-  2.0 fat     4016 b- defN 22-Oct-04 08:46 udl_vis/Basis/auxiliary/torchstat/compute_memory.py
--rw-rw-rw-  2.0 fat    10202 b- defN 22-Oct-04 08:46 udl_vis/Basis/auxiliary/torchstat/model_hook.py
--rw-rw-rw-  2.0 fat     3758 b- defN 22-Oct-04 08:46 udl_vis/Basis/auxiliary/torchstat/reporter.py
--rw-rw-rw-  2.0 fat     5899 b- defN 22-Oct-04 08:46 udl_vis/Basis/auxiliary/torchstat/stat_tree.py
--rw-rw-rw-  2.0 fat     3364 b- defN 22-Oct-04 08:46 udl_vis/Basis/auxiliary/torchstat/statistics.py
+-rw-rw-rw-  2.0 fat    10544 b- defN 23-Jul-05 09:27 udl_vis/Basis/auxiliary/torchstat/compute_flops.py
+-rw-rw-rw-  2.0 fat     6303 b- defN 23-Jul-04 16:13 udl_vis/Basis/auxiliary/torchstat/compute_madd.py
+-rw-rw-rw-  2.0 fat     4245 b- defN 23-Jul-04 16:13 udl_vis/Basis/auxiliary/torchstat/compute_memory.py
+-rw-rw-rw-  2.0 fat    12481 b- defN 23-Jul-04 18:37 udl_vis/Basis/auxiliary/torchstat/model_hook.py
+-rw-rw-rw-  2.0 fat     4193 b- defN 23-Jul-04 15:49 udl_vis/Basis/auxiliary/torchstat/reporter.py
+-rw-rw-rw-  2.0 fat     6134 b- defN 23-Jul-03 16:23 udl_vis/Basis/auxiliary/torchstat/stat_tree.py
+-rw-rw-rw-  2.0 fat     3761 b- defN 23-Jul-04 18:37 udl_vis/Basis/auxiliary/torchstat/statistics.py
 -rw-rw-rw-  2.0 fat      367 b- defN 22-Oct-04 08:46 udl_vis/mmcv/__init__.py
 -rw-rw-rw-  2.0 fat     1211 b- defN 22-Oct-04 08:46 udl_vis/mmcv/version.py
 -rw-rw-rw-  2.0 fat      137 b- defN 22-Oct-04 08:46 udl_vis/mmcv/arraymisc/__init__.py
 -rw-rw-rw-  2.0 fat     1879 b- defN 22-Oct-04 08:46 udl_vis/mmcv/arraymisc/quantization.py
 -rw-rw-rw-  2.0 fat     2479 b- defN 22-Oct-04 08:46 udl_vis/mmcv/cnn/__init__.py
 -rw-rw-rw-  2.0 fat     2051 b- defN 22-Oct-04 08:46 udl_vis/mmcv/cnn/alexnet.py
 -rw-rw-rw-  2.0 fat     1119 b- defN 22-Oct-04 08:46 udl_vis/mmcv/cnn/builder.py
@@ -144,66 +145,66 @@
 -rw-rw-rw-  2.0 fat     2454 b- defN 22-Oct-04 08:46 udl_vis/mmcv/parallel/data_container.py
 -rw-rw-rw-  2.0 fat     4588 b- defN 22-Oct-04 08:46 udl_vis/mmcv/parallel/data_parallel.py
 -rw-rw-rw-  2.0 fat     4945 b- defN 23-Jun-07 03:14 udl_vis/mmcv/parallel/distributed.py
 -rw-rw-rw-  2.0 fat     2887 b- defN 22-Oct-04 08:46 udl_vis/mmcv/parallel/distributed_deprecated.py
 -rw-rw-rw-  2.0 fat      328 b- defN 23-Jun-07 03:14 udl_vis/mmcv/parallel/registry.py
 -rw-rw-rw-  2.0 fat     2366 b- defN 22-Oct-04 08:46 udl_vis/mmcv/parallel/scatter_gather.py
 -rw-rw-rw-  2.0 fat      728 b- defN 22-Oct-04 08:46 udl_vis/mmcv/parallel/utils.py
--rw-rw-rw-  2.0 fat     4385 b- defN 22-Oct-04 08:46 udl_vis/mmcv/runner/__init__.py
+-rw-rw-rw-  2.0 fat     4428 b- defN 23-Jun-25 04:44 udl_vis/mmcv/runner/__init__.py
 -rw-rw-rw-  2.0 fat    14570 b- defN 23-Jun-07 03:14 udl_vis/mmcv/runner/base_module.py
--rw-rw-rw-  2.0 fat    24585 b- defN 23-Jun-22 07:57 udl_vis/mmcv/runner/base_runner.py
+-rw-rw-rw-  2.0 fat    24691 b- defN 23-Jul-09 15:52 udl_vis/mmcv/runner/base_runner.py
 -rw-rw-rw-  2.0 fat      690 b- defN 22-Oct-04 08:46 udl_vis/mmcv/runner/builder.py
--rw-rw-rw-  2.0 fat    34069 b- defN 23-Jun-22 08:29 udl_vis/mmcv/runner/checkpoint.py
+-rw-rw-rw-  2.0 fat    34094 b- defN 23-Jul-07 16:05 udl_vis/mmcv/runner/checkpoint.py
 -rw-rw-rw-  2.0 fat     1952 b- defN 22-Oct-04 08:46 udl_vis/mmcv/runner/default_constructor.py
 -rw-rw-rw-  2.0 fat     5559 b- defN 22-Oct-04 08:46 udl_vis/mmcv/runner/dist_utils.py
--rw-rw-rw-  2.0 fat    14412 b- defN 23-Jun-22 19:31 udl_vis/mmcv/runner/epoch_based_runner.py
+-rw-rw-rw-  2.0 fat    14439 b- defN 23-Jun-24 18:22 udl_vis/mmcv/runner/epoch_based_runner.py
 -rw-rw-rw-  2.0 fat    16873 b- defN 23-Jun-07 03:14 udl_vis/mmcv/runner/fp16_utils.py
 -rw-rw-rw-  2.0 fat    11438 b- defN 23-Jun-07 03:14 udl_vis/mmcv/runner/iter_based_runner.py
 -rw-rw-rw-  2.0 fat     1880 b- defN 22-Oct-04 08:46 udl_vis/mmcv/runner/log_buffer.py
 -rw-rw-rw-  2.0 fat     1241 b- defN 22-Oct-04 08:46 udl_vis/mmcv/runner/misc.py
 -rw-rw-rw-  2.0 fat     1658 b- defN 22-Oct-04 08:46 udl_vis/mmcv/runner/priority.py
--rw-rw-rw-  2.0 fat    11169 b- defN 23-Jun-17 15:49 udl_vis/mmcv/runner/record.py
+-rw-rw-rw-  2.0 fat    11372 b- defN 23-Jul-09 16:22 udl_vis/mmcv/runner/record.py
 -rw-rw-rw-  2.0 fat     3014 b- defN 23-Jun-07 03:14 udl_vis/mmcv/runner/utils.py
--rw-rw-rw-  2.0 fat     2402 b- defN 22-Oct-04 08:46 udl_vis/mmcv/runner/hooks/__init__.py
--rw-rw-rw-  2.0 fat    20344 b- defN 23-Jun-22 08:13 udl_vis/mmcv/runner/hooks/checkpoint.py
+-rw-rw-rw-  2.0 fat     2525 b- defN 23-Jun-25 04:44 udl_vis/mmcv/runner/hooks/__init__.py
+-rw-rw-rw-  2.0 fat    20433 b- defN 23-Jul-08 06:03 udl_vis/mmcv/runner/hooks/checkpoint.py
 -rw-rw-rw-  2.0 fat      280 b- defN 22-Oct-04 08:46 udl_vis/mmcv/runner/hooks/closure.py
 -rw-rw-rw-  2.0 fat     3674 b- defN 22-Oct-04 08:46 udl_vis/mmcv/runner/hooks/ema.py
 -rw-rw-rw-  2.0 fat    22917 b- defN 23-Jun-09 14:10 udl_vis/mmcv/runner/hooks/evaluation.py
--rw-rw-rw-  2.0 fat     2927 b- defN 23-Jun-18 03:48 udl_vis/mmcv/runner/hooks/hook.py
+-rw-rw-rw-  2.0 fat     8495 b- defN 23-Jun-25 04:47 udl_vis/mmcv/runner/hooks/hook.py
 -rw-rw-rw-  2.0 fat      521 b- defN 22-Oct-04 08:46 udl_vis/mmcv/runner/hooks/iter_timer.py
 -rw-rw-rw-  2.0 fat    27815 b- defN 23-Jun-07 03:14 udl_vis/mmcv/runner/hooks/lr_updater.py
 -rw-rw-rw-  2.0 fat      682 b- defN 22-Oct-04 08:46 udl_vis/mmcv/runner/hooks/memory.py
 -rw-rw-rw-  2.0 fat    23465 b- defN 23-Jun-07 03:14 udl_vis/mmcv/runner/hooks/momentum_updater.py
 -rw-rw-rw-  2.0 fat     1602 b- defN 22-Oct-04 08:46 udl_vis/mmcv/runner/hooks/nni_hook.py
--rw-rw-rw-  2.0 fat    25296 b- defN 23-Jun-07 03:14 udl_vis/mmcv/runner/hooks/optimizer.py
+-rw-rw-rw-  2.0 fat    26497 b- defN 23-Jun-25 04:02 udl_vis/mmcv/runner/hooks/optimizer.py
 -rw-rw-rw-  2.0 fat     8221 b- defN 22-Oct-04 08:46 udl_vis/mmcv/runner/hooks/profiler.py
 -rw-rw-rw-  2.0 fat      867 b- defN 22-Oct-04 08:46 udl_vis/mmcv/runner/hooks/sampler_seed.py
 -rw-rw-rw-  2.0 fat      729 b- defN 22-Oct-04 08:46 udl_vis/mmcv/runner/hooks/sync_buffer.py
 -rw-rw-rw-  2.0 fat      537 b- defN 22-Oct-04 08:46 udl_vis/mmcv/runner/hooks/logger/__init__.py
--rw-rw-rw-  2.0 fat     6492 b- defN 23-Jun-21 15:02 udl_vis/mmcv/runner/hooks/logger/base.py
+-rw-rw-rw-  2.0 fat     6568 b- defN 23-Jul-09 16:10 udl_vis/mmcv/runner/hooks/logger/base.py
 -rw-rw-rw-  2.0 fat     2294 b- defN 22-Oct-04 08:46 udl_vis/mmcv/runner/hooks/logger/dvclive.py
 -rw-rw-rw-  2.0 fat     2995 b- defN 23-Jun-07 03:14 udl_vis/mmcv/runner/hooks/logger/mlflow.py
 -rw-rw-rw-  2.0 fat     3338 b- defN 22-Oct-04 08:46 udl_vis/mmcv/runner/hooks/logger/neptune.py
 -rw-rw-rw-  2.0 fat     5269 b- defN 23-Jun-07 03:14 udl_vis/mmcv/runner/hooks/logger/pavi.py
--rw-rw-rw-  2.0 fat     2739 b- defN 23-Jun-07 03:14 udl_vis/mmcv/runner/hooks/logger/tensorboard.py
--rw-rw-rw-  2.0 fat    12470 b- defN 23-Jun-21 04:35 udl_vis/mmcv/runner/hooks/logger/text.py
+-rw-rw-rw-  2.0 fat     4256 b- defN 23-Jul-11 08:17 udl_vis/mmcv/runner/hooks/logger/tensorboard.py
+-rw-rw-rw-  2.0 fat    12607 b- defN 23-Jul-09 16:37 udl_vis/mmcv/runner/hooks/logger/text.py
 -rw-rw-rw-  2.0 fat     4009 b- defN 23-Jun-07 03:14 udl_vis/mmcv/runner/hooks/logger/wandb.py
 -rw-rw-rw-  2.0 fat      379 b- defN 22-Oct-04 08:46 udl_vis/mmcv/runner/optimizer/__init__.py
 -rw-rw-rw-  2.0 fat     1390 b- defN 22-Oct-04 08:46 udl_vis/mmcv/runner/optimizer/builder.py
 -rw-rw-rw-  2.0 fat    11971 b- defN 23-Jun-07 03:14 udl_vis/mmcv/runner/optimizer/default_constructor.py
 -rw-rw-rw-  2.0 fat      788 b- defN 22-Oct-04 08:46 udl_vis/mmcv/tensorrt/__init__.py
 -rw-rw-rw-  2.0 fat      916 b- defN 22-Oct-04 08:46 udl_vis/mmcv/tensorrt/init_plugins.py
 -rw-rw-rw-  2.0 fat     4459 b- defN 22-Oct-04 08:46 udl_vis/mmcv/tensorrt/preprocess.py
 -rw-rw-rw-  2.0 fat     8417 b- defN 22-Oct-04 08:46 udl_vis/mmcv/tensorrt/tensorrt_utils.py
--rw-rw-rw-  2.0 fat     4027 b- defN 22-Oct-04 08:46 udl_vis/mmcv/utils/__init__.py
+-rw-rw-rw-  2.0 fat     4059 b- defN 23-Jun-24 19:14 udl_vis/mmcv/utils/__init__.py
 -rw-rw-rw-  2.0 fat    27039 b- defN 22-Oct-04 08:46 udl_vis/mmcv/utils/config.py
 -rw-rw-rw-  2.0 fat     3407 b- defN 23-Jun-07 03:14 udl_vis/mmcv/utils/env.py
 -rw-rw-rw-  2.0 fat     2092 b- defN 22-Oct-04 08:46 udl_vis/mmcv/utils/ext_loader.py
 -rw-rw-rw-  2.0 fat     6167 b- defN 22-Oct-04 08:46 udl_vis/mmcv/utils/hub.py
--rw-rw-rw-  2.0 fat    14137 b- defN 23-Jun-22 19:25 udl_vis/mmcv/utils/logging.py
+-rw-rw-rw-  2.0 fat    14378 b- defN 23-Jul-08 15:48 udl_vis/mmcv/utils/logging.py
 -rw-rw-rw-  2.0 fat    11864 b- defN 22-Oct-04 08:46 udl_vis/mmcv/utils/misc.py
 -rw-rw-rw-  2.0 fat      940 b- defN 22-Oct-04 08:46 udl_vis/mmcv/utils/parrots_jit.py
 -rw-rw-rw-  2.0 fat     3643 b- defN 22-Oct-04 08:46 udl_vis/mmcv/utils/parrots_wrapper.py
 -rw-rw-rw-  2.0 fat     3516 b- defN 22-Oct-04 08:46 udl_vis/mmcv/utils/path.py
 -rw-rw-rw-  2.0 fat     7313 b- defN 22-Oct-04 08:46 udl_vis/mmcv/utils/progressbar.py
 -rw-rw-rw-  2.0 fat    11722 b- defN 22-Oct-04 08:46 udl_vis/mmcv/utils/registry.py
 -rw-rw-rw-  2.0 fat     4429 b- defN 22-Oct-04 08:46 udl_vis/mmcv/utils/testing.py
@@ -216,13 +217,13 @@
 -rw-rw-rw-  2.0 fat     5439 b- defN 23-Jun-07 03:14 udl_vis/mmcv/video/processing.py
 -rw-rw-rw-  2.0 fat      347 b- defN 22-Oct-04 08:46 udl_vis/mmcv/visualization/__init__.py
 -rw-rw-rw-  2.0 fat     1420 b- defN 23-Jun-07 03:14 udl_vis/mmcv/visualization/color.py
 -rw-rw-rw-  2.0 fat     5285 b- defN 23-Jun-07 03:14 udl_vis/mmcv/visualization/image.py
 -rw-rw-rw-  2.0 fat     3477 b- defN 23-Jun-07 03:14 udl_vis/mmcv/visualization/optflow.py
 -rw-rw-rw-  2.0 fat        0 b- defN 23-Jun-22 08:40 udl_vis/tests/__init__.py
 -rw-rw-rw-  2.0 fat     2143 b- defN 23-Jun-22 08:53 udl_vis/tests/test_pytorch_dataloader.py
--rw-rw-rw-  2.0 fat    35823 b- defN 23-Jun-22 19:34 udl_vis-0.3.5.dist-info/LICENSE
--rw-rw-rw-  2.0 fat     3501 b- defN 23-Jun-22 19:34 udl_vis-0.3.5.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 23-Jun-22 19:34 udl_vis-0.3.5.dist-info/WHEEL
--rw-rw-rw-  2.0 fat        8 b- defN 23-Jun-22 19:34 udl_vis-0.3.5.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat    20502 b- defN 23-Jun-22 19:34 udl_vis-0.3.5.dist-info/RECORD
-226 files, 1447427 bytes uncompressed, 391317 bytes compressed:  73.0%
+-rw-rw-rw-  2.0 fat    35823 b- defN 23-Jul-14 03:13 udl_vis-0.3.6.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat     3715 b- defN 23-Jul-14 03:13 udl_vis-0.3.6.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 23-Jul-14 03:13 udl_vis-0.3.6.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat        8 b- defN 23-Jul-14 03:13 udl_vis-0.3.6.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat    20590 b- defN 23-Jul-14 03:13 udl_vis-0.3.6.dist-info/RECORD
+227 files, 1478704 bytes uncompressed, 398145 bytes compressed:  73.1%
```

## zipnote {}

```diff
@@ -1,13 +1,16 @@
 Filename: udl_vis/__init__.py
 Comment: 
 
 Filename: udl_vis/AutoDL/__init__.py
 Comment: 
 
+Filename: udl_vis/AutoDL/base_runner.py
+Comment: 
+
 Filename: udl_vis/AutoDL/trainer.py
 Comment: 
 
 Filename: udl_vis/Basis/__init__.py
 Comment: 
 
 Filename: udl_vis/Basis/cal_ssim.py
@@ -657,23 +660,23 @@
 
 Filename: udl_vis/tests/__init__.py
 Comment: 
 
 Filename: udl_vis/tests/test_pytorch_dataloader.py
 Comment: 
 
-Filename: udl_vis-0.3.5.dist-info/LICENSE
+Filename: udl_vis-0.3.6.dist-info/LICENSE
 Comment: 
 
-Filename: udl_vis-0.3.5.dist-info/METADATA
+Filename: udl_vis-0.3.6.dist-info/METADATA
 Comment: 
 
-Filename: udl_vis-0.3.5.dist-info/WHEEL
+Filename: udl_vis-0.3.6.dist-info/WHEEL
 Comment: 
 
-Filename: udl_vis-0.3.5.dist-info/top_level.txt
+Filename: udl_vis-0.3.6.dist-info/top_level.txt
 Comment: 
 
-Filename: udl_vis-0.3.5.dist-info/RECORD
+Filename: udl_vis-0.3.6.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## udl_vis/AutoDL/trainer.py

```diff
@@ -24,69 +24,37 @@
 from udl_vis.mmcv.parallel import MMDataParallel, MMDistributedDataParallel
 from udl_vis.mmcv.runner import (DistSamplerSeedHook, EpochBasedRunner,
                                  Fp16OptimizerHook, OptimizerHook, build_optimizer,
                                  build_runner, get_dist_info)
 
 import inspect
 
+
 # 10s
 # from mmdet.datasets import (build_dataloader, build_dataset,
 #                             replace_ImageToTensor)
 
 def trainer(cfg, logger, build_model,
             getDataSession,
+            runner=None,
             distributed=False,
-            meta=None):
-
+            meta=None, **kwargs):
     # TODO: 对于多个model进行任务的封装的时候，放进构建器里，而不是这里？ 似乎会增加构建代价
     # TODO: 构建
     model, criterion, optimizer, scheduler = build_model(cfg.arch, cfg.task, cfg)
 
     print_log(cfg.pretty_text, logger=logger)
 
     if hasattr(model, 'init_weights'):
         model.init_weights()
 
-    ############################################################
-    # 不适合多任务
-    ############################################################
-    # datasets = [build_dataset(cfg.data.train)]
-    # if len(cfg.workflow) == 2:
-    #     val_dataset = copy.deepcopy(cfg.data.val)
-    #     val_dataset.pipeline = cfg.data.train.pipeline
-    #     datasets.append(build_dataset(val_dataset))
-    # model.CLASSES = datasets[0].CLASSES
-
-    # prepare data loaders
-    # datasets = datasets if isinstance(datasets, (list, tuple)) else [datasets]
-
-    # runner_type = 'EpochBasedRunner' if 'runner' not in cfg else cfg.runner[
-    #     'type']
-    # data_loaders = [
-    #     build_dataloader(
-    #         ds,
-    #         cfg.samples_per_gpu,
-    #         cfg.workers_per_gpu,
-    #         # `num_gpus` will be ignored if distributed
-    #         num_gpus=1,
-    #         dist=distributed,
-    #         seed=args.seed,
-    #         runner_type=runner_type,
-    #         persistent_workers=cfg.data.get('persistent_workers', False))
-    #     for ds in datasets
-    # ]
-
     sess = getDataSession(cfg)
     # cfg.valid_or_test = False
     if cfg.eval:
         cfg.workflow = [('test', 1)]
-    # if not any('train' in mode for mode, _ in cfg.workflow):
-    #     cfg.eval = True
-    # if not any('valid' in mode for mode, _ in cfg.workflow):
-    #     cfg.valid_or_test = True
 
     # put model on gpus
     if distributed:
         find_unused_parameters = cfg.get('find_unused_parameters', False)
         # Sets the `find_unused_parameters` parameter in
         # torch.nn.parallel.DistributedDataParallel
         model = MMDistributedDataParallel(
@@ -103,117 +71,124 @@
             if isinstance(model.model, dict):  # 实际运行的模型可以有多个，通过字典区分
                 for name, m in model.model.items():  # model不是模型, model.model是字典
                     model.model[name] = MMDataParallel(m, device_ids=cfg.gpu_ids)
             else:
                 model.model = MMDataParallel(model.model, device_ids=cfg.gpu_ids)
         else:
             model = MMDataParallel(model, device_ids=cfg.gpu_ids)
-
-    # 改到 build_model里，一次性设置，方便查找
-    if cfg.get('optimizer', None) is not None:
-        optimizer = build_optimizer(model.model.module, cfg.optimizer)
-
-    # 兼容argparser和配置文件的
-    if 'runner' not in cfg:
-        cfg.runner = {
-            'type': 'EpochBasedRunner',
-            'max_epochs': cfg.epochs  # argparser
-        }
-        warnings.warn(
-            'config is now expected to have a `runner` section, '
-            'please set `runner` in your config.', UserWarning)
-    else:
-        if 'epochs' in cfg and 'max_iters' not in cfg.runner:
-            cfg.runner['max_epochs'] = cfg.epochs
-            # assert cfg.epochs == cfg.runner['max_epochs'], print(cfg.epochs, cfg.runner['max_epochs'])
-
-    runner = build_runner(
-        cfg.runner,
-        default_args=dict(
-            model=model,
-            optimizer=optimizer,
-            seed=cfg.seed,
-            work_dir=cfg.work_dir,
-            logger=logger,
-            meta=meta,
-            opt_cfg={'log_interval': cfg.log_interval,
-                     'save_interval': cfg.save_interval,
-                     'accumulated_step': cfg.accumulated_step,
-                     'grad_clip': cfg.grad_clip,
-                     'dataset': cfg.dataset,
-                     'img_range': cfg.img_range,
-                     'metrics': cfg.metrics,
-                     'save_fmt': cfg.save_fmt,
-                     'mode': cfg.mode,
-                     'eval': cfg.eval,
-                     # 'val_mode': cfg.valid_or_test, # 在base_runner的resume里用于设置测试最大轮数来评估训练好的模型
-                     'save_dir': cfg.work_dir + "/results"}))
-
-    # an ugly workaround to make .log and .log.json filenames the same
-    # runner.timestamp = timestamp
-
-    # fp16 setting
-    fp16_cfg = cfg.get('fp16', None)
-    if fp16_cfg is not None:
-        optimizer_config = Fp16OptimizerHook(
-            **cfg.optimizer_config, **fp16_cfg, distributed=distributed)
-    elif distributed and 'type' not in cfg.optimizer_config:
-        optimizer_config = OptimizerHook(**cfg.optimizer_config)
+    if runner is not None:
+        runner = runner(cfg, model=model, optimizer=optimizer,
+                        scheduler=scheduler,
+                        logger=logger, **kwargs)
     else:
-        optimizer_config = cfg.get('optimizer_config', None)
+        # 改到 build_model里，一次性设置，方便查找
+        if cfg.get('optimizer', None) is not None:
+            optimizer = build_optimizer(model.model.module, cfg.optimizer)
+
+        # 兼容argparser和配置文件的
+        if 'runner' not in cfg:
+            cfg.runner = {
+                'type': 'EpochBasedRunner',
+                'max_epochs': cfg.epochs  # argparser
+            }
+            warnings.warn(
+                'config is now expected to have a `runner` section, '
+                'please set `runner` in your config.', UserWarning)
+        else:
+            if 'epochs' in cfg and 'max_iters' not in cfg.runner:
+                cfg.runner['max_epochs'] = cfg.epochs
+                # assert cfg.epochs == cfg.runner['max_epochs'], print(cfg.epochs, cfg.runner['max_epochs'])
+
+        runner = build_runner(
+            cfg.runner,
+            default_args=dict(
+                model=model,
+                optimizer=optimizer,
+                seed=cfg.seed,
+                work_dir=cfg.work_dir,
+                tfb_dir = cfg.tfb_dir,
+                logger=logger,
+                meta=meta,
+                opt_cfg={'log_interval': cfg.log_interval,
+                         'save_interval': cfg.save_interval,
+                         'accumulated_step': cfg.accumulated_step,
+                         'grad_clip': cfg.grad_clip,
+                         'dataset': cfg.dataset,
+                         'img_range': cfg.img_range,
+                         'metrics': cfg.metrics,
+                         'save_fmt': cfg.save_fmt,
+                         # 'mode': cfg.mode,
+                         'test': cfg.test,
+                         'eval': cfg.eval,
+                         # 'val_mode': cfg.valid_or_test, # 在base_runner的resume里用于设置测试最大轮数来评估训练好的模型
+                         'save_dir': cfg.work_dir + "/results"}))
+
+        # an ugly workaround to make .log and .log.json filenames the same
+        # runner.timestamp = timestamp
+
+        # fp16 setting
+        fp16_cfg = cfg.get('fp16', None)
+        if fp16_cfg is not None:
+            optimizer_config = Fp16OptimizerHook(
+                **cfg.optimizer_config, **fp16_cfg, distributed=distributed)
+        elif distributed and 'type' not in cfg.optimizer_config:
+            optimizer_config = OptimizerHook(**cfg.optimizer_config)
+        else:
+            optimizer_config = cfg.get('optimizer_config', None)
 
-    ############################################################
-    # register training hooks
-    ############################################################
-    if cfg.get('config', None) is not None and os.path.isfile(cfg.config):
-        '''
-        optimizer = dict(type='SGD', lr=0.1, momentum=0.9, weight_decay=0.0001)
-        optimizer_config = dict(grad_clip=None)
-        lr_config = dict(policy='step', step=[100, 150])
-        checkpoint_config = dict(interval=1)
-        log_config = dict(
-            interval=100,
-            hooks=[
-                dict(type='TextLoggerHook'),
-                # dict(type='TensorboardLoggerHook')
-            ])
-        '''
-        runner.register_training_hooks(
-            cfg.lr_config,
-            optimizer_config,
-            cfg.checkpoint_config,
-            cfg.log_config,
-            cfg.get('momentum_config', None),
-            custom_hooks_config=cfg.get('custom_hooks', None))
-
-
-    elif cfg.get('log_config', None) is None and len(cfg.workflow) and cfg.workflow[0][0] != 'simple_train':
-        # 提供time, data_time, memory等，并且用于mode里区别IterBasedRunner? 在train模式下提供了有无time的区别
-        if cfg.mode == 'nni':
-            runner.register_custom_hooks({'type': 'NNIHook', 'priority': 'very_low'})
-        if scheduler is not None:
-            runner.register_lr_hook(dict(policy=scheduler.__class__.__name__[:-2], step=scheduler.step_size))
-        runner.register_checkpoint_hook(
-            dict(type='ModelCheckpoint', indicator='loss', save_top_k=cfg.save_top_k,
-                 use_log_and_save=cfg.use_log_and_save, save_interval=cfg.save_interval))
-        runner.register_optimizer_hook(dict(grad_clip=cfg.grad_clip))  # ExternOptimizer
-        runner.register_timer_hook(dict(type='IterTimerHook'))
-        log_config = [dict(type='TextLoggerHook')]
-        if cfg.use_tfb:
-            log_config.append(dict(type='TensorboardLoggerHook'))
-        runner.register_logger_hooks(dict(
-            interval=cfg.log_interval,
-            hooks=log_config))
+        ############################################################
+        # register training hooks
+        ############################################################
+        if cfg.get('config', None) is not None and os.path.isfile(cfg.config):
+            '''
+            optimizer = dict(type='SGD', lr=0.1, momentum=0.9, weight_decay=0.0001)
+            optimizer_config = dict(grad_clip=None)
+            lr_config = dict(policy='step', step=[100, 150])
+            checkpoint_config = dict(interval=1)
+            log_config = dict(
+                interval=100,
+                hooks=[
+                    dict(type='TextLoggerHook'),
+                    # dict(type='TensorboardLoggerHook')
+                ])
+            '''
+            runner.register_training_hooks(
+                cfg.lr_config,
+                optimizer_config,
+                cfg.checkpoint_config,
+                cfg.log_config,
+                cfg.get('momentum_config', None),
+                custom_hooks_config=cfg.get('custom_hooks', None))
+
+
+        elif cfg.get('log_config', None) is None and len(cfg.workflow) and cfg.workflow[0][0] != 'simple_train':
+            # 提供time, data_time, memory等，并且用于mode里区别IterBasedRunner? 在train模式下提供了有无time的区别
+            if cfg.mode == 'nni':
+                runner.register_custom_hooks({'type': 'NNIHook', 'priority': 'very_low'})
+            if scheduler is not None:
+                runner.register_lr_hook(dict(policy=scheduler.__class__.__name__[:-2], step=scheduler.step_size))
+            runner.register_checkpoint_hook(
+                dict(type='ModelCheckpoint', indicator='loss', save_top_k=cfg.save_top_k,
+                     use_save=cfg.use_save, save_interval=cfg.save_interval, start_save_epoch=cfg.start_save_epoch))
+            runner.register_optimizer_hook(dict(grad_clip=cfg.grad_clip))  # ExternOptimizer
+            runner.register_timer_hook(dict(type='IterTimerHook'))
+            log_config = [dict(type='TextLoggerHook')]
+            if cfg.use_tfb:
+                log_config.append(dict(type='TensorboardLoggerHook'))
+            runner.register_logger_hooks(dict(
+                interval=cfg.log_interval,
+                hooks=log_config))
 
-    else:
-        runner.register_checkpoint_hook(dict(type='ModelCheckpoint', indicator='loss'))
+        else:
+            runner.register_checkpoint_hook(dict(type='ModelCheckpoint', indicator='loss'))
 
-    if distributed:
-        if isinstance(runner, EpochBasedRunner):
-            runner.register_hook(DistSamplerSeedHook())
+
+        if distributed:
+            if isinstance(runner, EpochBasedRunner):
+                runner.register_hook(DistSamplerSeedHook())
 
     ############################################################
     # register validate hooks
     ############################################################
     # if cfg.validate:
     #     # Support batch_size > 1 in validation
     #     val_samples_per_gpu = cfg.data.val.pop('samples_per_gpu', 1)
@@ -258,70 +233,74 @@
             # cfg.dataset = cfg.dataset + '_OrigScale_multiExm1.h5'
             # cfg.dataset = cfg.dataset + '_multiExm1.h5'
 
             eval_loader, eval_sampler = sess.get_eval_dataloader(cfg.dataset[mode], distributed)
 
             eval_cfg = cfg.get('evaluation', {})
             eval_cfg['by_epoch'] = cfg.runner['type'] != 'IterBasedRunner'
-            from udl_vis.mmcv.runner import EvalHook, DistEvalHook
-            eval_hook = DistEvalHook if distributed else EvalHook
-            # In this PR (https://github.com/open-mmlab/mmcv/pull/1193), the
-            # priority of IterTimerHook has been modified from 'NORMAL' to 'LOW'.
-            if mode != 'simple_val':
-                runner.register_hook(
-                    eval_hook(eval_loader, **eval_cfg), priority='LOW')
+            # from udl_vis.mmcv.runner import EvalHook, DistEvalHook
+            # eval_hook = DistEvalHook if distributed else EvalHook
+            # # In this PR (https://github.com/open-mmlab/mmcv/pull/1193), the
+            # # priority of IterTimerHook has been modified from 'NORMAL' to 'LOW'.
+            # if mode != 'simple_val':
+            #     runner.register_hook(
+            #         eval_hook(eval_loader, **eval_cfg), priority='LOW')
 
             data_loaders['test'] = eval_loader
             cfg.workflow[idx] = ('test', epoch)
             # if len(cfg.workflow) == 0:
             #     cfg.workflow.append(('val', 1))
 
         if 'valid' in mode:
             valid_loader, valid_sampler = sess.get_valid_dataloader(cfg.dataset[mode], distributed)
             if cfg.once_epoch:
                 valid_loader = iter(list(valid_loader))
             data_loaders['val'] = valid_loader
             cfg.workflow[idx] = ('val', epoch)
 
         if 'train' in mode:
-            train_loader, train_sampler, generator = sess.get_dataloader(cfg.dataset[mode], distributed, state_dataloader)
+            train_loader, train_sampler, generator = sess.get_dataloader(cfg.dataset[mode], distributed,
+                                                                         state_dataloader)
             # 保存generator状态用于恢复数据批次/轮次
             runner.generator = generator
             if cfg.once_epoch:
                 train_loader = iter(list(train_loader))
             data_loaders[mode] = train_loader
 
             if len(cfg.workflow) == 0:
                 cfg.workflow.append(('simple_train', 1))
 
-
     ############################################################
     # 载入数据，运行模型
     ############################################################
     # print(inspect.getfile(model.model.__class__).split(cfg.arch)[0])
-    if cfg.use_log_and_save and not os.path.exists("/".join([cfg.work_dir, "codes"])):
-        shutil.copytree("/".join([inspect.getfile(model.model.module.__class__).split(cfg.arch)[0], cfg.arch]),
+    if not os.path.exists("/".join([cfg.work_dir, "codes"])) and os.path.isdir(cfg.code_dir):
+        # "/".join([inspect.getfile(model.model.module.__class__).split(cfg.arch)[0], cfg.arch])
+        shutil.copytree(cfg.code_dir,
                         "/".join([cfg.work_dir, "codes"]))
 
+    print_log(cfg.pretty_text, logger=logger)
+
     runner.run(data_loaders, cfg.workflow)
 
 
-def main(cfg, build_model, getDataSession):
+def main(cfg, build_model, getDataSession, runner=None, **kwargs):
     # init distributed env first, since logger depends on the dist info.
     if cfg.launcher == 'none':
         distributed = False
     else:
         distributed = True
         init_dist(cfg.launcher, **cfg.dist_params)
         # re-set gpu_ids with distributed training mode
         _, world_size = get_dist_info()
         cfg.gpu_ids = range(world_size)
 
     logger, out_dir, model_save_dir, tfb_dir = create_logger(cfg, cfg.experimental_desc, 0)
     cfg.out_dir = cfg.work_dir = model_save_dir
+    cfg.tfb_dir = tfb_dir
     cfg.seed = init_random_seed(cfg.seed)
     print_log(f'Set random seed to {cfg.seed}', logger=logger)
 
     set_random_seed(cfg.seed)
 
     # if cfg.checkpoint_config is not None:
     #     # save mmdet version, config file content and class names in
@@ -332,9 +311,11 @@
     # add an attribute for visualization convenience
 
     trainer(
         cfg,
         logger,
         build_model,
         getDataSession,
+        runner,
         distributed=distributed,
-        meta={})
+        meta={},
+        **kwargs)
```

## udl_vis/Basis/logger.py

```diff
@@ -10,180 +10,180 @@
 import os
 import functools
 import torch.distributed as dist
 import colorlog
 import time
 from pathlib import Path
 
-logger_initialized = {}
-
-log_colors_config = {
-    'DEBUG': 'cyan',
-    'INFO': 'white',
-    'WARNING': 'yellow',
-    'ERROR': 'red',
-    'CRITICAL': 'red',
-}
-
-
-# def get_root_logger(name, log_file=None, log_level=logging.INFO):
-#     return get_logger('mmcls', log_file, log_level)
-def get_root_logger(name=None, cfg=None, cfg_name=None, log_level=logging.INFO):
-    return get_logger(name, cfg, cfg_name, log_level)
-# TODO: Depre
-# the same as "get_root_logger"
-def create_logger(cfg=None, cfg_name=None, dist_print=0, log_level=logging.INFO):
-    return get_logger(None, cfg, cfg_name, log_level)
-
-@functools.lru_cache()  # so that calling setup_logger multiple times won't add many handlers
-def setup_logger(name, final_log_file, color=True):
-    # LOG_DIR = cfg.log_dir
-    # LOG_FOUT = open(final_log_file, 'w')
-    # head = '%(asctime)-15s %(message)s'
-
-    logging.basicConfig(filename=str(final_log_file).replace('\\', '/'), format='%(message)s', level=logging.INFO)
-    # logger = logging.getLogger()
-    # logger.setLevel(logging.INFO)
-    # console = logging.StreamHandler()
-    # logging.getLogger('').addHandler(console)
-
-    logger = logging.getLogger(name)
-    # if name in logger_initialized:
-    #     return logger
-
-    for handler in logger.root.handlers:
-        if type(handler) is logging.StreamHandler:
-            handler.setLevel(logging.ERROR)
-
-    # stream_handler = logging.StreamHandler()
-    console = colorlog.StreamHandler()
-    handlers = [console]
-
-    # logger.setLevel(logging.INFO)
-    # formatter = colorlog.ColoredFormatter(
-    #     '%(log_color)s[%(asctime)s] [%(filename)s:%(lineno)d] [%(module)s:%(funcName)s] [%(levelname)s]- %(message)s',
-    #     log_colors=log_colors_config)  # 日志输出格式
-
-    if dist.is_available() and dist.is_initialized():
-        rank = dist.get_rank()
-    else:
-        rank = 0
-
-    if rank == 0:
-        # console = colorlog.StreamHandler()
-        # console.setLevel(logging.DEBUG)
-        handlers.append(console)
-        # if color:
-        #     formatter = _ColorfulFormatter(
-        #         colored("%(message)s", "green")
-        #     )
-        # else:
-    formatter = colorlog.ColoredFormatter(
-        '%(log_color)s- %(message)s',
-        log_colors=log_colors_config)  # 日志输出格式
-
-    # console.setFormatter(formatter)
-    # logger.addHandler(console)
-    for handler in handlers:
-        handler.setFormatter(formatter)
-        handler.setLevel(logging.INFO)  # log_level
-        logger.addHandler(handler)
-
-    # if rank == 0:
-    #     logger.setLevel(logging.INFO)  # log_level
-    # else:
-    #     logger.setLevel(logging.ERROR)
-
-    logger_initialized[name] = True
-
-    return logger
-
-
-def get_logger(name=None, cfg=None, cfg_name=None, phase='train', log_level=logging.INFO, file_mode='w'):  # log_file=None,
-    """Initialize and get a logger by name.
-
-    If the logger has not been initialized, this method will initialize the
-    logger by adding one or two handlers, otherwise the initialized logger will
-    be directly returned. During initialization, a StreamHandler will always be
-    added. If `log_file` is specified and the process rank is 0, a FileHandler
-    will also be added.
-
-    Args:
-        name (str): Logger name.
-        log_file (str | None): The log filename. If specified, a FileHandler
-            will be added to the logger.
-        log_level (int): The logger level. Note that only the process of
-            rank 0 is affected, and other processes will set the level to
-            "Error" thus be silent most of the time.
-        file_mode (str): The file mode used in opening log file.
-            Defaults to 'w'.
-
-    Returns:
-        logging.Logger: The expected logger.
-    """
-    if name in logger_initialized:
-        if cfg is None: # cfg.use_log
-            return logging.getLogger(name)
-        else:
-            return None
-    # handle hierarchical names
-    # e.g., logger "a" is initialized, then logger "a.b" will skip the
-    # initialization since it is a child of "a".
-    for logger_name in logger_initialized:
-        if name.startswith(logger_name):
-            if cfg.use_log_and_save:
-                return logging.getLogger(name)
-            else:
-                return None
-
-    logger = None
-    tensorboard_log_dir = None
-    root_output_dir = Path(cfg.out_dir)
-    # set up logger in root_path
-    if not root_output_dir.exists():
-        # if not dist_print: #rank 0-N, 0 is False
-        print('=> creating {}'.format(root_output_dir))
-        root_output_dir.mkdir(parents=True, exist_ok=True)
-
-    dataset = cfg.dataset
-    model = cfg.arch
-    cfg_name = os.path.basename(cfg_name).split('.')[0]
-    time_str = time.strftime('%Y-%m-%d-%H-%M-%S')
-
-    # store all output except tb_log file
-    final_output_dir = root_output_dir / dataset / model / cfg_name
-    if cfg.eval:
-        model_save_tmp = os.path.dirname(cfg.resume).split('/')[-1]
-    else:
-        model_save_tmp = "model_{}".format(time_str)
-
-    model_save_dir = final_output_dir / model_save_tmp
-    # if not dist_print:
-    log_string('=> creating {}'.format(final_output_dir))
-    final_output_dir.mkdir(parents=True, exist_ok=True)
-    model_save_dir.mkdir(parents=True, exist_ok=True)
-
-
-    if cfg.use_log_and_save:
-        cfg_name = '{}_{}'.format(cfg_name, time_str)
-        # a logger to save results
-        log_file = '{}_{}.log'.format(cfg_name, phase)
-        if cfg.eval:
-            final_log_file = model_save_dir / log_file
-        else:
-            final_log_file = final_output_dir / log_file
-            # tensorboard_log
-            tensorboard_log_dir = root_output_dir / Path(cfg.log_dir) / dataset / model / cfg_name
-            # if not dist_print:
-            print('=> creating tfb logs {}'.format(tensorboard_log_dir))
-            tensorboard_log_dir.mkdir(parents=True, exist_ok=True)
-        logger = setup_logger(name, final_log_file)
-
-    return logger, str(final_output_dir), str(model_save_dir), str(
-        tensorboard_log_dir)  # logger,
+# logger_initialized = {}
+#
+# log_colors_config = {
+#     'DEBUG': 'cyan',
+#     'INFO': 'white',
+#     'WARNING': 'yellow',
+#     'ERROR': 'red',
+#     'CRITICAL': 'red',
+# }
+#
+#
+# # def get_root_logger(name, log_file=None, log_level=logging.INFO):
+# #     return get_logger('mmcls', log_file, log_level)
+# def get_root_logger(name=None, cfg=None, cfg_name=None, log_level=logging.INFO):
+#     return get_logger(name, cfg, cfg_name, log_level)
+# # TODO: Depre
+# # the same as "get_root_logger"
+# def create_logger(cfg=None, cfg_name=None, dist_print=0, log_level=logging.INFO):
+#     return get_logger(None, cfg, cfg_name, log_level)
+#
+# @functools.lru_cache()  # so that calling setup_logger multiple times won't add many handlers
+# def setup_logger(name, final_log_file, color=True):
+#     # LOG_DIR = cfg.log_dir
+#     # LOG_FOUT = open(final_log_file, 'w')
+#     # head = '%(asctime)-15s %(message)s'
+#
+#     logging.basicConfig(filename=str(final_log_file).replace('\\', '/'), format='%(message)s', level=logging.INFO)
+#     # logger = logging.getLogger()
+#     # logger.setLevel(logging.INFO)
+#     # console = logging.StreamHandler()
+#     # logging.getLogger('').addHandler(console)
+#
+#     logger = logging.getLogger(name)
+#     # if name in logger_initialized:
+#     #     return logger
+#
+#     for handler in logger.root.handlers:
+#         if type(handler) is logging.StreamHandler:
+#             handler.setLevel(logging.ERROR)
+#
+#     # stream_handler = logging.StreamHandler()
+#     console = colorlog.StreamHandler()
+#     handlers = [console]
+#
+#     # logger.setLevel(logging.INFO)
+#     # formatter = colorlog.ColoredFormatter(
+#     #     '%(log_color)s[%(asctime)s] [%(filename)s:%(lineno)d] [%(module)s:%(funcName)s] [%(levelname)s]- %(message)s',
+#     #     log_colors=log_colors_config)  # 日志输出格式
+#
+#     if dist.is_available() and dist.is_initialized():
+#         rank = dist.get_rank()
+#     else:
+#         rank = 0
+#
+#     if rank == 0:
+#         # console = colorlog.StreamHandler()
+#         # console.setLevel(logging.DEBUG)
+#         handlers.append(console)
+#         # if color:
+#         #     formatter = _ColorfulFormatter(
+#         #         colored("%(message)s", "green")
+#         #     )
+#         # else:
+#     formatter = colorlog.ColoredFormatter(
+#         '%(log_color)s- %(message)s',
+#         log_colors=log_colors_config)  # 日志输出格式
+#
+#     # console.setFormatter(formatter)
+#     # logger.addHandler(console)
+#     for handler in handlers:
+#         handler.setFormatter(formatter)
+#         handler.setLevel(logging.INFO)  # log_level
+#         logger.addHandler(handler)
+#
+#     # if rank == 0:
+#     #     logger.setLevel(logging.INFO)  # log_level
+#     # else:
+#     #     logger.setLevel(logging.ERROR)
+#
+#     logger_initialized[name] = True
+#
+#     return logger
+#
+#
+# def get_logger(name=None, cfg=None, cfg_name=None, phase='train', log_level=logging.INFO, file_mode='w'):  # log_file=None,
+#     """Initialize and get a logger by name.
+#
+#     If the logger has not been initialized, this method will initialize the
+#     logger by adding one or two handlers, otherwise the initialized logger will
+#     be directly returned. During initialization, a StreamHandler will always be
+#     added. If `log_file` is specified and the process rank is 0, a FileHandler
+#     will also be added.
+#
+#     Args:
+#         name (str): Logger name.
+#         log_file (str | None): The log filename. If specified, a FileHandler
+#             will be added to the logger.
+#         log_level (int): The logger level. Note that only the process of
+#             rank 0 is affected, and other processes will set the level to
+#             "Error" thus be silent most of the time.
+#         file_mode (str): The file mode used in opening log file.
+#             Defaults to 'w'.
+#
+#     Returns:
+#         logging.Logger: The expected logger.
+#     """
+#     if name in logger_initialized:
+#         if cfg is None: # cfg.use_log
+#             return logging.getLogger(name)
+#         else:
+#             return None
+#     # handle hierarchical names
+#     # e.g., logger "a" is initialized, then logger "a.b" will skip the
+#     # initialization since it is a child of "a".
+#     for logger_name in logger_initialized:
+#         if name.startswith(logger_name):
+#             if cfg.use_log_and_save:
+#                 return logging.getLogger(name)
+#             else:
+#                 return None
+#
+#     logger = None
+#     tensorboard_log_dir = None
+#     root_output_dir = Path(cfg.out_dir)
+#     # set up logger in root_path
+#     if not root_output_dir.exists():
+#         # if not dist_print: #rank 0-N, 0 is False
+#         print('=> creating {}'.format(root_output_dir))
+#         root_output_dir.mkdir(parents=True, exist_ok=True)
+#
+#     dataset = cfg.dataset
+#     model = cfg.arch
+#     cfg_name = os.path.basename(cfg_name).split('.')[0]
+#     time_str = time.strftime('%Y-%m-%d-%H-%M-%S')
+#
+#     # store all output except tb_log file
+#     final_output_dir = root_output_dir / dataset / model / cfg_name
+#     if cfg.eval:
+#         model_save_tmp = os.path.dirname(cfg.resume).split('/')[-1]
+#     else:
+#         model_save_tmp = "model_{}".format(time_str)
+#
+#     model_save_dir = final_output_dir / model_save_tmp
+#     # if not dist_print:
+#     log_string('=> creating {}'.format(final_output_dir))
+#     final_output_dir.mkdir(parents=True, exist_ok=True)
+#     model_save_dir.mkdir(parents=True, exist_ok=True)
+#
+#
+#     if cfg.use_log_and_save:
+#         cfg_name = '{}_{}'.format(cfg_name, time_str)
+#         # a logger to save results
+#         log_file = '{}_{}.log'.format(cfg_name, phase)
+#         if cfg.eval:
+#             final_log_file = model_save_dir / log_file
+#         else:
+#             final_log_file = final_output_dir / log_file
+#             # tensorboard_log
+#             tensorboard_log_dir = root_output_dir / Path(cfg.log_dir) / dataset / model / cfg_name
+#             # if not dist_print:
+#             print('=> creating tfb logs {}'.format(tensorboard_log_dir))
+#             tensorboard_log_dir.mkdir(parents=True, exist_ok=True)
+#         logger = setup_logger(name, final_log_file)
+#
+#     return logger, str(final_output_dir), str(model_save_dir), str(
+#         tensorboard_log_dir)  # logger,
 
 def print_log(msg, logger=None, level=logging.INFO):
     """Print a log message.
 
     Args:
         msg (str): The message to be logged.
         logger (logging.Logger | str | None): The logger to be used.
```

## udl_vis/Basis/option.py

```diff
@@ -12,15 +12,15 @@
 from udl_vis.Basis.logger import print_log
 import warnings
 
 def common_cfg():
     parser = argparse.ArgumentParser(description='PyTorch Training')
 
     # * Logger
-    parser.add_argument('--use_log_and_save', default=True
+    parser.add_argument('--use_log', default=True
                         , type=bool)
     parser.add_argument('--log-dir', metavar='DIR', default='logs',
                         help='path to save log')
     parser.add_argument('--tfb-dir', metavar='DIR', default=None,
                         help='useless in this script.')
     parser.add_argument('--use-tfb', default=False, type=bool)
 
@@ -80,14 +80,19 @@
     args.start_epoch = 1
     assert args.accumulated_step > 0
     args.load_model_strict = True
     args.resume_mode = 'best'
     args.validate = False
     args.gpu_ids = [0]
     args.prefix_model = ''
+    args.use_colorlog = True
+    args.use_save= True
+    args.test = ""
+    args.code_dir = ""
+    args.start_save_epoch = 1
     # args.workflow = []
 
     return Config(args)
 
 
 def nni_cfg(args):
     if args.mode == 'nni':
@@ -123,19 +128,19 @@
         cfg = data_cfg(cfg)
         # print(cfg.pretty_text)
 
         self.merge_from_dict(cfg)
 
 
 def data_cfg(cfg):
-    if cfg.get('config', None) is not None and os.path.isfile(cfg.config):
-        print_log(f"reading {cfg.config}")
-        cfg.merge_from_dict(cfg.fromfile(cfg.config))
-    else:
-        print_log(f"reading {cfg.config} failed")
+    # if cfg.get('config', None) is not None and os.path.isfile(cfg.config):
+    #     print_log(f"reading {cfg.config}")
+    #     cfg.merge_from_dict(cfg.fromfile(cfg.config))
+    # else:
+    #     print_log(f"reading {cfg.config} failed")
 
     if cfg.get('data', None) is not None and callable(cfg.data):
         data_func = cfg.pop('data')
         cfg.merge_from_dict(Config(data_func(cfg.data_dir)))
 
     cfg.workflow = cfg.get('workflow', [])
     if cfg.get('norm_cfg', None) is not None and cfg.launcher == 'none':
```

## udl_vis/Basis/postprocess.py

```diff
@@ -284,16 +284,22 @@
     if isinstance(images, torch.Tensor):
         unnormlize = np.where(max(np.float(torch.max(images)), 1.0) > 1.0, 1.0, unnormlize)
         if first_channel:
             images = images.permute(1, 2, 0)
         output = images[..., [0, 2, 4]] * torch.tensor(unnormlize)
         output = torch.clamp(output, 0, 2047)
         output = output.cpu().detach().numpy()
-
-    norm_image = linstretch(output)
+    else:
+        unnormlize = np.where(max(np.float(images.max()), 1.0) > 1.0, 1.0, unnormlize)
+        output = images[..., [0, 2, 4]] * unnormlize
+        output = np.clip(output, 0, 2047)
+    try:
+        norm_image = linstretch(output)
+    except:
+        norm_image = output
     return norm_image[:, :, ::-1]
 
 
 def linstretch(images, tol=None):
     '''
     NM = N*M;
     for i=1:3
```

## udl_vis/Basis/python_sub_class.py

```diff
@@ -96,16 +96,80 @@
             warning = f'Got {key}={value} but expected ' \
                       f'one of {cls._task.keys()}'
             warnings.warn(warning)
             return Config()
 
         return cls(**kwargs)
 
+# class ModelDispatcher(object):
+#     _task = dict()
+# 
+#     def __init_subclass__(cls, name='', **kwargs):
+#         super().__init_subclass__(**kwargs)
+#         if name != '':
+#             cls._task[name] = cls
+#             cls._name = name
+#             # print(cls.__repr__, cls..__repr__)
+#         else:
+#             # warnings.warn(f'Creating a subclass of MetaModel {cls.__name__} with no name.')
+#             cls._task[cls.__name__] = cls
+#             cls._name = cls.__name__
+# 
+#     def __new__(cls, *args, **kwargs):
+#         if cls is ModelDispatcher:
+#             task = kwargs.get('task')
+#             try:
+#                 cls = cls._task[task]
+#             except KeyError:
+#                 raise ValueError(f'Got task={task} but expected'
+#                                  f'one of {cls._task.keys()}')
+# 
+#         instance = super().__new__(cls)
+# 
+#         return instance
+# 
+#     @classmethod
+#     def build_model(cls, cfg):
+# 
+#         arch = cfg.arch
+#         task = cfg.task
+#         model_style = cfg.model_style
+# 
+#         try:
+#             # 获得PansharpeningModel,进行分发
+#             cls = cls._task[task](None, None)
+#         except KeyError:
+#             raise ValueError(f'Got task={task} but expected '
+#                              f'one of {cls._task.keys()} in {cls}')
+#         try:
+#             # 获得具体的模型
+#             cls_arch = cls._models[arch]()
+#         except KeyError:
+#             raise ValueError(f'Got arch={arch} but expected '
+#                              f'one of {cls._models.keys()} in {cls}')
+# 
+#         model, criterion, optimizer, scheduler = cls_arch(cfg)
+# 
+#         if model_style is None:
+#             # 获得PansharpeningModel,model+head
+#             model_style = task
+# 
+#         if model_style is not None:
+#             try:
+#                 # 获得具体的模型
+#                 model = cls._task[model_style](model, criterion)
+#             except KeyError:
+#                 raise ValueError(f'Got model_style={model_style} but expected '
+#                                  f'one of {cls._models.keys()} (merged in _models) in {cls}')
+# 
+#         return model, criterion, optimizer, scheduler
+# 
+
 class ModelDispatcher(object):
-    _task = dict()
+    _task = dict()  # __init_subclass__调用优先级高于__init__, 无法使用self._task,可以使用cls._task
 
     def __init_subclass__(cls, name='', **kwargs):
         super().__init_subclass__(**kwargs)
         if name != '':
             cls._task[name] = cls
             cls._name = name
             # print(cls.__repr__, cls..__repr__)
@@ -124,22 +188,23 @@
                                  f'one of {cls._task.keys()}')
 
         instance = super().__new__(cls)
 
         return instance
 
     @classmethod
-    def build_model(cls, cfg):
-
+    def build_model_from_task(cls, cfg):
+        # TODO: baseline + head, structure like DETR/mmlab,
+        #  but you hardly direct know about the model structure
         arch = cfg.arch
         task = cfg.task
         model_style = cfg.model_style
 
         try:
-            # 获得PansharpeningModel,进行分发
+            #获得PansharpeningModel,进行分发
             cls = cls._task[task](None, None)
         except KeyError:
             raise ValueError(f'Got task={task} but expected '
                              f'one of {cls._task.keys()} in {cls}')
         try:
             # 获得具体的模型
             cls_arch = cls._models[arch]()
@@ -159,7 +224,22 @@
                 model = cls._task[model_style](model, criterion)
             except KeyError:
                 raise ValueError(f'Got model_style={model_style} but expected '
                                  f'one of {cls._models.keys()} (merged in _models) in {cls}')
 
         return model, criterion, optimizer, scheduler
 
+    @classmethod
+    def build_task_from_model(cls, cfg):
+
+        arch = cfg.arch
+
+        try:
+            # 获得具体的模型
+            cls_arch = cls._task[arch]()
+        except KeyError:
+            raise ValueError(f'Got arch={arch} but expected '
+                             f'one of {cls._task.keys()} in {cls}')
+
+        model, criterion, optimizer, scheduler = cls_arch(cfg)
+
+        return model, criterion, optimizer, scheduler
```

## udl_vis/Basis/auxiliary/torchstat/__init__.py

```diff
@@ -4,18 +4,18 @@
 # @Author  : Xiao Wu, LiangJian Deng
 # @reference:
 __copyright__ = 'Copyright (C) 2018 Swall0w'
 __version__ = '0.0.7'
 __author__ = 'Swall0w'
 __url__ = 'https://github.com/Swall0w/torchstat'
 
-from torchstat.compute_memory import compute_memory
-from torchstat.compute_madd import compute_madd
-from torchstat.compute_flops import compute_flops
-from torchstat.stat_tree import StatTree, StatNode
-from torchstat.model_hook import ModelHook
-from torchstat.reporter import report_format
-from torchstat.statistics import stat, ModelStat
+from .compute_memory import compute_memory
+from .compute_madd import compute_madd
+from .compute_flops import compute_flops
+from .stat_tree import StatTree, StatNode
+from .model_hook import ModelHook
+from .reporter import report_format
+from .statistics import stat, ModelStat
 
 __all__ = ['report_format', 'StatTree', 'StatNode', 'compute_madd',
            'compute_flops', 'ModelHook', 'stat', 'ModelStat', '__main__',
            'compute_memory']
```

## udl_vis/Basis/auxiliary/torchstat/compute_flops.py

```diff
@@ -7,36 +7,41 @@
 def compute_flops(module, inp, out):
     # print(module.__class__)
     # if 'attn' in module.__name__:
     #     print(module.__class__)
     # print(list(filter(lambda m: not m.startswith("__") and not m.endswith("__") and callable(getattr(module, m)), dir(module))))
     if isinstance(module, nn.Conv2d):
         return compute_Conv2d_flops(module, inp, out)
+    elif isinstance(module, nn.ConvTranspose2d):
+        return compute_ConvTranspose2d_flops(module, inp, out)
     elif isinstance(module, nn.BatchNorm2d):
         return compute_BatchNorm2d_flops(module, inp, out)
     elif isinstance(module, nn.LayerNorm) or 'LayerNorm' in type(module).__name__:
         return compute_LayerNorm_flops(module, inp, out)
     elif isinstance(module, (nn.AvgPool2d, nn.MaxPool2d)):
         return compute_Pool2d_flops(module, inp, out)
     elif isinstance(module, (nn.ReLU, nn.ReLU6, nn.PReLU, nn.ELU, nn.LeakyReLU)):
         return compute_ReLU_flops(module, inp, out)
     # elif isinstance(module, nn.Upsample):
     #     return compute_Upsample_flops(module, inp, out)
     elif isinstance(module, nn.Linear):
         return compute_Linear_flops(module, inp, out)
-    elif 'SwinTEB' in module.__class__.__name__:#
-        return compute_WindowAttention_flops(module, inp, out)
-    elif 'XCTEB' in module.__class__.__name__:
-        return compute_XCA_flops(module, inp, out)
-    elif 'MSA' in module.__class__.__name__:
-        return compute_MSA_flops(module, inp, out)
-    elif 'cGCN' == module.__class__.__name__:
-        return compute_cGCN_flops(module, inp, out)
-    elif 'sGCN' == module.__class__.__name__:
-        return compute_sGCN_flops(module, inp, out)
+    # elif 'SwinTEB' in module.__class__.__name__:#
+    #     return compute_WindowAttention_flops(module, inp, out)
+    # elif 'XCTEB' in module.__class__.__name__:
+    #     return compute_XCA_flops(module, inp, out)
+    # elif 'MSA' in module.__class__.__name__:
+    #     return compute_MSA_flops(module, inp, out)
+    # elif 'cGCN' == module.__class__.__name__:
+    #     return compute_cGCN_flops(module, inp, out)
+    # elif 'sGCN' == module.__class__.__name__:
+    #     return compute_sGCN_flops(module, inp, out)
+    elif hasattr(module, 'flops'):
+        module.__class__.__name__ = module.__class__.__name__ + "_flops"
+        return module.flops(*inp, out) if isinstance(inp, (tuple, list)) else module.flops(inp, out)
     else:
         print(f"[Flops]: {module.__class__.__name__} is not supported!")
         return 0
     pass
 
 
 def compute_cGCN_flops(module, inp, out):
@@ -96,14 +101,37 @@
     bias_flops = 0
     if module.bias is not None:
         bias_flops = out_c * active_elements_count
     # k * k * c * H * W * o = (乘法 + 加法 + bias) * active_elements_count
     total_flops = total_conv_flops + bias_flops
     return total_flops
 
+def compute_ConvTranspose2d_flops(module, inp, out):
+    # Can have multiple inputs, getting the first one
+    assert isinstance(module, nn.ConvTranspose2d)
+    assert len(inp.size()) == 4 and len(inp.size()) == len(out.size())
+
+    batch_size = inp.size()[0]
+    in_c = inp.size()[1]
+    k_h, k_w = module.kernel_size
+    out_c, out_h, out_w = out.size()[1:]
+    groups = module.groups
+
+    filters_per_channel = out_c // groups
+    conv_per_position_flops = k_h * k_w * in_c * filters_per_channel
+    active_elements_count = batch_size * out_h * out_w
+
+    total_conv_flops = conv_per_position_flops * active_elements_count
+
+    bias_flops = 0
+    if module.bias is not None:
+        bias_flops = out_c * active_elements_count
+    # k * k * c * H * W * o = (乘法 + 加法 + bias) * active_elements_count
+    total_flops = total_conv_flops + bias_flops
+    return total_flops
 
 def compute_BatchNorm2d_flops(module, inp, out):
     assert isinstance(module, nn.BatchNorm2d)
     assert len(inp.size()) == 4 and len(inp.size()) == len(out.size())
     in_c, in_h, in_w = inp.size()[1:]
     batch_flops = np.prod(inp.shape)
     if module.affine:
```

## udl_vis/Basis/auxiliary/torchstat/compute_madd.py

```diff
@@ -178,10 +178,15 @@
         return compute_ReLU_madd(module, inp, out)
     elif isinstance(module, nn.Softmax):
         return compute_Softmax_madd(module, inp, out)
     elif isinstance(module, nn.Linear):
         return compute_Linear_madd(module, inp, out)
     elif isinstance(module, nn.Bilinear):
         return compute_Bilinear_madd(module, inp[0], inp[1], out)
+    elif hasattr(module, 'madd'):
+        return module.madd(inp, out)
+    elif hasattr(module, 'flops'):
+        print(f"[MAdd]: {type(module).__name__.replace('_flops', '')} is not supported!")
+        return 0
     else:
         print(f"[MAdd]: {type(module).__name__} is not supported!")
         return 0
```

## udl_vis/Basis/auxiliary/torchstat/compute_memory.py

```diff
@@ -14,14 +14,19 @@
         return compute_BatchNorm2d_memory(module, inp, out)
     elif isinstance(module, nn.LayerNorm) or 'LayerNorm' in type(module).__name__:
         return compute_LayerNorm_memory(module, inp, out)
     elif isinstance(module, nn.Linear):
         return compute_Linear_memory(module, inp, out)
     elif isinstance(module, (nn.AvgPool2d, nn.MaxPool2d)):
         return compute_Pool2d_memory(module, inp, out)
+    elif hasattr(module, 'memory'):
+        return module.memory(inp, out)
+    elif hasattr(module, 'flops'):
+        print(f"[Memory]: {type(module).__name__.replace('_flops', '')} is not supported!")
+        return (0, 0)
     else:
         print(f"[Memory]: {type(module).__name__} is not supported!")
         return (0, 0)
     pass
 
 
 def num_params(module):
```

## udl_vis/Basis/auxiliary/torchstat/model_hook.py

```diff
@@ -1,44 +1,49 @@
 import time
 from collections import OrderedDict
 import numpy as np
 import torch
 import torch.nn as nn
 from functools import partial
-from torchstat import compute_madd
-from torchstat import compute_flops
-from torchstat import compute_memory
+from . import compute_madd
+from . import compute_flops
+from . import compute_memory
 
 
 class ModelHook(object):
-    def __init__(self, model, input_size, device="cuda", debug_layers=[]):
+    def __init__(self, model, input_size, device="cuda", keep_pair=False, ignore_flops=False): # , debug_layers=[]
         assert isinstance(model, nn.Module)
         assert isinstance(input_size, (list, tuple))
         self.leaf_modules = []
-        self.debug_layers = debug_layers
+        # self.debug_layers = debug_layers
         self._model = model
+        self.ignore_flops = ignore_flops
         self._input_size = input_size
         self._origin_call = dict()  # sub module call hook
         self.hooks = []
         self._hook_model()
         # x = [torch.rand(1, *self._input_size)]  # add module duration time
         device = device.lower()
         assert device in [
             "cuda",
             "cpu",
         ], "Input device is not valid, please specify 'cuda' or 'cpu'"
 
         if device == "cuda" and torch.cuda.is_available():
             dtype = torch.cuda.FloatTensor
+            # model.cuda()
         else:
             dtype = torch.FloatTensor
         x = [torch.rand(*in_size).type(dtype) for in_size in input_size]
         self._model.eval()
-        self._model(*x)
-
+        with torch.no_grad():
+            if keep_pair:
+                self._model(x)
+            else:
+                self._model(*x)
         # if len(debug_layers) > 0:
         #     self.debug_partial_layer(debug_layers)
 
 
     @staticmethod
     def _register_buffer(module):
         assert isinstance(module, nn.Module)
@@ -55,151 +60,185 @@
         module.register_buffer('Flops', torch.zeros(1).long())
         module.register_buffer('Memory', torch.zeros(2).long())
 
     def _sub_module_call_hook(self):
         def wrap_call(module, *input, **kwargs):
             assert module.__class__ in self._origin_call
             # Itemsize for memory
+            # if isinstance(input[0], (tuple, list)):
+            #     try:
+            #         itemsize = np.prod([inp[0].detach().numpy().itemsize for inp in input[0]])
+            #     except:
+            #         itemsize = np.prod([inp[0].detach().cpu().numpy().itemsize for inp in input[0]])
+            # else:
             try:
                 itemsize = input[0].detach().numpy().itemsize
             except:
                 itemsize = input[0].detach().cpu().numpy().itemsize
 
             start = time.time()
             output = self._origin_call[module.__class__](module, *input, **kwargs)  # 都是nn.Conv2D则有相同的_call__不需要重复存储
             end = time.time()
             module.duration = torch.from_numpy(
                 np.array([end - start], dtype=np.float32))
             # c, h, w
-            module.input_shape = torch.from_numpy(
-                np.array(input[0].size()[1:], dtype=np.int32))
-            module.output_shape = torch.from_numpy(
-                np.array(output.size()[1:], dtype=np.int32))
-            # print(module.name)
+            # print(type(module).__name__)
             parameter_quantity = 0
             inference_memory = 1
-            # iterate through parameters and count num params
-            if 'XCTEB' in module.__class__.__name__:
-                c, h, w = module.input_shape
-                num_heads = module.num_heads
-                parameter_quantity += c * c * num_heads
-            elif 'SwinTEB' in module.__class__.__name__:
-                if len(module.input_shape) == 3:
-                    # c, h, w = module.input_shape # c, h, w
-                    _, N, c = module.input_shape
-                    # N = h * w
-                elif len(module.input_shape) == 2:
-                    N = module.input_shape[0]
-                num_heads = module.num_heads
-                # hh = nH * h WindowAttention只减少了flops并没有减少显存占用，因此参数量按照图像大小算
-                parameter_quantity += N * N * num_heads
-                print(parameter_quantity, N, module.input_shape)
-            elif 'MSA' == module.__class__.__name__:
-                # L, B, D
-                # if hasattr(module, '__name__'):
-                #     print('model.body.decoder.layers.0.self_attn')
-                # print(module.__name__, module.input_shape)
-                module.input_shape = torch.from_numpy(
-                    np.array(input[0].permute(1, 2, 0).size()[1:], dtype=np.int32))
-                c, L = module.input_shape
-                num_heads = module.num_heads
-                parameter_quantity += L * L * num_heads
-                # print(L, c)
-            elif 'MSA_BNC' == module.__class__.__name__:
-                # B, L, C
-                module.input_shape = torch.from_numpy(
-                    np.array(input[0].permute(0, 2, 1).size()[1:], dtype=np.int32))
-                c, L = module.input_shape
-                num_heads = module.num_heads
-                parameter_quantity += L * L * num_heads
-                # print(L, c)
-            elif 'sGCN' == module.__class__.__name__:
-                module.input_shape = torch.from_numpy(
-                    np.array(input[0][0].permute(0, 2, 1).size(), dtype=np.int32))
-                c, H, W = module.input_shape
-                c = c // 2
-                parameter_quantity += c * c
-            elif 'cGCN' == module.__class__.__name__:
-                module.input_shape = torch.from_numpy(
-                    np.array(input[0][0].permute(0, 2, 1).size(), dtype=np.int32))
-                c, H, W = module.input_shape
-                c = c // 2
-                parameter_quantity += c * c // 2
-            else:
-                for s in output.size()[1:]:
-                    inference_memory *= s
-                # memory += parameters_number  # exclude parameter memory
+            madd = 0
+            flops = 0
+            Memory = (0, 0)
+
+            # # iterate through parameters and count num params
+            # if 'XCTEB' in module.__class__.__name__:
+            #     c, h, w = module.input_shape
+            #     num_heads = module.num_heads
+            #     parameter_quantity += c * c * num_heads
+            # elif 'SwinTEB' in module.__class__.__name__:
+            #     if len(module.input_shape) == 3:
+            #         # c, h, w = module.input_shape # c, h, w
+            #         _, N, c = module.input_shape
+            #         # N = h * w
+            #     elif len(module.input_shape) == 2:
+            #         N = module.input_shape[0]
+            #     num_heads = module.num_heads
+            #     # hh = nH * h WindowAttention只减少了flops并没有减少显存占用，因此参数量按照图像大小算
+            #     parameter_quantity += N * N * num_heads
+            #     print(parameter_quantity, N, module.input_shape)
+            # elif 'MSA' == module.__class__.__name__:
+            #     # L, B, D
+            #     # if hasattr(module, '__name__'):
+            #     #     print('model.body.decoder.layers.0.self_attn')
+            #     # print(module.__name__, module.input_shape)
+            #     module.input_shape = torch.from_numpy(
+            #         np.array(input[0].permute(1, 2, 0).size()[1:], dtype=np.int32))
+            #     c, L = module.input_shape
+            #     num_heads = module.num_heads
+            #     parameter_quantity += L * L * num_heads
+            #     # print(L, c)
+            # elif 'MSA_BNC' == module.__class__.__name__:
+            #     # B, L, C
+            #     module.input_shape = torch.from_numpy(
+            #         np.array(input[0].permute(0, 2, 1).size()[1:], dtype=np.int32))
+            #     c, L = module.input_shape
+            #     num_heads = module.num_heads
+            #     parameter_quantity += L * L * num_heads
+            #     # print(L, c)
+            # elif 'sGCN' == module.__class__.__name__:
+            #     module.input_shape = torch.from_numpy(
+            #         np.array(input[0][0].permute(0, 2, 1).size(), dtype=np.int32))
+            #     c, H, W = module.input_shape
+            #     c = c // 2
+            #     parameter_quantity += c * c
+            # elif 'cGCN' == module.__class__.__name__:
+            #     module.input_shape = torch.from_numpy(
+            #         np.array(input[0][0].permute(0, 2, 1).size(), dtype=np.int32))
+            #     c, H, W = module.input_shape
+            #     c = c // 2
+            #     parameter_quantity += c * c // 2
+            # print(f"log: {module.__class__.__name__:}")
+            # if hasattr(module, 'flops'):
+            #     # print(module)
+            #     shape = list(input[0].size())
+            #     shape = shape[1:] if shape[0] == 1 else shape
+            #     assert isinstance(shape, (list, tuple))
+            #     module.input_shape = torch.from_numpy(
+            #         np.array(shape, dtype=np.int32))
+            #     module.parameter_quantity = torch.from_numpy(
+            #         np.array([parameter_quantity], dtype=np.long))
+            #     module.__class__.__name__ = module.__class__.__name__ + "_flops"
+            #     # try:
+            #     flops += module.flops(*input, output)
+            #     # except:
+            #     #     print(f"error: {module.__class__.__name__}, {module}")
+            # else:
+            
+                # shape = list(input[0].size())
+                # shape = shape[1:] if shape[0] == 1 else shape
+                # assert isinstance(shape, (list, tuple))
+            # memory += parameters_number  # exclude parameter memory
             for name, p in module._parameters.items():
                 parameter_quantity += (0 if p is None else torch.numel(p.data))
             module.parameter_quantity = torch.from_numpy(
                 np.array([parameter_quantity], dtype=np.long))
 
-            inference_memory = inference_memory * 4 / (1024 ** 2)  # shown as MB unit
-            module.inference_memory = torch.from_numpy(
-                np.array([inference_memory], dtype=np.float32))
+            if len(output) == 1 or not isinstance(output, tuple):
+                module.output_shape = torch.from_numpy(
+                    np.array(output[0].size()[1:], dtype=np.int32))
+                for s in output.size()[1:]:
+                    inference_memory *= s
+            else:
+                print(" Only show first output's shape.", sep='')
+                module.output_shape = torch.from_numpy(
+                    np.array(output[0][0].size()[1:], dtype=np.int32))
 
-            if len(input) == 1:
+            if len(input) == 1 or not isinstance(input, tuple):
+                module.input_shape = torch.from_numpy(
+                    np.array(input[0].size()[1:], dtype=np.int32))
                 madd = compute_madd(module, input[0], output)
                 flops = compute_flops(module, input[0], output)
                 Memory = compute_memory(module, input[0], output)
-            elif len(input) > 1:
+            else:
+                print(" Only show first input's shape.", sep='')
+                module.input_shape = torch.from_numpy(
+                    np.array(input[0][0].size()[1:], dtype=np.int32))
                 madd = compute_madd(module, input, output)
                 flops = compute_flops(module, input, output)
                 Memory = compute_memory(module, input, output)
-            else:  # error
-                madd = 0
-                flops = 0
-                Memory = (0, 0)
+
+            inference_memory = inference_memory * 4 / (1024 ** 2)  # shown as MB unit
+            module.inference_memory = torch.from_numpy(
+                np.array([inference_memory], dtype=np.float32))
             module.MAdd = torch.from_numpy(
                 np.array([madd], dtype=np.int64))
             module.Flops = torch.from_numpy(
                 np.array([flops], dtype=np.int64))
             Memory = np.array(Memory, dtype=np.int64) * itemsize
             module.Memory = torch.from_numpy(Memory)
 
             return output
 
         leaf_modules = self.leaf_modules
-        # for m in self._model.modules():
-        #     print(m.__class__)
 
         for name, module in self._model.named_modules():
-            if len(list(module.children())) == 0:
+            num_children = len(list(module.children()))
+            # print(name, module.__class__.__name__, num_children)
+            if num_children == 0:
                 module.name = name
                 leaf_modules.append((name, module))
                 if module.__class__ not in self._origin_call:
                     # 只记录一类与具体实例无关的__call__
                     self._origin_call[module.__class__] = module.__class__.__call__
                     module.__class__.__call__ = wrap_call
-            elif name != '' and len(list(module.children())) > 0 and any([L in module.__class__.__name__ for L in self.debug_layers]):
+            elif name != '' and num_children > 0 and hasattr(module, 'flops') and not self.ignore_flops:#any([L in module.__class__.__name__ for L in self.debug_layers]):
                 #name in self.debug_layers:# module.__class__.__name__  in self.debug_layers
                 # if module.__class__.__name__ in self.debug_layers:
-                #     print("111")
                 leaf_modules.append((name, module))
                 if module.__class__ not in self._origin_call:
                     self._origin_call[module.__class__] = module.__class__.__call__
                     module.__class__.__call__ = wrap_call
-                    print(name, module.__class__.__name__)
+                    
 
         # for module in self._model.modules():
         #     if len(list(module.children())) == 0 and module.__class__ not in self._origin_call:
         #         self.hooks.append(module.register_forward_hook(wrap_call))
 
     def _hook_model(self):
         self._model.apply(self._register_buffer)
-        self._sub_module_call_hook()
+        self._sub_module_call_hook() # unregister_module_parameter_quantity
 
     def clear_hooks(self) -> None:
         """Clear model hooks"""
 
         # for handle in self.hook_handles:
         #     handle.pop()
         def unwarp_calls(module):
             if module.__class__ in self._origin_call:
                 module.__class__.__call__ = self._origin_call[module.__class__]
+                module.__class__.__name__ = module.__class__.__name__.replace("_flops", "")
                 # module.__delattr__('__name__')
 
         calls = list(map(unwarp_calls, self._model.modules()))
         del calls
         # for module in self._model.modules():
         #     if module.__class__ in self._origin_call:
         #         module.__class__.__call__ = self._origin_call[module.__class__]
@@ -212,16 +251,16 @@
     #             leaf_modules.append((name, m))
     #     return leaf_modules
 
     def retrieve_leaf_modules(self):
         return OrderedDict(self.leaf_modules)
         # return OrderedDict(self._retrieve_leaf_modules(self._model))
 
-    def debug_partial_layer(self, target_keys):
-        target_layers = []
-        submodule_name = dict(list(self._model.named_modules())[1:]).keys()
-        for t in target_keys:
-            for name in submodule_name:
-                if t in name:
-                    target_layers.append(name)
-
-        return target_layers
+    # def debug_partial_layer(self, target_keys):
+    #     target_layers = []
+    #     submodule_name = dict(list(self._model.named_modules())[1:]).keys()
+    #     for t in target_keys:
+    #         for name in submodule_name:
+    #             if t in name:
+    #                 target_layers.append(name)
+    #
+    #     return target_layers
```

## udl_vis/Basis/auxiliary/torchstat/reporter.py

```diff
@@ -6,28 +6,30 @@
 pd.set_option('display.max_columns', 10000)
 
 
 def round_value(value, binary=False):
     divisor = 1024. if binary else 1000.
 
     if value // divisor**4 > 0:
-        return str(round(value / divisor**4, 2)) + 'T'
+        return str(round(value / divisor**4, 2)) + 'T', str(round(value / divisor**5, 4)) + 'P'
     elif value // divisor**3 > 0:
-        return str(round(value / divisor**3, 2)) + 'G'
+        return str(round(value / divisor**3, 2)) + 'G', str(round(value / divisor**4, 4)) + 'T'
     elif value // divisor**2 > 0:
-        return str(round(value / divisor**2, 2)) + 'M'
+        return str(round(value / divisor**2, 2)) + 'M', str(round(value / divisor**3, 4)) + 'G'
     elif value // divisor > 0:
-        return str(round(value / divisor, 2)) + 'K'
+        return str(round(value / divisor, 2)) + 'K', str(round(value / divisor**2, 4)) + 'M'
     return str(value)
 
 
 def report_format(collected_nodes):
     data = list()
     properties = list()
     for node in collected_nodes:
+        # if node.mtype == "CAttention":
+        #     print("111")
         name = node.name
         mtype = node.mtype
         input_shape = ' '.join(['{:>3d}'] * len(node.input_shape)).format(
             *[e for e in node.input_shape])
         output_shape = ' '.join(['{:>3d}'] * len(node.output_shape)).format(
             *[e for e in node.output_shape])
         parameter_quantity = node.parameter_quantity
@@ -72,19 +74,20 @@
     df = df.fillna(' ')
     df['memory(MB)'] = df['memory(MB)'].apply(
         lambda x: '{:.2f}'.format(x))
     df['duration[%]'] = df['duration[%]'].apply(lambda x: '{:.2%}'.format(x))
     df['MAdd'] = df['MAdd'].apply(lambda x: '{:,}'.format(x))
     df['Flops'] = df['Flops'].apply(lambda x: '{:,}'.format(x))
 
+    binary = False
     summary = str(df) + '\n'
     summary += "=" * len(str(df).split('\n')[0])
-    summary += '\n'
-    summary += "Total params: {:,}\n".format(total_parameters_quantity)
+    summary += '\n(first four is divided by 1024)\n' if binary else '\n(first four is divided by 1000)\n'
+    summary += "Total params: {} {} ({:,})\n".format(*round_value(total_parameters_quantity, binary), total_parameters_quantity)
 
     summary += "-" * len(str(df).split('\n')[0])
     summary += '\n'
     summary += "Total memory: {:.2f}MB\n".format(total_memory)
-    summary += "Total MAdd: {}MAdd\n".format(round_value(total_operation_quantity))
-    summary += "Total Flops: {}Flops\n".format(round_value(total_flops))
-    summary += "Total MemR+W: {}B\n".format(round_value(total_memrw, True))
+    summary += "Total MAdd: {}MAdd {}MAdd\n".format(*round_value(total_operation_quantity, binary))
+    summary += "Total Flops: {}Flops {}Flops\n".format(*round_value(total_flops, binary))
+    summary += "Total MemR+W: {}B {}B\n".format(*round_value(total_memrw, True))
     return summary
```

## udl_vis/Basis/auxiliary/torchstat/stat_tree.py

```diff
@@ -18,23 +18,28 @@
         q.put(self.root_node)
         while not q.empty():
             node = q.get()
             node.granularity = self.get_same_level_max_node_depth(node)
             for child in node.children:
                 q.put(child)
 
-    def get_collected_stat_nodes(self, debug_layers, query_granularity):
+    def get_collected_stat_nodes(self, query_granularity): #debug_layers
         self.update_stat_nodes_granularity()
 
         collected_nodes = []
         stack = list()
         stack.append(self.root_node)
         while len(stack) > 0:
             node = stack.pop()
-            if any([L in node.mtype for L in debug_layers]): #node.name
+            # if node.mtype == "PanFormerEncoderLayer":
+            #     print(node.mtype)
+            # if any([L in node.mtype for L in debug_layers]): #node.name
+            if 'flops' in node.mtype:
+                node.mtype = node.mtype.replace("_flops", '')
+                # print(node.mtype)
                 collected_nodes.append(node)
             for child in reversed(node.children):
                 stack.append(child)
             if node.depth == query_granularity:
                 collected_nodes.append(node)
             if node.depth < query_granularity <= node.granularity:
                 collected_nodes.append(node)
```

## udl_vis/Basis/auxiliary/torchstat/statistics.py

```diff
@@ -1,12 +1,14 @@
+import warnings
+
 import torch
 import torch.nn as nn
-from torchstat import ModelHook
+from . import ModelHook
 from collections import OrderedDict
-from torchstat import StatTree, StatNode, report_format
+from . import StatTree, StatNode, report_format
 
 
 def get_parent_node(root_node, stat_node_name):
     assert isinstance(root_node, StatNode)
 
     node = root_node
     names = stat_node_name.split('.')
@@ -20,16 +22,16 @@
 
 def convert_leaf_modules_to_stat_tree(leaf_modules):
     assert isinstance(leaf_modules, OrderedDict)
 
     create_index = 1
     root_node = StatNode(name='root', parent=None)
     for leaf_module_name, leaf_module in leaf_modules.items():
-        if 'model.body.decoder.layers.0.self_attn' in leaf_module_name:
-            print("111", leaf_module_name, leaf_module.__class__.__name__)
+        # if 'model.body.decoder.layers.0.self_attn' in leaf_module_name:
+        #     print("111", leaf_module_name, leaf_module.__class__.__name__)
         names = leaf_module_name.split('.')
         for i in range(len(names)):
             create_index += 1
             stat_node_name = '.'.join(names[0:i+1])
             parent_node = get_parent_node(root_node, stat_node_name)
             node = StatNode(name=stat_node_name, mtype=leaf_module.__base__ if hasattr(leaf_module, '__base__') else leaf_module.__class__.__name__, parent=parent_node)#.__class__.__name__
             parent_node.add_child(node)
@@ -44,31 +46,35 @@
                 node.Flops = leaf_module.Flops.numpy()[0]
                 node.duration = leaf_module.duration.numpy()[0]
                 node.Memory = leaf_module.Memory.numpy().tolist()
     return StatTree(root_node)
 
 
 class ModelStat(object):
-    def __init__(self, model, input_size, query_granularity=1, debug_layers=[]):
+    def __init__(self, model, input_size, query_granularity=1, device="cuda", keep_pair=False, ignore_flops=False): # , debug_layers=[]
         assert isinstance(model, nn.Module)
         # assert isinstance(input_size, (tuple, list)) and len(input_size) == 3
         self._model = model
         self._input_size = input_size
         self._query_granularity = query_granularity
-        self.debug_layers = debug_layers
+        # self.debug_layers = debug_layers
+        self.keep_pair = keep_pair
+        self.device = device
+        self.ignore_flops = ignore_flops
 
     def _analyze_model(self):
-        model_hook = ModelHook(self._model, self._input_size, debug_layers=self.debug_layers)
+        model_hook = ModelHook(self._model, self._input_size, self.device, self.keep_pair, self.ignore_flops) # , debug_layers=self.debug_layers
         leaf_modules = model_hook.retrieve_leaf_modules()
         stat_tree = convert_leaf_modules_to_stat_tree(leaf_modules)
-        collected_nodes = stat_tree.get_collected_stat_nodes(self.debug_layers, self._query_granularity)
+        collected_nodes = stat_tree.get_collected_stat_nodes(self._query_granularity) # self.debug_layers,
         model_hook.clear_hooks()
         return collected_nodes
 
     def show_report(self):
         collected_nodes = self._analyze_model()
         report = report_format(collected_nodes)
         print(report)
 
-def stat(model, input_size, query_granularity=1, debug_layers=["MSA", "SwinTEB", "XCTEB", "MSA_BNC", 'cGCN', 'sGCN']):
-    ms = ModelStat(model, input_size, query_granularity, debug_layers)
+def stat(model, input_size, query_granularity=1, device="cuda", keep_pair=False, ignore_flops=False):#, debug_layers=["MSA", "SwinTEB", "XCTEB", "MSA_BNC", 'cGCN', 'sGCN']):
+    warnings.warn("Note that for LayerNorm, the function name uses the full name")
+    ms = ModelStat(model, input_size, query_granularity, device, keep_pair, ignore_flops) #debug_layers
     ms.show_report()
```

## udl_vis/mmcv/runner/__init__.py

```diff
@@ -12,15 +12,16 @@
 from .fp16_utils import LossScaler, auto_fp16, force_fp32, wrap_fp16_model
 from .hooks import (HOOKS, CheckpointHook, ClosureHook, DistEvalHook,
                     DistSamplerSeedHook, DvcliveLoggerHook, EMAHook, EvalHook,
                     Fp16OptimizerHook, GradientCumulativeFp16OptimizerHook,
                     GradientCumulativeOptimizerHook, Hook, IterTimerHook,
                     LoggerHook, MlflowLoggerHook, NeptuneLoggerHook,
                     OptimizerHook, PaviLoggerHook, SyncBuffersHook,
-                    TensorboardLoggerHook, TextLoggerHook, WandbLoggerHook)
+                    TensorboardLoggerHook, TextLoggerHook, WandbLoggerHook, NNIHook,
+                    detect_anomalous_parameters, clip_grads, Hook_v2)
 from .hooks.lr_updater import StepLrUpdaterHook  # noqa
 from .hooks.lr_updater import (CosineAnnealingLrUpdaterHook,
                                CosineRestartLrUpdaterHook, CyclicLrUpdaterHook,
                                ExpLrUpdaterHook, FixedLrUpdaterHook,
                                FlatCosineAnnealingLrUpdaterHook,
                                InvLrUpdaterHook, LrUpdaterHook,
                                OneCycleLrUpdaterHook, PolyLrUpdaterHook)
@@ -33,15 +34,14 @@
 from .log_buffer import LogBuffer
 from .optimizer import (OPTIMIZER_BUILDERS, OPTIMIZERS,
                         DefaultOptimizerConstructor, build_optimizer,
                         build_optimizer_constructor)
 from .priority import Priority, get_priority
 from .utils import get_host_info, get_time_str, obj_from_dict, set_random_seed
 from .record import MetricLogger
-from .hooks.nni_hook import NNIHook
 from .misc import find_latest_checkpoint
 
 __all__ = [
     'BaseRunner', 'Runner', 'EpochBasedRunner', 'IterBasedRunner', 'LogBuffer',
     'HOOKS', 'Hook', 'CheckpointHook', 'ClosureHook', 'LrUpdaterHook',
     'FixedLrUpdaterHook', 'StepLrUpdaterHook', 'ExpLrUpdaterHook',
     'PolyLrUpdaterHook', 'InvLrUpdaterHook', 'CosineAnnealingLrUpdaterHook',
```

## udl_vis/mmcv/runner/base_runner.py

```diff
@@ -52,14 +52,15 @@
 
     def __init__(self,
                  model,
                  batch_processor=None,
                  optimizer=None,
                  seed=None,
                  work_dir=None,
+                 tfb_dir = None,
                  logger=None,
                  meta=None,
                  max_iters=None,
                  max_epochs=None,
                  opt_cfg=None):
         if batch_processor is not None:
             if not callable(batch_processor):
@@ -112,24 +113,25 @@
         self.opt_cfg = opt_cfg
         self.earlyStop = False
         self.seed = seed
         # create work_dir
         save_dir = opt_cfg['save_dir']
         if mmcv.is_str(work_dir):
             self.work_dir = osp.abspath(work_dir)
-            self.save_dir = osp.abspath(save_dir)
+            self.save_dir = self.work_dir + "/results"
             if os.path.isdir(work_dir):
                 mmcv.mkdir_or_exist(self.save_dir)
                 # mmcv.mkdir_or_exist(self.work_dir)
         elif work_dir is None:
             self.work_dir = None
             self.save_dir = None
         else:
             raise TypeError(f'"work_dir: {work_dir}" must be a str or None')
-
+        if tfb_dir is not None:
+            self.tfb_dir = tfb_dir
 
 
 
         # get model name from the model class
         if hasattr(self.model, 'module'):
             self._model_name = self.model.module.__class__.__name__
         else:
```

## udl_vis/mmcv/runner/checkpoint.py

```diff
@@ -613,15 +613,15 @@
             the prefix 'module.' by [(r'^module\\.', '')].
 
     Returns:
         dict or OrderedDict: The loaded checkpoint.
     """
     ################
     # 只从work_dir里读ckpt，用于模型的继续训练
-    if not os.path.isfile(filename):
+    if not os.path.isfile(filename) and work_dir is not None:
         resume_mode = resume_mode.lower()
         if resume_mode == 'best':
             _, best_k_fname = get_best_k_model(os.path.join(work_dir, "checkpoint"), None)
             if len(best_k_fname) > 0:
                 best_k_model = sorted(best_k_fname)[-1]
                 filename = os.path.join(work_dir, best_k_model, '.pth.tar')
             else:
```

## udl_vis/mmcv/runner/epoch_based_runner.py

```diff
@@ -73,16 +73,16 @@
         for i, data_batch in enumerate(self.data_loader):
             self._inner_iter = i
             self.call_hook('before_train_iter')
             self.run_iter(data_batch, train_mode=True, **kwargs)
             self.call_hook('after_train_iter')
             self._iter += 1
             # break
-        self.metrics = {k: meter.avg for k, meter in self.log_buffer.meters.items()}
         self.metrics.update(epoch_time=time.time() - tic)
+        self.metrics = {k: meter.avg for k, meter in self.log_buffer.meters.items()}
         self.call_hook('after_train_epoch')
         self._epoch += 1
 
     def simple_train(self, data_loader, **kwargs):
         optimizer = self.optimizer
         accumulated_step = self.opt_cfg.get('accumulated_step', 1)
         grad_clip = self.opt_cfg.get('grad_clip', 0)
@@ -166,15 +166,15 @@
         self.call_hook('before_val_epoch')
         time.sleep(2)  # Prevent possible deadlock during epoch transition
         tic = time.time()
         for i, data_batch in enumerate(self.data_loader):
             self._inner_iter = i
             self.call_hook('before_val_iter')
             self.run_iter(data_batch, train_mode=False, idx=i,
-                          img_range=self.opt_cfg['img_range'], eval=self.opt_cfg['eval'],
+                          img_range=self.opt_cfg['img_range'], eval=self.opt_cfg['eval'], test=self.opt_cfg['test'],
                           save_fmt=self.opt_cfg['save_fmt'], filename=data_batch.get('filename', [None])[0], save_dir=self.save_dir,
                           **kwargs)
                           # val_mode=self.opt_cfg['val_mode'])
             self.call_hook('after_val_iter')
             # break
         print_log(f"test time: {time.time() - tic}", logger=self.logger)
         self.call_hook('after_val_epoch')
```

## udl_vis/mmcv/runner/record.py

```diff
@@ -139,14 +139,17 @@
     def reset(self, window_size):
         self.deque = deque(maxlen=window_size)
         self.val = 0
         self.avg = 0
         self.total = 0
         self.count = 0
 
+    def store(self, tensor):
+        self.image = tensor
+
     def update(self, value, n=1):
         self.deque.append(value)
         self.val = value
         self.count += n
         self.total += value * n
         self.avg = self.total / self.count
 
@@ -242,16 +245,19 @@
     def update_dict(self, kwargs: dict, n=1):
         # dist.barrier()
         for k, v in kwargs.items():
             if isinstance(v, torch.Tensor):
                 v = torch.mean(v)
                 if hasattr(v, 'item'):
                     v = v.item()
-            assert isinstance(v, (float, int, str)), print("type: ", type(v))
-            self.meters[k].update(v, n)
+            # assert isinstance(v, (float, int, str)), print("type: ", type(v))
+            if isinstance(v, (float, int, str)):
+                self.meters[k].update(v, n)
+            elif isinstance(v, np.ndarray):
+                self.meters[k].store(v)
 
     def __getattr__(self, attr):
         if attr in self.meters:
             return self.meters[attr]
         if attr in self.__dict__:
             return self.__dict__[attr]
         raise AttributeError("'{}' object has no attribute '{}'".format(
```

## udl_vis/mmcv/runner/hooks/__init__.py

```diff
@@ -1,13 +1,13 @@
 # Copyright (c) OpenMMLab. All rights reserved.
-from .checkpoint import CheckpointHook
+from .checkpoint import CheckpointHook, ModelCheckpoint
 from .closure import ClosureHook
 from .ema import EMAHook
 from .evaluation import DistEvalHook, EvalHook
-from .hook import HOOKS, Hook
+from .hook import HOOKS, Hook, Hook_v2
 from .iter_timer import IterTimerHook
 from .logger import (DvcliveLoggerHook, LoggerHook, MlflowLoggerHook,
                      NeptuneLoggerHook, PaviLoggerHook, TensorboardLoggerHook,
                      TextLoggerHook, WandbLoggerHook)
 from .lr_updater import (CosineAnnealingLrUpdaterHook,
                          CosineRestartLrUpdaterHook, CyclicLrUpdaterHook,
                          ExpLrUpdaterHook, FixedLrUpdaterHook,
@@ -16,18 +16,20 @@
                          PolyLrUpdaterHook, StepLrUpdaterHook)
 from .memory import EmptyCacheHook
 from .momentum_updater import (CosineAnnealingMomentumUpdaterHook,
                                CyclicMomentumUpdaterHook, MomentumUpdaterHook,
                                OneCycleMomentumUpdaterHook,
                                StepMomentumUpdaterHook)
 from .optimizer import (Fp16OptimizerHook, GradientCumulativeFp16OptimizerHook,
-                        GradientCumulativeOptimizerHook, OptimizerHook)
+                        GradientCumulativeOptimizerHook, OptimizerHook,
+                        detect_anomalous_parameters, clip_grads)
 from .profiler import ProfilerHook
 from .sampler_seed import DistSamplerSeedHook
 from .sync_buffer import SyncBuffersHook
+from .nni_hook import NNIHook
 
 __all__ = [
     'HOOKS', 'Hook', 'CheckpointHook', 'ClosureHook', 'LrUpdaterHook',
     'FixedLrUpdaterHook', 'StepLrUpdaterHook', 'ExpLrUpdaterHook',
     'PolyLrUpdaterHook', 'InvLrUpdaterHook', 'CosineAnnealingLrUpdaterHook',
     'FlatCosineAnnealingLrUpdaterHook', 'CosineRestartLrUpdaterHook',
     'CyclicLrUpdaterHook', 'OneCycleLrUpdaterHook', 'OptimizerHook',
```

## udl_vis/mmcv/runner/hooks/checkpoint.py

```diff
@@ -182,33 +182,33 @@
         'acc', 'top', 'AR@', 'auc', 'precision', 'mAP', 'mDice', 'mIoU',
         'mAcc', 'aAcc', 'psnr', 'ssim', 'q'
     ]
     _default_best_prec1 = {'greater': -inf, 'less': inf}
     _default_less_keys = ['loss', 'sam', 'ergas']
 
     def __init__(self, indicator: str, formatter_filename="model_best_{epoch},{best_metric}", save_interval=1,
-                 save_top_k: int = 1, use_log_and_save=True,
+                 save_top_k: int = 1, use_save=True, start_save_epoch=1,
                  greater_keys=None, less_keys=None, best_prec1=None, best_epoch=0, sync_buffer=False):
         '''
         Args:
             save_interval:
             save_top_k: ``save_top_k == k``,
                         if ``save_top_k == 0``, no models are saved.
                         if ``save_top_k == -1``, all models are saved.
                         Please note that the monitors are checked every ``every_n_epochs`` epochs.
             Returns:
         '''
-        self.use_log_and_save = use_log_and_save
+        self.use_save = use_save
         self.best_epoch = best_epoch
         self.save_interval = save_interval
         self.save_top_k = save_top_k
         self.sync_buffer = sync_buffer
         self.indicator = 'top-1' if indicator == 'top' else indicator
         self.formatter_filename = formatter_filename
-
+        self.start_save_epoch = start_save_epoch
         # indicator_lc = indicator.lower()
 
         if greater_keys is None:
             greater_keys = ModelCheckpoint._default_greater_keys
         else:
             if not isinstance(greater_keys, (list, tuple)):
                 greater_keys = (greater_keys,)
@@ -252,42 +252,44 @@
             return True
 
     def after_train_epoch(self, runner):
         if self.sync_buffer:
             allreduce_params(runner.model.buffers())
         metrics = runner.metrics  # metrics = {k: meter.avg for k, meter in runner.log_buffer.meters.items()}
         runner.earlyStop = self.earlyStopping(metrics.get('grad_norm', 0))
-        self.save_checkpoint(runner, metrics)
+        if runner.epoch + 1 >= self.start_save_epoch:
+            self.save_checkpoint(runner, metrics)
 
         # print_log(' * Best training metrics so far@ {best_metric} in epoch {best_epoch}'.format(
         #     best_metric=metrics['best_metric'], best_epoch=metrics['best_epoch']), logger=runner.logger)
 
     def _save_checkpoint(self, meta, out_dir, filename, is_best, create_symlink=True):
         if meta is None:
             meta = {}
         elif not isinstance(meta, dict):
             raise TypeError(
                 f'meta should be a dict or None, but got {type(meta)}')
         # meta.update(epoch=meta.pop('epoch') + 1, iter=meta.pop('iter'))
         filepath = os.path.join(out_dir, filename)
         # save_checkpoint(meta.pop('model'), filepath, optimizer=meta.pop('optimizer'), meta=meta)
-        if self.use_log_and_save:
+        if self.use_save:
             save_checkpoint(filepath, meta=meta)
             if create_symlink and is_best:
                 dst_file = os.path.join(out_dir, f'model_best_{filename}')
                 if platform.system() != 'Windows':
                     mmcv.symlink(filename, dst_file)
                 else:
                     shutil.copy(filepath, dst_file)
 
     @master_only
     def save_checkpoint(self, runner, metrics):
         flag = False
         epoch =  runner.epoch + 1
         iter = runner.iter + 1
+
         if not hasattr(runner.model, 'train') and isinstance(runner.model.model, dict):
             flag = True
             stats = {}
             for k, m in runner.model.model.items():
                 stats[k] = {
                     'epoch': epoch,
                     'iter': iter,
@@ -378,15 +380,15 @@
                 best_k_model = best_k_model[:-1]
                 # best_k_model = [{'epoch': k, 'score': v} for k, v in best_k_model.items()]
                 best_k_model = [{'epoch': epoch, 'best_metric': score} for (epoch, score, _) in best_k_model]
                 with open(self.ckpt, 'w') as f:
                     outs = [self.formatter_filename.format(**line) + "\n" for line in best_k_model]
                     f.writelines(outs)
             else:
-                if not flag and self.use_log_and_save:
+                if not flag and self.use_save:
                     with open(self.ckpt, 'a') as f:
                         outs = self.formatter_filename.format(**stats) + "\n"
                         f.writelines(outs)
                 # 训练初期，不满topk时候, 模型是否保存下来
                 # if save_top_k == 1:
                 # if len(best_k_model) < save_top_k:
                 #     new_best_k_model_flag = [True]
```

## udl_vis/mmcv/runner/hooks/hook.py

```diff
@@ -58,22 +58,22 @@
     def every_n_inner_iters(self, runner, n):
         return (runner.inner_iter + 1) % n == 0 if n > 0 else False
 
     def every_n_iters(self, runner, n):
         return (runner.iter + 1) % n == 0 if n > 0 else False
 
     def end_of_n_inner_iters(self, runner):
-        self.status_end_of_n_inner_iters  = runner.inner_iter + 1 == len(runner.data_loader)
+        self.status_end_of_n_inner_iters = runner.inner_iter + 1 == len(runner.data_loader)
         return self.status_end_of_n_inner_iters
 
     def is_last_epoch(self, runner):
-        return runner.epoch + 1 == runner._max_epochs
+        return runner.epoch + 1 == runner.max_epochs
 
     def is_last_iter(self, runner):
-        return runner.iter + 1 == runner._max_iters
+        return runner.iter + 1 == runner.max_iters
 
     def get_triggered_stages(self):
         trigger_stages = set()
         for stage in Hook.stages:
             if is_method_overridden(stage, Hook, self):
                 trigger_stages.add(stage)
 
@@ -87,7 +87,170 @@
         }
 
         for method, map_stages in method_stages_map.items():
             if is_method_overridden(method, Hook, self):
                 trigger_stages.update(map_stages)
 
         return [stage for stage in Hook.stages if stage in trigger_stages]
+
+
+from ..priority import get_priority, Priority
+class Hook_v2:
+    stages = ('before_run', 'before_train_epoch', 'before_train_iter',
+              'after_train_iter', 'after_train_epoch', 'before_val_epoch',
+              'before_val_iter', 'after_val_iter', 'after_val_epoch',
+              'after_run')
+    __hooks = []
+
+    def before_run(self, runner):
+        pass
+
+    def after_run(self, runner):
+        pass
+
+    def before_epoch(self, runner):
+        pass
+
+    def after_epoch(self, runner):
+        pass
+
+    def before_iter(self, runner):
+        pass
+
+    def after_iter(self, runner):
+        pass
+
+    def before_train_epoch(self, runner):
+        self.before_epoch(runner)
+
+    def before_val_epoch(self, runner):
+        self.before_epoch(runner)
+
+    def after_train_epoch(self, runner):
+        self.after_epoch(runner)
+
+    def after_val_epoch(self, runner):
+        self.after_epoch(runner)
+
+    def before_train_iter(self, runner):
+        self.before_iter(runner)
+
+    def before_val_iter(self, runner):
+        self.before_iter(runner)
+
+    def after_train_iter(self, runner):
+        self.after_iter(runner)
+
+    def after_val_iter(self, runner):
+        self.after_iter(runner)
+
+    def every_n_epochs(self, runner, n):
+        return (runner.epoch + 1) % n == 0 if n > 0 else False
+
+    def every_n_inner_iters(self, runner, n):
+        return (runner.inner_iter + 1) % n == 0 if n > 0 else False
+
+    def every_n_iters(self, runner, n):
+        return (runner.iter + 1) % n == 0 if n > 0 else False
+
+    def end_of_n_inner_iters(self, runner):
+        self.status_end_of_n_inner_iters = runner.inner_iter + 1 == len(runner.data_loader)
+        return self.status_end_of_n_inner_iters
+
+    def is_last_epoch(self, runner):
+        return runner.epoch + 1 == runner.max_epochs
+
+    def is_last_iter(self, runner):
+        return runner.iter + 1 == runner.max_iters
+
+    def get_triggered_stages(self):
+        trigger_stages = set()
+        for stage in Hook.stages:
+            if is_method_overridden(stage, Hook, self):
+                trigger_stages.add(stage)
+
+        # some methods will be triggered in multi stages
+        # use this dict to map method to stages.
+        method_stages_map = {
+            'before_epoch': ['before_train_epoch', 'before_val_epoch'],
+            'after_epoch': ['after_train_epoch', 'after_val_epoch'],
+            'before_iter': ['before_train_iter', 'before_val_iter'],
+            'after_iter': ['after_train_iter', 'after_val_iter'],
+        }
+
+        for method, map_stages in method_stages_map.items():
+            if is_method_overridden(method, Hook, self):
+                trigger_stages.update(map_stages)
+
+        return [stage for stage in Hook.stages if stage in trigger_stages]
+
+    # TODO: MyHook
+    def call_hook(self, fn_name):
+        """Call all hooks.
+
+        Args:
+            fn_name (str): The function name in each hook to be called, such as
+                "before_train_epoch".
+        """
+        for hook in self.__hooks:
+            getattr(hook, fn_name)(self)
+
+    def __register_hook(self, hook_cfg):
+        """Register a hook into the hook list.
+
+        The hook will be inserted into a priority queue, with the specified
+        priority (See :class:`Priority` for details of priorities).
+        For hooks with the same priority, they will be triggered in the same
+        order as they are registered.
+
+        Args:
+            hook (:obj:`Hook`): The hook to be registered.
+            priority (int or str or :obj:`Priority`): Hook priority.
+                Lower value means higher priority.
+        """
+        args = hook_cfg.copy()
+        if isinstance(args, dict):
+            obj_cls = args.pop('type')
+            priority = args.pop('priority')
+            hook = obj_cls(**args)
+
+        assert isinstance(hook, Hook)
+        if hasattr(hook, 'priority'):
+            raise ValueError('"priority" is a reserved attribute for hooks')
+        priority = get_priority(priority)
+        hook.priority = priority
+        # insert the hook to a sorted list
+        inserted = False
+        for i in range(len(self.__hooks) - 1, -1, -1):
+            if priority >= self.__hooks[i].priority:
+                self.__hooks.insert(i + 1, hook)
+                inserted = True
+                break
+        if not inserted:
+            self.__hooks.insert(0, hook)
+
+    def init_hook(self, hooks):
+        for hook_name, hook in hooks.items():
+            self.__register_hook(hook)
+
+    def get_hook_info(self):
+        # Get hooks info in each stage
+        stage_hook_map = {stage: [] for stage in Hook.stages}
+        for hook in self.__hooks:
+            try:
+                priority = Priority(hook.priority).name
+            except ValueError:
+                priority = hook.priority
+            classname = hook.__class__.__name__
+            hook_info = f'({priority:<12}) {classname:<35}'
+            for trigger_stage in hook.get_triggered_stages():
+                stage_hook_map[trigger_stage].append(hook_info)
+
+        stage_hook_infos = []
+        for stage in Hook.stages:
+            hook_infos = stage_hook_map[stage]
+            if len(hook_infos) > 0:
+                info = f'{stage}:\n'
+                info += '\n'.join(hook_infos)
+                info += '\n -------------------- '
+                stage_hook_infos.append(info)
+        return '\n'.join(stage_hook_infos)
```

## udl_vis/mmcv/runner/hooks/optimizer.py

```diff
@@ -15,14 +15,47 @@
     # If PyTorch version >= 1.6.0, torch.cuda.amp.GradScaler would be imported
     # and used; otherwise, auto fp16 will adopt mmcv's implementation.
     from torch.cuda.amp import GradScaler
 except ImportError:
     pass
 
 
+def clip_grads(grad_clip_value, params):
+
+    params, grad_norm = get_grad_norm(params)
+    if len(params) > 0 and grad_clip_value:
+        return clip_grad.clip_grad_norm_(params, grad_clip_value)
+    else:
+        return grad_norm
+
+def detect_anomalous_parameters(model, loss, logger):
+        parameters_in_graph = set()
+        visited = set()
+
+        def traverse(grad_fn):
+            if grad_fn is None:
+                return
+            if grad_fn not in visited:
+                visited.add(grad_fn)
+                if hasattr(grad_fn, 'variable'):
+                    parameters_in_graph.add(grad_fn.variable)
+                parents = grad_fn.next_functions
+                if parents is not None:
+                    for parent in parents:
+                        grad_fn = parent[0]
+                        traverse(grad_fn)
+
+        traverse(loss.grad_fn)
+        for n, p in model.named_parameters():
+            if p not in parameters_in_graph and p.requires_grad:
+                logger.log(
+                    level=logging.ERROR,
+                    msg=f'{n} with shape {p.size()} is not '
+                    f'in the computational graph \n')
+
 @HOOKS.register_module()
 class OptimizerHook(Hook):
     """A hook contains custom operations for the optimizer.
 
     Args:
         grad_clip (dict, optional): A float to control the clip_grad.
             Default: None. (not a config dict)
```

## udl_vis/mmcv/runner/hooks/logger/base.py

```diff
@@ -105,15 +105,15 @@
                 tags[f'learning_rate/{name}'] = value[0]
         else:
             tags['learning_rate'] = lrs[0]
         return tags
 
     def get_momentum_tags(self, runner):
         tags = {}
-        momentums = runner.current_momentum()
+        momentums = self.current_momentum(runner)
         if isinstance(momentums, dict):
             for name, value in momentums.items():
                 tags[f'momentum/{name}'] = value[0]
         else:
             tags['momentum'] = momentums[0]
         return tags
 
@@ -127,14 +127,16 @@
         for var, val in runner.metrics.items():#log_buffer.output
             if var in tags_to_skip:
                 continue
             if self.is_scalar(val) and not allow_scalar:
                 continue
             if isinstance(val, str) and not allow_text:
                 continue
+            if isinstance(val, np.ndarray):
+                print("")
             if add_mode:
                 var = f'{self.get_mode(runner)}/{var}'
             tags[var] = val
         tags.update(self.get_lr_tags(runner))
         tags.update(self.get_momentum_tags(runner))
         return tags
```

## udl_vis/mmcv/runner/hooks/logger/tensorboard.py

```diff
@@ -1,14 +1,15 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 import os.path as osp
-
+import torch
 from udl_vis.mmcv.utils import TORCH_VERSION, digit_version
 from ...dist_utils import master_only
 from ..hook import HOOKS
 from .base import LoggerHook
+import numpy as np
 
 
 @HOOKS.register_module()
 class TensorboardLoggerHook(LoggerHook):
     """Class to log metrics to Tensorboard.
 
     Args:
@@ -48,22 +49,58 @@
             except ImportError:
                 raise ImportError(
                     'Please run "pip install future tensorboard" to install '
                     'the dependencies to use torch.utils.tensorboard '
                     '(applicable to PyTorch 1.1 or higher)')
 
         if self.log_dir is None:
-            self.log_dir = osp.join(runner.work_dir, 'tf_logs')
+            self.log_dir = osp.join(runner.work_dir, 'tf_logs')  # runner.tfb_dir #
         self.writer = SummaryWriter(self.log_dir)
 
     @master_only
     def log(self, runner):
         tags = self.get_loggable_tags(runner, allow_text=True)
         for tag, val in tags.items():
             if isinstance(val, str):
                 self.writer.add_text(tag, val, self.get_iter(runner))
-            else:
+            elif not isinstance(val, np.ndarray):
                 self.writer.add_scalar(tag, val, self.get_iter(runner))
 
+            if isinstance(val, np.ndarray):
+                if runner.epoch % self.interval == 0:
+                    self.writer.add_image(tag, val, dataformats="HWC", global_step=self.get_iter(runner))
+
     @master_only
     def after_run(self, runner):
         self.writer.close()
+
+    def current_momentum(self, runner):
+        """Get current momentums.
+
+        Returns:
+            list[float] | dict[str, list[float]]: Current momentums of all
+            param groups. If the runner has a dict of optimizers, this method
+            will return a dict.
+        """
+
+        def _get_momentum(optimizer):
+            momentums = []
+            for group in optimizer.param_groups:
+                if 'momentum' in group.keys():
+                    momentums.append(group['momentum'])
+                elif 'betas' in group.keys():
+                    momentums.append(group['betas'][0])
+                else:
+                    momentums.append(0)
+            return momentums
+
+        optimizer = runner.optimizer
+        if optimizer is None:
+            raise RuntimeError(
+                'momentum is not applicable because optimizer does not exist.')
+        elif isinstance(optimizer, torch.optim.Optimizer):
+            momentums = _get_momentum(optimizer)
+        elif isinstance(optimizer, dict):
+            momentums = dict()
+            for name, optim in optimizer.items():
+                momentums[name] = _get_momentum(optim)
+        return momentums
```

## udl_vis/mmcv/runner/hooks/logger/text.py

```diff
@@ -158,20 +158,20 @@
                 eta_str = str(datetime.timedelta(seconds=int(eta_sec)))
                 log_str += f'eta: {eta_str}, '
                 log_str += f'time: {log_dict["time"]:.3f}, ' \
                            f'data_time: {log_dict["data_time"]:.3f}, '
                 # statistic memory
                 if torch.cuda.is_available():
                     log_str += f'memory: {log_dict["memory"]}MB, '
-                if self.status_end_of_n_inner_iters:
-                    new_opt_param_groups = np.sum([param.cpu().detach().numpy().sum() for param in
-                                        runner.optimizer.param_groups[0]['params']])
-                    new_opt_state = np.sum([param.cpu().detach().numpy().sum() for _, v in
-                                        runner.optimizer.state.items()  for param in list(v.values())])
-                    log_str += f"opt_param_groups: {new_opt_param_groups}, opt_state: {new_opt_state} "
+                # if self.status_end_of_n_inner_iters:
+                #     new_opt_param_groups = np.sum([param.cpu().detach().numpy().sum() for param in
+                #                         runner.optimizer.param_groups[0]['params']])
+                #     new_opt_state = np.sum([param.cpu().detach().numpy().sum() for _, v in
+                #                         runner.optimizer.state.items()  for param in list(v.values())])
+                #     log_str += f"opt_param_groups: {new_opt_param_groups}, opt_state: {new_opt_state} "
         else:
             # val/test time
             # here 1000 is the length of the val dataloader
             # by epoch: Epoch[val] [4][1000]
             # by iter: Iter[val] [1000]
             if self.by_epoch:
                 log_str = f'Iter [{log_dict["iter"]}] Epoch({log_dict["mode"]}) ' \
@@ -185,14 +185,16 @@
             # TODO: resolve this hack
             # these items have been in log_str
             if name in [
                     'mode', 'Epoch', 'iter', 'lr', 'time', 'data_time',
                     'memory', 'epoch', 'inner_iter'
             ]:
                 continue
+            if isinstance(val, np.ndarray):
+                continue
             if isinstance(val, float):
                 val = f'{val:.5f}'
             log_items.append(f'{name}: {val}')
         log_str += ', '.join(log_items)
         print_log(log_str, logger=runner.logger)
 
     def _dump_log(self, log_dict, runner):
@@ -239,19 +241,19 @@
                 log_dict['lr'].update({k: lr_[0]})
 
         if 'time' in runner.log_buffer.meters:#output
             # statistic memory
             if torch.cuda.is_available():
                 log_dict['memory'] = self._get_max_memory(runner)
 
-        runner.metrics = {k: meter.avg for k, meter in runner.log_buffer.meters.items()}
+        runner.metrics = {k: meter.avg if not hasattr(meter, 'image') else meter.image for k, meter in runner.log_buffer.meters.items()}
         log_dict = dict(log_dict, **runner.metrics) #output
-        if self.status_end_of_n_inner_iters:
-            values = [np.mean(v['exp_avg'].cpu().numpy()) for k, v in runner.optimizer.state_dict()['state'].items()]
-            log_dict.update(opt=np.mean(values))
+        # if self.status_end_of_n_inner_iters:
+        #     values = [np.mean(v['exp_avg'].cpu().numpy()) for k, v in runner.optimizer.state_dict()['state'].items()]
+        #     log_dict.update(opt=np.mean(values))
 
 
         self._log_info(log_dict, runner)
         if self.use_json and runner.logger is not None:
             self._dump_log(log_dict, runner)
         return log_dict
```

## udl_vis/mmcv/utils/__init__.py

```diff
@@ -33,26 +33,26 @@
         'assert_dict_contains_subset', 'assert_attrs_equal',
         'assert_dict_has_keys', 'assert_keys_equal', 'check_python_script',
         'to_1tuple', 'to_2tuple', 'to_3tuple', 'to_4tuple', 'to_ntuple',
         'is_method_overridden', 'has_method'
     ]
 else:
     from .env import collect_env
-    from .logging import get_logger, print_log
+    from .logging import get_logger, print_log, create_logger
     from .parrots_jit import jit, skip_no_elena
     from .parrots_wrapper import (
         TORCH_VERSION, BuildExtension, CppExtension, CUDAExtension, DataLoader,
         PoolDataLoader, SyncBatchNorm, _AdaptiveAvgPoolNd, _AdaptiveMaxPoolNd,
         _AvgPoolNd, _BatchNorm, _ConvNd, _ConvTransposeMixin, _InstanceNorm,
         _MaxPoolNd, get_build_config, is_rocm_pytorch, _get_cuda_home)
     from .registry import Registry, build_from_cfg
     from .trace import is_jit_tracing
     from .hub import load_url
     __all__ = [
-        'Config', 'ConfigDict', 'DictAction', 'collect_env', 'get_logger',
+        'Config', 'ConfigDict', 'DictAction', 'collect_env', 'get_logger', 'create_logger',
         'print_log', 'is_str', 'iter_cast', 'list_cast', 'tuple_cast',
         'is_seq_of', 'is_list_of', 'is_tuple_of', 'slice_list', 'concat_list',
         'check_prerequisites', 'requires_package', 'requires_executable',
         'is_filepath', 'fopen', 'check_file_exist', 'mkdir_or_exist',
         'symlink', 'scandir', 'ProgressBar', 'track_progress',
         'track_iter_progress', 'track_parallel_progress', 'Registry',
         'build_from_cfg', 'Timer', 'TimerError', 'check_time', 'SyncBatchNorm',
```

## udl_vis/mmcv/utils/logging.py

```diff
@@ -190,17 +190,22 @@
         # console.setLevel(logging.DEBUG)
         handlers.append(console)
         # if color:
         #     formatter = _ColorfulFormatter(
         #         colored("%(message)s", "green")
         #     )
         # else:
-    formatter = colorlog.ColoredFormatter(
-        '%(log_color)s- %(message)s',
-        log_colors=log_colors_config)  # 日志输出格式
+
+    if color:
+        formatter = colorlog.ColoredFormatter(
+            '%(log_color)s- %(message)s',
+            log_colors=log_colors_config)  # 日志输出格式
+    else:
+        formatter = logging.Formatter(
+            '%(message)s')
 
     # console.setFormatter(formatter)
     # logger.addHandler(console)
     for handler in handlers:
         handler.setFormatter(formatter)
         handler.setLevel(logging.INFO)  # log_level
         logger.addHandler(handler)
@@ -249,76 +254,81 @@
     # for logger_name in logger_initialized:
     #     if name.startswith(logger_name):
     #         if cfg.use_log:
     #             return logging.getLogger(name)
     #         else:
     #             return None
 
-    logger = None
-    tensorboard_log_dir = None
+    logger = tensorboard_log_dir = final_output_dir = model_save_dir =None
+
     root_output_dir = Path(cfg.out_dir)
     # set up logger in root_path
     if not root_output_dir.exists():
         # if not dist_print: #rank 0-N, 0 is False
         print('=> creating {}'.format(root_output_dir))
         root_output_dir.mkdir(parents=True, exist_ok=True)
 
     dataset = cfg.dataset
     assert isinstance(dataset, dict), print(f"{dataset}'s type is {type(dataset)}, not a dict. ")
 
     # if not dist_print:
-
-    if cfg.use_log_and_save:
-
-        if os.path.exists(cfg.resume_from) and (dataset.get('train', None) is None or cfg.eval):
-            model_save_dir = os.path.dirname(cfg.resume_from.replace('\\', '/'))
-            log_file = '{}_{}.log'.format(cfg_name, model_save_dir.split('/')[-1].split('_')[-1])
-            final_output_dir = model_save_dir
-            final_log_file = Path(model_save_dir) / log_file
-
+    if os.path.exists(cfg.resume_from) and (dataset.get('train', None) is None or cfg.eval):
+        model_save_dir = os.path.dirname(cfg.resume_from.replace('\\', '/'))
+        log_file = '{}_{}.log'.format(cfg_name, model_save_dir.split('/')[-1].split('_')[-1])
+        final_output_dir = model_save_dir
+        final_log_file = Path(model_save_dir) / log_file
+
+    else:
+        if cfg.eval:
+            dataset = dataset.get("test")
         else:
-            if cfg.eval:
-                dataset = dataset.get("test")
-            else:
-                dataset = dataset.get('train') if dataset.get('train', None) is not None else dataset.get('val')
-            model = cfg.arch
-            cfg_name = os.path.basename(cfg_name).split('.')[0]
-            time_str = time.strftime('%Y-%m-%d-%H-%M-%S')
-
-            # store all output except tb_log file
-            final_output_dir = root_output_dir / dataset / model / cfg_name
-            if cfg.eval:
-                model_save_tmp = os.path.dirname(cfg.resume_from).split('/')[-1]
-            else:
-                model_save_tmp = "model_{}".format(time_str)
+            dataset = dataset.get('train') if dataset.get('train', None) is not None else dataset.get('val')
+        model = cfg.arch
+        cfg_name_base = os.path.basename(cfg_name).split('.')[0]
+        time_str = time.strftime('%Y-%m-%d-%H-%M-%S')
+
+        # store all output except tb_log file
+        final_output_dir = root_output_dir / dataset / model / cfg_name
+        if cfg.eval:
+            model_save_tmp = os.path.dirname(cfg.resume_from).split('/')[-1]
+        else:
+            model_save_tmp = "model_{}".format(time_str)
 
-            model_save_dir = final_output_dir / model_save_tmp
+        model_save_dir = final_output_dir / model_save_tmp
 
-            print_log('=> creating {}'.format(final_output_dir))
-            final_output_dir.mkdir(parents=True, exist_ok=True)
+        print_log('=> creating {}'.format(final_output_dir))
+        if cfg.use_save:
             model_save_dir.mkdir(parents=True, exist_ok=True)
+        if cfg.use_log:
+            final_output_dir.mkdir(parents=True, exist_ok=True)
 
-            cfg_name = '{}_{}'.format(cfg_name, time_str)
-            # a logger to save results
-            log_file = '{}.log'.format(cfg_name)
-            # if cfg.eval:
-            #     final_log_file = model_save_dir / log_file
-            # else:
-            #     final_log_file = final_output_dir / log_file
-            final_log_file = final_output_dir / log_file
-            # tensorboard_log
-            tensorboard_log_dir = root_output_dir / Path(cfg.log_dir) / dataset / model / cfg_name
-            # if not dist_print:
-            print_log('=> creating tfb logs {}'.format(tensorboard_log_dir))
-            tensorboard_log_dir.mkdir(parents=True, exist_ok=True)
-        logger = setup_logger(name, final_log_file)
+        cfg_name = '{}_{}'.format(cfg_name_base, time_str)
+        # a logger to save results
+        log_file = '{}.log'.format(cfg_name)
+        # if cfg.eval:
+        #     final_log_file = model_save_dir / log_file
+        # else:
+        #     final_log_file = final_output_dir / log_file
+        final_log_file = final_output_dir / log_file
+        # tensorboard_log
+        tensorboard_log_dir = root_output_dir / Path(cfg.log_dir) / dataset / model / cfg_name
+        # if not dist_print:
+        print_log('=> creating tfb logs {}'.format(tensorboard_log_dir))
+        tensorboard_log_dir.mkdir(parents=True, exist_ok=True)
+
+    if cfg.use_log:
+        logger = setup_logger(name, final_log_file, cfg.use_colorlog)
+    if not cfg.use_save:
+        tensorboard_log_dir = model_save_dir = ""
+        return logger, final_output_dir, model_save_dir, tensorboard_log_dir
+    else:
+        return logger, str(final_output_dir), str(model_save_dir), str(
+            tensorboard_log_dir)  # logger,
 
 
-    return logger, str(final_output_dir), str(model_save_dir), str(
-        tensorboard_log_dir)  # logger,
 
 
 def print_log(msg, logger=None, level=logging.INFO, clear_logger=False):
     """Print a log message.
 
     Args:
         msg (str): The message to be logged.
```

## Comparing `udl_vis-0.3.5.dist-info/LICENSE` & `udl_vis-0.3.6.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `udl_vis-0.3.5.dist-info/METADATA` & `udl_vis-0.3.6.dist-info/METADATA`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: udl-vis
-Version: 0.3.5
+Version: 0.3.6
 Summary: unified pytorch framework for vision task
 Home-page: https://github.com/XiaoXiao-Woo/PanCollection
 Author: XiaoXiao-Woo
 Author-email: wxwsx1997@gmail.com
 License: GPLv3
 Classifier: Programming Language :: Python :: 3
 Classifier: License :: OSI Approved :: GNU General Public License v3 (GPLv3)
@@ -23,43 +23,46 @@
 Requires-Dist: colorlog
 Requires-Dist: scipy
 Requires-Dist: h5py
 Requires-Dist: regex
 Requires-Dist: packaging
 Requires-Dist: pyyaml
 
-# UDL (Make Available on PyPI :tada:)
+# UDL (Make Available on [PyPI](https://pypi.org/project/udl-vis/) :tada:)
 
-UDL is a unified pytorch framework for vision research:
+UDL is a unified Pytorch framework for vision research:
 
-* UDL has faster library loading speed and more convenient reflection mechanism to call different models and methods.
+* UDL has a faster library loading speed and a more convenient reflection mechanism to call different models and methods.
 * UDL is based on MMCV which provides the following functionalities.
-* UDL is based on NNI to peform automatic machine learning.
-
+* UDL is based on NNI to perform automatic machine learning.
 
 
 
 [English](https://github.com/XiaoXiao-Woo/UDL/edit/dev/README.md) | [简体中文](https://github.com/XiaoXiao-Woo/UDL/edit/dev/README_zh.md)
 
 See the [repo](https://github.com/liangjiandeng/PanCollection) for more detailed descriptions. 
 
 ## Note
 
 For the implementation of DCFNet as described in the ICCV paper "Dynamic Cross Feature Fusion for Remote Sensing Pansharpening," please refer to the [branch](https://github.com/XiaoXiao-Woo/UDL/blob/UDL_DCFNet) in the this repository.
 
 ## Requirements
-* Python3.7+, Pytorch>=1.6.0
+* Python3.7+, Pytorch>=1.9.0
 * NVIDIA GPU + CUDA
 * Run `python setup.py develop`
 
 Note: Our project is based on MMCV, but you needn't to install it currently.
 
 ## Quick Start
 
-> pip install -i udl-vis https://pypi.org/simple
+> pip install udl-vis -i https://pypi.org/simple
+
+Also, you can quick start from PanCollection, which is remote sensing pansharpening and is one of our applications.
+
+> pip install pancollection -i https://pypi.org/simple
 
 ### Quick Start for developer
 
 **Step0.** We use UDL in PanCollection, first please set your Python environment.
 
 >git clone https://github.com/XiaoXiao-Woo/UDL
 >
```

## Comparing `udl_vis-0.3.5.dist-info/RECORD` & `udl_vis-0.3.6.dist-info/RECORD`

 * *Files 4% similar despite different names*

```diff
@@ -1,37 +1,38 @@
 udl_vis/__init__.py,sha256=jPqedAPKI8MRMSK8o-F-9WuCAgsfZ5AFg4cUBj6x1zU,250
 udl_vis/AutoDL/__init__.py,sha256=ovguP4wzQEDNguczwiZnhMm4dRRVcvnzmHrfQtlRCNQ,15
-udl_vis/AutoDL/trainer.py,sha256=TAqJEYB6fL_ncx631SazL0jrrQLj-tNDzYh3HmIVLsQ,14554
+udl_vis/AutoDL/base_runner.py,sha256=VK4sZFUssbgijk4fzUl5ciYEZtEhOds2inPz3f-vMCs,13274
+udl_vis/AutoDL/trainer.py,sha256=wOZyHFBtB3MeiEtG6fdltifbyXRrhSyr51odUyP1ZBk,14215
 udl_vis/Basis/__init__.py,sha256=ovguP4wzQEDNguczwiZnhMm4dRRVcvnzmHrfQtlRCNQ,15
 udl_vis/Basis/cal_ssim.py,sha256=BuPhoTypDOPaxaZiz0tJvKZoGk5UTIgA35EuIk98EYY,2851
 udl_vis/Basis/config.py,sha256=R1R663Oe9eWtUgaEI2X7WBw3vJDXWyMWmBTdnsC7RK8,29004
 udl_vis/Basis/criterion_metrics.py,sha256=hNLNcvsmO1AUwyB9kI5G_pxEMpjz-1K38RUHARL0EP0,4087
 udl_vis/Basis/dist_utils.py,sha256=hFoog343eLiQI9sLm7LUVdDWxoklcQxUjIVC_XjS4gg,6622
 udl_vis/Basis/launch.py,sha256=zd8AuWJzRbXS3BF9p2K8sHv-i69M2o6-4Edi0m-KNFg,14882
-udl_vis/Basis/logger.py,sha256=p3Wsr0dKoZ16PhB2OaYb3riEi8Oq5OunKg4fWdyp1fU,8772
+udl_vis/Basis/logger.py,sha256=gq6eCd3fpZAZhbvSQo8Iujynyrwb4VEQNgnRsGP2WOE,9077
 udl_vis/Basis/metrics.py,sha256=Fg8wV8p0rrLN7HQ24BrkxaE7xUXMOORPIbdUaQDcE8Q,3145
 udl_vis/Basis/module.py,sha256=gip797wlrR7rEpCHznAlAIkokNmqA82QHD2MeuTj76Y,16806
 udl_vis/Basis/optim.py,sha256=2dypTCovd1xV7eIaHpFz2fepERPploatT0ZLvu5TOvE,9926
-udl_vis/Basis/option.py,sha256=jnj7Dhh9h3tds3fm1Xu8f7OBVyk1VV8bzWDKZG-Ju0o,6333
-udl_vis/Basis/postprocess.py,sha256=-A7YPKsTHI4xlL-6fzfl1Ws6xgrvv3zD9mo1loIQ43o,17306
-udl_vis/Basis/python_sub_class.py,sha256=rRV5ZSKy7BILVBclYasFs9d6_1PvGrPF4E3XcfQMtis,5187
+udl_vis/Basis/option.py,sha256=KNPODpnrjEFCHX9Gim1UW23ABSpyjlsG-XRJu8Oy81M,6464
+udl_vis/Basis/postprocess.py,sha256=gxTbU8Fqsso75gfdxn1-8bdaSu0qMNsz1vDCdE6PihU,17556
+udl_vis/Basis/python_sub_class.py,sha256=huBqcu9yYRNQs_EY7_fY7EVpWQ7UhuUkKrWWyQEBa9c,8175
 udl_vis/Basis/variance_sacling_initializer.py,sha256=Ub0HTUQsjkjMI1rse2uUSrp0l0UmvJX1JMxaco7DfkE,3145
 udl_vis/Basis/auxiliary/__init__.py,sha256=N7S6PyM1oUxq90HraUHPVaywibZ08YTzucpz-OHTh8U,134
 udl_vis/Basis/auxiliary/base.py,sha256=wCkUqLKhi8tiDEeAikt4EJrZuthnCPwvgEZW2HCr-G4,1084
 udl_vis/Basis/auxiliary/fp16_utils.py,sha256=IJ6iVgq_N5expZz7mO609bfPJQ9I5koEZfSc39xhEEk,7303
 udl_vis/Basis/auxiliary/utils.py,sha256=of_TyTV-yi0vH7T9mkG0mXwYM8yb6XWLaeP_oNb4HE8,11956
-udl_vis/Basis/auxiliary/torchstat/__init__.py,sha256=rmgfbQJhlKTwkHDRvHGvF_PZyHgRAWlZohMOY-d5PuE,787
+udl_vis/Basis/auxiliary/torchstat/__init__.py,sha256=nl15C286Dzv1DOW5GmUJp4YxgZj42VOL1doN2RCjW8s,724
 udl_vis/Basis/auxiliary/torchstat/__main__.py,sha256=ZA3Vy4VOyk0-7EQbMD9BmlPvVqI2qbcir7ngP9XW2RU,1142
-udl_vis/Basis/auxiliary/torchstat/compute_flops.py,sha256=VQGcLx2iOONiDPuhC7RYEHlqIK_lnVyw-dF9Q5ljfFQ,9296
-udl_vis/Basis/auxiliary/torchstat/compute_madd.py,sha256=opfejlnQC36_tHiAfkdpY6zaCjGTAkiRVX09cKldyqE,6085
-udl_vis/Basis/auxiliary/torchstat/compute_memory.py,sha256=lg9bwTmDDKqAmu2YtU4CPwhlnqwhcKmJfamtRtpMwXE,4016
-udl_vis/Basis/auxiliary/torchstat/model_hook.py,sha256=afP3D2O_R3taHNxKFCKvsavqhBI6WxUjG52N2ZJBzZ8,10202
-udl_vis/Basis/auxiliary/torchstat/reporter.py,sha256=1KCeotL8NUkzIfdT2PjvTc1Thk7gMjd0MmB-mpGy1No,3758
-udl_vis/Basis/auxiliary/torchstat/stat_tree.py,sha256=ZMU6M-5H9XOW4TDsjzB5BggkFDlQ5525_3Ip6JalDB4,5899
-udl_vis/Basis/auxiliary/torchstat/statistics.py,sha256=VRAJc0FiyYYtYqEseKK5Wpbd6Qqaz7HEMv8rHxmGOXM,3364
+udl_vis/Basis/auxiliary/torchstat/compute_flops.py,sha256=MR90WnDTWfSNBKWruAWQrAXRWFD6nCguzmUFgIWlITw,10544
+udl_vis/Basis/auxiliary/torchstat/compute_madd.py,sha256=z4IM2L8oXDznBxpz1g9Zl0_yPF2EaEZZGhFDYKdCwtU,6303
+udl_vis/Basis/auxiliary/torchstat/compute_memory.py,sha256=0lXTxHhyuF_8xiQRB7DbxT0cLd5ExZSFiH2lT_llC7w,4245
+udl_vis/Basis/auxiliary/torchstat/model_hook.py,sha256=ri-2Rpn9WxcaeTmVARC4thc7lyYMxVrr-LUm_ZfsUko,12481
+udl_vis/Basis/auxiliary/torchstat/reporter.py,sha256=lkn1eWwJd5ZGc48Ij8kXpNlNfg_esfKtQvIKy5TBZWk,4193
+udl_vis/Basis/auxiliary/torchstat/stat_tree.py,sha256=PAt22ZSs8d8M8HDu8zZxuo5-dGjAQu-3fKC-ppE4Y-Y,6134
+udl_vis/Basis/auxiliary/torchstat/statistics.py,sha256=TaLb-S6nEVCT-ym-KsO5NYORk_a5ii-_xTAy_5asq9k,3761
 udl_vis/mmcv/__init__.py,sha256=poBZJDY0hSh51YKteCBE93Aqdbi3jLNjt-gTetRRE1E,367
 udl_vis/mmcv/version.py,sha256=fKeOnShZWjaTQmYodfxGlqDNvvCgDIOGIK5VEtaXfvk,1211
 udl_vis/mmcv/arraymisc/__init__.py,sha256=HxXujiBxIvR-lOZ1BSnOxJk6kb-9jCuM8J-11xd-pps,137
 udl_vis/mmcv/arraymisc/quantization.py,sha256=zdGHPaEZktwf4fuOJD2-nZfwuJIgfEfiU5C-SXRdA0A,1879
 udl_vis/mmcv/cnn/__init__.py,sha256=aR17Af5YukZdQoqZAojJIotMw4T4RNbhnoMHFw39gK0,2479
 udl_vis/mmcv/cnn/alexnet.py,sha256=yCPuuFrDg3OtrJFoMBHSkDwqcvPcopcpD1-SDTLuU3c,2051
 udl_vis/mmcv/cnn/builder.py,sha256=bGwhOUS2WtzhwSkbpAMQqFvXapCwUK4bdx_9AlnRYVU,1119
@@ -143,66 +144,66 @@
 udl_vis/mmcv/parallel/data_container.py,sha256=1OzV9uT9F2Zm2kapOSPh_AZEZXvQmIkOqRhyQTUgMTQ,2454
 udl_vis/mmcv/parallel/data_parallel.py,sha256=5sC45vtwmHPQM51D2b7LbOI8EiH9pcs2EDTekVER3uI,4588
 udl_vis/mmcv/parallel/distributed.py,sha256=ByVq79hUJj8ic1f56IQM1PzncsbJI12b6GdpzmHdJCE,4945
 udl_vis/mmcv/parallel/distributed_deprecated.py,sha256=KWgCZMwzoroaoV5YecUpOCf6_ubPMvt9Ve5giT3wdbk,2887
 udl_vis/mmcv/parallel/registry.py,sha256=AxyS-c-pCPJ7-bRiKxz_8oJ1k7z5mQpw6XuvHgmOVV4,328
 udl_vis/mmcv/parallel/scatter_gather.py,sha256=AxxkS6uq-2c0_8k4lRtRxv0Bix9eKO-Bpsb0V3aXCPk,2366
 udl_vis/mmcv/parallel/utils.py,sha256=wg6-rhJqFl3-NaU5vMw9CG5UBxcuOr4aJMlBBG1D0Dk,728
-udl_vis/mmcv/runner/__init__.py,sha256=sxC4fyVcxZjiHLL3_5EcvDywM1cXjNoU3ZlG6G_ti9I,4385
+udl_vis/mmcv/runner/__init__.py,sha256=BkcGpBqb7gg8Zmd3q-a1rJlKspMoMbyHHAtrWoB4O3U,4428
 udl_vis/mmcv/runner/base_module.py,sha256=pajuXr-sZz5J046chHyn8khrSS1vg4PuV2TQ82GK69Q,14570
-udl_vis/mmcv/runner/base_runner.py,sha256=L1A2lpqO6xwKAdF3Sfms0JyFDW7JX46CX0L4k-bbIN0,24585
+udl_vis/mmcv/runner/base_runner.py,sha256=Er_cVXoz5b6HgjMxHA1joVr89wDAyZbxCdJUiU_ZK0Y,24691
 udl_vis/mmcv/runner/builder.py,sha256=yEp9ctE7Kw4gbGbY2Yf88Yc48RrZQLM-cR0WGkYgKyk,690
-udl_vis/mmcv/runner/checkpoint.py,sha256=io85Y3N9dK2tu8llXSncKw9zbYbbn5PAeUL5VUhY9kY,34069
+udl_vis/mmcv/runner/checkpoint.py,sha256=rqaCmNn667aFe8dKQKMtq2by4iK7nbNhKPwKlOGXGJQ,34094
 udl_vis/mmcv/runner/default_constructor.py,sha256=b6edxh4NotRdYNdqZ9a0NE5KKInXSvvbzqLyi6PPazw,1952
 udl_vis/mmcv/runner/dist_utils.py,sha256=56RURtqjEHSH7Nh933b53cg0T4mAzXv6yF8vFq4NRgY,5559
-udl_vis/mmcv/runner/epoch_based_runner.py,sha256=f0L8x_8F8K3hY7jmwsMvfZdq2WUNG5H80eSPAp0_McI,14412
+udl_vis/mmcv/runner/epoch_based_runner.py,sha256=bf3TWrg-lfIup6SZTTSi0SCk51NcCiEF6YV07T6gCCc,14439
 udl_vis/mmcv/runner/fp16_utils.py,sha256=nO36ezgpKzcewM4hSTxXbPihhO8w2xrGMne4bV-vrX0,16873
 udl_vis/mmcv/runner/iter_based_runner.py,sha256=x7ZtmX04mhrENaO9Hz7I_xlkweYYswTAQejDZFMf49Y,11438
 udl_vis/mmcv/runner/log_buffer.py,sha256=zdjg0Aphh9zJgNScEiauGpgR-u5bGRubuXYwoQ2Vk1E,1880
 udl_vis/mmcv/runner/misc.py,sha256=U7MPrB1srnbVPupNxVw34yOn93_k5TeYJmCnSd7mll4,1241
 udl_vis/mmcv/runner/priority.py,sha256=G8CBRfmUWlIp2zAP9twjEjlMlos98_6OAXG5KHOXB30,1658
-udl_vis/mmcv/runner/record.py,sha256=Bdr0_9NMZm_suWVvY2xgCGRq0PZ1oBiGJPNu4Q1xLLk,11169
+udl_vis/mmcv/runner/record.py,sha256=esM3ZZu7uA6QF7rwZM3Nj4op2ra6rHpm7SfE4Ym4srQ,11372
 udl_vis/mmcv/runner/utils.py,sha256=FaOBbQlCBbH8ms24YXTmg5PX02KgOaUfKOSSokRkCGc,3014
-udl_vis/mmcv/runner/hooks/__init__.py,sha256=wGiPEb1jNBOH4QJCX-mNszEbEM6n9HVjef2EVQnwS_A,2402
-udl_vis/mmcv/runner/hooks/checkpoint.py,sha256=51OFPcLGeWsiA3uSUlycQiMdI9toeORa3tO-hwBFgik,20344
+udl_vis/mmcv/runner/hooks/__init__.py,sha256=O2KInoJBSX9x7lq54DWel9rzBMFAdjZ9K26HDAJk8vk,2525
+udl_vis/mmcv/runner/hooks/checkpoint.py,sha256=Dy9mCdukp6cjtfTO6gv_Kb36Gxl91K7aqdzWh8r3d_o,20433
 udl_vis/mmcv/runner/hooks/closure.py,sha256=-cC2C-FdrLroTtckeUsmRN5ylZIhyD8JrKo3cqtOhac,280
 udl_vis/mmcv/runner/hooks/ema.py,sha256=V-RiL_Qg2yTRKapcSN1ymHid1vsQMq-GWh3l-jPoOBg,3674
 udl_vis/mmcv/runner/hooks/evaluation.py,sha256=G-5wezy_DZ0hfZptCn-HtI_99N5uKsCM7dw_9bSUlKc,22917
-udl_vis/mmcv/runner/hooks/hook.py,sha256=cI-XTMtGvRCyOLb5FcFsPVWBI07Gxq9xjKxTKoKRsdo,2927
+udl_vis/mmcv/runner/hooks/hook.py,sha256=hg-gZ6cd90GPbKanSC9HussQp6hssSYYx1We77dqmNU,8495
 udl_vis/mmcv/runner/hooks/iter_timer.py,sha256=BGIOgNW-v8-a5KCdSX_HIeZgH5Kcq_-mReIcGEDFCQ8,521
 udl_vis/mmcv/runner/hooks/lr_updater.py,sha256=gi05MgmVkuzLi-EcteByzbsf7p70aoLeSYbRPkOq9Hw,27815
 udl_vis/mmcv/runner/hooks/memory.py,sha256=ynvJeHEHlELy_mQRgo0JEInQhULP7t5JCHNjNwfwMzY,682
 udl_vis/mmcv/runner/hooks/momentum_updater.py,sha256=BO1gfg6BLPTg34CN1Ck-ZUCn_UOqxLQknnijI-mYDRA,23465
 udl_vis/mmcv/runner/hooks/nni_hook.py,sha256=tby26kxtSPTHrmdDlSZVZBHFZGVwRcl3GMKm8_HGmIU,1602
-udl_vis/mmcv/runner/hooks/optimizer.py,sha256=arP82gGwku17ihDsnwnfIsifuMPAPS8B5jUVwAl7KcY,25296
+udl_vis/mmcv/runner/hooks/optimizer.py,sha256=lh5aCz18KTodH32XmyLzObWK-3XS8_ltAMvqIs3qxPw,26497
 udl_vis/mmcv/runner/hooks/profiler.py,sha256=sLZk1hUYzB5t0m4U5UvZcnu8AEpMxTeWOwP9ICswj0s,8221
 udl_vis/mmcv/runner/hooks/sampler_seed.py,sha256=MU4haFBYJUBNxGLe5GTzVO4BZqhX-nDDzHPbvqrqXZM,867
 udl_vis/mmcv/runner/hooks/sync_buffer.py,sha256=eHxJGlwXWm_h-hoVe-6UvVmkXlyobwc1TsMjl9tAGSM,729
 udl_vis/mmcv/runner/hooks/logger/__init__.py,sha256=UaUE-IAETUKskXuwqhUg1B8Q26lpEw8siktBtqsn-l0,537
-udl_vis/mmcv/runner/hooks/logger/base.py,sha256=FmALy6bQeWqUdmzfiqn0QGiI-AEAJfbLIA0bN2awjYY,6492
+udl_vis/mmcv/runner/hooks/logger/base.py,sha256=xiTs1ynUOnysqnd_FzivL3R9xXMWV9YCaQmdDrICTM0,6568
 udl_vis/mmcv/runner/hooks/logger/dvclive.py,sha256=vuaeGoKtBwF_1rLe7a1iOcNQRwyOkDCyol25ceC5xu4,2294
 udl_vis/mmcv/runner/hooks/logger/mlflow.py,sha256=TlCsfS2P1OXjqdKq4m9ukjwEpYLlHkVB9c-I2e4CTFE,2995
 udl_vis/mmcv/runner/hooks/logger/neptune.py,sha256=1iaIiZ_M2GcJ-u9EOQvZUnOR0qKzZ-C4aTEgKnqiBXs,3338
 udl_vis/mmcv/runner/hooks/logger/pavi.py,sha256=pyF8bLp2GxhwoFqEaVEUJNCJ5x59bQEU4mYr5nQeb9Q,5269
-udl_vis/mmcv/runner/hooks/logger/tensorboard.py,sha256=UGJxaQPs85CIoZu6E0cWNoLLc2LLv6xoCuapYlXfLV4,2739
-udl_vis/mmcv/runner/hooks/logger/text.py,sha256=WLqS9bBm81qKbtwDBCa-2cMxzWBWLu_6tZSuktYfsRc,12470
+udl_vis/mmcv/runner/hooks/logger/tensorboard.py,sha256=PWE46Dscp9-qDO7_qGaTU6855tqgETsbeszkeaC7wQQ,4256
+udl_vis/mmcv/runner/hooks/logger/text.py,sha256=m3MbMfUVmLfr3_kWu-gXGasBDPGZPii_QAPb5kJqSPs,12607
 udl_vis/mmcv/runner/hooks/logger/wandb.py,sha256=Iq4605_B0qF7jnDOpbTjxA6qQdMOv-oScqt4G2nc_1Y,4009
 udl_vis/mmcv/runner/optimizer/__init__.py,sha256=KQgKnlQvYl2GyC-SvJBG4921AYnR9KQGN0DdbCdF1CE,379
 udl_vis/mmcv/runner/optimizer/builder.py,sha256=AzEYtI2qddTJsDPf6tSibjujq6rr7ab10LnQYiPlPH8,1390
 udl_vis/mmcv/runner/optimizer/default_constructor.py,sha256=FuqUgtUi8v4WkFndf3MXlihycrKIUP0t1qq2J2X7pqE,11971
 udl_vis/mmcv/tensorrt/__init__.py,sha256=-Z0RXE8vJJV2soQ5RqnWno1OEs61oZJqWEYl8ys61UM,788
 udl_vis/mmcv/tensorrt/init_plugins.py,sha256=miTA83z2oemLK6TPnh8pPpiwmDQhsF6ET5GCRaWaWHw,916
 udl_vis/mmcv/tensorrt/preprocess.py,sha256=U_ozP91sK7VUqdzxnwx3eWtr8hduuh-FGoSHOMzsWiM,4459
 udl_vis/mmcv/tensorrt/tensorrt_utils.py,sha256=k1H7ciwICpgTOviVxAkbm7hlaZhmRrlmkc3iCtEHG4U,8417
-udl_vis/mmcv/utils/__init__.py,sha256=2-PyhGZrqWphp3p65zcyxY4WKmv3OaXdqqAEvRIPZMM,4027
+udl_vis/mmcv/utils/__init__.py,sha256=TegCnsmfLOlsQlsAtG9U-e5PG3d9wkL6x096rgfYJ9A,4059
 udl_vis/mmcv/utils/config.py,sha256=vgOZXX1qWZZsFTRj0JHEuYU5MoEtngDEpZKL8DFT9pg,27039
 udl_vis/mmcv/utils/env.py,sha256=dQfrJudYLZK7HEv6dnTatvggQC8o73iZE8C8DvdsFWY,3407
 udl_vis/mmcv/utils/ext_loader.py,sha256=bVwtzF6oAvmKCBM7TAWvMUPb5-iVME1YNQVgcH89TBU,2092
 udl_vis/mmcv/utils/hub.py,sha256=0jfv4LD6f8J0tIruxBum2Oa-mUVU5zuv5Y9VJC-zOjE,6167
-udl_vis/mmcv/utils/logging.py,sha256=1L6Mg5x0JpGoEl6U2svRMO8S6FMQS4twwINQrpnJbq4,14137
+udl_vis/mmcv/utils/logging.py,sha256=KMzmnAl4iKemiUIyHfMhcrQaKGCZ2R2hhvy4Gm67IwE,14378
 udl_vis/mmcv/utils/misc.py,sha256=690_cNNkMZsQQVxlouO3rwdlLeeRGtCHk0obOEnujJE,11864
 udl_vis/mmcv/utils/parrots_jit.py,sha256=qs-pmzBm2mIpezgiBMjs4Qs-Venlh7wF0rGogX8yDoo,940
 udl_vis/mmcv/utils/parrots_wrapper.py,sha256=UoGceWp1WeBUSR5oC0T-JwRWQpyqMEqP5RIDkeGn9D8,3643
 udl_vis/mmcv/utils/path.py,sha256=vRWXZfkm8PnS9jXXxOlvvHVoqzCzgieXZhUjMC8dlJA,3516
 udl_vis/mmcv/utils/progressbar.py,sha256=StJ17Rm4Gzn_zl3DZWdQ2twoEIB1Xl-EEd3dA5t-7Pk,7313
 udl_vis/mmcv/utils/registry.py,sha256=--0W9bJTtqIDxlAOI6ivSQfkAGRtbwNAAV69W7n0pFU,11722
 udl_vis/mmcv/utils/testing.py,sha256=uS--CYMC31g0f8SR45eu1tIuiMZW1h9-0RI5F_OgTrM,4429
@@ -215,12 +216,12 @@
 udl_vis/mmcv/video/processing.py,sha256=ReMLLg8M-TpkmMP2lhVkEhIeYbKuBAw6UdBnxEGYrj0,5439
 udl_vis/mmcv/visualization/__init__.py,sha256=7bjyjDI2DNV8SfsDa7MaB3Yr7tjMCS3_zflkNMfdSjY,347
 udl_vis/mmcv/visualization/color.py,sha256=ldTEA13GOlmCUzuuZkrlHSh6rHCKcxuRUC2cBXwjNwY,1420
 udl_vis/mmcv/visualization/image.py,sha256=9W78dT6U84cY5i1Cm5LheB1sNPxoWYVWc7FgRTRTv_o,5285
 udl_vis/mmcv/visualization/optflow.py,sha256=PWdjNrqjl77czn4787rE_ir6nilHIAvV6Wq8aKr2dfA,3477
 udl_vis/tests/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 udl_vis/tests/test_pytorch_dataloader.py,sha256=-Fgae8RScyOnVFKlgjR6kbVEwLhm4RJmaiA0Hfw6Jdo,2143
-udl_vis-0.3.5.dist-info/LICENSE,sha256=IwGE9guuL-ryRPEKi6wFPI_zOhg7zDZbTYuHbSt_SAk,35823
-udl_vis-0.3.5.dist-info/METADATA,sha256=ObFLUXUDkNEbnQ03L-JztUOtmdTAEV1sXGw3L_HwAbc,3501
-udl_vis-0.3.5.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-udl_vis-0.3.5.dist-info/top_level.txt,sha256=lwXVNhXgYH7yLAdg0S8Gb4hYbomoBdaja9PeHE6iDoU,8
-udl_vis-0.3.5.dist-info/RECORD,,
+udl_vis-0.3.6.dist-info/LICENSE,sha256=IwGE9guuL-ryRPEKi6wFPI_zOhg7zDZbTYuHbSt_SAk,35823
+udl_vis-0.3.6.dist-info/METADATA,sha256=OGt0-1YZsjWheiPSOoKtj5qrc9kmKZgmbNJK-DJBGRY,3715
+udl_vis-0.3.6.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
+udl_vis-0.3.6.dist-info/top_level.txt,sha256=lwXVNhXgYH7yLAdg0S8Gb4hYbomoBdaja9PeHE6iDoU,8
+udl_vis-0.3.6.dist-info/RECORD,,
```

