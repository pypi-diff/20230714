# Comparing `tmp/fairbench-0.2.8-py3-none-any.whl.zip` & `tmp/fairbench-0.2.9-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,49 +1,49 @@
-Zip file size: 37696 bytes, number of entries: 47
+Zip file size: 38308 bytes, number of entries: 47
 -rw-rw-rw-  2.0 fat      411 b- defN 23-Jun-23 14:12 fairbench/__init__.py
 -rw-rw-rw-  2.0 fat     1952 b- defN 23-Jun-01 09:25 fairbench/export.py
 -rw-rw-rw-  2.0 fat     5762 b- defN 23-Jun-28 17:37 fairbench/stamps.py
 -rw-rw-rw-  2.0 fat       38 b- defN 23-Jun-14 13:57 fairbench/bench/__init__.py
 -rw-rw-rw-  2.0 fat     2341 b- defN 23-Jun-15 21:51 fairbench/bench/demos.py
 -rw-rw-rw-  2.0 fat     1660 b- defN 23-Jun-15 09:42 fairbench/bench/loader.py
 -rw-rw-rw-  2.0 fat      134 b- defN 23-Jun-28 12:59 fairbench/export/__init__.py
--rw-rw-rw-  2.0 fat    16295 b- defN 23-Jul-05 08:03 fairbench/export/interactive.py
--rw-rw-rw-  2.0 fat     2912 b- defN 23-Jun-16 09:23 fairbench/export/native.py
+-rw-rw-rw-  2.0 fat    16566 b- defN 23-Jul-11 12:04 fairbench/export/interactive.py
+-rw-rw-rw-  2.0 fat     3022 b- defN 23-Jul-07 09:37 fairbench/export/native.py
 -rw-rw-rw-  2.0 fat      173 b- defN 23-Jun-28 12:05 fairbench/export/modelcards/__init__.py
 -rw-rw-rw-  2.0 fat     2856 b- defN 23-Jun-28 13:13 fairbench/export/modelcards/tohtml.py
 -rw-rw-rw-  2.0 fat     2423 b- defN 23-Jun-28 13:12 fairbench/export/modelcards/tomarkdown.py
 -rw-rw-rw-  2.0 fat      562 b- defN 23-Jun-28 12:59 fairbench/export/modelcards/toyaml.py
 -rw-rw-rw-  2.0 fat      122 b- defN 23-May-13 22:58 fairbench/forks/__init__.py
 -rw-rw-rw-  2.0 fat     2130 b- defN 23-May-25 07:44 fairbench/forks/categorical.py
 -rw-rw-rw-  2.0 fat     2936 b- defN 23-Jun-28 12:59 fairbench/forks/explanation.py
--rw-rw-rw-  2.0 fat    26874 b- defN 23-Jun-28 12:59 fairbench/forks/fork.py
+-rw-rw-rw-  2.0 fat    26940 b- defN 23-Jul-10 09:08 fairbench/forks/fork.py
 -rw-rw-rw-  2.0 fat      239 b- defN 23-Jun-20 08:51 fairbench/metrics/__init__.py
--rw-rw-rw-  2.0 fat     4008 b- defN 23-Jul-06 00:14 fairbench/metrics/classification.py
+-rw-rw-rw-  2.0 fat     4016 b- defN 23-Jul-07 09:37 fairbench/metrics/classification.py
 -rw-rw-rw-  2.0 fat     2066 b- defN 23-May-31 07:41 fairbench/metrics/disparate_impact.py
 -rw-rw-rw-  2.0 fat     1494 b- defN 23-May-31 07:41 fairbench/metrics/disparate_mistreatment.py
--rw-rw-rw-  2.0 fat      668 b- defN 23-Jun-20 11:41 fairbench/metrics/ranking.py
+-rw-rw-rw-  2.0 fat     3760 b- defN 23-Jul-10 11:32 fairbench/metrics/ranking.py
 -rw-rw-rw-  2.0 fat     2970 b- defN 23-Jul-06 06:59 fairbench/metrics/regression.py
 -rw-rw-rw-  2.0 fat      100 b- defN 23-May-31 10:15 fairbench/mitigation/__init__.py
 -rw-rw-rw-  2.0 fat     1790 b- defN 23-May-31 10:16 fairbench/mitigation/postprocessing.py
 -rw-rw-rw-  2.0 fat      207 b- defN 23-Jan-15 14:06 fairbench/reports/__init__.py
 -rw-rw-rw-  2.0 fat     1491 b- defN 23-Jun-07 08:52 fairbench/reports/accumulate.py
--rw-rw-rw-  2.0 fat     2004 b- defN 23-Jun-23 13:28 fairbench/reports/adhoc.py
+-rw-rw-rw-  2.0 fat     2075 b- defN 23-Jul-10 10:19 fairbench/reports/adhoc.py
 -rw-rw-rw-  2.0 fat     2590 b- defN 23-Jun-20 08:05 fairbench/reports/base.py
 -rw-rw-rw-  2.0 fat      883 b- defN 23-May-14 15:09 fairbench/reports/surrogate.py
 -rw-rw-rw-  2.0 fat      155 b- defN 23-Feb-20 08:42 fairbench/reports/reduction/__init__.py
 -rw-rw-rw-  2.0 fat     2260 b- defN 23-Jun-16 09:23 fairbench/reports/reduction/expanders.py
 -rw-rw-rw-  2.0 fat     2637 b- defN 23-Jun-20 14:07 fairbench/reports/reduction/reduce.py
 -rw-rw-rw-  2.0 fat     2834 b- defN 23-Jun-16 06:45 fairbench/reports/reduction/reducers.py
 -rw-rw-rw-  2.0 fat        0 b- defN 23-May-26 21:15 tests/__init__.py
 -rw-rw-rw-  2.0 fat     1394 b- defN 23-Jun-12 10:06 tests/test_batching.py
 -rw-rw-rw-  2.0 fat     1278 b- defN 23-Jun-01 09:09 tests/test_benchmarks.py
 -rw-rw-rw-  2.0 fat     1103 b- defN 23-Jun-16 09:20 tests/test_demos.py
 -rw-rw-rw-  2.0 fat     5807 b- defN 23-Jun-15 09:42 tests/test_forks.py
--rw-rw-rw-  2.0 fat     2685 b- defN 23-Jun-20 14:07 tests/test_metrics.py
+-rw-rw-rw-  2.0 fat     3391 b- defN 23-Jul-07 09:35 tests/test_metrics.py
 -rw-rw-rw-  2.0 fat     1150 b- defN 23-Jun-29 09:15 tests/test_modelcards.py
 -rw-rw-rw-  2.0 fat     1624 b- defN 23-Jun-14 06:40 tests/test_reduction.py
 -rw-rw-rw-  2.0 fat     6556 b- defN 23-May-31 14:04 tests/test_reports.py
--rw-rw-rw-  2.0 fat      852 b- defN 23-Jul-06 18:43 fairbench-0.2.8.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 23-Jul-06 18:43 fairbench-0.2.8.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       16 b- defN 23-Jul-06 18:43 fairbench-0.2.8.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     3985 b- defN 23-Jul-06 18:43 fairbench-0.2.8.dist-info/RECORD
-47 files, 124519 bytes uncompressed, 31344 bytes compressed:  74.8%
+-rw-rw-rw-  2.0 fat      877 b- defN 23-Jul-11 12:06 fairbench-0.2.9.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 23-Jul-11 12:06 fairbench-0.2.9.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       16 b- defN 23-Jul-11 12:06 fairbench-0.2.9.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat     3986 b- defN 23-Jul-11 12:06 fairbench-0.2.9.dist-info/RECORD
+47 files, 128869 bytes uncompressed, 31956 bytes compressed:  75.2%
```

## zipnote {}

```diff
@@ -123,20 +123,20 @@
 
 Filename: tests/test_reduction.py
 Comment: 
 
 Filename: tests/test_reports.py
 Comment: 
 
-Filename: fairbench-0.2.8.dist-info/METADATA
+Filename: fairbench-0.2.9.dist-info/METADATA
 Comment: 
 
-Filename: fairbench-0.2.8.dist-info/WHEEL
+Filename: fairbench-0.2.9.dist-info/WHEEL
 Comment: 
 
-Filename: fairbench-0.2.8.dist-info/top_level.txt
+Filename: fairbench-0.2.9.dist-info/top_level.txt
 Comment: 
 
-Filename: fairbench-0.2.8.dist-info/RECORD
+Filename: fairbench-0.2.9.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## fairbench/export/interactive.py

```diff
@@ -42,15 +42,15 @@
 ):  # pragma: no cover
     """
     Creates an interactive visualization over a fairness report.
     :param report: A fairness report.
     :param name: Default is 'report'.
     :param width: The minimum width of interactive plot screens.
     :param height: The minimum height of interactive plot screens.
-    :param spacing: The minimum spacing between bars of bar plots. If None, internally set to 50 for horizontal mode and 100 otherwise.
+    :param spacing: The minimum spacing between bars of bar plots. If None, internally set to 30 for horizontal mode and 80 otherwise.
     :param horizontal: Whether bar plots should be horizontally or vertically aligned.
     """
     from bokeh.models import (
         ColumnDataSource,
         Select,
         Button,
         Div,
@@ -65,21 +65,24 @@
     import webbrowser
     from bokeh.transform import factor_cmap
     from bokeh.palettes import Category20
     from bokeh.core.validation import silence
     from bokeh.core.validation.warnings import MISSING_RENDERERS
 
     if spacing is None:
-        spacing = 50 if horizontal else 100
+        spacing = 30 if horizontal else 80
 
     silence(MISSING_RENDERERS, True)
+    order = (lambda x: x) if not horizontal else reversed
 
     def modify_doc(doc):
         if horizontal:
-            plot = figure(y_range=["1", "2"], width=width, height=height)
+            plot = figure(
+                y_range=["1", "2"], width=width, height=height, x_axis_location="above"
+            )
         else:
             plot = figure(x_range=["1", "2"], width=width, height=height)
         plot.add_tools(HoverTool(tooltips=[("Name", "@keys"), ("Value", "@values")]))
         select_branch = RadioButtonGroup(
             labels=["ALL"] + list(report.branches().keys()), active=0
         )
         select_view = Select(
@@ -173,35 +176,37 @@
                         for k, v in _asdict(branches[branch]).items():
                             if k not in _source:
                                 _source[k] = list()
                             _source[k].append(branch)
                     try:
                         values = [
                             tofloat(_asdict(branches[branch])[metric])
-                            for metric in _source
-                            for branch in _source[metric]
+                            for metric in order(_source)
+                            for branch in order(_source[metric])
                         ]
                     except TypeError:
                         return
                 else:
                     for branch in branches.keys():
                         for k, v in _asdict(branches[branch]).items():
                             if branch not in _source:
                                 _source[branch] = list()
                             _source[branch].append(k)
                     try:
                         values = [
                             tofloat(_asdict(branches[branch])[metric])
-                            for branch in _source
-                            for metric in _source[branch]
+                            for branch in order(_source)
+                            for metric in order(_source[branch])
                         ]
                     except TypeError:
                         return
                 keys = [
-                    (metric, branch) for metric in _source for branch in _source[metric]
+                    (metric, branch)
+                    for metric in order(_source)
+                    for branch in order(_source[metric])
                 ]
                 if horizontal:
                     plot.height = max(height, spacing * len(keys))
                     plot.y_range.factors = keys
                     plot.y_range.range_padding = 0.1
                 else:
                     plot.width = max(width, spacing * len(keys))
@@ -247,14 +252,15 @@
                 select_view.disabled = not True
             else:
                 select_view.disabled = not False
                 label.text = f"<h1>{'.'.join([t for t in previous_title if t!='ALL'])}.<em>{selected_branch}</em></h1>Select ALL to switch between branch and entry views."
                 plot_data = _branch(selected_branch)
                 explain.visible = hasattr(plot_data, "explain")
                 plot_data = _asdict(plot_data)
+                plot_data = {k: plot_data[k] for k in order(plot_data)}
                 keys = list(plot_data.keys())
                 if horizontal:
                     plot.height = max(height, spacing * len(keys))
                     plot.y_range.factors = keys
                 else:
                     plot.width = max(width, spacing * len(keys))
                     plot.x_range.factors = keys
```

## fairbench/export/native.py

```diff
@@ -53,15 +53,15 @@
                     + [f"{entry:.3f}".ljust(spacing) for entry in report[metric]]
                 )
                 + "\n"
             )
     print(ret)
 
 
-def visualize(report: Fork, hold: bool = False):
+def visualize(report: Fork, hold: bool = False, xrotation: int = 0):
     report = json.loads(tojson(report))
 
     i = 1
     for metric in report:
         if metric != "header":
             plt.subplot(2, len(report) // 2, i)
             barplots = False
@@ -73,14 +73,15 @@
                     plt.plot(
                         report[metric][j]["x"],
                         report[metric][j]["y"],
                         label=report["header"][1 + j],
                     )
             if barplots:
                 plt.xticks(list(range(len(report["header"][1:]))), report["header"][1:])
+                plt.xticks(rotation=-xrotation, ha="right" if xrotation < 0 else "left")
             else:
                 plt.legend()
             plt.title(metric)
             i += 1
     plt.tight_layout()
     if not hold:
         plt.show()
```

## fairbench/forks/fork.py

```diff
@@ -12,15 +12,14 @@
     if isinstance(v, Fork):
         v = v.branches()
     if isinstance(v, dict):
         complicated = False
         for val in v.values():
             if isinstance(val, Fork) or isinstance(val, dict):
                 complicated = True
-        tmp = list(v.values())[0]
         return "\n".join(
             "   " * tabs
             + k
             + ": "
             + ("\n" if complicated else "")
             + _str_foreign(fromtensor(v), tabs + 1)
             for k, v in v.items()
@@ -770,15 +769,17 @@
 
 
 def unit_bounded(method):
     @wraps(method)
     def wrapper(*args, **kwargs):
         for iter in [args, kwargs.values()]:
             for arg in iter:
-                if isinstance(arg, ep.Tensor):
+                if (
+                    isinstance(arg, ep.Tensor) and arg.shape
+                ):  # do not check for single number parameters
                     assert (
                         arg.min() >= 0 and arg.max() <= 1
                     ), f"{method.__name__} inputs should lie in the range [0,1]. Maybe use fairbench.categories to transform categorical data."
         return method(*args, **kwargs)
 
     return wrapper
```

## fairbench/metrics/classification.py

```diff
@@ -3,15 +3,17 @@
 from eagerpy import Tensor
 from typing import Optional
 
 
 @role("metric")
 @parallel
 @unit_bounded
-def accuracy(predictions: Tensor, labels: Tensor, sensitive: Optional[Tensor] = None) -> Explainable:
+def accuracy(
+    predictions: Tensor, labels: Tensor, sensitive: Optional[Tensor] = None
+) -> Explainable:
     if sensitive is None:
         sensitive = predictions.ones_like()
     num_sensitive = sensitive.sum()
     true = ((predictions - labels) * sensitive).abs().sum()
     return Explainable(
         0 if num_sensitive == 0 else 1 - true / num_sensitive,
         samples=num_sensitive,
```

## fairbench/metrics/ranking.py

```diff
@@ -1,10 +1,27 @@
 from fairbench.forks import parallel, unit_bounded, role
 from fairbench.forks.explanation import Explainable, ExplanationCurve
 from eagerpy import Tensor
+import numpy as np
+from typing import Optional
+
+
+@role("metric")
+@parallel
+@unit_bounded
+def phi(scores: Tensor, sensitive: Optional[Tensor] = None):
+    if sensitive is None:
+        sensitive = scores.ones_like()
+    sum_sensitive = sensitive.sum()
+    sum_positives = (scores * sensitive).sum()
+    return Explainable(
+        0 if sum_sensitive == 0 else (sum_positives / sum_sensitive),
+        samples=sum_sensitive,
+        sensitive_scores=sum_positives,
+    )
 
 
 @role("metric")
 @parallel
 @unit_bounded
 def auc(scores: Tensor, labels: Tensor, sensitive: Tensor = None):
     import sklearn
@@ -15,7 +32,98 @@
     labels = labels[sensitive == 1]
     fpr, tpr, _ = sklearn.metrics.roc_curve(labels.numpy(), scores.numpy())
     return Explainable(
         sklearn.metrics.auc(fpr, tpr),
         curve=ExplanationCurve(fpr, tpr, "ROC"),
         samples=sensitive.sum(),
     )
+
+
+@role("metric")
+@parallel
+@unit_bounded
+def hr(scores: Tensor, labels: Tensor, sensitive: Tensor = None, top: int = 3):
+    k = int(top.numpy())
+    assert 0 < k <= scores.shape[0]
+    if sensitive is None:
+        sensitive = scores.ones_like()
+    scores = scores[sensitive == 1]
+    labels = labels[sensitive == 1]
+    indexes = scores.argsort()
+    indexes = indexes[-k:]
+    return Explainable(
+        labels[indexes].mean(),
+        top=k,
+        true_top=labels[indexes].sum(),
+        samples=sensitive.sum(),
+    )
+
+
+@role("metric")
+@parallel
+@unit_bounded
+def reck(scores: Tensor, labels: Tensor, sensitive: Tensor = None, top: int = 3):
+    k = int(top.numpy())
+    assert 0 < k <= scores.shape[0]
+    if sensitive is None:
+        sensitive = scores.ones_like()
+    scores = scores[sensitive == 1]
+    labels = labels[sensitive == 1]
+    indexes = scores.argsort()
+    indexes = indexes[-k:]
+    return Explainable(
+        labels[indexes].sum() / labels.sum(),
+        top=k,
+        true_top=labels[indexes].sum(),
+        samples=sensitive.sum(),
+    )
+
+
+@role("metric")
+@parallel
+@unit_bounded
+def f1k(scores: Tensor, labels: Tensor, sensitive: Tensor = None, top: int = 3):
+    k = int(top.numpy())
+    assert 0 < k <= scores.shape[0]
+    if sensitive is None:
+        sensitive = scores.ones_like()
+    scores = scores[sensitive == 1]
+    labels = labels[sensitive == 1]
+    indexes = scores.argsort()
+    indexes = indexes[-k:]
+    prec = labels[indexes].mean()
+    rec = labels[indexes].sum() / labels.sum()
+    return Explainable(
+        2 * prec * rec / (prec + rec),
+        top=k,
+        true_top=labels[indexes].sum(),
+        samples=sensitive.sum(),
+    )
+
+
+@role("metric")
+@parallel
+@unit_bounded
+def ap(scores: Tensor, labels: Tensor, sensitive: Tensor = None, top: int = 3):
+    k = int(top.numpy())
+    assert 0 < k <= scores.shape[0]
+    if sensitive is None:
+        sensitive = scores.ones_like()
+    scores = scores[sensitive == 1]
+    labels = labels[sensitive == 1]
+    indexes = scores.argsort()
+    curve = list()
+    accum = 0
+    for num in range(1, k + 1):
+        accum += labels[indexes[-num]].numpy()
+        curve.append(accum / num * labels[indexes[-num]].numpy())
+    curve = [v / k for v in curve]
+    return Explainable(
+        sum(curve),
+        top=k,
+        curve=ExplanationCurve(
+            np.array(list(range(len(curve))), dtype=float),
+            np.array(curve, dtype=float),
+            "hks",
+        ),
+        samples=sensitive.sum(),
+    )
```

## fairbench/reports/adhoc.py

```diff
@@ -10,14 +10,18 @@
 common_metrics = (metrics.accuracy, metrics.prule, metrics.dfpr, metrics.dfnr)
 acc_metrics = (
     metrics.accuracy,
     metrics.pr,
     metrics.tpr,
     metrics.tnr,
     metrics.auc,
+    metrics.phi,
+    metrics.hr,
+    metrics.reck,
+    metrics.ap,
     metrics.r2,
 )
 common_reduction = (
     {"reducer": reduction.min},
     {"reducer": reduction.wmean},
     {"reducer": reduction.min, "expand": reduction.ratio},
     {"reducer": reduction.max, "expand": reduction.diff},
```

## tests/test_metrics.py

```diff
@@ -37,14 +37,34 @@
 
 
 def test_auc():
     for _ in environment():
         assert fb.auc(scores=fb.astensor([0.5, 0.8, 0.3, 0.2]), labels=fb.astensor([1, 1, 1, 0])) > 0.5
 
 
+def test_f1k():
+    for _ in environment():
+        assert fb.f1k(scores=fb.astensor([0.5, 0.8, 0.3, 0.2, 0, 0.1, 0.12]), labels=fb.astensor([1, 1, 1, 0, 0, 0, 0])) == 1
+
+
+def test_hr():
+    for _ in environment():
+        assert fb.hr(scores=fb.astensor([0.5, 0.8, 0.3, 0.2, 0, 0.1, 0.12]), labels=fb.astensor([1, 1, 1, 0, 0, 0, 0])) == 1
+
+
+def test_reck():
+    for _ in environment():
+        assert fb.reck(scores=fb.astensor([0.5, 0.8, 0.3, 0.2, 0, 0.1, 0.12]), labels=fb.astensor([1, 1, 1, 0, 0, 0, 0])) == 1
+
+
+def test_ap():
+    for _ in environment():
+        assert fb.ap(scores=fb.astensor([0.5, 0.8, 0.3, 0.2, 0, 0.1, 0.12]), labels=fb.astensor([1, 1, 1, 0, 0, 0, 0])) == 1
+
+
 def test_pinball():
     for _ in environment():
         mae = fb.metrics.mae(scores=fb.astensor([0.5, 0.8, 0.3, 0.2]), targets=fb.astensor([1, 1, 1, 0]))
         pinball = fb.metrics.pinball(scores=fb.astensor([0.5, 0.8, 0.3, 0.2]), targets=fb.astensor([1, 1, 1, 0]), alpha=0.5)
         assert pinball == 0.5*mae
```

## Comparing `fairbench-0.2.8.dist-info/METADATA` & `fairbench-0.2.9.dist-info/METADATA`

 * *Files 16% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: fairbench
-Version: 0.2.8
+Version: 0.2.9
 Summary: Fairness model assessment framework
 Home-page: https://github.com/mever-team/FairBench
 Author: Emmanouil (Manios) Krasanakis
 Author-email: maniospas@hotmail.com
 License: UNKNOWN
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3
@@ -19,9 +19,9 @@
 Requires-Dist: wget
 Requires-Dist: scikit-learn
 Requires-Dist: pandas
 Requires-Dist: objwrap
 Requires-Dist: bokeh
 Requires-Dist: pyyaml
 
-For tutorials, documentation, and contribution guidelines, please visit the project's homepage at https://github.com/mever-team/FairBench
+A comprehensive AI fairness exploration framework.<br>**Homepage:** https://github.com/mever-team/FairBench<br>**Documentation:** https://fairbench.readthedocs.io
```

## Comparing `fairbench-0.2.8.dist-info/RECORD` & `fairbench-0.2.9.dist-info/RECORD`

 * *Files 6% similar despite different names*

```diff
@@ -1,47 +1,47 @@
 fairbench/__init__.py,sha256=Ido_Y_0n5fjUZDnEXn5vFcHTNnSJBJ95PI8sp_Y_Gpo,411
 fairbench/export.py,sha256=_u97-Yi8VArkwof63mcVXgnUD-i9AY1vjxtCnQ_jKcs,1952
 fairbench/stamps.py,sha256=naC2tvM9ihnMyFqlInP57thAgCf5af4000fjY8snNCE,5762
 fairbench/bench/__init__.py,sha256=piA_S0SAZ96_-MxdcGO_V7pCKngOFBeAv8KGj9rkGRA,38
 fairbench/bench/demos.py,sha256=ODVbSjCVrl_S1lP7cZOyoaVFPUZmsz75SYbeD3sflFY,2341
 fairbench/bench/loader.py,sha256=P1PiDtR67NFSk_lIQvzQ0Wez2ZvcxprP7zMpsLcsTYo,1660
 fairbench/export/__init__.py,sha256=ZQ3m4piX_o-z5mlmG_H7O26SGuhKo7b38Nc64Bs8pik,134
-fairbench/export/interactive.py,sha256=N38BHs9PukvikIg8dasefsWArYWMa3ZxPzmlluxa5YI,16295
-fairbench/export/native.py,sha256=s9houeUkqPRIx76Ke4NxqHED650XWAZu6s27ndXiCnA,2912
+fairbench/export/interactive.py,sha256=yoa0Xdp8wrsjYIDU9ps-QVRFIC7M6zsfv2kKYDXdY8Y,16566
+fairbench/export/native.py,sha256=dTgt72i0uTPdD59ruMC7jQKB_qD4qp148_bCmOzqtuE,3022
 fairbench/export/modelcards/__init__.py,sha256=B9gsTh-sWzIkY5pIPSPJ8ZdurlBsr2MS4o95n3WXXj0,173
 fairbench/export/modelcards/tohtml.py,sha256=IkVy8NpNBe-OYjc68QPWZeSfAHp5b9rvZ6fl8y5-J0Y,2856
 fairbench/export/modelcards/tomarkdown.py,sha256=Jesxn2AF0lRrDZxCjhj47phKI-PmLIdQ02GNrzoZjKk,2423
 fairbench/export/modelcards/toyaml.py,sha256=-DW4YcICcuWiNGdtECTKam7XlDXliuf4S3UvJzJrp6g,562
 fairbench/forks/__init__.py,sha256=eBRDSxc31Pc1yKCc1VjFBvPMsBVaOzVOfXHpkYQodG0,122
 fairbench/forks/categorical.py,sha256=nfsA1ZjrrywMzOhEoPUF6_9oazIJWcZEI4gvil-1VIs,2130
 fairbench/forks/explanation.py,sha256=x6zqF4b9dc1-XBg_xAQXu43czI4f900mrwEgoo-vR6o,2936
-fairbench/forks/fork.py,sha256=cpWhnUNZFYfJDq_k8k1IJGgKZ-Cb88z-6a6PAk-Mbto,26874
+fairbench/forks/fork.py,sha256=-UCnFFthc9WPBqoaovACVek7xvN12pmerJpy44Y0kDc,26940
 fairbench/metrics/__init__.py,sha256=kqs5vladIyri5R_l4u6fcte-O4j_n-jt9OeiIRA2KPo,239
-fairbench/metrics/classification.py,sha256=TcenssnwmcwQkn19B5537mGuKvBWZigCo8f6vmUP1CA,4008
+fairbench/metrics/classification.py,sha256=aqsbknk20xfIFposX-WpWs3RB59YQ-8JkEp4DB-QoBQ,4016
 fairbench/metrics/disparate_impact.py,sha256=VJw9Q4cA1YqP9fTTE-aMMUzjeuk072L-HRow_vaLkZA,2066
 fairbench/metrics/disparate_mistreatment.py,sha256=ORoUcMU_Dh18Ytz4dgQn8wPyaNJ_w98m_oegxI90vc4,1494
-fairbench/metrics/ranking.py,sha256=jnj-Ig9JgxeHqkt7v7uDfosEk-0gGv8lhXYGlkx7YKM,668
+fairbench/metrics/ranking.py,sha256=bX_ckM7k8syEBgoT7PcKMG5NPu_xuXcqGl4iAecZ7DY,3760
 fairbench/metrics/regression.py,sha256=w7zVO0hfLABovxIzGZ7XkWzYfT9eJOA2ceIME7XPMeQ,2970
 fairbench/mitigation/__init__.py,sha256=f-owl1xAKGweXEuVbFJGYSel3-7kccvTo6SEziKmXRk,100
 fairbench/mitigation/postprocessing.py,sha256=CF2WcAWQKKfNRJ4yvNqIjuNdLaCBrSIpI_XNhmfiUkY,1790
 fairbench/reports/__init__.py,sha256=YQm7A6K3PUB4uNOw3iDu2RUaoWa07Rsn3GJr4wZkn2M,207
 fairbench/reports/accumulate.py,sha256=jDlJFb1bK9nRfumzSWFbCpAGWi8aCOFil4L8Hj44xe0,1491
-fairbench/reports/adhoc.py,sha256=RWoOXYLZRLvKEXO5FAhTltXWZiv1WkoRv9xlsnx99D0,2004
+fairbench/reports/adhoc.py,sha256=KNrF0CmdQasVVJBYQ2g-x9JqLxjTM_LePtwlUQZipgo,2075
 fairbench/reports/base.py,sha256=KQHdSzzU7c3uH0cmV7CmU1YUlIwKhRviGtAGucNEzk4,2590
 fairbench/reports/surrogate.py,sha256=LnV6kkNPMGL2SYEBSSIFWjkXQ8N5m2k4y5as2zUyDt8,883
 fairbench/reports/reduction/__init__.py,sha256=a3c0w4bKV3f6NXnGMPPGFRxdfg_YIrcccy_mEBSr8zk,155
 fairbench/reports/reduction/expanders.py,sha256=DZvDpfXw8LI3ZquA5c22-25PSuLsOhpp_B_IYo5VeVY,2260
 fairbench/reports/reduction/reduce.py,sha256=H6_rIdw4JZRkhpS2ldpi3kPgBKuHjcQ6OOuXwPyRIUk,2637
 fairbench/reports/reduction/reducers.py,sha256=EQlN9rCKDBJmls0rtoA9vyAlrIyIBzQCCcpRDBMRlMA,2834
 tests/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 tests/test_batching.py,sha256=bXewsKXAlDTeyZZ8OlG74-qYQ_hhiwcYJigxy0M_YKY,1394
 tests/test_benchmarks.py,sha256=RZwehxePgR6fwFCh9HWbK3rXsXQNLHomrn7M6BboiMw,1278
 tests/test_demos.py,sha256=VgXBcev7XAnvy2OWiOZf0yBNXzhkjWvf0Bl6of00WzM,1103
 tests/test_forks.py,sha256=Dyu5UPAFG8fxVCfeZ9JwisVwUtZT0NV1WqAMe2GXt3Y,5807
-tests/test_metrics.py,sha256=mbG4oetiyfihNwIKj620SZmARxX8YuGeuAhebOv9gH4,2685
+tests/test_metrics.py,sha256=44ukRpPdweJEIjk5GtsKR1mhBeZzN3dVdAIwVC_ENeo,3391
 tests/test_modelcards.py,sha256=v73DtkMJZGnfnTEymv48ad2qsQCn7XI6nOs7OedTckE,1150
 tests/test_reduction.py,sha256=JMXZeh3Catiboki8TG50A_caMh4zpSt_NKScqGS2Pr0,1624
 tests/test_reports.py,sha256=2nNSZjkrTdZl8P5bvEz8dDdRd_W7CmSvn51GDdHFPvo,6556
-fairbench-0.2.8.dist-info/METADATA,sha256=lQJlnEv1BcVJmYM9akxXdF8lnBRAX4mGZvW-Bpd1Nrs,852
-fairbench-0.2.8.dist-info/WHEEL,sha256=OqRkF0eY5GHssMorFjlbTIq072vpHpF60fIQA6lS9xA,92
-fairbench-0.2.8.dist-info/top_level.txt,sha256=lrkG910bN_2UdVUqCXaR6aeRjjXfOQX2-wSeVjhhFnM,16
-fairbench-0.2.8.dist-info/RECORD,,
+fairbench-0.2.9.dist-info/METADATA,sha256=eF2oXQnrzMhOOyvgkZnK7Jz1oasChqGAAz4POA3EQyY,877
+fairbench-0.2.9.dist-info/WHEEL,sha256=OqRkF0eY5GHssMorFjlbTIq072vpHpF60fIQA6lS9xA,92
+fairbench-0.2.9.dist-info/top_level.txt,sha256=lrkG910bN_2UdVUqCXaR6aeRjjXfOQX2-wSeVjhhFnM,16
+fairbench-0.2.9.dist-info/RECORD,,
```

